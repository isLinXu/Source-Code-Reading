# YOLOv5 ğŸš€ by Ultralytics, AGPL-3.0 license
"""
Validate a trained YOLOv5 detection model on a detection dataset.

Usage:
    $ python val.py --weights yolov5s.pt --data coco128.yaml --img 640

Usage - formats:
    $ python val.py --weights yolov5s.pt                 # PyTorch
                              yolov5s.torchscript        # TorchScript
                              yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                              yolov5s_openvino_model     # OpenVINO
                              yolov5s.engine             # TensorRT
                              yolov5s.mlmodel            # CoreML (macOS-only)
                              yolov5s_saved_model        # TensorFlow SavedModel
                              yolov5s.pb                 # TensorFlow GraphDef
                              yolov5s.tflite             # TensorFlow Lite
                              yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
                              yolov5s_paddle_model       # PaddlePaddle
"""

import argparse
import json
import os
import subprocess
import sys
from pathlib import Path

import numpy as np
import torch
from tqdm import tqdm


FILE = Path(__file__).resolve()  # è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
ROOT = FILE.parents[0]  # YOLOv5 æ ¹ç›®å½•
if str(ROOT) not in sys.path:  # æ£€æŸ¥æ ¹ç›®å½•æ˜¯å¦åœ¨ç³»ç»Ÿè·¯å¾„ä¸­
    sys.path.append(str(ROOT))  # å°†æ ¹ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„ä¸­
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # ç›¸å¯¹è·¯å¾„

from models.common import DetectMultiBackend  # ä» models.common å¯¼å…¥ DetectMultiBackend ç±»
from utils.callbacks import Callbacks  # ä» utils.callbacks å¯¼å…¥ Callbacks ç±»
from utils.dataloaders import create_dataloader  # ä» utils.dataloaders å¯¼å…¥ create_dataloader å‡½æ•°
from utils.general import (  # ä» utils.general å¯¼å…¥å¤šä¸ªå·¥å…·å‡½æ•°
    LOGGER,  # æ—¥å¿—è®°å½•å™¨
    TQDM_BAR_FORMAT,  # TQDM è¿›åº¦æ¡æ ¼å¼
    Profile,  # æ€§èƒ½åˆ†æç±»
    check_dataset,  # æ£€æŸ¥æ•°æ®é›†çš„å‡½æ•°
    check_img_size,  # æ£€æŸ¥å›¾åƒå¤§å°çš„å‡½æ•°
    check_requirements,  # æ£€æŸ¥ä¾èµ–é¡¹çš„å‡½æ•°
    check_yaml,  # æ£€æŸ¥ YAML æ–‡ä»¶çš„å‡½æ•°
    coco80_to_coco91_class,  # COCO 80 ç±»æ˜ å°„åˆ° COCO 91 ç±»çš„å‡½æ•°
    colorstr,  # é¢œè‰²å­—ç¬¦ä¸²æ ¼å¼åŒ–å‡½æ•°
    increment_path,  # å¢åŠ è·¯å¾„çš„å‡½æ•°
    non_max_suppression,  # éæå¤§å€¼æŠ‘åˆ¶å‡½æ•°
    print_args,  # æ‰“å°å‚æ•°çš„å‡½æ•°
    scale_boxes,  # ç¼©æ”¾è¾¹ç•Œæ¡†çš„å‡½æ•°
    xywh2xyxy,  # å°† xywh æ ¼å¼è½¬æ¢ä¸º xyxy æ ¼å¼çš„å‡½æ•°
    xyxy2xywh,  # å°† xyxy æ ¼å¼è½¬æ¢ä¸º xywh æ ¼å¼çš„å‡½æ•°
)
from utils.metrics import ConfusionMatrix, ap_per_class, box_iou  # ä» utils.metrics å¯¼å…¥æ··æ·†çŸ©é˜µã€æ¯ç±»å¹³å‡ç²¾åº¦å’Œæ¡†çš„ IoU è®¡ç®—å‡½æ•°
from utils.plots import output_to_target, plot_images, plot_val_study  # ä» utils.plots å¯¼å…¥ç»˜å›¾ç›¸å…³å‡½æ•°
from utils.torch_utils import select_device, smart_inference_mode  # ä» utils.torch_utils å¯¼å…¥è®¾å¤‡é€‰æ‹©å’Œæ™ºèƒ½æ¨ç†æ¨¡å¼å‡½æ•°


def save_one_txt(predn, save_conf, shape, file):
    """Saves one detection result to a txt file in normalized xywh format, optionally including confidence."""
    # å°†ä¸€ä¸ªæ£€æµ‹ç»“æœä¿å­˜åˆ° txt æ–‡ä»¶ä¸­ï¼Œé‡‡ç”¨å½’ä¸€åŒ–çš„ xywh æ ¼å¼ï¼Œé€‰é¡¹ä¸Šå¯ä»¥åŒ…å«ç½®ä¿¡åº¦
    gn = torch.tensor(shape)[[1, 0, 1, 0]]  # normalization gain whwh  # å½’ä¸€åŒ–å¢ç›Š
    for *xyxy, conf, cls in predn.tolist():  # éå†é¢„æµ‹ç»“æœ
        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh  # å½’ä¸€åŒ–çš„ xywh
        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format  # æ ‡ç­¾æ ¼å¼
        with open(file, "a") as f:  # ä»¥è¿½åŠ æ¨¡å¼æ‰“å¼€æ–‡ä»¶
            f.write(("%g " * len(line)).rstrip() % line + "\n")  # å†™å…¥ç»“æœ


def save_one_json(predn, jdict, path, class_map):
    """
    Saves one JSON detection result with image ID, category ID, bounding box, and score.

    Example: {"image_id": 42, "category_id": 18, "bbox": [258.15, 41.29, 348.26, 243.78], "score": 0.236}
    """
    # ä¿å­˜ä¸€ä¸ª JSON æ ¼å¼çš„æ£€æµ‹ç»“æœï¼ŒåŒ…æ‹¬å›¾åƒ IDã€ç±»åˆ« IDã€è¾¹ç•Œæ¡†å’Œåˆ†æ•°
    image_id = int(path.stem) if path.stem.isnumeric() else path.stem  # è·å–å›¾åƒ ID
    box = xyxy2xywh(predn[:, :4])  # xywh  # å°†é¢„æµ‹ç»“æœè½¬æ¢ä¸º xywh æ ¼å¼
    box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner  # ä»ä¸­å¿ƒç‚¹è½¬æ¢ä¸ºå·¦ä¸Šè§’åæ ‡
    for p, b in zip(predn.tolist(), box.tolist()):  # éå†é¢„æµ‹ç»“æœå’Œè¾¹ç•Œæ¡†
        jdict.append(  # å°†ç»“æœæ·»åŠ åˆ° JSON å­—å…¸ä¸­
            {
                "image_id": image_id,  # å›¾åƒ ID
                "category_id": class_map[int(p[5])],  # ç±»åˆ« ID
                "bbox": [round(x, 3) for x in b],  # è¾¹ç•Œæ¡†ï¼Œä¿ç•™ä¸‰ä½å°æ•°
                "score": round(p[4], 5),  # åˆ†æ•°ï¼Œä¿ç•™äº”ä½å°æ•°
            }
        )


def process_batch(detections, labels, iouv):
    """
    Return correct prediction matrix.

    Arguments:
        detections (array[N, 6]), x1, y1, x2, y2, conf, class  # æ£€æµ‹ç»“æœæ•°ç»„
        labels (array[M, 5]), class, x1, y1, x2, y2  # æ ‡ç­¾æ•°ç»„
    Returns:
        correct (array[N, 10]), for 10 IoU levels  # è¿”å›æ­£ç¡®é¢„æµ‹çŸ©é˜µ
    """
    correct = np.zeros((detections.shape[0], iouv.shape[0])).astype(bool)  # åˆå§‹åŒ–æ­£ç¡®é¢„æµ‹çŸ©é˜µ
    iou = box_iou(labels[:, 1:], detections[:, :4])  # è®¡ç®— IoU
    correct_class = labels[:, 0:1] == detections[:, 5]  # ç±»åˆ«åŒ¹é…
    for i in range(len(iouv)):  # éå† IoU é˜ˆå€¼
        x = torch.where((iou >= iouv[i]) & correct_class)  # IoU > threshold and classes match  # IoU å¤§äºé˜ˆå€¼ä¸”ç±»åˆ«åŒ¹é…
        if x[0].shape[0]:  # å¦‚æœæœ‰åŒ¹é…
            matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()  # [label, detect, iou]
            if x[0].shape[0] > 1:  # å¦‚æœæœ‰å¤šä¸ªåŒ¹é…
                matches = matches[matches[:, 2].argsort()[::-1]]  # æŒ‰ç…§ IoU é™åºæ’åº
                matches = matches[np.unique(matches[:, 1], return_index=True)[1]]  # å»é‡
                matches = matches[np.unique(matches[:, 0], return_index=True)[1]]  # å»é‡
            correct[matches[:, 1].astype(int), i] = True  # æ›´æ–°æ­£ç¡®é¢„æµ‹çŸ©é˜µ
    return torch.tensor(correct, dtype=torch.bool, device=iouv.device)  # è¿”å›æ­£ç¡®é¢„æµ‹çŸ©é˜µ

@smart_inference_mode()  # ä½¿ç”¨æ™ºèƒ½æ¨ç†æ¨¡å¼è£…é¥°å™¨
def run(
    data,  # è¾“å…¥æ•°æ®
    weights=None,  # model.pt path(s)  # æ¨¡å‹æƒé‡è·¯å¾„
    batch_size=32,  # batch size  # æ‰¹å¤„ç†å¤§å°
    imgsz=640,  # inference size (pixels)  # æ¨ç†å›¾åƒå¤§å°ï¼ˆåƒç´ ï¼‰
    conf_thres=0.001,  # confidence threshold  # ç½®ä¿¡åº¦é˜ˆå€¼
    iou_thres=0.6,  # NMS IoU threshold  # éæå¤§å€¼æŠ‘åˆ¶çš„ IoU é˜ˆå€¼
    max_det=300,  # maximum detections per image  # æ¯å¼ å›¾åƒçš„æœ€å¤§æ£€æµ‹æ•°é‡
    task="val",  # train, val, test, speed or study  # ä»»åŠ¡ç±»å‹ï¼šè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•ã€é€Ÿåº¦æµ‹è¯•æˆ–ç ”ç©¶
    device="",  # cuda device, i.e. 0 or 0,1,2,3 or cpu  # è®¾å¤‡é€‰æ‹©ï¼Œä¾‹å¦‚ CUDA è®¾å¤‡æˆ– CPU
    workers=8,  # max dataloader workers (per RANK in DDP mode)  # æœ€å¤§æ•°æ®åŠ è½½å·¥ä½œçº¿ç¨‹æ•°ï¼ˆåœ¨ DDP æ¨¡å¼ä¸‹æ¯ä¸ª RANKï¼‰
    single_cls=False,  # treat as single-class dataset  # å°†æ•°æ®é›†è§†ä¸ºå•ç±»æ•°æ®é›†
    augment=False,  # augmented inference  # æ˜¯å¦ä½¿ç”¨å¢å¼ºæ¨ç†
    verbose=False,  # verbose output  # æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
    save_txt=False,  # save results to *.txt  # æ˜¯å¦å°†ç»“æœä¿å­˜åˆ° *.txt æ–‡ä»¶
    save_hybrid=False,  # save label+prediction hybrid results to *.txt  # æ˜¯å¦ä¿å­˜æ ‡ç­¾å’Œé¢„æµ‹çš„æ··åˆç»“æœåˆ° *.txt æ–‡ä»¶
    save_conf=False,  # save confidences in --save-txt labels  # æ˜¯å¦åœ¨ --save-txt æ ‡ç­¾ä¸­ä¿å­˜ç½®ä¿¡åº¦
    save_json=False,  # save a COCO-JSON results file  # æ˜¯å¦ä¿å­˜ COCO-JSON æ ¼å¼çš„ç»“æœæ–‡ä»¶
    project=ROOT / "runs/val",  # save to project/name  # ä¿å­˜åˆ°é¡¹ç›®/åç§°
    name="exp",  # save to project/name  # ä¿å­˜åˆ°é¡¹ç›®/åç§°
    exist_ok=False,  # existing project/name ok, do not increment  # å¦‚æœé¡¹ç›®/åç§°å·²å­˜åœ¨ï¼Œåˆ™ä¸é€’å¢
    half=True,  # use FP16 half-precision inference  # æ˜¯å¦ä½¿ç”¨ FP16 åŠç²¾åº¦æ¨ç†
    dnn=False,  # use OpenCV DNN for ONNX inference  # æ˜¯å¦ä½¿ç”¨ OpenCV DNN è¿›è¡Œ ONNX æ¨ç†
    model=None,  # æ¨¡å‹
    dataloader=None,  # æ•°æ®åŠ è½½å™¨
    save_dir=Path(""),  # ä¿å­˜ç›®å½•
    plots=True,  # æ˜¯å¦ç»˜åˆ¶å›¾åƒ
    callbacks=Callbacks(),  # å›è°ƒå‡½æ•°
    compute_loss=None,  # è®¡ç®—æŸå¤±çš„å‡½æ•°
):
    # Initialize/load model and set device  # åˆå§‹åŒ–/åŠ è½½æ¨¡å‹å¹¶è®¾ç½®è®¾å¤‡
    training = model is not None  # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if training:  # called by train.py  # å¦‚æœæ˜¯ç”± train.py è°ƒç”¨
        device, pt, jit, engine = next(model.parameters()).device, True, False, False  # get model device, PyTorch model  # è·å–æ¨¡å‹è®¾å¤‡ï¼ŒPyTorch æ¨¡å‹
        half &= device.type != "cpu"  # half precision only supported on CUDA  # åŠç²¾åº¦ä»…åœ¨ CUDA ä¸Šæ”¯æŒ
        model.half() if half else model.float()  # è®¾ç½®æ¨¡å‹ä¸ºåŠç²¾åº¦æˆ–å•ç²¾åº¦
    else:  # called directly  # å¦‚æœæ˜¯ç›´æ¥è°ƒç”¨
        device = select_device(device, batch_size=batch_size)  # é€‰æ‹©è®¾å¤‡

        # Directories  # ç›®å½•è®¾ç½®
        save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run  # å¢åŠ è¿è¡Œæ¬¡æ•°
        (save_dir / "labels" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir  # åˆ›å»ºç›®å½•

        # Load model  # åŠ è½½æ¨¡å‹
        model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)  # åˆå§‹åŒ–å¤šåç«¯æ£€æµ‹æ¨¡å‹
        stride, pt, jit, engine = model.stride, model.pt, model.jit, model.engine  # è·å–æ¨¡å‹çš„æ­¥å¹…å’Œå…¶ä»–å‚æ•°
        imgsz = check_img_size(imgsz, s=stride)  # check image size  # æ£€æŸ¥å›¾åƒå¤§å°
        half = model.fp16  # FP16 supported on limited backends with CUDA  # FP16 ä»…åœ¨æœ‰é™çš„ CUDA åç«¯æ”¯æŒ
        if engine:  # å¦‚æœæ¨¡å‹æ˜¯å¼•æ“ç±»å‹
            batch_size = model.batch_size  # è·å–æ¨¡å‹çš„æ‰¹å¤„ç†å¤§å°
        else:
            device = model.device  # è·å–æ¨¡å‹è®¾å¤‡
            if not (pt or jit):  # å¦‚æœä¸æ˜¯ PyTorch æˆ– JIT æ¨¡å‹
                batch_size = 1  # export.py models default to batch-size 1  # export.py æ¨¡å‹é»˜è®¤æ‰¹å¤„ç†å¤§å°ä¸º 1
                LOGGER.info(f"Forcing --batch-size 1 square inference (1,3,{imgsz},{imgsz}) for non-PyTorch models")  # å¼ºåˆ¶è®¾ç½®æ‰¹å¤„ç†å¤§å°ä¸º 1

        # Data  # æ•°æ®å¤„ç†
        data = check_dataset(data)  # check  # æ£€æŸ¥æ•°æ®é›†

    # Configure  # é…ç½®æ¨¡å‹
    model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    cuda = device.type != "cpu"  # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ CUDA
    is_coco = isinstance(data.get("val"), str) and data["val"].endswith(f"coco{os.sep}val2017.txt")  # COCO dataset  # æ£€æŸ¥æ˜¯å¦ä¸º COCO æ•°æ®é›†
    nc = 1 if single_cls else int(data["nc"])  # number of classes  # ç±»åˆ«æ•°é‡
    iouv = torch.linspace(0.5, 0.95, 10, device=device)  # iou vector for mAP@0.5:0.95  # åˆ›å»º IoU å‘é‡
    niou = iouv.numel()  # IoU æ•°é‡

    # Dataloader  # æ•°æ®åŠ è½½å™¨
    if not training:  # å¦‚æœä¸æ˜¯è®­ç»ƒæ¨¡å¼
        if pt and not single_cls:  # check --weights are trained on --data  # æ£€æŸ¥æƒé‡æ˜¯å¦åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒ
            ncm = model.model.nc  # è·å–æ¨¡å‹ç±»åˆ«æ•°é‡
            assert ncm == nc, (  # æ–­è¨€ç±»åˆ«æ•°é‡ä¸€è‡´
                f"{weights} ({ncm} classes) trained on different --data than what you passed ({nc} "
                f"classes). Pass correct combination of --weights and --data that are trained together."
            )
        model.warmup(imgsz=(1 if pt else batch_size, 3, imgsz, imgsz))  # warmup  # é¢„çƒ­æ¨¡å‹
        pad, rect = (0.0, False) if task == "speed" else (0.5, pt)  # square inference for benchmarks  # æ ¹æ®ä»»åŠ¡è®¾ç½®å¡«å……å’ŒçŸ©å½¢æ¨ç†
        task = task if task in ("train", "val", "test") else "val"  # path to train/val/test images  # è®¾ç½®ä»»åŠ¡ç±»å‹
        dataloader = create_dataloader(  # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            data[task],  # ä»»åŠ¡å¯¹åº”çš„æ•°æ®
            imgsz,  # å›¾åƒå¤§å°
            batch_size,  # æ‰¹å¤„ç†å¤§å°
            stride,  # æ­¥å¹…
            single_cls,  # æ˜¯å¦ä¸ºå•ç±»
            pad=pad,  # å¡«å……
            rect=rect,  # çŸ©å½¢æ¨ç†
            workers=workers,  # å·¥ä½œçº¿ç¨‹æ•°
            prefix=colorstr(f"{task}: "),  # å‰ç¼€
        )[0]  # è¿”å›æ•°æ®åŠ è½½å™¨
    
    seen = 0  # å·²å¤„ç†çš„å›¾åƒæ•°é‡
    confusion_matrix = ConfusionMatrix(nc=nc)  # åˆå§‹åŒ–æ··æ·†çŸ©é˜µï¼Œç±»åˆ«æ•°é‡ä¸º nc
    names = model.names if hasattr(model, "names") else model.module.names  # get class names  # è·å–ç±»åˆ«åç§°
    if isinstance(names, (list, tuple)):  # old format  # å¦‚æœæ˜¯æ—§æ ¼å¼
        names = dict(enumerate(names))  # å°†ç±»åˆ«åç§°è½¬æ¢ä¸ºå­—å…¸
    class_map = coco80_to_coco91_class() if is_coco else list(range(1000))  # æ ¹æ®æ˜¯å¦ä¸º COCO æ•°æ®é›†è®¾ç½®ç±»åˆ«æ˜ å°„
    s = ("%22s" + "%11s" * 6) % ("Class", "Images", "Instances", "P", "R", "mAP50", "mAP50-95")  # æ‰“å°æ ¼å¼
    tp, fp, p, r, f1, mp, mr, map50, ap50, map = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  # åˆå§‹åŒ–æŒ‡æ ‡
    dt = Profile(device=device), Profile(device=device), Profile(device=device)  # profiling times  # åˆå§‹åŒ–æ€§èƒ½åˆ†æ
    loss = torch.zeros(3, device=device)  # åˆå§‹åŒ–æŸå¤±
    jdict, stats, ap, ap_class = [], [], [], []  # åˆå§‹åŒ–ç»“æœå­—å…¸å’Œç»Ÿè®¡ä¿¡æ¯
    callbacks.run("on_val_start")  # è¿è¡ŒéªŒè¯å¼€å§‹çš„å›è°ƒ
    pbar = tqdm(dataloader, desc=s, bar_format=TQDM_BAR_FORMAT)  # progress bar  # è¿›åº¦æ¡
    for batch_i, (im, targets, paths, shapes) in enumerate(pbar):  # éå†æ•°æ®åŠ è½½å™¨
        callbacks.run("on_val_batch_start")  # è¿è¡Œæ¯ä¸ªæ‰¹æ¬¡å¼€å§‹çš„å›è°ƒ
        with dt[0]:  # è®°å½•æ—¶é—´
            if cuda:  # å¦‚æœä½¿ç”¨ CUDA
                im = im.to(device, non_blocking=True)  # å°†å›¾åƒç§»åŠ¨åˆ°è®¾å¤‡
                targets = targets.to(device)  # å°†ç›®æ ‡ç§»åŠ¨åˆ°è®¾å¤‡
            im = im.half() if half else im.float()  # uint8 to fp16/32  # å°†å›¾åƒè½¬æ¢ä¸ºåŠç²¾åº¦æˆ–å•ç²¾åº¦
            im /= 255  # 0 - 255 to 0.0 - 1.0  # å°†å›¾åƒå½’ä¸€åŒ–åˆ° [0, 1]
            nb, _, height, width = im.shape  # batch size, channels, height, width  # è·å–å›¾åƒçš„å½¢çŠ¶

        # Inference  # æ¨ç†
        with dt[1]:  # è®°å½•æ—¶é—´
            preds, train_out = model(im) if compute_loss else (model(im, augment=augment), None)  # è¿›è¡Œæ¨ç†

        # Loss  # è®¡ç®—æŸå¤±
        if compute_loss:  # å¦‚æœéœ€è¦è®¡ç®—æŸå¤±
            loss += compute_loss(train_out, targets)[1]  # box, obj, cls  # ç´¯åŠ æŸå¤±

        # NMS  # éæå¤§å€¼æŠ‘åˆ¶
        targets[:, 2:] *= torch.tensor((width, height, width, height), device=device)  # to pixels  # å°†ç›®æ ‡è½¬æ¢ä¸ºåƒç´ åæ ‡
        lb = [targets[targets[:, 0] == i, 1:] for i in range(nb)] if save_hybrid else []  # for autolabelling  # ç”¨äºè‡ªåŠ¨æ ‡æ³¨
        with dt[2]:  # è®°å½•æ—¶é—´
            preds = non_max_suppression(  # è¿›è¡Œéæå¤§å€¼æŠ‘åˆ¶
                preds, conf_thres, iou_thres, labels=lb, multi_label=True, agnostic=single_cls, max_det=max_det
            )

        # Metrics  # è®¡ç®—æŒ‡æ ‡
        for si, pred in enumerate(preds):  # éå†æ¯ä¸ªé¢„æµ‹ç»“æœ
            labels = targets[targets[:, 0] == si, 1:]  # è·å–å½“å‰å›¾åƒçš„æ ‡ç­¾
            nl, npr = labels.shape[0], pred.shape[0]  # number of labels, predictions  # æ ‡ç­¾å’Œé¢„æµ‹çš„æ•°é‡
            path, shape = Path(paths[si]), shapes[si][0]  # è·å–å›¾åƒè·¯å¾„å’Œå½¢çŠ¶
            correct = torch.zeros(npr, niou, dtype=torch.bool, device=device)  # init  # åˆå§‹åŒ–æ­£ç¡®é¢„æµ‹çŸ©é˜µ
            seen += 1  # å·²å¤„ç†å›¾åƒæ•°é‡å¢åŠ 

            if npr == 0:  # å¦‚æœæ²¡æœ‰é¢„æµ‹ç»“æœ
                if nl:  # å¦‚æœæœ‰æ ‡ç­¾
                    stats.append((correct, *torch.zeros((2, 0), device=device), labels[:, 0]))  # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                    if plots:  # å¦‚æœéœ€è¦ç»˜å›¾
                        confusion_matrix.process_batch(detections=None, labels=labels[:, 0])  # å¤„ç†æ··æ·†çŸ©é˜µ
                continue  # ç»§ç»­ä¸‹ä¸€ä¸ªæ‰¹æ¬¡

            # Predictions  # å¤„ç†é¢„æµ‹ç»“æœ
            if single_cls:  # å¦‚æœæ˜¯å•ç±»
                pred[:, 5] = 0  # å°†ç±»åˆ«è®¾ç½®ä¸º 0
            predn = pred.clone()  # å…‹éš†é¢„æµ‹ç»“æœ
            scale_boxes(im[si].shape[1:], predn[:, :4], shape, shapes[si][1])  # native-space pred  # å°†é¢„æµ‹ç»“æœç¼©æ”¾åˆ°åŸå§‹ç©ºé—´

            # Evaluate  # è¯„ä¼°
            if nl:  # å¦‚æœæœ‰æ ‡ç­¾
                tbox = xywh2xyxy(labels[:, 1:5])  # target boxes  # è·å–ç›®æ ‡æ¡†
                scale_boxes(im[si].shape[1:], tbox, shape, shapes[si][1])  # native-space labels  # å°†ç›®æ ‡æ¡†ç¼©æ”¾åˆ°åŸå§‹ç©ºé—´
                labelsn = torch.cat((labels[:, 0:1], tbox), 1)  # native-space labels  # åˆå¹¶æ ‡ç­¾
                correct = process_batch(predn, labelsn, iouv)  # å¤„ç†æ‰¹æ¬¡
                if plots:  # å¦‚æœéœ€è¦ç»˜å›¾
                    confusion_matrix.process_batch(predn, labelsn)  # å¤„ç†æ··æ·†çŸ©é˜µ
            stats.append((correct, pred[:, 4], pred[:, 5], labels[:, 0]))  # è®°å½•ç»Ÿè®¡ä¿¡æ¯

            # Save/log  # ä¿å­˜/è®°å½•ç»“æœ
            if save_txt:  # å¦‚æœéœ€è¦ä¿å­˜ä¸º txt æ–‡ä»¶
                (save_dir / "labels").mkdir(parents=True, exist_ok=True)  # åˆ›å»ºæ ‡ç­¾ç›®å½•
                save_one_txt(predn, save_conf, shape, file=save_dir / "labels" / f"{path.stem}.txt")  # ä¿å­˜é¢„æµ‹ç»“æœåˆ° txt æ–‡ä»¶
            if save_json:  # å¦‚æœéœ€è¦ä¿å­˜ä¸º JSON æ–‡ä»¶
                save_one_json(predn, jdict, path, class_map)  # append to COCO-JSON dictionary  # æ·»åŠ åˆ° COCO-JSON å­—å…¸
            callbacks.run("on_val_image_end", pred, predn, path, names, im[si])  # è¿è¡Œæ¯ä¸ªå›¾åƒç»“æŸçš„å›è°ƒ

        # Plot images  # ç»˜åˆ¶å›¾åƒ
        if plots and batch_i < 3:  # å¦‚æœéœ€è¦ç»˜å›¾ä¸”æ‰¹æ¬¡å°äº 3
            plot_images(im, targets, paths, save_dir / f"val_batch{batch_i}_labels.jpg", names)  # labels  # ç»˜åˆ¶æ ‡ç­¾å›¾åƒ
            plot_images(im, output_to_target(preds), paths, save_dir / f"val_batch{batch_i}_pred.jpg", names)  # pred  # ç»˜åˆ¶é¢„æµ‹å›¾åƒ

        callbacks.run("on_val_batch_end", batch_i, im, targets, paths, shapes, preds)  # è¿è¡Œæ¯ä¸ªæ‰¹æ¬¡ç»“æŸçš„å›è°ƒ

    # Compute metrics
    stats = [torch.cat(x, 0).cpu().numpy() for x in zip(*stats)]  # to numpy
    # å°†ç»Ÿè®¡æ•°æ®ä¸­çš„æ¯ä¸ªå…ƒç´ æ²¿ç€ç¬¬0ç»´è¿æ¥ï¼Œå¹¶å°†ç»“æœä»GPUè½¬ç§»åˆ°CPUï¼Œæœ€åè½¬æ¢ä¸ºNumPyæ•°ç»„

    if len(stats) and stats[0].any():
        # æ£€æŸ¥ç»Ÿè®¡æ•°æ®æ˜¯å¦å­˜åœ¨ä¸”ç¬¬ä¸€ä¸ªç»Ÿè®¡æ•°æ®ä¸­æœ‰ä»»æ„å€¼
        tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats, plot=plots, save_dir=save_dir, names=names)
        # è°ƒç”¨ap_per_classå‡½æ•°è®¡ç®—æ¯ä¸ªç±»åˆ«çš„çœŸé˜³æ€§ã€å‡é˜³æ€§ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ã€å¹³å‡ç²¾ç¡®åº¦å’Œç±»åˆ«

        ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95
        # è·å–AP@0.5çš„å€¼å’ŒAPåœ¨ä¸åŒé˜ˆå€¼ä¸‹çš„å¹³å‡å€¼

        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()
        # è®¡ç®—å¹³å‡ç²¾ç¡®ç‡ã€å¹³å‡å¬å›ç‡ã€AP@0.5çš„å¹³å‡å€¼å’Œæ€»ä½“APçš„å¹³å‡å€¼

    nt = np.bincount(stats[3].astype(int), minlength=nc)  # number of targets per class
    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„ç›®æ ‡æ•°é‡ï¼Œä½¿ç”¨np.bincountå¯¹ç»Ÿè®¡æ•°æ®è¿›è¡Œè®¡æ•°

    # Print results
    pf = "%22s" + "%11i" * 2 + "%11.3g" * 4  # print format
    # å®šä¹‰æ‰“å°æ ¼å¼

    LOGGER.info(pf % ("all", seen, nt.sum(), mp, mr, map50, map))
    # è®°å½•æ‰€æœ‰ç±»åˆ«çš„ç»“æœï¼ŒåŒ…æ‹¬å·²è§æ ·æœ¬æ•°ã€ç›®æ ‡æ€»æ•°ã€å¹³å‡ç²¾ç¡®ç‡ã€å¹³å‡å¬å›ç‡ã€AP@0.5çš„å¹³å‡å€¼å’Œæ€»ä½“AP

    if nt.sum() == 0:
        LOGGER.warning(f"WARNING âš ï¸ no labels found in {task} set, can not compute metrics without labels")
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ ‡ç­¾ï¼Œå‘å‡ºè­¦å‘Šï¼Œæ— æ³•è®¡ç®—æŒ‡æ ‡

    # Print results per class
    if (verbose or (nc < 50 and not training)) and nc > 1 and len(stats):
        # å¦‚æœéœ€è¦è¯¦ç»†è¾“å‡ºæˆ–ç±»åˆ«å°‘äº50ä¸”ä¸æ˜¯è®­ç»ƒæ¨¡å¼ï¼Œå¹¶ä¸”ç»Ÿè®¡æ•°æ®å­˜åœ¨
        for i, c in enumerate(ap_class):
            LOGGER.info(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))
            # è®°å½•æ¯ä¸ªç±»åˆ«çš„ç»“æœï¼ŒåŒ…æ‹¬ç±»åˆ«åç§°ã€å·²è§æ ·æœ¬æ•°ã€ç›®æ ‡æ•°ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€AP@0.5å’ŒAP

    # Print speeds
    t = tuple(x.t / seen * 1e3 for x in dt)  # speeds per image
    # è®¡ç®—æ¯å¼ å›¾åƒçš„å¤„ç†é€Ÿåº¦ï¼Œå•ä½ä¸ºæ¯«ç§’

    if not training:
        shape = (batch_size, 3, imgsz, imgsz)
        LOGGER.info(f"Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {shape}" % t)
        # å¦‚æœä¸æ˜¯è®­ç»ƒæ¨¡å¼ï¼Œè®°å½•æ¯å¼ å›¾åƒçš„é¢„å¤„ç†ã€æ¨ç†å’ŒNMSçš„é€Ÿåº¦

    # Plots
    if plots:
        confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))
        # å¦‚æœéœ€è¦ç»˜åˆ¶å›¾è¡¨ï¼Œç»˜åˆ¶æ··æ·†çŸ©é˜µå¹¶ä¿å­˜åˆ°æŒ‡å®šç›®å½•

        callbacks.run("on_val_end", nt, tp, fp, p, r, f1, ap, ap50, ap_class, confusion_matrix)
        # è¿è¡Œå›è°ƒå‡½æ•°ï¼Œä¼ é€’éªŒè¯ç»“æŸæ—¶çš„ç»Ÿè®¡æ•°æ®

    # Save JSON
    if save_json and len(jdict):
        # å¦‚æœéœ€è¦ä¿å­˜JSONå¹¶ä¸”jdictä¸ä¸ºç©º
        w = Path(weights[0] if isinstance(weights, list) else weights).stem if weights is not None else ""  # weights
        # è·å–æƒé‡æ–‡ä»¶çš„åŸºæœ¬åç§°

        anno_json = str(Path("../datasets/coco/annotations/instances_val2017.json"))  # annotations
        # è®¾ç½®COCOæ ¼å¼çš„æ³¨é‡Šæ–‡ä»¶è·¯å¾„

        if not os.path.exists(anno_json):
            anno_json = os.path.join(data["path"], "annotations", "instances_val2017.json")
            # å¦‚æœæ³¨é‡Šæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ•°æ®è·¯å¾„ä¸‹çš„æ³¨é‡Šæ–‡ä»¶

        pred_json = str(save_dir / f"{w}_predictions.json")  # predictions
        # è®¾ç½®é¢„æµ‹ç»“æœçš„ä¿å­˜è·¯å¾„

        LOGGER.info(f"\nEvaluating pycocotools mAP... saving {pred_json}...")
        # è®°å½•æ­£åœ¨è¯„ä¼°pycocotoolsçš„mAPï¼Œå¹¶ä¿å­˜é¢„æµ‹ç»“æœ

        with open(pred_json, "w") as f:
            json.dump(jdict, f)
            # å°†é¢„æµ‹ç»“æœä»¥JSONæ ¼å¼ä¿å­˜

        try:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb
            check_requirements("pycocotools>=2.0.6")
            # æ£€æŸ¥pycocotoolsçš„ç‰ˆæœ¬è¦æ±‚

            from pycocotools.coco import COCO
            from pycocotools.cocoeval import COCOeval
            # å¯¼å…¥pycocotoolsåº“

            anno = COCO(anno_json)  # init annotations api
            # åˆå§‹åŒ–æ³¨é‡ŠAPI

            pred = anno.loadRes(pred_json)  # init predictions api
            # åŠ è½½é¢„æµ‹ç»“æœ

            eval = COCOeval(anno, pred, "bbox")
            # åˆå§‹åŒ–COCOè¯„ä¼°å¯¹è±¡ï¼Œè¯„ä¼°ç›®æ ‡æ¡†

            if is_coco:
                eval.params.imgIds = [int(Path(x).stem) for x in dataloader.dataset.im_files]  # image IDs to evaluate
                # å¦‚æœæ˜¯COCOæ•°æ®é›†ï¼Œè®¾ç½®è¦è¯„ä¼°çš„å›¾åƒID

            eval.evaluate()
            eval.accumulate()
            eval.summarize()
            # è¿›è¡Œè¯„ä¼°ã€ç´¯ç§¯ç»“æœå¹¶æ€»ç»“

            map, map50 = eval.stats[:2]  # update results (mAP@0.5:0.95, mAP@0.5)
            # æ›´æ–°ç»“æœï¼Œè·å–mAP@0.5:0.95å’ŒmAP@0.5çš„å€¼
        except Exception as e:
            LOGGER.info(f"pycocotools unable to run: {e}")
            # å¦‚æœè¿è¡Œpycocotoolsæ—¶å‘ç”Ÿå¼‚å¸¸ï¼Œè®°å½•é”™è¯¯ä¿¡æ¯

    # Return results
    model.float()  # for training
    # å°†æ¨¡å‹è½¬æ¢ä¸ºæµ®ç‚¹æ¨¡å¼ï¼Œä»¥ä¾¿è¿›è¡Œè®­ç»ƒ

    if not training:
        s = f"\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}" if save_txt else ""
        # å¦‚æœä¸æ˜¯è®­ç»ƒæ¨¡å¼ï¼Œè®°å½•ä¿å­˜çš„æ ‡ç­¾æ–‡ä»¶æ•°é‡

        LOGGER.info(f"Results saved to {colorstr('bold', save_dir)}{s}")
        # è®°å½•ç»“æœä¿å­˜çš„ç›®å½•

    maps = np.zeros(nc) + map
    # åˆ›å»ºä¸€ä¸ªä¸ç±»åˆ«æ•°é‡ç›¸åŒçš„æ•°ç»„ï¼Œå¹¶å°†mapçš„å€¼èµ‹ç»™å®ƒ

    for i, c in enumerate(ap_class):
        maps[c] = ap[i]
    # å°†æ¯ä¸ªç±»åˆ«çš„APå€¼å­˜å…¥mapsæ•°ç»„

    return (mp, mr, map50, map, *(loss.cpu() / len(dataloader)).tolist()), maps, t
    # è¿”å›å¹³å‡ç²¾ç¡®ç‡ã€å¹³å‡å¬å›ç‡ã€AP@0.5çš„å¹³å‡å€¼ã€æ€»ä½“APã€æŸå¤±å€¼åˆ—è¡¨ã€mapsæ•°ç»„å’Œå¤„ç†é€Ÿåº¦


def parse_opt():
    """Parses command-line options for YOLOv5 model inference configuration."""
    # è§£æYOLOv5æ¨¡å‹æ¨ç†é…ç½®çš„å‘½ä»¤è¡Œé€‰é¡¹
    parser = argparse.ArgumentParser()
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨

    parser.add_argument("--data", type=str, default=ROOT / "data/coco128.yaml", help="dataset.yaml path")
    # æ·»åŠ æ•°æ®é›†è·¯å¾„å‚æ•°ï¼Œé»˜è®¤ä¸ºcoco128.yaml

    parser.add_argument("--weights", nargs="+", type=str, default=ROOT / "yolov5s.pt", help="model path(s)")
    # æ·»åŠ æƒé‡æ–‡ä»¶è·¯å¾„å‚æ•°ï¼Œæ”¯æŒå¤šä¸ªæƒé‡æ–‡ä»¶ï¼Œé»˜è®¤ä¸ºyolov5s.pt

    parser.add_argument("--batch-size", type=int, default=32, help="batch size")
    # æ·»åŠ æ‰¹å¤„ç†å¤§å°å‚æ•°ï¼Œé»˜è®¤ä¸º32

    parser.add_argument("--imgsz", "--img", "--img-size", type=int, default=640, help="inference size (pixels)")
    # æ·»åŠ å›¾åƒå¤§å°å‚æ•°ï¼Œé»˜è®¤ä¸º640åƒç´ 

    parser.add_argument("--conf-thres", type=float, default=0.001, help="confidence threshold")
    # æ·»åŠ ç½®ä¿¡åº¦é˜ˆå€¼å‚æ•°ï¼Œé»˜è®¤ä¸º0.001

    parser.add_argument("--iou-thres", type=float, default=0.6, help="NMS IoU threshold")
    # æ·»åŠ NMSçš„IoUé˜ˆå€¼å‚æ•°ï¼Œé»˜è®¤ä¸º0.6

    parser.add_argument("--max-det", type=int, default=300, help="maximum detections per image")
    # æ·»åŠ æ¯å¼ å›¾åƒçš„æœ€å¤§æ£€æµ‹æ•°é‡å‚æ•°ï¼Œé»˜è®¤ä¸º300

    parser.add_argument("--task", default="val", help="train, val, test, speed or study")
    # æ·»åŠ ä»»åŠ¡ç±»å‹å‚æ•°ï¼Œé»˜è®¤ä¸ºéªŒè¯ï¼ˆvalï¼‰

    parser.add_argument("--device", default="", help="cuda device, i.e. 0 or 0,1,2,3 or cpu")
    # æ·»åŠ è®¾å¤‡å‚æ•°ï¼ŒæŒ‡å®šä½¿ç”¨çš„CUDAè®¾å¤‡æˆ–CPU

    parser.add_argument("--workers", type=int, default=8, help="max dataloader workers (per RANK in DDP mode)")
    # æ·»åŠ æœ€å¤§æ•°æ®åŠ è½½å·¥ä½œçº¿ç¨‹æ•°å‚æ•°ï¼Œé»˜è®¤ä¸º8

    parser.add_argument("--single-cls", action="store_true", help="treat as single-class dataset")
    # æ·»åŠ å•ç±»æ•°æ®é›†æ ‡å¿—ï¼Œå¦‚æœè®¾ç½®åˆ™å°†æ•°æ®é›†è§†ä¸ºå•ç±»

    parser.add_argument("--augment", action="store_true", help="augmented inference")
    # æ·»åŠ å¢å¼ºæ¨ç†æ ‡å¿—ï¼Œå¦‚æœè®¾ç½®åˆ™å¯ç”¨å¢å¼ºæ¨ç†

    parser.add_argument("--verbose", action="store_true", help="report mAP by class")
    # æ·»åŠ è¯¦ç»†è¾“å‡ºæ ‡å¿—ï¼Œå¦‚æœè®¾ç½®åˆ™æŒ‰ç±»åˆ«æŠ¥å‘ŠmAP

    parser.add_argument("--save-txt", action="store_true", help="save results to *.txt")
    # æ·»åŠ ä¿å­˜ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶çš„æ ‡å¿—

    parser.add_argument("--save-hybrid", action="store_true", help="save label+prediction hybrid results to *.txt")
    # æ·»åŠ ä¿å­˜æ ‡ç­¾å’Œé¢„æµ‹æ··åˆç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶çš„æ ‡å¿—

    parser.add_argument("--save-conf", action="store_true", help="save confidences in --save-txt labels")
    # æ·»åŠ ä¿å­˜ç½®ä¿¡åº¦åˆ°æ–‡æœ¬æ–‡ä»¶çš„æ ‡å¿—

    parser.add_argument("--save-json", action="store_true", help="save a COCO-JSON results file")
    # æ·»åŠ ä¿å­˜COCOæ ¼å¼JSONç»“æœæ–‡ä»¶çš„æ ‡å¿—

    parser.add_argument("--project", default=ROOT / "runs/val", help="save to project/name")
    # æ·»åŠ é¡¹ç›®ä¿å­˜è·¯å¾„å‚æ•°ï¼Œé»˜è®¤ä¸ºruns/val

    parser.add_argument("--name", default="exp", help="save to project/name")
    # æ·»åŠ å®éªŒåç§°å‚æ•°ï¼Œé»˜è®¤ä¸º"exp"

    parser.add_argument("--exist-ok", action="store_true", help="existing project/name ok, do not increment")
    # æ·»åŠ å…è®¸å­˜åœ¨çš„é¡¹ç›®åç§°æ ‡å¿—ï¼Œå¦‚æœè®¾ç½®åˆ™ä¸é€’å¢é¡¹ç›®åç§°

    parser.add_argument("--half", action="store_true", help="use FP16 half-precision inference")
    # æ·»åŠ ä½¿ç”¨FP16åŠç²¾åº¦æ¨ç†çš„æ ‡å¿—

    parser.add_argument("--dnn", action="store_true", help="use OpenCV DNN for ONNX inference")
    # æ·»åŠ ä½¿ç”¨OpenCV DNNè¿›è¡ŒONNXæ¨ç†çš„æ ‡å¿—

    opt = parser.parse_args()
    # è§£æå‘½ä»¤è¡Œå‚æ•°

    opt.data = check_yaml(opt.data)  # check YAML
    # æ£€æŸ¥YAMLæ–‡ä»¶çš„æœ‰æ•ˆæ€§

    opt.save_json |= opt.data.endswith("coco.yaml")
    # å¦‚æœæ•°æ®é›†è·¯å¾„ä»¥coco.yamlç»“å°¾ï¼Œåˆ™è®¾ç½®ä¿å­˜JSONçš„æ ‡å¿—

    opt.save_txt |= opt.save_hybrid
    # å¦‚æœè®¾ç½®äº†ä¿å­˜æ··åˆç»“æœçš„æ ‡å¿—ï¼Œåˆ™ä¹Ÿè®¾ç½®ä¿å­˜æ–‡æœ¬çš„æ ‡å¿—

    print_args(vars(opt))
    # æ‰“å°è§£æåçš„å‚æ•°

    return opt
    # è¿”å›è§£æåçš„é€‰é¡¹


def main(opt):
    """Executes YOLOv5 tasks like training, validation, testing, speed, and study benchmarks based on provided
    options.
    """
    # æ ¹æ®æä¾›çš„é€‰é¡¹æ‰§è¡ŒYOLOv5ä»»åŠ¡ï¼Œå¦‚è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•ã€é€Ÿåº¦å’Œç ”ç©¶åŸºå‡†

    check_requirements(ROOT / "requirements.txt", exclude=("tensorboard", "thop"))
    # æ£€æŸ¥æ‰€éœ€çš„ä¾èµ–é¡¹ï¼Œæ’é™¤tensorboardå’Œthop

    if opt.task in ("train", "val", "test"):  # run normally
        # å¦‚æœä»»åŠ¡ä¸ºè®­ç»ƒã€éªŒè¯æˆ–æµ‹è¯•ï¼Œæ­£å¸¸è¿è¡Œ
        if opt.conf_thres > 0.001:  # https://github.com/ultralytics/yolov5/issues/1466
            LOGGER.info(f"WARNING âš ï¸ confidence threshold {opt.conf_thres} > 0.001 produces invalid results")
            # å¦‚æœç½®ä¿¡åº¦é˜ˆå€¼å¤§äº0.001ï¼Œè®°å½•è­¦å‘Šä¿¡æ¯

        if opt.save_hybrid:
            LOGGER.info("WARNING âš ï¸ --save-hybrid will return high mAP from hybrid labels, not from predictions alone")
            # å¦‚æœè®¾ç½®äº†ä¿å­˜æ··åˆç»“æœçš„æ ‡å¿—ï¼Œè®°å½•è­¦å‘Šä¿¡æ¯

        run(**vars(opt))
        # è¿è¡Œä¸»ç¨‹åºï¼Œä¼ é€’è§£æåçš„é€‰é¡¹å‚æ•°

    else:
        weights = opt.weights if isinstance(opt.weights, list) else [opt.weights]
        # å¦‚æœæƒé‡æ˜¯åˆ—è¡¨ï¼Œåˆ™ä½¿ç”¨è¯¥åˆ—è¡¨ï¼Œå¦åˆ™å°†å•ä¸ªæƒé‡æ”¾å…¥åˆ—è¡¨ä¸­

        opt.half = torch.cuda.is_available() and opt.device != "cpu"  # FP16 for fastest results
        # å¦‚æœCUDAå¯ç”¨ä¸”è®¾å¤‡ä¸æ˜¯CPUï¼Œåˆ™è®¾ç½®ä½¿ç”¨FP16ä»¥è·å¾—æœ€å¿«çš„ç»“æœ

        if opt.task == "speed":  # speed benchmarks
            # å¦‚æœä»»åŠ¡ä¸ºé€Ÿåº¦åŸºå‡†æµ‹è¯•
            # python val.py --task speed --data coco.yaml --batch 1 --weights yolov5n.pt yolov5s.pt...
            opt.conf_thres, opt.iou_thres, opt.save_json = 0.25, 0.45, False
            # è®¾ç½®ç½®ä¿¡åº¦é˜ˆå€¼ã€IoUé˜ˆå€¼å’Œä¿å­˜JSONçš„æ ‡å¿—

            for opt.weights in weights:
                run(**vars(opt), plots=False)
                # å¯¹æ¯ä¸ªæƒé‡è¿è¡Œä¸»ç¨‹åºï¼Œä¸ç»˜åˆ¶å›¾è¡¨

        elif opt.task == "study":  # speed vs mAP benchmarks
            # å¦‚æœä»»åŠ¡ä¸ºç ”ç©¶åŸºå‡†æµ‹è¯•
            # python val.py --task study --data coco.yaml --iou 0.7 --weights yolov5n.pt yolov5s.pt...
            for opt.weights in weights:
                f = f"study_{Path(opt.data).stem}_{Path(opt.weights).stem}.txt"  # filename to save to
                # ç”Ÿæˆç”¨äºä¿å­˜ç»“æœçš„æ–‡ä»¶å

                x, y = list(range(256, 1536 + 128, 128)), []  # x axis (image sizes), y axis
                # è®¾ç½®xè½´ä¸ºå›¾åƒå¤§å°èŒƒå›´ï¼Œyè½´ä¸ºç©ºåˆ—è¡¨

                for opt.imgsz in x:  # img-size
                    # å¯¹æ¯ä¸ªå›¾åƒå¤§å°è¿›è¡Œå¾ªç¯
                    LOGGER.info(f"\nRunning {f} --imgsz {opt.imgsz}...")
                    # è®°å½•æ­£åœ¨è¿è¡Œçš„å›¾åƒå¤§å°

                    r, _, t = run(**vars(opt), plots=False)
                    # è¿è¡Œä¸»ç¨‹åºï¼Œè·å¾—ç»“æœå’Œæ—¶é—´ï¼Œä¸ç»˜åˆ¶å›¾è¡¨

                    y.append(r + t)  # results and times
                    # å°†ç»“æœå’Œæ—¶é—´æ·»åŠ åˆ°yè½´åˆ—è¡¨ä¸­

                np.savetxt(f, y, fmt="%10.4g")  # save
                # å°†yè½´æ•°æ®ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶

            subprocess.run(["zip", "-r", "study.zip", "study_*.txt"])
            # å°†æ‰€æœ‰ç ”ç©¶ç»“æœæ–‡ä»¶å‹ç¼©ä¸ºstudy.zip

            plot_val_study(x=x)  # plot
            # ç»˜åˆ¶ç ”ç©¶ç»“æœå›¾

        else:
            raise NotImplementedError(f'--task {opt.task} not in ("train", "val", "test", "speed", "study")')
            # å¦‚æœä»»åŠ¡ä¸åœ¨å·²å®šä¹‰çš„èŒƒå›´å†…ï¼Œåˆ™å¼•å‘æœªå®ç°é”™è¯¯


if __name__ == "__main__":
    opt = parse_opt()
    # è§£æå‘½ä»¤è¡Œé€‰é¡¹

    main(opt)
    # æ‰§è¡Œä¸»ç¨‹åº