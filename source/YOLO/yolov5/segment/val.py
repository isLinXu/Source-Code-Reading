# YOLOv5 ğŸš€ by Ultralytics, AGPL-3.0 license
"""
Validate a trained YOLOv5 segment model on a segment dataset.

Usage:
    $ bash data/scripts/get_coco.sh --val --segments  # download COCO-segments val split (1G, 5000 images)
    $ python segment/val.py --weights yolov5s-seg.pt --data coco.yaml --img 640  # validate COCO-segments

Usage - formats:
    $ python segment/val.py --weights yolov5s-seg.pt                 # PyTorch
                                      yolov5s-seg.torchscript        # TorchScript
                                      yolov5s-seg.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                      yolov5s-seg_openvino_label     # OpenVINO
                                      yolov5s-seg.engine             # TensorRT
                                      yolov5s-seg.mlmodel            # CoreML (macOS-only)
                                      yolov5s-seg_saved_model        # TensorFlow SavedModel
                                      yolov5s-seg.pb                 # TensorFlow GraphDef
                                      yolov5s-seg.tflite             # TensorFlow Lite
                                      yolov5s-seg_edgetpu.tflite     # TensorFlow Edge TPU
                                      yolov5s-seg_paddle_model       # PaddlePaddle
"""

import argparse  # å¯¼å…¥argparseåº“ï¼Œç”¨äºå¤„ç†å‘½ä»¤è¡Œå‚æ•°
import json  # å¯¼å…¥jsonåº“ï¼Œç”¨äºå¤„ç†JSONæ•°æ®
import os  # å¯¼å…¥osåº“ï¼Œç”¨äºä¸æ“ä½œç³»ç»Ÿäº¤äº’
import subprocess  # å¯¼å…¥subprocessåº“ï¼Œç”¨äºæ‰§è¡Œå­è¿›ç¨‹
import sys  # å¯¼å…¥sysåº“ï¼Œç”¨äºè®¿é—®ä¸Pythonè§£é‡Šå™¨ç›¸å…³çš„å˜é‡å’Œå‡½æ•°
from multiprocessing.pool import ThreadPool  # ä»multiprocessing.poolå¯¼å…¥ThreadPoolç±»ï¼Œç”¨äºåˆ›å»ºçº¿ç¨‹æ± 
from pathlib import Path  # ä»pathlibå¯¼å…¥Pathç±»ï¼Œç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„

import numpy as np  # å¯¼å…¥numpyåº“ï¼Œå¸¸ç”¨äºæ•°å€¼è®¡ç®—
import torch  # å¯¼å…¥PyTorchåº“ï¼Œç”¨äºæ·±åº¦å­¦ä¹ 
from tqdm import tqdm  # ä»tqdmå¯¼å…¥tqdmç±»ï¼Œç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡

FILE = Path(__file__).resolve()  # è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
ROOT = FILE.parents[1]  # è·å–YOLOv5æ ¹ç›®å½•ï¼ˆå½“å‰æ–‡ä»¶çš„çˆ¶ç›®å½•çš„çˆ¶ç›®å½•ï¼‰
if str(ROOT) not in sys.path:  # å¦‚æœæ ¹ç›®å½•ä¸åœ¨ç³»ç»Ÿè·¯å¾„ä¸­
    sys.path.append(str(ROOT))  # å°†æ ¹ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„ä¸­
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # è·å–ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•çš„æ ¹ç›®å½•è·¯å¾„

import torch.nn.functional as F  # å¯¼å…¥PyTorchçš„åŠŸèƒ½æ€§æ¨¡å—ï¼Œé€šå¸¸ç”¨äºç¥ç»ç½‘ç»œçš„æ“ä½œ

from models.common import DetectMultiBackend  # ä»models.commonå¯¼å…¥DetectMultiBackendç±»
from models.yolo import SegmentationModel  # ä»models.yoloå¯¼å…¥SegmentationModelç±»
from utils.callbacks import Callbacks  # ä»utils.callbackså¯¼å…¥Callbacksç±»
from utils.general import (  # ä»utils.generalå¯¼å…¥å¤šä¸ªå®ç”¨å‡½æ•°
    LOGGER,  # æ—¥å¿—è®°å½•å™¨
    NUM_THREADS,  # çº¿ç¨‹æ•°é‡
    TQDM_BAR_FORMAT,  # tqdmè¿›åº¦æ¡æ ¼å¼
    Profile,  # æ€§èƒ½åˆ†æç±»
    check_dataset,  # æ£€æŸ¥æ•°æ®é›†çš„å‡½æ•°
    check_img_size,  # æ£€æŸ¥å›¾åƒå¤§å°çš„å‡½æ•°
    check_requirements,  # æ£€æŸ¥ä¾èµ–é¡¹çš„å‡½æ•°
    check_yaml,  # æ£€æŸ¥YAMLæ–‡ä»¶çš„å‡½æ•°
    coco80_to_coco91_class,  # COCO 80ç±»åˆ°91ç±»çš„è½¬æ¢å‡½æ•°
    colorstr,  # é¢œè‰²å­—ç¬¦ä¸²æ ¼å¼åŒ–å‡½æ•°
    increment_path,  # å¢åŠ è·¯å¾„çš„å‡½æ•°
    non_max_suppression,  # éæå¤§å€¼æŠ‘åˆ¶å‡½æ•°
    print_args,  # æ‰“å°å‚æ•°çš„å‡½æ•°
    scale_boxes,  # ç¼©æ”¾æ¡†çš„å‡½æ•°
    xywh2xyxy,  # ä»xywhæ ¼å¼è½¬æ¢åˆ°xyxyæ ¼å¼çš„å‡½æ•°
    xyxy2xywh,  # ä»xyxyæ ¼å¼è½¬æ¢åˆ°xywhæ ¼å¼çš„å‡½æ•°
)
from utils.metrics import ConfusionMatrix, box_iou  # ä»utils.metricså¯¼å…¥æ··æ·†çŸ©é˜µå’Œæ¡†çš„IOUè®¡ç®—å‡½æ•°
from utils.plots import output_to_target, plot_val_study  # ä»utils.plotså¯¼å…¥è¾“å‡ºåˆ°ç›®æ ‡å’Œç»˜åˆ¶éªŒè¯ç ”ç©¶çš„å‡½æ•°
from utils.segment.dataloaders import create_dataloader  # ä»utils.segment.dataloaderså¯¼å…¥åˆ›å»ºæ•°æ®åŠ è½½å™¨çš„å‡½æ•°
from utils.segment.general import (  # ä»utils.segment.generalå¯¼å…¥å¤šä¸ªå®ç”¨å‡½æ•°
    mask_iou,  # è®¡ç®—æ©ç çš„IOUå‡½æ•°
    process_mask,  # å¤„ç†æ©ç çš„å‡½æ•°
    process_mask_native,  # åŸç”Ÿå¤„ç†æ©ç çš„å‡½æ•°
    scale_image,  # ç¼©æ”¾å›¾åƒçš„å‡½æ•°
)
from utils.segment.metrics import Metrics, ap_per_class_box_and_mask  # ä»utils.segment.metricså¯¼å…¥Metricsç±»å’ŒæŒ‰ç±»åˆ«è®¡ç®—APçš„å‡½æ•°
from utils.segment.plots import plot_images_and_masks  # ä»utils.segment.plotså¯¼å…¥ç»˜åˆ¶å›¾åƒå’Œæ©ç çš„å‡½æ•°
from utils.torch_utils import de_parallel, select_device, smart_inference_mode  # ä»utils.torch_utilså¯¼å…¥å¤šä¸ªå®ç”¨å‡½æ•°

def save_one_txt(predn, save_conf, shape, file):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä¿å­˜æ£€æµ‹ç»“æœåˆ°txtæ–‡ä»¶
    """Saves detection results in txt format; includes class, xywh (normalized), optionally confidence if `save_conf` is
    True.  # å°†æ£€æµ‹ç»“æœä»¥txtæ ¼å¼ä¿å­˜ï¼›åŒ…æ‹¬ç±»åˆ«ã€xywhï¼ˆå½’ä¸€åŒ–ï¼‰ï¼Œå¦‚æœ`save_conf`ä¸ºTrueåˆ™å¯é€‰ä¿å­˜ç½®ä¿¡åº¦ã€‚
    """
    gn = torch.tensor(shape)[[1, 0, 1, 0]]  # normalization gain whwh  # å½’ä¸€åŒ–å¢ç›Šwhwh
    for *xyxy, conf, cls in predn.tolist():  # éå†é¢„æµ‹ç»“æœï¼Œå°†xyxyã€ç½®ä¿¡åº¦å’Œç±»åˆ«è§£åŒ…
        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh  # å½’ä¸€åŒ–çš„xywh
        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format  # æ ‡ç­¾æ ¼å¼
        with open(file, "a") as f:  # ä»¥è¿½åŠ æ¨¡å¼æ‰“å¼€æ–‡ä»¶
            f.write(("%g " * len(line)).rstrip() % line + "\n")  # å°†ç»“æœå†™å…¥æ–‡ä»¶


def save_one_json(predn, jdict, path, class_map, pred_masks):
    """
    Saves a JSON file with detection results including bounding boxes, category IDs, scores, and segmentation masks.
    ä¿å­˜ä¸€ä¸ªåŒ…å«æ£€æµ‹ç»“æœçš„ JSON æ–‡ä»¶ï¼ŒåŒ…æ‹¬è¾¹ç•Œæ¡†ã€ç±»åˆ« IDã€å¾—åˆ†å’Œåˆ†å‰²æ©ç ã€‚

    Example JSON result: {"image_id": 42, "category_id": 18, "bbox": [258.15, 41.29, 348.26, 243.78], "score": 0.236}.
    ç¤ºä¾‹ JSON ç»“æœï¼š{"image_id": 42, "category_id": 18, "bbox": [258.15, 41.29, 348.26, 243.78], "score": 0.236}ã€‚
    """
    from pycocotools.mask import encode
    # ä» pycocotools.mask å¯¼å…¥ encode å‡½æ•°ï¼Œç”¨äºç¼–ç åˆ†å‰²æ©ç ã€‚

    def single_encode(x):
        # å®šä¹‰ä¸€ä¸ªå†…éƒ¨å‡½æ•° single_encodeï¼Œç”¨äºå¯¹å•ä¸ªæ©ç è¿›è¡Œç¼–ç ã€‚
        rle = encode(np.asarray(x[:, :, None], order="F", dtype="uint8"))[0]
        # å°†è¾“å…¥æ•°ç»„ x è½¬æ¢ä¸º RLEï¼ˆæ¸¸ç¨‹é•¿åº¦ç¼–ç ï¼‰æ ¼å¼ã€‚
        rle["counts"] = rle["counts"].decode("utf-8")
        # å°† RLE çš„ counts å­—æ®µä»å­—èŠ‚è§£ç ä¸ºå­—ç¬¦ä¸²ã€‚
        return rle

    image_id = int(path.stem) if path.stem.isnumeric() else path.stem
    # ä»è·¯å¾„ä¸­æå–å›¾åƒ IDï¼Œå¦‚æœè·¯å¾„çš„ stem æ˜¯æ•°å­—ï¼Œåˆ™è½¬æ¢ä¸ºæ•´æ•°ï¼Œå¦åˆ™ä¿æŒä¸ºå­—ç¬¦ä¸²ã€‚
    box = xyxy2xywh(predn[:, :4])  # xywh
    # å°†é¢„æµ‹çš„è¾¹ç•Œæ¡†ä» (x1, y1, x2, y2) æ ¼å¼è½¬æ¢ä¸º (x_center, y_center, width, height) æ ¼å¼ã€‚
    box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner
    # å°†ä¸­å¿ƒåæ ‡è½¬æ¢ä¸ºå·¦ä¸Šè§’åæ ‡ï¼Œé€šè¿‡å‡å»å®½åº¦å’Œé«˜åº¦çš„ä¸€åŠã€‚
    pred_masks = np.transpose(pred_masks, (2, 0, 1))
    # è½¬ç½®é¢„æµ‹æ©ç çš„ç»´åº¦ï¼Œä»¥ä¾¿äºåç»­å¤„ç†ï¼Œè°ƒæ•´ç»´åº¦é¡ºåºä¸º (num_masks, height, width)ã€‚
    with ThreadPool(NUM_THREADS) as pool:
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ©ç ç¼–ç ã€‚
        rles = pool.map(single_encode, pred_masks)
        # å¯¹æ¯ä¸ªé¢„æµ‹æ©ç è°ƒç”¨ single_encode å‡½æ•°è¿›è¡Œç¼–ç ï¼Œè¿”å› RLE åˆ—è¡¨ã€‚

    for i, (p, b) in enumerate(zip(predn.tolist(), box.tolist())):
        # éå†é¢„æµ‹ç»“æœå’Œè¾¹ç•Œæ¡†ï¼Œä½¿ç”¨ enumerate è·å–ç´¢å¼• iã€‚
        jdict.append(
            {
                "image_id": image_id,
                # æ·»åŠ å›¾åƒ ID åˆ° JSON å­—å…¸ã€‚
                "category_id": class_map[int(p[5])],
                # æ ¹æ®é¢„æµ‹ç»“æœä¸­çš„ç±»åˆ«ç´¢å¼•è·å–ç±»åˆ« IDã€‚
                "bbox": [round(x, 3) for x in b],
                # å°†è¾¹ç•Œæ¡†åæ ‡å››èˆäº”å…¥åˆ°å°æ•°ç‚¹åä¸‰ä½ã€‚
                "score": round(p[4], 5),
                # å°†å¾—åˆ†å››èˆäº”å…¥åˆ°å°æ•°ç‚¹åäº”ä½ã€‚
                "segmentation": rles[i],
                # å°†ç¼–ç åçš„åˆ†å‰²æ©ç æ·»åŠ åˆ° JSON å­—å…¸ä¸­ã€‚
            }
        )


def process_batch(detections, labels, iouv, pred_masks=None, gt_masks=None, overlap=False, masks=False):
    """
    Return correct prediction matrix
    è¿”å›æ­£ç¡®çš„é¢„æµ‹çŸ©é˜µ
    Arguments:
        detections (array[N, 6]), x1, y1, x2, y2, conf, class
        detectionsï¼ˆæ•°ç»„[N, 6]ï¼‰ï¼Œx1, y1, x2, y2, ç½®ä¿¡åº¦, ç±»åˆ«
        labels (array[M, 5]), class, x1, y1, x2, y2
        labelsï¼ˆæ•°ç»„[M, 5]ï¼‰ï¼Œç±»åˆ«, x1, y1, x2, y2
    Returns:
        correct (array[N, 10]), for 10 IoU levels
        correctï¼ˆæ•°ç»„[N, 10]ï¼‰ï¼Œç”¨äº 10 ä¸ª IoU æ°´å¹³
    """
    if masks:
        # å¦‚æœä½¿ç”¨æ©ç 
        if overlap:
            # å¦‚æœå…è®¸é‡å 
            nl = len(labels)
            index = torch.arange(nl, device=gt_masks.device).view(nl, 1, 1) + 1
            # åˆ›å»ºä¸€ä¸ªç´¢å¼•æ•°ç»„ï¼Œå½¢çŠ¶ä¸º(nl, 1, 1)ï¼Œç”¨äºåç»­å¤„ç†
            gt_masks = gt_masks.repeat(nl, 1, 1)  # shape(1,640,640) -> (n,640,640)
            # å°† gt_masks æ‰©å±•ä¸º(nl, 640, 640)çš„å½¢çŠ¶
            gt_masks = torch.where(gt_masks == index, 1.0, 0.0)
            # å°† gt_masks ä¸­ç­‰äºç´¢å¼•çš„å€¼è®¾ä¸º 1.0ï¼Œå…¶ä½™è®¾ä¸º 0.0
        if gt_masks.shape[1:] != pred_masks.shape[1:]:
            # å¦‚æœ gt_masks å’Œ pred_masks çš„å½¢çŠ¶ä¸ä¸€è‡´
            gt_masks = F.interpolate(gt_masks[None], pred_masks.shape[1:], mode="bilinear", align_corners=False)[0]
            # ä½¿ç”¨åŒçº¿æ€§æ’å€¼è°ƒæ•´ gt_masks çš„å¤§å°ä»¥åŒ¹é… pred_masks
            gt_masks = gt_masks.gt_(0.5)
            # å°† gt_masks ä¸­å¤§äº 0.5 çš„å€¼è®¾ä¸º True
        iou = mask_iou(gt_masks.view(gt_masks.shape[0], -1), pred_masks.view(pred_masks.shape[0], -1))
        # è®¡ç®— gt_masks å’Œ pred_masks ä¹‹é—´çš„ IoU
    else:  # boxes
        # å¦‚æœä¸ä½¿ç”¨æ©ç ï¼Œåˆ™è®¡ç®—è¾¹ç•Œæ¡†çš„ IoU
        iou = box_iou(labels[:, 1:], detections[:, :4])

    correct = np.zeros((detections.shape[0], iouv.shape[0])).astype(bool)
    # åˆ›å»ºä¸€ä¸ªæ­£ç¡®é¢„æµ‹çŸ©é˜µï¼Œåˆå§‹åŒ–ä¸º False
    correct_class = labels[:, 0:1] == detections[:, 5]
    # æ£€æŸ¥é¢„æµ‹ç±»åˆ«ä¸çœŸå®ç±»åˆ«æ˜¯å¦åŒ¹é…
    for i in range(len(iouv)):
        # éå†æ¯ä¸ª IoU é˜ˆå€¼
        x = torch.where((iou >= iouv[i]) & correct_class)  # IoU > threshold and classes match
        # æ‰¾åˆ° IoU å¤§äºå½“å‰é˜ˆå€¼ä¸”ç±»åˆ«åŒ¹é…çš„é¢„æµ‹
        if x[0].shape[0]:
            matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()  # [label, detect, iou]
            # å°†åŒ¹é…çš„æ ‡ç­¾ã€æ£€æµ‹å’Œ IoU ç»„åˆæˆä¸€ä¸ªæ•°ç»„
            if x[0].shape[0] > 1:
                matches = matches[matches[:, 2].argsort()[::-1]]
                # æŒ‰ç…§ IoU ä»å¤§åˆ°å°æ’åº
                matches = matches[np.unique(matches[:, 1], return_index=True)[1]]
                # å»é™¤é‡å¤çš„æ£€æµ‹
                matches = matches[np.unique(matches[:, 0], return_index=True)[1]]
                # å»é™¤é‡å¤çš„æ ‡ç­¾
            correct[matches[:, 1].astype(int), i] = True
            # æ›´æ–°æ­£ç¡®é¢„æµ‹çŸ©é˜µ
    return torch.tensor(correct, dtype=torch.bool, device=iouv.device)
    # è¿”å›æ­£ç¡®é¢„æµ‹çŸ©é˜µä½œä¸ºå¸ƒå°”å¼ é‡


@smart_inference_mode()
def run(
    data,
    weights=None,  # model.pt path(s)
    # æ¨¡å‹è·¯å¾„
    batch_size=32,  # batch size
    # æ‰¹å¤„ç†å¤§å°
    imgsz=640,  # inference size (pixels)
    # æ¨ç†å›¾åƒå¤§å°ï¼ˆåƒç´ ï¼‰
    conf_thres=0.001,  # confidence threshold
    # ç½®ä¿¡åº¦é˜ˆå€¼
    iou_thres=0.6,  # NMS IoU threshold
    # NMS IoU é˜ˆå€¼
    max_det=300,  # maximum detections per image
    # æ¯å¼ å›¾åƒçš„æœ€å¤§æ£€æµ‹æ•°
    task="val",  # train, val, test, speed or study
    # ä»»åŠ¡ç±»å‹ï¼šè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•ã€é€Ÿåº¦æˆ–å­¦ä¹ 
    device="",  # cuda device, i.e. 0 or 0,1,2,3 or cpu
    # CUDA è®¾å¤‡ï¼Œä¾‹å¦‚ 0 æˆ– 0,1,2,3 æˆ– cpu
    workers=8,  # max dataloader workers (per RANK in DDP mode)
    # æœ€å¤§æ•°æ®åŠ è½½å™¨å·¥ä½œçº¿ç¨‹ï¼ˆåœ¨ DDP æ¨¡å¼ä¸‹æ¯ä¸ª RANKï¼‰
    single_cls=False,  # treat as single-class dataset
    # å°†æ•°æ®é›†è§†ä¸ºå•ç±»æ•°æ®é›†
    augment=False,  # augmented inference
    # å¢å¼ºæ¨ç†
    verbose=False,  # verbose output
    # è¯¦ç»†è¾“å‡º
    save_txt=False,  # save results to *.txt
    # å°†ç»“æœä¿å­˜åˆ° *.txt
    save_hybrid=False,  # save label+prediction hybrid results to *.txt
    # å°†æ ‡ç­¾+é¢„æµ‹æ··åˆç»“æœä¿å­˜åˆ° *.txt
    save_conf=False,  # save confidences in --save-txt labels
    # åœ¨ --save-txt æ ‡ç­¾ä¸­ä¿å­˜ç½®ä¿¡åº¦
    save_json=False,  # save a COCO-JSON results file
    # ä¿å­˜ COCO-JSON ç»“æœæ–‡ä»¶
    project=ROOT / "runs/val-seg",  # save to project/name
    # ä¿å­˜åˆ°é¡¹ç›®/åç§°
    name="exp",  # save to project/name
    # ä¿å­˜åˆ°é¡¹ç›®/åç§°
    exist_ok=False,  # existing project/name ok, do not increment
    # å…è®¸å­˜åœ¨çš„é¡¹ç›®/åç§°ï¼Œä¸é€’å¢
    half=True,  # use FP16 half-precision inference
    # ä½¿ç”¨ FP16 åŠç²¾åº¦æ¨ç†
    dnn=False,  # use OpenCV DNN for ONNX inference
    # ä½¿ç”¨ OpenCV DNN è¿›è¡Œ ONNX æ¨ç†
    model=None,
    # æ¨¡å‹
    dataloader=None,
    # æ•°æ®åŠ è½½å™¨
    save_dir=Path(""),
    # ä¿å­˜ç›®å½•
    plots=True,
    # æ˜¯å¦ç”Ÿæˆå›¾è¡¨
    overlap=False,
    # æ˜¯å¦å…è®¸é‡å 
    mask_downsample_ratio=1,
    # æ©ç ä¸‹é‡‡æ ·æ¯”ä¾‹
    compute_loss=None,
    # è®¡ç®—æŸå¤±å‡½æ•°
    callbacks=Callbacks(),
):
    if save_json:
        # å¦‚æœéœ€è¦ä¿å­˜ JSON
        check_requirements("pycocotools>=2.0.6")
        # æ£€æŸ¥ pycocotools çš„ç‰ˆæœ¬è¦æ±‚
        process = process_mask_native  # more accurate
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ©ç å¤„ç†
    else:
        process = process_mask  # faster
        # ä½¿ç”¨æ›´å¿«çš„æ©ç å¤„ç†

    # Initialize/load model and set device
    # åˆå§‹åŒ–/åŠ è½½æ¨¡å‹å¹¶è®¾ç½®è®¾å¤‡
    training = model is not None
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ä»¥ç¡®å®šæ˜¯å¦åœ¨è®­ç»ƒ
    if training:  # called by train.py
        # å¦‚æœæ˜¯ç”± train.py è°ƒç”¨
        device, pt, jit, engine = next(model.parameters()).device, True, False, False  # get model device, PyTorch model
        # è·å–æ¨¡å‹è®¾å¤‡ï¼ŒPyTorch æ¨¡å‹
        half &= device.type != "cpu"  # half precision only supported on CUDA
        # ä»…åœ¨ CUDA ä¸Šæ”¯æŒåŠç²¾åº¦
        model.half() if half else model.float()
        # å°†æ¨¡å‹è½¬æ¢ä¸ºåŠç²¾åº¦æˆ–å•ç²¾åº¦
        nm = de_parallel(model).model[-1].nm  # number of masks
        # è·å–æ©ç æ•°é‡
    else:  # called directly
        # å¦‚æœæ˜¯ç›´æ¥è°ƒç”¨
        device = select_device(device, batch_size=batch_size)
        # é€‰æ‹©è®¾å¤‡

        # Directories
        save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run
        # å¢åŠ è¿è¡Œç›®å½•
        (save_dir / "labels" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir
        # å¦‚æœéœ€è¦ä¿å­˜æ–‡æœ¬ï¼Œåˆ™åˆ›å»ºæ ‡ç­¾ç›®å½•

        # Load model
        model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)
        # åŠ è½½å¤šåç«¯æ£€æµ‹æ¨¡å‹
        stride, pt, jit, engine = model.stride, model.pt, model.jit, model.engine
        # è·å–æ¨¡å‹çš„æ­¥å¹…ã€PyTorch çŠ¶æ€ã€JIT çŠ¶æ€å’Œå¼•æ“çŠ¶æ€
        imgsz = check_img_size(imgsz, s=stride)  # check image size
        # æ£€æŸ¥å›¾åƒå¤§å°
        half = model.fp16  # FP16 supported on limited backends with CUDA
        # FP16 ä»…åœ¨æœ‰é™çš„åç«¯æ”¯æŒ
        nm = de_parallel(model).model.model[-1].nm if isinstance(model, SegmentationModel) else 32  # number of masks
        # è·å–æ©ç æ•°é‡ï¼Œå¦‚æœæ¨¡å‹æ˜¯åˆ†å‰²æ¨¡å‹
        if engine:
            batch_size = model.batch_size
        else:
            device = model.device
            if not (pt or jit):
                batch_size = 1  # export.py models default to batch-size 1
                # export.py æ¨¡å‹é»˜è®¤ä¸ºæ‰¹å¤„ç†å¤§å° 1
                LOGGER.info(f"Forcing --batch-size 1 square inference (1,3,{imgsz},{imgsz}) for non-PyTorch models")
                # æ—¥å¿—è®°å½•ï¼Œå¼ºåˆ¶é PyTorch æ¨¡å‹ä½¿ç”¨æ‰¹å¤„ç†å¤§å° 1

        # Data
        data = check_dataset(data)  # check
        # æ£€æŸ¥æ•°æ®é›†
    
    # Configure
    model.eval()
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    cuda = device.type != "cpu"
    # æ£€æŸ¥è®¾å¤‡æ˜¯å¦ä¸º CUDA
    is_coco = isinstance(data.get("val"), str) and data["val"].endswith(f"coco{os.sep}val2017.txt")  # COCO dataset
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸º COCO æ•°æ®é›†
    nc = 1 if single_cls else int(data["nc"])  # number of classes
    # ç¡®å®šç±»åˆ«æ•°é‡
    iouv = torch.linspace(0.5, 0.95, 10, device=device)  # iou vector for mAP@0.5:0.95
    # åˆ›å»º IoU å‘é‡ç”¨äºè®¡ç®— mAP
    niou = iouv.numel()
    # è·å– IoU å‘é‡çš„å…ƒç´ æ•°é‡

    # Dataloader
    if not training:
        # å¦‚æœä¸æ˜¯åœ¨è®­ç»ƒæ¨¡å¼
        if pt and not single_cls:  # check --weights are trained on --data
            # å¦‚æœä½¿ç”¨ PyTorch æ¨¡å‹å¹¶ä¸”ä¸æ˜¯å•ç±»æ•°æ®é›†
            ncm = model.model.nc
            # è·å–æ¨¡å‹çš„ç±»åˆ«æ•°é‡
            assert ncm == nc, (
                f"{weights} ({ncm} classes) trained on different --data than what you passed ({nc} "
                f"classes). Pass correct combination of --weights and --data that are trained together."
            )
            # ç¡®ä¿æ¨¡å‹è®­ç»ƒçš„ç±»åˆ«æ•°é‡ä¸ä¼ å…¥çš„æ•°æ®é›†ç±»åˆ«æ•°é‡ä¸€è‡´
        model.warmup(imgsz=(1 if pt else batch_size, 3, imgsz, imgsz))  # warmup
        # å¯¹æ¨¡å‹è¿›è¡Œé¢„çƒ­ï¼Œè®¾ç½®å›¾åƒå¤§å°
        pad, rect = (0.0, False) if task == "speed" else (0.5, pt)  # square inference for benchmarks
        # æ ¹æ®ä»»åŠ¡ç±»å‹è®¾ç½®å¡«å……å’ŒçŸ©å½¢æ¨ç†æ ‡å¿—
        task = task if task in ("train", "val", "test") else "val"  # path to train/val/test images
        # ç¡®ä¿ä»»åŠ¡ç±»å‹æœ‰æ•ˆï¼Œé»˜è®¤ä¸ºéªŒè¯æ¨¡å¼
        dataloader = create_dataloader(
            data[task],
            imgsz,
            batch_size,
            stride,
            single_cls,
            pad=pad,
            rect=rect,
            workers=workers,
            prefix=colorstr(f"{task}: "),
            overlap_mask=overlap,
            mask_downsample_ratio=mask_downsample_ratio,
        )[0]
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå…ƒç´ 

    seen = 0
    # è®°å½•å·²å¤„ç†çš„æ ·æœ¬æ•°é‡
    confusion_matrix = ConfusionMatrix(nc=nc)
    # åˆå§‹åŒ–æ··æ·†çŸ©é˜µ
    names = model.names if hasattr(model, "names") else model.module.names  # get class names
    # è·å–ç±»åˆ«åç§°
    if isinstance(names, (list, tuple)):  # old format
        names = dict(enumerate(names))
        # å¦‚æœåç§°æ˜¯åˆ—è¡¨æˆ–å…ƒç»„ï¼Œåˆ™è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    class_map = coco80_to_coco91_class() if is_coco else list(range(1000))
    # å¦‚æœæ•°æ®é›†æ˜¯ COCO æ ¼å¼ï¼Œåˆ™è·å– COCO 80 åˆ° COCO 91 çš„ç±»åˆ«æ˜ å°„ï¼Œå¦åˆ™ä½¿ç”¨ 0 åˆ° 999 çš„èŒƒå›´
    s = ("%22s" + "%11s" * 10) % (
        "Class",
        "Images",
        "Instances",
        "Box(P",
        "R",
        "mAP50",
        "mAP50-95)",
        "Mask(P",
        "R",
        "mAP50",
        "mAP50-95)",
    )
    # å®šä¹‰è¾“å‡ºæ ¼å¼å­—ç¬¦ä¸²
    dt = Profile(device=device), Profile(device=device), Profile(device=device)
    # åˆ›å»ºä¸‰ä¸ªæ€§èƒ½åˆ†æå™¨
    metrics = Metrics()
    # åˆå§‹åŒ–åº¦é‡å¯¹è±¡
    loss = torch.zeros(4, device=device)
    # åˆå§‹åŒ–æŸå¤±ä¸ºé›¶
    jdict, stats = [], []
    # åˆå§‹åŒ– JSON å­—å…¸å’Œç»Ÿè®¡ä¿¡æ¯åˆ—è¡¨
    # callbacks.run('on_val_start')
    pbar = tqdm(dataloader, desc=s, bar_format=TQDM_BAR_FORMAT)  # progress bar
    # åˆ›å»ºè¿›åº¦æ¡

    for batch_i, (im, targets, paths, shapes, masks) in enumerate(pbar):
        # éå†æ•°æ®åŠ è½½å™¨ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡
        # callbacks.run('on_val_batch_start')
        with dt[0]:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ€§èƒ½åˆ†æå™¨
            if cuda:
                im = im.to(device, non_blocking=True)
                # å°†å›¾åƒæ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
                targets = targets.to(device)
                # å°†ç›®æ ‡æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
                masks = masks.to(device)
                # å°†æ©ç æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
            masks = masks.float()
            # å°†æ©ç è½¬æ¢ä¸ºæµ®ç‚¹å‹
            im = im.half() if half else im.float()  # uint8 to fp16/32
            # å°†å›¾åƒæ•°æ®è½¬æ¢ä¸ºåŠç²¾åº¦æˆ–å•ç²¾åº¦
            im /= 255  # 0 - 255 to 0.0 - 1.0
            # å°†å›¾åƒæ•°æ®å½’ä¸€åŒ–åˆ° [0.0, 1.0] èŒƒå›´
            nb, _, height, width = im.shape  # batch size, channels, height, width
            # è·å–æ‰¹æ¬¡å¤§å°ã€é€šé“æ•°ã€é«˜åº¦å’Œå®½åº¦

        # Inference
        with dt[1]:
            # ä½¿ç”¨ç¬¬äºŒä¸ªæ€§èƒ½åˆ†æå™¨
            preds, protos, train_out = model(im) if compute_loss else (*model(im, augment=augment)[:2], None)
            # è¿›è¡Œæ¨ç†ï¼Œè·å–é¢„æµ‹ç»“æœå’ŒåŸå‹è¾“å‡º

        # Loss
        if compute_loss:
            # å¦‚æœéœ€è¦è®¡ç®—æŸå¤±
            loss += compute_loss((train_out, protos), targets, masks)[1]  # box, obj, cls
            # æ›´æ–°æŸå¤±å€¼

        # NMS
        targets[:, 2:] *= torch.tensor((width, height, width, height), device=device)  # to pixels
        # å°†ç›®æ ‡æ¡†çš„åæ ‡è½¬æ¢ä¸ºåƒç´ å€¼
        lb = [targets[targets[:, 0] == i, 1:] for i in range(nb)] if save_hybrid else []  # for autolabelling
        # ä¸ºæ¯ä¸ªæ ·æœ¬å‡†å¤‡æ ‡ç­¾
        with dt[2]:
            # ä½¿ç”¨ç¬¬ä¸‰ä¸ªæ€§èƒ½åˆ†æå™¨
            preds = non_max_suppression(
                preds, conf_thres, iou_thres, labels=lb, multi_label=True, agnostic=single_cls, max_det=max_det, nm=nm
            )
            # è¿›è¡Œéæå¤§å€¼æŠ‘åˆ¶

        # Metrics
        plot_masks = []  # masks for plotting
        # åˆå§‹åŒ–ç»˜å›¾æ©ç åˆ—è¡¨
        for si, (pred, proto) in enumerate(zip(preds, protos)):
            # éå†æ¯ä¸ªé¢„æµ‹å’ŒåŸå‹
            labels = targets[targets[:, 0] == si, 1:]
            # è·å–å½“å‰æ ·æœ¬çš„æ ‡ç­¾
            nl, npr = labels.shape[0], pred.shape[0]  # number of labels, predictions
            # è·å–æ ‡ç­¾å’Œé¢„æµ‹çš„æ•°é‡
            path, shape = Path(paths[si]), shapes[si][0]
            # è·å–å½“å‰æ ·æœ¬çš„è·¯å¾„å’Œå½¢çŠ¶
            correct_masks = torch.zeros(npr, niou, dtype=torch.bool, device=device)  # init
            # åˆå§‹åŒ–æ­£ç¡®æ©ç 
            correct_bboxes = torch.zeros(npr, niou, dtype=torch.bool, device=device)  # init
            # åˆå§‹åŒ–æ­£ç¡®è¾¹ç•Œæ¡†
            seen += 1
            # æ›´æ–°å·²å¤„ç†æ ·æœ¬è®¡æ•°

            if npr == 0:
                # å¦‚æœæ²¡æœ‰é¢„æµ‹
                if nl:
                    stats.append((correct_masks, correct_bboxes, *torch.zeros((2, 0), device=device), labels[:, 0]))
                    # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                    if plots:
                        confusion_matrix.process_batch(detections=None, labels=labels[:, 0])
                        # æ›´æ–°æ··æ·†çŸ©é˜µ
                continue

            # Masks
            midx = [si] if overlap else targets[:, 0] == si
            # æ ¹æ®é‡å æƒ…å†µé€‰æ‹©æ©ç ç´¢å¼•
            gt_masks = masks[midx]
            # è·å–çœŸå®æ©ç 
            pred_masks = process(proto, pred[:, 6:], pred[:, :4], shape=im[si].shape[1:])
            # å¤„ç†é¢„æµ‹æ©ç 

            # Predictions
            if single_cls:
                pred[:, 5] = 0
                # å¦‚æœæ˜¯å•ç±»æ•°æ®é›†ï¼Œå°†ç±»åˆ«ç´¢å¼•è®¾ä¸º 0
            predn = pred.clone()
            # å…‹éš†é¢„æµ‹ç»“æœ
            scale_boxes(im[si].shape[1:], predn[:, :4], shape, shapes[si][1])  # native-space pred
            # å°†é¢„æµ‹æ¡†çš„åæ ‡è½¬æ¢ä¸ºåŸå§‹ç©ºé—´

            # Evaluate
            if nl:
                # å¦‚æœæœ‰æ ‡ç­¾
                tbox = xywh2xyxy(labels[:, 1:5])  # target boxes
                # å°†ç›®æ ‡æ¡†ä» (x_center, y_center, width, height) è½¬æ¢ä¸º (x1, y1, x2, y2) æ ¼å¼
                scale_boxes(im[si].shape[1:], tbox, shape, shapes[si][1])  # native-space labels
                # å°†ç›®æ ‡æ¡†çš„åæ ‡è½¬æ¢ä¸ºåŸå§‹ç©ºé—´
                labelsn = torch.cat((labels[:, 0:1], tbox), 1)  # native-space labels
                # åˆå¹¶æ ‡ç­¾å’Œç›®æ ‡æ¡†
                correct_bboxes = process_batch(predn, labelsn, iouv)
                # å¤„ç†è¾¹ç•Œæ¡†
                correct_masks = process_batch(predn, labelsn, iouv, pred_masks, gt_masks, overlap=overlap, masks=True)
                # å¤„ç†æ©ç 
                if plots:
                    confusion_matrix.process_batch(predn, labelsn)
                    # æ›´æ–°æ··æ·†çŸ©é˜µ
            stats.append((correct_masks, correct_bboxes, pred[:, 4], pred[:, 5], labels[:, 0]))  # (conf, pcls, tcls)
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯

            pred_masks = torch.as_tensor(pred_masks, dtype=torch.uint8)
            # å°†é¢„æµ‹æ©ç è½¬æ¢ä¸º uint8 ç±»å‹
            if plots and batch_i < 3:
                plot_masks.append(pred_masks[:15])  # filter top 15 to plot
                # è¿‡æ»¤å‰ 15 ä¸ªæ©ç ç”¨äºç»˜å›¾

            # Save/log
            if save_txt:
                save_one_txt(predn, save_conf, shape, file=save_dir / "labels" / f"{path.stem}.txt")
                # ä¿å­˜é¢„æµ‹ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶
            if save_json:
                pred_masks = scale_image(
                    im[si].shape[1:], pred_masks.permute(1, 2, 0).contiguous().cpu().numpy(), shape, shapes[si][1]
                )
                # å¯¹é¢„æµ‹æ©ç è¿›è¡Œç¼©æ”¾
                save_one_json(predn, jdict, path, class_map, pred_masks)  # append to COCO-JSON dictionary
                # å°†é¢„æµ‹ç»“æœæ·»åŠ åˆ° COCO-JSON å­—å…¸ä¸­
            # callbacks.run('on_val_image_end', pred, predn, path, names, im[si])
            
            # Plot images
            # ç»˜åˆ¶å›¾åƒ
            if plots and batch_i < 3:
                # å¦‚æœéœ€è¦ç»˜åˆ¶å›¾åƒå¹¶ä¸”å½“å‰æ‰¹æ¬¡å°äº3
                if len(plot_masks):
                    # å¦‚æœæœ‰æ©ç 
                    plot_masks = torch.cat(plot_masks, dim=0)
                    # å°†æ‰€æœ‰æ©ç åœ¨ç¬¬0ç»´åº¦ä¸Šè¿æ¥æˆä¸€ä¸ªå¼ é‡
                plot_images_and_masks(im, targets, masks, paths, save_dir / f"val_batch{batch_i}_labels.jpg", names)
                # ç»˜åˆ¶çœŸå®å›¾åƒå’Œæ©ç ï¼Œå¹¶ä¿å­˜ä¸ºâ€œval_batch{batch_i}_labels.jpgâ€
                plot_images_and_masks(
                    im,
                    output_to_target(preds, max_det=15),
                    plot_masks,
                    paths,
                    save_dir / f"val_batch{batch_i}_pred.jpg",
                    names,
                )  # pred
                # ç»˜åˆ¶é¢„æµ‹å›¾åƒå’Œæ©ç ï¼Œå¹¶ä¿å­˜ä¸ºâ€œval_batch{batch_i}_pred.jpgâ€

            # callbacks.run('on_val_batch_end')
            # è°ƒç”¨å›è°ƒå‡½æ•°ï¼Œæ‰§è¡ŒéªŒè¯æ‰¹æ¬¡ç»“æŸæ—¶çš„æ“ä½œ

    # Compute metrics
    stats = [torch.cat(x, 0).cpu().numpy() for x in zip(*stats)]  # to numpy
    # å°†ç»Ÿè®¡ä¿¡æ¯ä»å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„ï¼Œä½¿ç”¨ torch.cat å°†æ¯ä¸ªç»Ÿè®¡ä¿¡æ¯æŒ‰ç»´åº¦ 0 è¿æ¥èµ·æ¥ï¼Œå¹¶å°†å…¶ç§»åŠ¨åˆ° CPU ä¸Šã€‚

    if len(stats) and stats[0].any():
        results = ap_per_class_box_and_mask(*stats, plot=plots, save_dir=save_dir, names=names)
        # å¦‚æœ stats ä¸ä¸ºç©ºä¸”ç¬¬ä¸€ä¸ªç»Ÿè®¡ä¿¡æ¯ä¸­æœ‰æ•°æ®ï¼Œåˆ™è°ƒç”¨ ap_per_class_box_and_mask å‡½æ•°è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ APï¼ˆå¹³å‡ç²¾åº¦ï¼‰åŠå…¶ä»–æŒ‡æ ‡ã€‚

        metrics.update(results)
        # æ›´æ–° metrics å¯¹è±¡ï¼ŒåŠ å…¥æ–°è®¡ç®—çš„ç»“æœã€‚

    nt = np.bincount(stats[4].astype(int), minlength=nc)  # number of targets per class
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç›®æ ‡æ•°é‡ï¼Œä½¿ç”¨ np.bincount ç»Ÿè®¡ stats[4] ä¸­æ¯ä¸ªç±»åˆ«çš„å‡ºç°æ¬¡æ•°ï¼Œç¡®ä¿é•¿åº¦è‡³å°‘ä¸º ncã€‚

    # Print results
    pf = "%22s" + "%11i" * 2 + "%11.3g" * 8  # print format
    # å®šä¹‰æ‰“å°æ ¼å¼ï¼ŒåŒ…å«ç±»åˆ«åç§°ã€å·²è§ç›®æ ‡æ•°é‡ã€æ¯ä¸ªç±»åˆ«çš„ç›®æ ‡æ•°é‡å’Œå…¶ä»–æŒ‡æ ‡çš„æ ¼å¼ã€‚

    LOGGER.info(pf % ("all", seen, nt.sum(), *metrics.mean_results()))
    # æ‰“å°æ‰€æœ‰ç±»åˆ«çš„ç»“æœï¼ŒåŒ…æ‹¬æ€»ç›®æ ‡æ•°é‡å’Œå¹³å‡ç»“æœã€‚

    if nt.sum() == 0:
        LOGGER.warning(f"WARNING âš ï¸ no labels found in {task} set, can not compute metrics without labels")
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ ‡ç­¾ï¼Œå‘å‡ºè­¦å‘Šï¼Œæ— æ³•è®¡ç®—æŒ‡æ ‡ã€‚

    # Print results per class
    if (verbose or (nc < 50 and not training)) and nc > 1 and len(stats):
        for i, c in enumerate(metrics.ap_class_index):
            LOGGER.info(pf % (names[c], seen, nt[c], *metrics.class_result(i)))
            # å¦‚æœéœ€è¦è¯¦ç»†è¾“å‡ºæˆ–ç±»åˆ«æ•°å°äº 50 ä¸”ä¸æ˜¯è®­ç»ƒçŠ¶æ€ï¼Œåˆ™æ‰“å°æ¯ä¸ªç±»åˆ«çš„ç»“æœã€‚

    # Print speeds
    t = tuple(x.t / seen * 1e3 for x in dt)  # speeds per image
    # è®¡ç®—æ¯å¼ å›¾ç‰‡çš„å¤„ç†é€Ÿåº¦ï¼Œå°†æ—¶é—´è½¬æ¢ä¸ºæ¯«ç§’ã€‚

    if not training:
        shape = (batch_size, 3, imgsz, imgsz)
        LOGGER.info(f"Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {shape}" % t)
        # å¦‚æœä¸æ˜¯è®­ç»ƒçŠ¶æ€ï¼Œæ‰“å°æ¯å¼ å›¾ç‰‡çš„é¢„å¤„ç†ã€æ¨ç†å’Œ NMS çš„é€Ÿåº¦ã€‚

    # Plots
    if plots:
        confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))
        # å¦‚æœéœ€è¦ç»˜åˆ¶å›¾è¡¨ï¼Œç»˜åˆ¶æ··æ·†çŸ©é˜µå¹¶ä¿å­˜åˆ°æŒ‡å®šç›®å½•ã€‚

    mp_bbox, mr_bbox, map50_bbox, map_bbox, mp_mask, mr_mask, map50_mask, map_mask = metrics.mean_results()
    # ä» metrics ä¸­è·å–å¹³å‡ç»“æœï¼ŒåŒ…æ‹¬å¤šç§æŒ‡æ ‡ã€‚

    # Save JSON
    if save_json and len(jdict):
        w = Path(weights[0] if isinstance(weights, list) else weights).stem if weights is not None else ""  # weights
        # å¦‚æœéœ€è¦ä¿å­˜ JSON ä¸” jdict ä¸ä¸ºç©ºï¼Œè·å–æƒé‡çš„æ–‡ä»¶åã€‚

        anno_json = str(Path("../datasets/coco/annotations/instances_val2017.json"))  # annotations
        # å®šä¹‰ COCO æ•°æ®é›†çš„æ³¨é‡Šæ–‡ä»¶è·¯å¾„ã€‚

        pred_json = str(save_dir / f"{w}_predictions.json")  # predictions
        # å®šä¹‰é¢„æµ‹ç»“æœçš„ JSON æ–‡ä»¶è·¯å¾„ã€‚

        LOGGER.info(f"\nEvaluating pycocotools mAP... saving {pred_json}...")
        # æ‰“å°ä¿¡æ¯ï¼Œè¡¨ç¤ºæ­£åœ¨è¯„ä¼° pycocotools çš„ mAPï¼Œå¹¶ä¿å­˜é¢„æµ‹ç»“æœã€‚

        with open(pred_json, "w") as f:
            json.dump(jdict, f)
            # å°† jdict ä¿å­˜ä¸º JSON æ–‡ä»¶ã€‚

        try:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb
            from pycocotools.coco import COCO
            from pycocotools.cocoeval import COCOeval
            # å¯¼å…¥ COCO API çš„ç›¸å…³æ¨¡å—ã€‚

            anno = COCO(anno_json)  # init annotations api
            # åˆå§‹åŒ– COCO æ³¨é‡Š APIã€‚

            pred = anno.loadRes(pred_json)  # init predictions api
            # åŠ è½½é¢„æµ‹ç»“æœï¼Œåˆå§‹åŒ–é¢„æµ‹ APIã€‚

            results = []
            for eval in COCOeval(anno, pred, "bbox"), COCOeval(anno, pred, "segm"):
                if is_coco:
                    eval.params.imgIds = [int(Path(x).stem) for x in dataloader.dataset.im_files]  # img ID to evaluate
                    # å¦‚æœæ˜¯ COCO æ•°æ®é›†ï¼Œè®¾ç½®è¦è¯„ä¼°çš„å›¾åƒ IDã€‚

                eval.evaluate()
                eval.accumulate()
                eval.summarize()
                results.extend(eval.stats[:2])  # update results (mAP@0.5:0.95, mAP@0.5)
                # è¯„ä¼°ã€ç´¯ç§¯å¹¶æ€»ç»“ç»“æœï¼Œå°† mAP ç›¸å…³çš„ç»Ÿè®¡ä¿¡æ¯æ·»åŠ åˆ°ç»“æœåˆ—è¡¨ä¸­ã€‚

            map_bbox, map50_bbox, map_mask, map50_mask = results
            # ä»ç»“æœä¸­è·å–ä¸åŒçš„ mAP æŒ‡æ ‡ã€‚

        except Exception as e:
            LOGGER.info(f"pycocotools unable to run: {e}")
            # æ•æ‰å¼‚å¸¸ï¼Œå¦‚æœ pycocotools æ— æ³•è¿è¡Œï¼Œè®°å½•é”™è¯¯ä¿¡æ¯ã€‚

    # Return results
    model.float()  # for training
    # å°†æ¨¡å‹è½¬æ¢ä¸ºæµ®ç‚¹æ¨¡å¼ï¼Œä»¥å¤‡è®­ç»ƒä½¿ç”¨ã€‚

    if not training:
        s = f"\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}" if save_txt else ""
        LOGGER.info(f"Results saved to {colorstr('bold', save_dir)}{s}")
        # å¦‚æœä¸æ˜¯è®­ç»ƒçŠ¶æ€ï¼Œè®°å½•ä¿å­˜çš„æ ‡ç­¾æ•°é‡å’Œä¿å­˜ç›®å½•ã€‚

    final_metric = mp_bbox, mr_bbox, map50_bbox, map_bbox, mp_mask, mr_mask, map50_mask, map_mask
    # æœ€ç»ˆçš„æŒ‡æ ‡ç»“æœï¼ŒåŒ…æ‹¬å¤šç§æŒ‡æ ‡ã€‚

    return (*final_metric, *(loss.cpu() / len(dataloader)).tolist()), metrics.get_maps(nc), t
    # è¿”å›æœ€ç»ˆæŒ‡æ ‡ã€æ¯ä¸ªç±»åˆ«çš„ mAP å’Œå¤„ç†é€Ÿåº¦ã€‚


def parse_opt():
    """Parses command line arguments for configuring YOLOv5 options like dataset path, weights, batch size, and
    inference settings.
    """
    # è§£æå‘½ä»¤è¡Œå‚æ•°ä»¥é…ç½® YOLOv5 é€‰é¡¹ï¼Œä¾‹å¦‚æ•°æ®é›†è·¯å¾„ã€æƒé‡ã€æ‰¹å¤§å°å’Œæ¨ç†è®¾ç½®ã€‚
    
    parser = argparse.ArgumentParser()
    # åˆ›å»ºä¸€ä¸ªå‚æ•°è§£æå™¨å¯¹è±¡ã€‚

    parser.add_argument("--data", type=str, default=ROOT / "data/coco128-seg.yaml", help="dataset.yaml path")
    # æ·»åŠ æ•°æ®é›† YAML æ–‡ä»¶è·¯å¾„å‚æ•°ï¼Œé»˜è®¤ä¸º coco128-seg.yamlã€‚

    parser.add_argument("--weights", nargs="+", type=str, default=ROOT / "yolov5s-seg.pt", help="model path(s)")
    # æ·»åŠ æƒé‡æ–‡ä»¶è·¯å¾„å‚æ•°ï¼Œæ”¯æŒå¤šä¸ªæƒé‡æ–‡ä»¶ï¼Œé»˜è®¤ä¸º yolov5s-seg.ptã€‚

    parser.add_argument("--batch-size", type=int, default=32, help="batch size")
    # æ·»åŠ æ‰¹å¤§å°å‚æ•°ï¼Œé»˜è®¤ä¸º 32ã€‚

    parser.add_argument("--imgsz", "--img", "--img-size", type=int, default=640, help="inference size (pixels)")
    # æ·»åŠ å›¾åƒå¤§å°å‚æ•°ï¼Œé»˜è®¤ä¸º 640 åƒç´ ã€‚

    parser.add_argument("--conf-thres", type=float, default=0.001, help="confidence threshold")
    # æ·»åŠ ç½®ä¿¡åº¦é˜ˆå€¼å‚æ•°ï¼Œé»˜è®¤ä¸º 0.001ã€‚

    parser.add_argument("--iou-thres", type=float, default=0.6, help="NMS IoU threshold")
    # æ·»åŠ  NMS çš„ IoU é˜ˆå€¼å‚æ•°ï¼Œé»˜è®¤ä¸º 0.6ã€‚

    parser.add_argument("--max-det", type=int, default=300, help="maximum detections per image")
    # æ·»åŠ æ¯å¼ å›¾åƒæœ€å¤§æ£€æµ‹æ•°å‚æ•°ï¼Œé»˜è®¤ä¸º 300ã€‚

    parser.add_argument("--task", default="val", help="train, val, test, speed or study")
    # æ·»åŠ ä»»åŠ¡ç±»å‹å‚æ•°ï¼Œé»˜è®¤ä¸ºéªŒè¯ï¼ˆvalï¼‰ã€‚

    parser.add_argument("--device", default="", help="cuda device, i.e. 0 or 0,1,2,3 or cpu")
    # æ·»åŠ è®¾å¤‡å‚æ•°ï¼ŒæŒ‡å®šä½¿ç”¨çš„ CUDA è®¾å¤‡æˆ– CPUã€‚

    parser.add_argument("--workers", type=int, default=8, help="max dataloader workers (per RANK in DDP mode)")
    # æ·»åŠ æ•°æ®åŠ è½½å™¨çš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°å‚æ•°ï¼Œé»˜è®¤ä¸º 8ã€‚

    parser.add_argument("--single-cls", action="store_true", help="treat as single-class dataset")
    # æ·»åŠ å•ç±»æ•°æ®é›†å‚æ•°ï¼Œè‹¥è®¾ç½®åˆ™å°†æ•°æ®é›†è§†ä¸ºå•ç±»ã€‚

    parser.add_argument("--augment", action="store_true", help="augmented inference")
    # æ·»åŠ å¢å¼ºæ¨ç†å‚æ•°ï¼Œè‹¥è®¾ç½®åˆ™ä½¿ç”¨å¢å¼ºæ¨ç†ã€‚

    parser.add_argument("--verbose", action="store_true", help="report mAP by class")
    # æ·»åŠ è¯¦ç»†è¾“å‡ºå‚æ•°ï¼Œè‹¥è®¾ç½®åˆ™æŒ‰ç±»åˆ«æŠ¥å‘Š mAPã€‚

    parser.add_argument("--save-txt", action="store_true", help="save results to *.txt")
    # æ·»åŠ ä¿å­˜ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶çš„å‚æ•°ï¼Œè‹¥è®¾ç½®åˆ™ä¿å­˜ç»“æœã€‚

    parser.add_argument("--save-hybrid", action="store_true", help="save label+prediction hybrid results to *.txt")
    # æ·»åŠ ä¿å­˜æ··åˆç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶çš„å‚æ•°ï¼Œè‹¥è®¾ç½®åˆ™ä¿å­˜æ ‡ç­¾å’Œé¢„æµ‹çš„æ··åˆç»“æœã€‚

    parser.add_argument("--save-conf", action="store_true", help="save confidences in --save-txt labels")
    # æ·»åŠ ä¿å­˜ç½®ä¿¡åº¦åˆ°æ–‡æœ¬æ–‡ä»¶çš„å‚æ•°ï¼Œè‹¥è®¾ç½®åˆ™åœ¨ä¿å­˜çš„æ ‡ç­¾ä¸­åŒ…å«ç½®ä¿¡åº¦ã€‚

    parser.add_argument("--save-json", action="store_true", help="save a COCO-JSON results file")
    # æ·»åŠ ä¿å­˜ COCO-JSON ç»“æœæ–‡ä»¶çš„å‚æ•°ï¼Œè‹¥è®¾ç½®åˆ™ä¿å­˜ä¸º JSON æ ¼å¼ã€‚

    parser.add_argument("--project", default=ROOT / "runs/val-seg", help="save results to project/name")
    # æ·»åŠ é¡¹ç›®ä¿å­˜è·¯å¾„å‚æ•°ï¼Œé»˜è®¤ä¸º runs/val-segã€‚

    parser.add_argument("--name", default="exp", help="save to project/name")
    # æ·»åŠ é¡¹ç›®åç§°å‚æ•°ï¼Œé»˜è®¤ä¸º "exp"ã€‚

    parser.add_argument("--exist-ok", action="store_true", help="existing project/name ok, do not increment")
    # æ·»åŠ å…è®¸å­˜åœ¨çš„é¡¹ç›®åç§°å‚æ•°ï¼Œè‹¥è®¾ç½®åˆ™ä¸é€’å¢é¡¹ç›®åç§°ã€‚

    parser.add_argument("--half", action="store_true", help="use FP16 half-precision inference")
    # æ·»åŠ ä½¿ç”¨ FP16 åŠç²¾åº¦æ¨ç†çš„å‚æ•°ï¼Œè‹¥è®¾ç½®åˆ™å¯ç”¨åŠç²¾åº¦ã€‚

    parser.add_argument("--dnn", action="store_true", help="use OpenCV DNN for ONNX inference")
    # æ·»åŠ ä½¿ç”¨ OpenCV DNN è¿›è¡Œ ONNX æ¨ç†çš„å‚æ•°ï¼Œè‹¥è®¾ç½®åˆ™å¯ç”¨ DNNã€‚

    opt = parser.parse_args()
    # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶å°†ç»“æœå­˜å‚¨åœ¨ opt ä¸­ã€‚

    opt.data = check_yaml(opt.data)  # check YAML
    # æ£€æŸ¥æ•°æ®é›† YAML æ–‡ä»¶çš„æœ‰æ•ˆæ€§ã€‚

    # opt.save_json |= opt.data.endswith('coco.yaml')
    # å¦‚æœæ•°æ®é›† YAML æ–‡ä»¶ä»¥ coco.yaml ç»“å°¾ï¼Œåˆ™è®¾ç½®ä¿å­˜ JSON çš„å‚æ•°ã€‚

    opt.save_txt |= opt.save_hybrid
    # å¦‚æœè®¾ç½®äº†ä¿å­˜æ··åˆç»“æœï¼Œåˆ™ä¹Ÿè®¾ç½®ä¿å­˜æ–‡æœ¬æ–‡ä»¶çš„å‚æ•°ã€‚

    print_args(vars(opt))
    # æ‰“å°è§£æåçš„å‚æ•°ã€‚

    return opt
    # è¿”å›è§£æåçš„å‚æ•°å¯¹è±¡ã€‚


def main(opt):
    """Executes YOLOv5 tasks including training, validation, testing, speed, and study with configurable options."""
    # æ‰§è¡Œ YOLOv5 ä»»åŠ¡ï¼ŒåŒ…æ‹¬è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•ã€é€Ÿåº¦è¯„ä¼°å’Œç ”ç©¶ï¼Œä½¿ç”¨å¯é…ç½®é€‰é¡¹ã€‚

    check_requirements(ROOT / "requirements.txt", exclude=("tensorboard", "thop"))
    # æ£€æŸ¥æ‰€éœ€çš„ Python åŒ…æ˜¯å¦å·²å®‰è£…ï¼Œæ’é™¤ tensorboard å’Œ thopã€‚

    if opt.task in ("train", "val", "test"):  # run normally
        # å¦‚æœä»»åŠ¡æ˜¯è®­ç»ƒã€éªŒè¯æˆ–æµ‹è¯•ï¼Œåˆ™æ­£å¸¸è¿è¡Œã€‚
        
        if opt.conf_thres > 0.001:  # https://github.com/ultralytics/yolov5/issues/1466
            LOGGER.warning(f"WARNING âš ï¸ confidence threshold {opt.conf_thres} > 0.001 produces invalid results")
            # å¦‚æœç½®ä¿¡åº¦é˜ˆå€¼å¤§äº 0.001ï¼Œåˆ™å‘å‡ºè­¦å‘Šï¼Œå¯èƒ½å¯¼è‡´æ— æ•ˆç»“æœã€‚

        if opt.save_hybrid:
            LOGGER.warning("WARNING âš ï¸ --save-hybrid returns high mAP from hybrid labels, not from predictions alone")
            # å¦‚æœè®¾ç½®äº†ä¿å­˜æ··åˆç»“æœï¼Œåˆ™å‘å‡ºè­¦å‘Šï¼Œè¯´æ˜é«˜ mAP æ˜¯ç”±äºæ··åˆæ ‡ç­¾ï¼Œè€Œéä»…ä»…æ˜¯é¢„æµ‹ç»“æœã€‚

        run(**vars(opt))
        # è°ƒç”¨ run å‡½æ•°æ‰§è¡Œä»»åŠ¡ï¼Œä¼ å…¥æ‰€æœ‰å‚æ•°ã€‚

    else:
        weights = opt.weights if isinstance(opt.weights, list) else [opt.weights]
        # å¦‚æœæƒé‡æ˜¯åˆ—è¡¨åˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™å°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨ã€‚

        opt.half = torch.cuda.is_available() and opt.device != "cpu"  # FP16 for fastest results
        # å¦‚æœå¯ç”¨ CUDA ä¸”è®¾å¤‡ä¸æ˜¯ CPUï¼Œåˆ™è®¾ç½®ä½¿ç”¨ FP16 ä»¥è·å¾—æœ€å¿«çš„ç»“æœã€‚

        if opt.task == "speed":  # speed benchmarks
            # å¦‚æœä»»åŠ¡æ˜¯é€Ÿåº¦è¯„ä¼°ï¼Œåˆ™è®¾ç½®ç›¸å…³å‚æ•°ã€‚
            # python val.py --task speed --data coco.yaml --batch 1 --weights yolov5n.pt yolov5s.pt...
            opt.conf_thres, opt.iou_thres, opt.save_json = 0.25, 0.45, False
            # è®¾ç½®ç½®ä¿¡åº¦é˜ˆå€¼ã€IoU é˜ˆå€¼å’Œä¿å­˜ JSON çš„å‚æ•°ã€‚

            for opt.weights in weights:
                run(**vars(opt), plots=False)
                # å¯¹æ¯ä¸ªæƒé‡è°ƒç”¨ run å‡½æ•°ï¼Œæ‰§è¡Œé€Ÿåº¦è¯„ä¼°ï¼Œä¸ç»˜åˆ¶å›¾è¡¨ã€‚

        elif opt.task == "study":  # speed vs mAP benchmarks
            # å¦‚æœä»»åŠ¡æ˜¯ç ”ç©¶é€Ÿåº¦ä¸ mAP çš„å…³ç³»ï¼Œåˆ™æ‰§è¡Œç›¸å…³æ“ä½œã€‚
            # python val.py --task study --data coco.yaml --iou 0.7 --weights yolov5n.pt yolov5s.pt...
            for opt.weights in weights:
                f = f"study_{Path(opt.data).stem}_{Path(opt.weights).stem}.txt"  # filename to save to
                # ç”Ÿæˆä¿å­˜ç»“æœçš„æ–‡ä»¶åã€‚

                x, y = list(range(256, 1536 + 128, 128)), []  # x axis (image sizes), y axis
                # å®šä¹‰ x è½´ï¼ˆå›¾åƒå°ºå¯¸ï¼‰å’Œ y è½´ï¼ˆç»“æœï¼‰ã€‚

                for opt.imgsz in x:  # img-size
                    # éå†ä¸åŒçš„å›¾åƒå°ºå¯¸ã€‚
                    LOGGER.info(f"\nRunning {f} --imgsz {opt.imgsz}...")
                    # æ‰“å°å½“å‰è¿è¡Œçš„ä¿¡æ¯ã€‚

                    r, _, t = run(**vars(opt), plots=False)
                    # è°ƒç”¨ run å‡½æ•°è¿è¡Œï¼Œå¹¶è·å–ç»“æœå’Œæ—¶é—´ã€‚

                    y.append(r + t)  # results and times
                    # å°†ç»“æœå’Œæ—¶é—´æ·»åŠ åˆ° y è½´æ•°æ®ä¸­ã€‚

                np.savetxt(f, y, fmt="%10.4g")  # save
                # å°†ç»“æœä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶ä¸­ã€‚

            subprocess.run(["zip", "-r", "study.zip", "study_*.txt"])
            # å°†æ‰€æœ‰ç ”ç©¶ç»“æœæ–‡ä»¶å‹ç¼©ä¸º study.zipã€‚

            plot_val_study(x=x)  # plot
            # ç»˜åˆ¶ç ”ç©¶ç»“æœçš„å›¾è¡¨ã€‚

        else:
            raise NotImplementedError(f'--task {opt.task} not in ("train", "val", "test", "speed", "study")')
            # å¦‚æœä»»åŠ¡ä¸åœ¨å·²å®ç°çš„èŒƒå›´å†…ï¼Œåˆ™æŠ›å‡ºæœªå®ç°çš„å¼‚å¸¸ã€‚


if __name__ == "__main__":
    opt = parse_opt()
    # å¦‚æœè¯¥è„šæœ¬æ˜¯ä¸»ç¨‹åºï¼Œåˆ™è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚

    main(opt)
    # è°ƒç”¨ main å‡½æ•°æ‰§è¡Œä»»åŠ¡ã€‚