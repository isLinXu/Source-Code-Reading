# yolof

Directory tree
- tools
  - test_net.py
  - benchmark.py
  - visualize_data.py
  - caffe2_converter.py
  - plain_train_net.py
  - rm_files.py
  - visualize_json_results.py
  - dev
    - run_instant_tests.sh
    - linter.sh
    - parse_results.sh
    - run_inference_tests.sh
  - train_net.py
  - debug_net.py
- docker
- cvpods
  - checkpoint
    - catalog.py
    - checkpoint.py
    - c2_model_loading.py
    - __init__.py
    - utils.py
  - layers
    - deform_conv.py
    - activation_funcs.py
    - csrc
      - cocoeval
        - cocoeval.h
        - cocoeval.cpp
      - PSROIPool
        - psroi_pool_kernel.cu
        - psroi_pool_cuda.h
      - border_align
        - border_align.h
        - border_align_kernel.cu
      - cuda_version.cu
      - ROIAlignRotated
        - ROIAlignRotated_cuda.cu
        - ROIAlignRotated_cpu.cpp
        - ROIAlignRotated.h
      - sigmoid_focal_loss
        - SigmoidFocalLoss.h
        - SigmoidFocalLoss_cuda.cu
      - SwapAlign2Nat
        - SwapAlign2Nat_cuda.cu
        - SwapAlign2Nat.h
      - ml_nms
        - ml_nms.h
        - ml_nms.cu
      - tree_filter
        - boruvka.cpp
        - mst.hpp
        - refine.cu
        - rst.hpp
        - boruvka_rst.cpp
        - refine.hpp
        - bfs.hpp
        - boruvka_rst.hpp
        - rst.cu
        - boruvka.hpp
        - mst.cu
        - bfs.cu
      - box_iou_rotated
        - box_iou_rotated.h
        - box_iou_rotated_utils.h
        - box_iou_rotated_cpu.cpp
        - box_iou_rotated_cuda.cu
      - ROIAlign
        - ROIAlign.h
        - ROIAlign_cpu.cpp
        - ROIAlign_cuda.cu
      - deformable
        - deform_conv_cuda_kernel.cu
        - deform_conv_cuda.cu
        - deform_conv.h
      - vision.cpp
      - nms_rotated
        - nms_rotated_cuda.cu
        - nms_rotated_cpu.cpp
        - nms_rotated.h
    - shape_spec.py
    - psroi_pool.py
    - swap_align2nat.py
    - roi_align.py
    - roi_align_rotated.py
    - position_encoding.py
    - nms.py
    - __init__.py
    - mask_ops.py
    - wrappers.py
    - tree_filter_core.py
    - larc.py
    - deform_conv_with_off.py
    - batch_norm.py
    - tree_filter_v2.py
    - rotated_boxes.py
    - border_align.py
  - structures
    - instances.py
    - __init__.py
    - boxes.py
    - keypoints.py
    - masks.py
    - image_list.py
    - rotated_boxes.py
  - __init__.py
  - utils
    - benchmark
      - benchmark.py
      - timer.py
      - __init__.py
    - metrics
      - __init__.py
      - accuracy.py
    - dump
      - events.py
      - __init__.py
      - logger.py
      - history_buffer.py
    - memory.py
    - file
      - serialize.py
      - download.py
      - __init__.py
      - file_io.py
    - distributed
      - comm.py
      - __init__.py
    - registry.py
    - __init__.py
    - imports.py
    - env
      - env.py
      - collect_env.py
      - __init__.py
    - apex_wrapper.py
    - visualizer
      - colormap.py
      - show.py
      - video_visualizer.py
      - __init__.py
      - visualizer.py
  - solver
    - optimizer_builder.py
    - build.py
    - lr_scheduler.py
    - scheduler_builder.py
    - __init__.py
  - modeling
    - losses
      - dice_loss.py
      - circle_loss.py
      - __init__.py
      - sigmoid_focal_loss.py
      - focal_loss.py
      - reg_l1_loss.py
      - label_smooth_ce_loss.py
      - iou_loss.py
      - smooth_l1_loss.py
    - test_time_augmentation.py
    - poolers.py
    - matcher.py
    - nn_utils
      - flop_count.py
      - __init__.py
      - activation_count.py
      - module_converter.py
      - precise_bn.py
      - feature_utils.py
      - jit_handles.py
      - scale_grad.py
      - weight_init.py
      - parameter_count.py
    - __init__.py
    - box_regression.py
    - anchor_generator.py
    - basenet
      - __init__.py
      - basenet.py
    - sampling.py
    - meta_arch
      - rcnn.py
      - panoptic_fpn.py
      - detr.py
      - yolov3.py
      - tensormask.py
      - fcn.py
      - pointrend.py
      - efficientdet.py
      - ssd.py
      - centernet.py
      - imagenet.py
      - solo.py
      - __init__.py
      - dynamic4seg.py
      - reppoints.py
      - fcos.py
      - solo_decoupled.py
      - auto_assign.py
      - borderdet.py
      - retinanet.py
      - moco.py
      - semantic_seg.py
      - free_anchor.py
      - atss.py
    - postprocessing.py
    - proposal_generator
      - rpn.py
      - rrpn.py
      - rrpn_outputs.py
      - __init__.py
      - rpn_outputs.py
      - proposal_utils.py
    - roi_heads
      - mask_head.py
      - fast_rcnn.py
      - box_head.py
      - keypoint_head.py
      - __init__.py
      - rotated_fast_rcnn.py
      - cascade_rcnn.py
      - roi_heads.py
    - backbone
      - bifpn.py
      - fpn.py
      - vgg.py
      - dynamic_arch
        - cal_op_flops.py
        - op_with_flops.py
        - __init__.py
        - dynamic_backbone.py
        - dynamic_cell.py
      - backbone.py
      - splat.py
      - __init__.py
      - snet.py
      - efficientnet.py
      - darknet.py
      - mobilenet.py
      - shufflenet.py
      - resnet.py
      - transformer.py
  - analyser
    - module_profiler.py
    - tide
      - functions.py
      - plotting.py
      - datasets.py
      - __init__.py
      - ap.py
      - errors
        - error.py
        - qualifiers.py
        - main_errors.py
      - data.py
      - quantify.py
  - configs
    - pointrend_config.py
    - config_helper.py
    - retinanet_config.py
    - base_config.py
    - dynamic_routing_config.py
    - panoptic_seg_config.py
    - rcnn_fpn_config.py
    - __init__.py
    - keypoint_config.py
    - base_classification_config.py
    - efficientdet_config.py
    - segm_config.py
    - fcos_config.py
    - solo_config.py
    - rcnn_config.py
    - yolo_config.py
    - ssd_config.py
    - base_detection_config.py
  - evaluation
    - eval_MR_multisetup.py
    - citypersons_evaluation.py
    - build.py
    - crowdhumantools.py
    - fast_eval_api.py
    - panoptic_evaluation.py
    - registry.py
    - classification_evaluation.py
    - __init__.py
    - cityscapes_evaluation.py
    - coco_evaluation.py
    - crowdhuman_evaluation.py
    - widerface_evaluation.py
    - sem_seg_evaluation.py
    - widerfacetools.py
    - pascal_voc_evaluation.py
    - lvis_evaluation.py
    - testing.py
    - evaluator.py
    - rotated_coco_evaluation.py
  - data
    - build.py
    - base_dataset.py
    - wrapped_dataset.py
    - registry.py
    - datasets
      - coco.py
      - cityscapes.py
      - paths_route.py
      - objects365.py
      - imagenet.py
      - __init__.py
      - lvis.py
      - torchvision_datasets.py
      - citypersons.py
      - crowdhuman.py
      - voc.py
      - widerface.py
      - lvis_categories.py
      - objects365_categories.py
      - builtin_meta.py
    - __init__.py
    - detection_utils.py
    - transforms
      - transform_util.py
      - auto_aug.py
      - __init__.py
      - transform_gen.py
      - transform.py
    - samplers
      - __init__.py
      - distributed_sampler.py
  - engine
    - predictor.py
    - runner.py
    - hooks.py
    - base_runner.py
    - __init__.py
    - launch.py
- playground
  - detection
    - coco
      - yolof
        - yolof.res50.DC5.1x
          - config.py
          - net.py
        - yolof.res101.C5.1x
          - config.py
          - net.py
        - yolof.res50.C5.1x
          - config.py
          - net.py
        - yolof.cspdarknet53.DC5.9x.stage2.3x
          - config.py
          - net.py
          - cspdarknet.py
        - yolof_base
          - decoder.py
          - box_ops.py
          - uniform_matcher.py
          - __init__.py
          - encoder.py
          - yolof.py
          - utils.py
        - yolof.res101.DC5.1x
          - config.py
          - net.py
        - yolof.cspdarknet53.DC5.3x
          - config.py
          - net.py
          - cspdarknet.py
        - yolof.X101.64x4d.C5.1x
          - config.py
          - net.py
        - yolof.cspdarknet53.DC5.9x
          - config.py
          - net.py
          - cspdarknet.py
- images
  - yolof.png
- datasets
- tests
  - checkpoint
    - test_checkpoint.py
  - layers
    - test_nms.py
    - test_roi_align.py
    - test_mask_ops.py
    - test_roi_align_rotated.py
    - test_nms_rotated.py
  - structures
    - test_imagelist.py
    - test_masks.py
    - test_rotated_boxes.py
    - test_boxes.py
    - test_instances.py
  - __init__.py
  - utils
    - test_events.py
  - modeling
    - test_matcher.py
    - test_roi_pooler.py
    - test_anchor_generator.py
    - test_roi_heads.py
    - test_rpn.py
    - test_fast_rcnn.py
    - test_box2box_transform.py
  - configs
    - test_config.py
  - test_psroi_pool.py
  - data
    - test_coco.py
    - test_rotation_transform.py
    - test_transforms.py
    - test_coco_evaluation.py
  - engine
    - test_engine.py
- docs
  - index.rst
  - conf.py
  - _static
    - css
      - custom.css
  - modules
    - checkpoint.rst
    - evaluation.rst
    - index.rst
    - solver.rst
    - layers.rst
    - engine.rst
    - modeling.rst
    - configs.rst
    - data.rst
    - structures.rst
    - utils.rst
- README_cvpods.md
- CONTRIBUTING.md
- pylint.cfg
- setup.cfg

---
<!-- TOC -->
# pylint.cfg

```
[MASTER]

# A comma-separated list of package or module names from where C extensions may
# be loaded. Extensions are loading into the active Python interpreter and may
# run arbitrary code.
extension-pkg-whitelist=

# Specify a score threshold to be exceeded before program exits with error.
fail-under=100

# Add files or directories to the blacklist. They should be base names, not
# paths.
ignore=
    fpn.py,
    jit_handles.py,
    anchor_generator.py,
    classification_evaluation.py,
    caffe2_modeling.py,
    # RCNN related
    proposal_generator,
    roi_heads,
    # no need to fix
    transform.py,
    transform_gen.py,
    batch_norm.py,
    basenet.py,
    resnet.py,
    dynamic_arch,

# Add files or directories matching the regex patterns to the blacklist. The
# regex matches against base names, not paths.
ignore-patterns=

# Use multiple processes to speed up Pylint. Specifying 0 will auto-detect the
# number of processors available to use.
jobs=0


[MESSAGES CONTROL]

# Only show warnings with the listed confidence levels. Leave empty to show
# all. Valid levels: HIGH, INFERENCE, INFERENCE_FAILURE, UNDEFINED.
confidence=

# Disable the message, report, category or checker with the given id(s). You
# can either give multiple identifiers separated by comma (,) or put this
# option multiple times (only on the command line, not in the configuration
# file where it should appear only once). You can also use "--disable=all" to
# disable everything first and then reenable specific checks. For example, if
# you want to run only the similarities checker, you can use "--disable=all
# --enable=similarities". If you want to run only the classes checker, but have
# no Warning level messages displayed, use "--disable=all --enable=classes
# --disable=W".
disable=all

# Enable the message, report, category or checker with the given id(s). You can
# either give multiple identifier separated by comma (,) or put this option
# multiple time (only on the command line, not in the configuration file where
# it should appear only once). See also the "--disable" option for examples.
enable=unused-argument


[VARIABLES]

# List of additional names supposed to be defined in builtins. Remember that
# you should avoid defining new builtins when possible.
additional-builtins=

# Tells whether unused global variables should be treated as a violation.
allow-global-unused-variables=yes

# List of strings which can identify a callback function by name. A callback
# name must start or end with one of those strings.
callbacks=cb_,
          _cb

# A regular expression matching the name of dummy variables (i.e. expected to
# not be used).
dummy-variables-rgx=_+$|(_[a-zA-Z0-9_]*[a-zA-Z0-9]+?$)|dummy|^ignored_|^unused_

# Argument names that match this expression will be ignored. Default to name
# with leading underscore.
# ignored-argument-names=_.*|^ignored_|^unused_
# ignored-argument-names=.*_unused$


# Tells whether we should check for unused import in __init__ files.
init-import=no

# List of qualified module names which can have objects that can redefine
# builtins.
redefining-builtins-modules=six.moves,past.builtins,future.builtins,builtins,io
```

# setup.cfg

```
[isort]
line_length = 100
multi_line_output = 3
balanced_wrapping = True
skip = tools/test_net.py, tools/train_net.py
known_standard_library = setuptools,mock
known_myself = cvpods
known_third_party = appdirs,colorama,easydict,portalocker,yacs,termcolor,tabulate,tqdm,psutil,pkg_resources
known_data_processing = cv2,numpy,scipy,PIL,matplotlib
known_datasets = pycocotools,cityscapesscripts,lvis
known_deeplearning = torch,torchvision,caffe2,onnx
sections = FUTURE,STDLIB,THIRDPARTY,data_processing,datasets,deeplearning,myself,FIRSTPARTY,LOCALFOLDER
no_lines_before=STDLIB,THIRDPARTY,datasets
default_section = FIRSTPARTY

[flake8]
ignore = W503, E221
max-line-length = 100
max-complexity = 18
select = B,C,E,F,W,T4,B9
exclude = build,__init__.py

[pep8]
ignore = W503, E203, E221, E402, E741, C901, W504, E731, F541, E722
max-line-length = 100 

[yapf]
based_on_style = pep8
spaces_before_comment = 4
split_before_logical_operator = true

[mypy]
python_version=3.6
ignore_missing_imports = True
warn_unused_configs = True
disallow_untyped_defs = True
check_untyped_defs = True
warn_unused_ignores = True
warn_redundant_casts = True
show_column_numbers = True
follow_imports = silent
allow_redefinition = True
; Require all functions to be annotated
disallow_incomplete_defs = True
```

## tools/test_net.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

# pylint: disable=W0613

"""
Testing Script for cvpods.

This scripts reads a given config file and runs the training or evaluation.
It is an entry point that is made to train standard models in cvpods.

In order to let one script support training of many models,
this script contains logic that are specific to these built-in models and therefore
may not be suitable for your own project.
For example, your research project perhaps only needs a single "evaluator".

Therefore, we recommend you to use cvpods as an library and take
this file as an example of how to use the library.
You may want to write your own script with your datasets and other customizations.
"""
import glob
import logging
import os
import re
from collections import OrderedDict
from pprint import pformat

from torch.nn.parallel import DistributedDataParallel

from cvpods.checkpoint import DefaultCheckpointer
from cvpods.engine import RUNNERS, default_argument_parser, default_setup, launch
from cvpods.evaluation import build_evaluator, verify_results
from cvpods.modeling import GeneralizedRCNN, GeneralizedRCNNWithTTA, TTAWarper
from cvpods.utils import PathManager, comm

from config import config
from net import build_model


def runner_decrator(cls):
    """
    We use the "DefaultTrainer" which contains a number pre-defined logic for
    standard training workflow. They may not work for you, especially if you
    are working on a new research project. In that case you can use the cleaner
    "SimpleTrainer", or write your own training loop.
    """

    def custom_build_evaluator(cls, cfg, dataset_name, dataset, output_folder=None):
        """
        Create evaluator(s) for a given dataset.
        This uses the special metadata "evaluator_type" associated with each builtin dataset.
        For your own dataset, you can simply create an evaluator manually in your
        script and do not have to worry about the hacky if-else logic here.
        """
        dump_test = config.GLOBAL.DUMP_TEST
        return build_evaluator(cfg, dataset_name, dataset, output_folder, dump=dump_test)

    def custom_test_with_TTA(cls, cfg, model):
        logger = logging.getLogger("cvpods.runner")
        # In the end of training, run an evaluation with TTA
        # Only support some R-CNN models.
        logger.info("Running inference with test-time augmentation ...")

        module = model
        if isinstance(module, DistributedDataParallel):
            module = model.module
        if isinstance(module, GeneralizedRCNN):
            model = GeneralizedRCNNWithTTA(cfg, model)
        else:
            model = TTAWarper(cfg, model)
        res = cls.test(cfg, model, output_folder=os.path.join(cfg.OUTPUT_DIR, "inference_TTA"))
        res = OrderedDict({k + "_TTA": v for k, v in res.items()})
        return res

    cls.build_evaluator = classmethod(custom_build_evaluator)
    cls.test_with_TTA = classmethod(custom_test_with_TTA)

    return cls


def test_argument_parser():
    parser = default_argument_parser()
    parser.add_argument("--start-iter", type=int, default=None, help="start iter used to test")
    parser.add_argument("--end-iter", type=int, default=None, help="end iter used to test")
    parser.add_argument("--debug", action="store_true", help="use debug mode or not")
    return parser


def filter_by_iters(file_list, start_iter, end_iter):
    # sort file_list by modified time
    if file_list[0].startswith("s3://"):
        file_list.sort(key=lambda x: PathManager.stat(x).m_date)
    else:
        file_list.sort(key=os.path.getmtime)

    if start_iter is None:
        if end_iter is None:
            # use latest ckpt if start_iter and end_iter are not given
            return [file_list[-1]]
        else:
            start_iter = 0
    elif end_iter is None:
        end_iter = float("inf")

    iter_infos = [re.split(r"model_|\.pth", f)[-2] for f in file_list]
    keep_list = [0] * len(iter_infos)
    start_index = 0
    if "final" in iter_infos and iter_infos[-1] != "final":
        start_index = iter_infos.index("final")

    for i in range(len(iter_infos) - 1, start_index, -1):
        if iter_infos[i] == "final":
            if end_iter == float("inf"):
                keep_list[i] = 1
        elif float(start_iter) < float(iter_infos[i]) < float(end_iter):
            keep_list[i] = 1
            if float(iter_infos[i - 1]) > float(iter_infos[i]):
                break

    return [filename for keep, filename in zip(keep_list, file_list) if keep == 1]


def get_valid_files(args, cfg, logger):

    if "MODEL.WEIGHTS" in args.opts:
        model_weights = cfg.MODEL.WEIGHTS
        assert PathManager.exists(model_weights), "{} not exist!!!".format(model_weights)
        return [model_weights]

    file_list = glob.glob(os.path.join(cfg.OUTPUT_DIR, "model_*.pth"))
    if len(file_list) == 0:  # local file invalid, get it from oss
        model_prefix = cfg.OUTPUT_DIR.split("cvpods_playground")[-1][1:]
        remote_file_path = os.path.join(cfg.OSS.DUMP_PREFIX, model_prefix)
        logger.warning(
            "No checkpoint file was found locally, try to "
            f"load the corresponding dump file on OSS site: {remote_file_path}."
        )
        file_list = [
            str(filename) for filename in PathManager.ls(remote_file_path)
            if re.match(r"model_.+\.pth", filename.name) is not None
        ]
        assert len(file_list) != 0, "No valid file found on OSS"

    file_list = filter_by_iters(file_list, args.start_iter, args.end_iter)
    assert file_list, "No checkpoint valid in {}.".format(cfg.OUTPUT_DIR)
    logger.info("All files below will be tested in order:\n{}".format(pformat(file_list)))
    return file_list


def main(args):
    config.merge_from_list(args.opts)
    cfg, logger = default_setup(config, args)
    if args.debug:
        batches = int(cfg.SOLVER.IMS_PER_DEVICE * args.num_gpus)
        if cfg.SOLVER.IMS_PER_BATCH != batches:
            cfg.SOLVER.IMS_PER_BATCH = batches
            logger.warning("SOLVER.IMS_PER_BATCH is changed to {}".format(batches))

    valid_files = get_valid_files(args, cfg, logger)
    # * means all if need specific format then *.csv
    for current_file in valid_files:
        cfg.MODEL.WEIGHTS = current_file
        model = build_model(cfg)

        DefaultCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(
            cfg.MODEL.WEIGHTS, resume=args.resume
        )
        if cfg.TEST.AUG.ENABLED:
            res = runner_decrator(RUNNERS.get(cfg.TRAINER.NAME)).test_with_TTA(cfg, model)
        else:
            res = runner_decrator(RUNNERS.get(cfg.TRAINER.NAME)).test(cfg, model)

        if comm.is_main_process():
            verify_results(cfg, res)

    # return res


if __name__ == "__main__":
    args = test_argument_parser().parse_args()
    print("Command Line Args:", args)
    launch(
        main,
        args.num_gpus,
        num_machines=args.num_machines,
        machine_rank=args.machine_rank,
        dist_url=args.dist_url,
        args=(args,),
    )
```

## tools/benchmark.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
"""
A script to benchmark builtin models.

Note: this script has an extra dependency of psutil.
"""

import itertools
import logging
import psutil
import tqdm

import torch
from torch.nn.parallel import DistributedDataParallel

from cvpods.checkpoint import DefaultCheckpointer
from cvpods.config import get_cfg
from cvpods.data import DatasetFromList, build_test_loader, build_train_loader
from cvpods.engine import SimpleTrainer, default_argument_parser, hooks, launch
from cvpods.modeling import build_model
from cvpods.solver import build_optimizer
from cvpods.utils import CommonMetricPrinter, Timer, comm, setup_logger

logger = logging.getLogger("cvpods")


def setup(args):
    cfg = get_cfg()
    cfg.merge_from_file(args.config_file)
    cfg.SOLVER.BASE_LR = 0.001  # Avoid NaNs. Not useful in this script anyway.
    cfg.merge_from_list(args.opts)
    cfg.freeze()
    setup_logger(distributed_rank=comm.get_rank())
    return cfg


def benchmark_data(args):
    cfg = setup(args)

    dataloader = build_train_loader(cfg)

    timer = Timer()
    itr = iter(dataloader)
    for i in range(10):  # warmup
        next(itr)
        if i == 0:
            startup_time = timer.seconds()
    timer = Timer()
    max_iter = 1000
    for _ in tqdm.trange(max_iter):
        next(itr)
    logger.info(
        "{} iters ({} images) in {} seconds.".format(
            max_iter, max_iter * cfg.SOLVER.IMS_PER_BATCH, timer.seconds()
        )
    )
    logger.info("Startup time: {} seconds".format(startup_time))
    vram = psutil.virtual_memory()
    logger.info(
        "RAM Usage: {:.2f}/{:.2f} GB".format(
            (vram.total - vram.available) / 1024 ** 3, vram.total / 1024 ** 3
        )
    )


def benchmark_train(args):
    cfg = setup(args)
    model = build_model(cfg)
    logger.info("Model:\n{}".format(model))
    if comm.get_world_size() > 1:
        model = DistributedDataParallel(
            model, device_ids=[comm.get_local_rank()], broadcast_buffers=False
        )
    optimizer = build_optimizer(cfg, model)
    checkpointer = DefaultCheckpointer(model, optimizer=optimizer)
    checkpointer.load(cfg.MODEL.WEIGHTS)

    cfg.defrost()
    cfg.DATALOADER.NUM_WORKERS = 0
    data_loader = build_train_loader(cfg)
    dummy_data = list(itertools.islice(data_loader, 100))

    def f():
        while True:
            yield from DatasetFromList(dummy_data, copy=False)

    max_iter = 400
    trainer = SimpleTrainer(model, f(), optimizer)
    trainer.register_hooks(
        [hooks.IterationTimer(), hooks.PeriodicWriter([CommonMetricPrinter(max_iter)])]
    )
    trainer.train(1, max_iter)


@torch.no_grad()
def benchmark_eval(args):
    cfg = setup(args)
    model = build_model(cfg)
    model.eval()
    logger.info("Model:\n{}".format(model))
    DefaultCheckpointer(model).load(cfg.MODEL.WEIGHTS)

    cfg.defrost()
    cfg.DATALOADER.NUM_WORKERS = 0
    data_loader = build_test_loader(cfg, cfg.DATASETS.TEST[0])
    dummy_data = list(itertools.islice(data_loader, 100))

    def f():
        while True:
            yield from DatasetFromList(dummy_data, copy=False)

    for _ in range(5):  # warmup
        model(dummy_data[0])

    max_iter = 400
    timer = Timer()
    with tqdm.tqdm(total=max_iter) as pbar:
        for idx, d in enumerate(f()):
            if idx == max_iter:
                break
            model(d)
            pbar.update()
    logger.info("{} iters in {} seconds.".format(max_iter, timer.seconds()))


if __name__ == "__main__":
    parser = default_argument_parser()
    parser.add_argument("--task", choices=["train", "eval", "data"], required=True)
    args = parser.parse_args()
    assert not args.eval_only

    if args.task == "data":
        f = benchmark_data
    elif args.task == "train":
        """
        Note: training speed may not be representative.
        The training cost of a R-CNN model varies with the content of the data
        and the quality of the model.
        """
        f = benchmark_train
    elif args.task == "eval":
        f = benchmark_eval
        # only benchmark single-GPU inference.
        assert args.num_gpus == 1 and args.num_machines == 1
    launch(f, args.num_gpus, args.num_machines, args.machine_rank, args.dist_url, args=(args,))
```

## tools/visualize_data.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import argparse
import os
from itertools import chain
import tqdm

import cv2
import numpy as np
from PIL import Image

from cvpods.config import get_cfg
from cvpods.data import DatasetCatalog, MetadataCatalog, build_train_loader
from cvpods.data import detection_utils as utils
from cvpods.data.build import filter_images_with_few_keypoints
from cvpods.utils import Visualizer, setup_logger


def setup(args):
    cfg = get_cfg()
    if args.config_file:
        cfg.merge_from_file(args.config_file)
    cfg.merge_from_list(args.opts)
    cfg.freeze()
    return cfg


def parse_args(in_args=None):
    parser = argparse.ArgumentParser(description="Visualize ground-truth data")
    parser.add_argument(
        "--source",
        choices=["annotation", "dataloader"],
        required=True,
        help="visualize the annotations or the data loader (with pre-processing)",
    )
    parser.add_argument("--config-file", default="", metavar="FILE", help="path to config file")
    parser.add_argument("--output-dir", default="./", help="path to output directory")
    parser.add_argument("--show", action="store_true", help="show output in a window")
    parser.add_argument(
        "opts",
        help="Modify config options using the command-line",
        default=None,
        nargs=argparse.REMAINDER,
    )
    return parser.parse_args(in_args)


if __name__ == "__main__":
    args = parse_args()
    logger = setup_logger()
    logger.info("Arguments: " + str(args))
    cfg = setup(args)

    dirname = args.output_dir
    os.makedirs(dirname, exist_ok=True)
    metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])

    def output(vis, fname):
        if args.show:
            print(fname)
            cv2.imshow("window", vis.get_image()[:, :, ::-1])
            cv2.waitKey()
        else:
            filepath = os.path.join(dirname, fname)
            print("Saving to {} ...".format(filepath))
            vis.save(filepath)

    scale = 2.0 if args.show else 1.0
    if args.source == "dataloader":
        train_data_loader = build_train_loader(cfg)
        for batch in train_data_loader:
            for per_image in batch:
                # Pytorch tensor is in (C, H, W) format
                img = per_image["image"].permute(1, 2, 0)
                if cfg.INPUT.FORMAT == "BGR":
                    img = img[:, :, [2, 1, 0]]
                else:
                    img = np.asarray(Image.fromarray(img, mode=cfg.INPUT.FORMAT).convert("RGB"))

                visualizer = Visualizer(img, metadata=metadata, scale=scale)
                target_fields = per_image["instances"].get_fields()
                labels = [metadata.thing_classes[i] for i in target_fields["gt_classes"]]
                vis = visualizer.overlay_instances(
                    labels=labels,
                    boxes=target_fields.get("gt_boxes", None),
                    masks=target_fields.get("gt_masks", None),
                    keypoints=target_fields.get("gt_keypoints", None),
                )
                output(vis, str(per_image["image_id"]) + ".jpg")
    else:
        dicts = list(chain.from_iterable([DatasetCatalog.get(k) for k in cfg.DATASETS.TRAIN]))
        if cfg.MODEL.KEYPOINT_ON:
            dicts = filter_images_with_few_keypoints(dicts, 1)
        for dic in tqdm.tqdm(dicts):
            img = utils.read_image(dic["file_name"], "RGB")
            visualizer = Visualizer(img, metadata=metadata, scale=scale)
            vis = visualizer.draw_dataset_dict(dic)
            output(vis, os.path.basename(dic["file_name"]))
```

## tools/caffe2_converter.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
import argparse
import os

from cvpods.checkpoint import DefaultCheckpointer
from cvpods.config import get_cfg
from cvpods.data import build_test_loader
from cvpods.evaluation import COCOEvaluator, inference_on_dataset, print_csv_format
from cvpods.export import add_export_config, export_caffe2_model
from cvpods.modeling import build_model
from cvpods.utils import setup_logger


def setup_cfg(args):
    cfg = get_cfg()
    # cuda context is initialized before creating dataloader, so we don't fork anymore
    cfg.DATALOADER.NUM_WORKERS = 0
    cfg = add_export_config(cfg)
    cfg.merge_from_file(args.config_file)
    cfg.merge_from_list(args.opts)
    cfg.freeze()
    return cfg


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Convert a model to Caffe2")
    parser.add_argument("--config-file", default="", metavar="FILE", help="path to config file")
    parser.add_argument("--run-eval", action="store_true")
    parser.add_argument("--output", help="output directory for the converted caffe2 model")
    parser.add_argument(
        "opts",
        help="Modify config options using the command-line",
        default=None,
        nargs=argparse.REMAINDER,
    )
    args = parser.parse_args()
    logger = setup_logger()
    logger.info("Command line arguments: " + str(args))

    cfg = setup_cfg(args)

    # create a torch model
    torch_model = build_model(cfg)
    DefaultCheckpointer(torch_model).resume_or_load(cfg.MODEL.WEIGHTS)

    # get a sample data
    data_loader = build_test_loader(cfg, cfg.DATASETS.TEST[0])
    first_batch = next(iter(data_loader))

    # convert and save caffe2 model
    caffe2_model = export_caffe2_model(cfg, torch_model, first_batch)
    caffe2_model.save_protobuf(args.output)
    # draw the caffe2 graph
    caffe2_model.save_graph(os.path.join(args.output, "model.svg"), inputs=first_batch)

    # run evaluation with the converted model
    if args.run_eval:
        dataset = cfg.DATASETS.TEST[0]
        data_loader = build_test_loader(cfg, dataset)
        # NOTE: hard-coded evaluator. change to the evaluator for your dataset
        evaluator = COCOEvaluator(dataset, cfg, True, args.output)
        metrics = inference_on_dataset(caffe2_model, data_loader, evaluator)
        print_csv_format(metrics)
```

## tools/plain_train_net.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
"""
cvpods training script with a plain training loop.
This scripts reads a given config file and runs the training or evaluation.
It is an entry point that is able to train standard models in cvpods.
In order to let one script support training of many models,
this script contains logic that are specific to these built-in models and therefore
may not be suitable for your own project.
For example, your research project perhaps only needs a single "evaluator".
Therefore, we recommend you to use cvpods as an library and take
this file as an example of how to use the library.
You may want to write your own script with your datasets and other customizations.
Compared to "train_net.py", this script supports fewer default features.
It also includes fewer abstraction, therefore is easier to add custom logic.
"""

import logging
import os
from collections import OrderedDict

import torch
from torch.nn.parallel import DistributedDataParallel

from cvpods.checkpoint import DefaultCheckpointer, PeriodicCheckpointer
from cvpods.config import get_cfg
from cvpods.data import MetadataCatalog, build_test_loader, build_train_loader
from cvpods.engine import default_argument_parser, default_setup, launch
from cvpods.evaluation import (
    CityscapesEvaluator,
    COCOEvaluator,
    COCOPanopticEvaluator,
    DatasetEvaluators,
    LVISEvaluator,
    PascalVOCDetectionEvaluator,
    SemSegEvaluator,
    inference_on_dataset,
    print_csv_format
)
from cvpods.solver import build_lr_scheduler, build_optimizer
from cvpods.utils import CommonMetricPrinter, EventStorage, JSONWriter, TensorboardXWriter, comm

from config import config
from net import build_model

logger = logging.getLogger("cvpods")


def get_evaluator(cfg, dataset_name, output_folder=None):
    """
    Create evaluator(s) for a given dataset.
    This uses the special metadata "evaluator_type" associated with each builtin dataset.
    For your own dataset, you can simply create an evaluator manually in your
    script and do not have to worry about the hacky if-else logic here.
    """
    if output_folder is None:
        output_folder = os.path.join(cfg.OUTPUT_DIR, "inference")
    evaluator_list = []
    evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type
    if evaluator_type in ["sem_seg", "coco_panoptic_seg"]:
        evaluator_list.append(
            SemSegEvaluator(
                dataset_name,
                distributed=True,
                num_classes=cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES,
                ignore_label=cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE,
                output_dir=output_folder,
            )
        )
    if evaluator_type in ["coco", "coco_panoptic_seg"]:
        evaluator_list.append(COCOEvaluator(dataset_name, cfg, True, output_folder))
    if evaluator_type == "coco_panoptic_seg":
        evaluator_list.append(COCOPanopticEvaluator(dataset_name, output_folder))
    if evaluator_type == "cityscapes":
        assert (
            torch.cuda.device_count() >= comm.get_rank()
        ), "CityscapesEvaluator currently do not work with multiple machines."
        return CityscapesEvaluator(dataset_name)
    if evaluator_type == "pascal_voc":
        return PascalVOCDetectionEvaluator(dataset_name)
    if evaluator_type == "lvis":
        return LVISEvaluator(dataset_name, cfg, True, output_folder)
    if len(evaluator_list) == 0:
        raise NotImplementedError(
            "no Evaluator for the dataset {} with the type {}".format(dataset_name, evaluator_type)
        )
    if len(evaluator_list) == 1:
        return evaluator_list[0]
    return DatasetEvaluators(evaluator_list)


def do_test(cfg, model):
    results = OrderedDict()
    for dataset_name in cfg.DATASETS.TEST:
        data_loader = build_test_loader(cfg, dataset_name)
        evaluator = get_evaluator(
            cfg, dataset_name, os.path.join(cfg.OUTPUT_DIR, "inference", dataset_name)
        )
        results_i = inference_on_dataset(model, data_loader, evaluator)
        results[dataset_name] = results_i
        if comm.is_main_process():
            logger.info("Evaluation results for {} in csv format:".format(dataset_name))
            print_csv_format(results_i)
    if len(results) == 1:
        results = list(results.values())[0]
    return results


def do_train(cfg, model, resume=False):
    model.train()
    optimizer = build_optimizer(cfg, model)
    scheduler = build_lr_scheduler(cfg, optimizer)

    checkpointer = DefaultCheckpointer(
        model, cfg.OUTPUT_DIR, optimizer=optimizer, scheduler=scheduler
    )
    start_iter = (
        checkpointer.resume_or_load(cfg.MODEL.WEIGHTS, resume=resume).get("iteration", -1) + 1
    )
    max_iter = cfg.SOLVER.MAX_ITER

    periodic_checkpointer = PeriodicCheckpointer(
        checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD, max_iter=max_iter
    )

    writers = (
        [
            CommonMetricPrinter(max_iter),
            JSONWriter(os.path.join(cfg.OUTPUT_DIR, "metrics.json")),
            TensorboardXWriter(cfg.OUTPUT_DIR),
        ]
        if comm.is_main_process()
        else []
    )

    # compared to "train_net.py", we do not support accurate timing and
    # precise BN here, because they are not trivial to implement
    data_loader = build_train_loader(cfg)
    logger.info("Starting training from iteration {}".format(start_iter))
    with EventStorage(start_iter) as storage:
        for data, iteration in zip(data_loader, range(start_iter, max_iter)):
            iteration = iteration + 1
            storage.step()

            loss_dict = model(data)
            losses = sum(loss for loss in loss_dict.values())
            assert torch.isfinite(losses).all(), loss_dict

            loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}
            losses_reduced = sum(loss for loss in loss_dict_reduced.values())
            if comm.is_main_process():
                storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)

            optimizer.zero_grad()
            losses.backward()
            optimizer.step()
            storage.put_scalar("lr", optimizer.param_groups[0]["lr"], smoothing_hint=False)
            scheduler.step()

            if (
                cfg.TEST.EVAL_PERIOD > 0
                and iteration % cfg.TEST.EVAL_PERIOD == 0
                and iteration != max_iter
            ):
                do_test(cfg, model)
                # Compared to "train_net.py", the test results are not dumped to EventStorage
                comm.synchronize()

            if iteration - start_iter > 5 and (iteration % 20 == 0 or iteration == max_iter):
                for writer in writers:
                    writer.write()
            periodic_checkpointer.step(iteration)


def setup(args):
    """
    Create configs and perform basic setups.
    """
    cfg = get_cfg()
    cfg.merge_from_file(args.config_file)
    cfg.merge_from_list(args.opts)
    cfg.freeze()
    default_setup(
        cfg, args
    )  # if you don't like any of the default setup, write your own setup code
    return cfg


def main(args):
    config.merge_from_list(args.opts)
    cfg = setup(args)
    model = build_model(cfg)

    logger.info("Model:\n{}".format(model))
    if args.eval_only:
        DefaultCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(
            cfg.MODEL.WEIGHTS, resume=args.resume
        )
        return do_test(cfg, model)

    distributed = comm.get_world_size() > 1
    if distributed:
        model = DistributedDataParallel(
            model, device_ids=[comm.get_local_rank()], broadcast_buffers=False
        )

    do_train(cfg, model)
    return do_test(cfg, model)


if __name__ == "__main__":
    args = default_argument_parser().parse_args()
    print("Command Line Args:", args)
    launch(
        main,
        args.num_gpus,
        num_machines=args.num_machines,
        machine_rank=args.machine_rank,
        dist_url=args.dist_url,
        args=(args,),
    )
```

## tools/rm_files.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-

import argparse
import os
import re
from colorama import Fore, Style


def remove_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument("--start-iter", "-s", type=int, default=0, help="start iter to remove")
    parser.add_argument("--end-iter", "-e", type=int, default=0, help="end iter to remove")
    parser.add_argument("--prefix", "-p", type=str, default="model_",
                        help="prefix of model to remove")
    parser.add_argument("--dir", "-d", type=str, default="/data/Outputs",
                        help="dir to remove pth model")
    parser.add_argument("--real", "-r", action="store_true",
                        help="really delete or just show what you will delete")
    return parser


def remove_files(args):
    start = args.start_iter
    end = args.end_iter
    prefix = args.prefix
    for folder, _, files in os.walk(args.dir):
        # l = [x for x in f if x.endswith(".pth")]
        models = [f for f in files if re.search(prefix + r"[0123456789]*\.pth", f)]
        delete = [os.path.join(folder, model) for model in models
                  if start <= int(model[len(prefix):-len(".pth")]) <= end]
        if delete:
            for f in delete:
                if args.real:
                    print(f"remove {f}")
                    os.remove(f)
                else:
                    print(f"you may remove {f}")
    if not args.real:
        print(Fore.RED + "use --real parameter to really delete models" + Style.RESET_ALL)


def main():
    args = remove_parser().parse_args()
    remove_files(args)


if __name__ == "__main__":
    main()
```

## tools/visualize_json_results.py

```python
#!/usr/bin/env python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

import argparse
import json
import os
from collections import defaultdict
import tqdm

import cv2
import numpy as np

from cvpods.data import build_dataset
from cvpods.structures import Boxes, BoxMode, Instances
from cvpods.utils import PathManager, Visualizer, dynamic_import, setup_logger


def setup_cfg(path, logger):
    # load config from file and command-line arguments
    assert path.endswith(".py")
    path, module = os.path.split(path)
    module = module.rstrip(".py")
    cfg = dynamic_import(module, path).config
    if cfg.DATASETS.CUSTOM_TYPE != ["ConcatDataset", dict()]:
        logger.warning("Ignore cfg.DATASETS.CUSTOM_TYPE: {}. "
                       "Using (\"ConcatDataset\", dict())".format(cfg.DATASETS.CUSTOM_TYPE))
    cfg.DATASETS.CUSTOM_TYPE = ["ConcatDataset", dict()]

    return cfg


def create_instances(predictions, image_size):
    ret = Instances(image_size)

    score = np.asarray([x["score"] for x in predictions])
    chosen = (score > args.conf_threshold).nonzero()[0]
    score = score[chosen]
    bbox = np.asarray([predictions[i]["bbox"] for i in chosen])

    if score.shape[0] == 0:
        bbox = np.zeros((0, 4))
    else:
        bbox = BoxMode.convert(bbox, BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)

    labels = np.asarray([dataset_id_map(predictions[i]["category_id"]) for i in chosen])

    ret.scores = score
    ret.pred_boxes = Boxes(bbox)
    ret.pred_classes = labels

    try:
        ret.pred_masks = [predictions[i]["segmentation"] for i in chosen]
    except KeyError:
        pass
    return ret


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="A script that visualizes the json predictions from COCO or LVIS dataset."
    )
    parser.add_argument("--input", required=True, help="JSON file produced by the model")
    parser.add_argument("--output", required=True, help="output directory")
    parser.add_argument("--config", required=True,
                        help="path to a python file with a definition of `config`")
    parser.add_argument("--dataset",
                        help="name of the dataset. Use DATASETS.TEST[0] if not specified.",
                        default="")
    parser.add_argument("--conf-threshold", default=0.5, type=float, help="confidence threshold")
    args = parser.parse_args()

    logger = setup_logger()
    cfg = setup_cfg(args.config, logger)
    with PathManager.open(args.input, "r") as f:
        predictions = json.load(f)

    pred_by_image = defaultdict(list)
    for p in predictions:
        pred_by_image[p["image_id"]].append(p)

    # TODO: add DatasetCatalog, MetadataCatalog
    dataset = build_dataset(
        cfg,
        [args.dataset] if args.dataset else [cfg.DATASETS.TEST[0]],
        transforms=[],
        is_train=False)
    dicts = dataset.datasets[0].dataset_dicts
    metadata = dataset.meta
    if hasattr(metadata, "thing_dataset_id_to_contiguous_id"):

        def dataset_id_map(ds_id):
            return metadata.thing_dataset_id_to_contiguous_id[ds_id]

    elif "lvis" in args.dataset:
        # LVIS results are in the same format as COCO results, but have a different
        # mapping from dataset category id to contiguous category id in [0, #categories - 1]
        def dataset_id_map(ds_id):
            return ds_id - 1

    else:
        raise ValueError("Unsupported dataset: {}".format(args.dataset))

    os.makedirs(args.output, exist_ok=True)

    for dic in tqdm.tqdm(dicts):
        img = cv2.imread(dic["file_name"], cv2.IMREAD_COLOR)[:, :, ::-1]
        basename = os.path.basename(dic["file_name"])

        predictions = create_instances(pred_by_image[dic["image_id"]], img.shape[:2])
        vis = Visualizer(img, metadata)
        vis_pred = vis.draw_instance_predictions(predictions).get_image()

        vis = Visualizer(img, metadata)
        vis_gt = vis.draw_dataset_dict(dic).get_image()

        concat = np.concatenate((vis_pred, vis_gt), axis=1)
        cv2.imwrite(os.path.join(args.output, basename), concat[:, :, ::-1])
```

## tools/train_net.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
# Modified by BaseDetection, Inc. and its affiliates. All Rights Reserved

# pylint: disable=W0613

"""
Detection Training Script.
This scripts reads a given config file and runs the training or evaluation.
It is an entry point that is made to train standard models in cvpods.
In order to let one script support training of many models,
this script contains logic that are specific to these built-in models and therefore
may not be suitable for your own project.
For example, your research project perhaps only needs a single "evaluator".
Therefore, we recommend you to use cvpods as an library and take
this file as an example of how to use the library.
You may want to write your own script with your datasets and other customizations.
"""
import logging
import os
from collections import OrderedDict
from colorama import Fore, Style

from cvpods.engine import RUNNERS, default_argument_parser, default_setup, hooks, launch
from cvpods.evaluation import build_evaluator
from cvpods.modeling import GeneralizedRCNNWithTTA

from config import config
from net import build_model


def runner_decrator(cls):
    """
    We use the "DefaultRunner" which contains pre-defined default logic for
    standard training workflow. They may not work for you, especially if you
    are working on a new research project. In that case you can use the cleaner
    "SimpleRunner", or write your own training loop. You can use
    "tools/plain_train_net.py" as an example.
    """

    def custom_build_evaluator(cls, cfg, dataset_name, dataset, output_folder=None):
        """
        Create evaluator(s) for a given dataset.
        This uses the special metadata "evaluator_type" associated with each builtin dataset.
        For your own dataset, you can simply create an evaluator manually in your
        script and do not have to worry about the hacky if-else logic here.
        """
        dump_train = config.GLOBAL.DUMP_TRAIN
        return build_evaluator(cfg, dataset_name, dataset, output_folder, dump=dump_train)

    def custom_test_with_TTA(cls, cfg, model):
        logger = logging.getLogger("cvpods.runner")
        # In the end of training, run an evaluation with TTA
        # Only support some R-CNN models.
        logger.info("Running inference with test-time augmentation ...")
        model = GeneralizedRCNNWithTTA(cfg, model)
        res = cls.test(cfg, model, output_folder=os.path.join(cfg.OUTPUT_DIR, "inference_TTA"))
        res = OrderedDict({k + "_TTA": v for k, v in res.items()})
        return res

    cls.build_evaluator = classmethod(custom_build_evaluator)
    cls.test_with_TTA = classmethod(custom_test_with_TTA)

    return cls


def main(args):
    config.merge_from_list(args.opts)
    cfg, logger = default_setup(config, args)

    """
    If you'd like to do anything fancier than the standard training logic,
    consider writing your own training loop or subclassing the runner.
    """
    runner = runner_decrator(RUNNERS.get(cfg.TRAINER.NAME))(cfg, build_model)
    runner.resume_or_load(resume=args.resume)

    # check wheather worksapce has enough storeage space
    # assume that a single dumped model is 700Mb
    file_sys = os.statvfs(cfg.OUTPUT_DIR)
    free_space_Gb = (file_sys.f_bfree * file_sys.f_frsize) / 2**30
    eval_space_Gb = (cfg.SOLVER.LR_SCHEDULER.MAX_ITER // cfg.SOLVER.CHECKPOINT_PERIOD) * 700 / 2**10
    if eval_space_Gb > free_space_Gb:
        logger.warning(f"{Fore.RED}Remaining space({free_space_Gb}GB) "
                       f"is less than ({eval_space_Gb}GB){Style.RESET_ALL}")

    if cfg.TEST.AUG.ENABLED:
        runner.register_hooks(
            [hooks.EvalHook(0, lambda: runner.test_with_TTA(cfg, runner.model))]
        )

    logger.info("Running with full config:\n{}".format(cfg))
    base_config = cfg.__class__.__base__()
    logger.info("different config with base class:\n{}".format(cfg.diff(base_config)))

    runner.train()


if __name__ == "__main__":
    args = default_argument_parser().parse_args()
    print("Command Line Args:", args)
    config.link_log()
    print("soft link to {}".format(config.OUTPUT_DIR))
    launch(
        main,
        args.num_gpus,
        num_machines=args.num_machines,
        machine_rank=args.machine_rank,
        dist_url=args.dist_url,
        args=(args,),
    )
```

## tools/debug_net.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved
"""
Debuging Script.
"""
import logging
import os
import re

from cvpods.checkpoint import Checkpointer
from cvpods.engine import TrainerBase, default_argument_parser, default_setup, launch
from cvpods.solver import build_optimizer
from cvpods.utils import comm

from config import config
from net import build_model


class DebugTrainer(TrainerBase):

    def __init__(self, model, data, optimizer):
        model.train()

        self.data = data
        self.model = model
        self.optimizer = optimizer

    def run_step(self):
        """
        Implement the standard training logic described above.
        """
        assert self.model.training, "[SimpleTrainer] model was changed to eval mode!"

        self.optimizer.zero_grad()

        # for each mini step
        loss_dict = self.model(self.data)

        for metrics_name, metrics_value in loss_dict.items():
            # Actually, some metrics are not loss, such as
            # top1_acc, top5_acc in classification, filter them out
            if metrics_value.requires_grad:
                loss_dict[metrics_name] = metrics_value

        losses = sum([
            metrics_value for metrics_value in loss_dict.values()
            if metrics_value.requires_grad
        ])
        losses.backward()

        self.optimizer.step()


def debug_parser():
    parser = default_argument_parser()
    parser.add_argument(
        "--ckpt-file", type=str, default=None, help="path of debug checkpoint file"
    )
    return parser


def stage_main(args, cfg, build):
    logger = logging.getLogger(__name__)
    assert comm.get_world_size() == 1, "DEBUG mode only supported for 1 GPU"

    cfg.merge_from_list(args.opts)
    cfg, logger = default_setup(cfg, args)
    model = build(cfg)
    optimizer = build_optimizer(cfg, model)
    debug_ckpt = Checkpointer(model, resume=True, optimizer=optimizer)
    ckpt_file = args.ckpt_file
    if ckpt_file is None:
        # find latest checkpoint in log dir if ckpt_file is not given
        log_dir = "./log"
        matched_files = [
            os.path.join(log_dir, files) for files in os.listdir(log_dir)
            if re.match("debug_.*.pth", files) is not None
        ]
        ckpt_file = sorted(matched_files, key=os.path.getatime)[-1]

    left_dict = debug_ckpt.load(ckpt_file)
    assert "inputs" in left_dict, "input data not found in checkpoints"
    data = left_dict["inputs"]

    trainer = DebugTrainer(model, data, optimizer)
    logger.info("start run models")
    trainer.run_step()
    logger.info("finish debuging")


def main(args):
    if isinstance(config, list):
        assert isinstance(build_model, list) and len(config) == len(build_model)
        for cfg, build in zip(config, build_model):
            stage_main(args, cfg, build)
    else:
        stage_main(args, config, build_model)


if __name__ == "__main__":
    args = debug_parser().parse_args()

    if isinstance(config, list):
        assert len(config) > 0
        print("soft link first config in list to {}".format(config[0].OUTPUT_DIR))
        config[0].link_log()
    else:
        print("soft link to {}".format(config.OUTPUT_DIR))
        config.link_log()
    print("Command Line Args:", args)
    launch(
        main,
        args.num_gpus,
        num_machines=args.num_machines,
        machine_rank=args.machine_rank,
        dist_url=args.dist_url,
        args=(args,),
    )
```

### tools/dev/run_instant_tests.sh

```bash
#!/bin/bash -e
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

BIN="python tools/train_net.py"
OUTPUT="instant_test_output"
NUM_GPUS=2

CFG_LIST=( "${@:1}" )
if [ ${#CFG_LIST[@]} -eq 0 ]; then
  CFG_LIST=( ./configs/quick_schedules/*instant_test.yaml )
fi

echo "========================================================================"
echo "Configs to run:"
echo "${CFG_LIST[@]}"
echo "========================================================================"

for cfg in "${CFG_LIST[@]}"; do
    echo "========================================================================"
    echo "Running $cfg ..."
    echo "========================================================================"
    $BIN --num-gpus $NUM_GPUS --config-file "$cfg" \
      SOLVER.IMS_PER_BATCH $(($NUM_GPUS * 2)) \
      OUTPUT_DIR "$OUTPUT"
    rm -rf "$OUTPUT"
done

```

### tools/dev/linter.sh

```bash
#!/bin/bash -e
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

# Run this script at project root by "./dev/linter.sh" before you commit

{
	black --version | grep "19.3b0" > /dev/null
} || {
	echo "Linter requires black==19.3b0 !"
	exit 1
}

set -v

echo "Running isort ..."
isort -y --multi-line 3 --trailing-comma -sp . --skip datasets --skip docs --skip-glob '*/__init__.py' --atomic

echo "Running black ..."
black -l 100 .

echo "Running flake8 ..."
if [ -x "$(command -v flake8-3)" ]; then
  flake8-3 .
else
  python3 -m flake8 .
fi

# echo "Running mypy ..."
# Pytorch does not have enough type annotations
# mypy cvpods/solver cvpods/structures cvpods/config

echo "Running clang-format ..."
find . -regex ".*\.\(cpp\|c\|cc\|cu\|cxx\|h\|hh\|hpp\|hxx\|tcc\|mm\|m\)" -print0 | xargs -0 clang-format -i

command -v arc > /dev/null && arc lint
```

### tools/dev/parse_results.sh

```bash
#!/bin/bash
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

# A shell script that parses metrics from the log file.
# Make it easier for developers to track performance of models.

LOG="$1"

if [[ -z "$LOG" ]]; then
	echo "Usage: $0 /path/to/log/file"
	exit 1
fi

# [12/15 11:47:32] trainer INFO: Total training time: 12:15:04.446477 (0.4900 s / it)
# [12/15 11:49:03] inference INFO: Total inference time: 0:01:25.326167 (0.13652186737060548 s / img per device, on 8 devices)
# [12/15 11:49:03] inference INFO: Total inference pure compute time: .....

# training time
trainspeed=$(grep -o 'Overall training.*' "$LOG" | grep -Eo '\(.*\)' | grep -o '[0-9\.]*')
echo "Training speed: $trainspeed s/it"

# inference time: there could be multiple inference during training
inferencespeed=$(grep -o 'Total inference pure.*' "$LOG" | tail -n1 | grep -Eo '\(.*\)' | grep -o '[0-9\.]*' | head -n1)
echo "Inference speed: $inferencespeed s/it"

# [12/15 11:47:18] trainer INFO: eta: 0:00:00  iter: 90000  loss: 0.5407 (0.7256)  loss_classifier: 0.1744 (0.2446)  loss_box_reg: 0.0838 (0.1160)  loss_mask: 0.2159 (0.2722)  loss_objectness: 0.0244 (0.0429)  loss_rpn_box_reg: 0.0279 (0.0500)  time: 0.4487 (0.4899)  data: 0.0076 (0.0975) lr: 0.000200  max mem: 4161
memory=$(grep -o 'max[_ ]mem: [0-9]*' "$LOG" | tail -n1 | grep -o '[0-9]*')
echo "Training memory: $memory MB"

echo "Easy to copypaste:"
echo "$trainspeed","$inferencespeed","$memory"

echo "------------------------------"

# [12/26 17:26:32] engine.coco_evaluation: copypaste: Task: bbox
# [12/26 17:26:32] engine.coco_evaluation: copypaste: AP,AP50,AP75,APs,APm,APl
# [12/26 17:26:32] engine.coco_evaluation: copypaste: 0.0017,0.0024,0.0017,0.0005,0.0019,0.0011
# [12/26 17:26:32] engine.coco_evaluation: copypaste: Task: segm
# [12/26 17:26:32] engine.coco_evaluation: copypaste: AP,AP50,AP75,APs,APm,APl
# [12/26 17:26:32] engine.coco_evaluation: copypaste: 0.0014,0.0021,0.0016,0.0005,0.0016,0.0011

echo "COCO Results:"
num_tasks=$(grep -o 'copypaste:.*Task.*' "$LOG" | sort -u | wc -l)
# each task has 3 lines
grep -o 'copypaste:.*' "$LOG" | cut -d ' ' -f 2- | tail -n $((num_tasks * 3))
```

### tools/dev/run_inference_tests.sh

```bash
#!/bin/bash -e
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

BIN="python tools/train_net.py"
OUTPUT="inference_test_output"
NUM_GPUS=2

CFG_LIST=( "${@:1}" )

if [ ${#CFG_LIST[@]} -eq 0 ]; then
  CFG_LIST=( ./configs/quick_schedules/*inference_acc_test.yaml )
fi

echo "========================================================================"
echo "Configs to run:"
echo "${CFG_LIST[@]}"
echo "========================================================================"


for cfg in "${CFG_LIST[@]}"; do
    echo "========================================================================"
    echo "Running $cfg ..."
    echo "========================================================================"
    $BIN \
      --eval-only \
      --num-gpus $NUM_GPUS \
      --config-file "$cfg" \
      OUTPUT_DIR $OUTPUT
      rm -rf $OUTPUT
done


echo "========================================================================"
echo "Running demo.py ..."
echo "========================================================================"
DEMO_BIN="python demo/demo.py"
COCO_DIR=datasets/coco/val2014
mkdir -pv $OUTPUT

set -v

$DEMO_BIN --config-file ./configs/quick_schedules/panoptic_fpn_R_50_inference_acc_test.yaml \
  --input $COCO_DIR/COCO_val2014_0000001933* --output $OUTPUT
rm -rf $OUTPUT
```

## cvpods/__init__.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.

from .utils import setup_environment

setup_environment()

# This line will be programatically read/write by setup.py.
# Leave them at the bottom of this file and don't touch them.
__version__ = "0.1"
```

### cvpods/checkpoint/catalog.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import logging

from cvpods.utils import PathHandler, PathManager


class ModelCatalog(object):
    """
    Store mappings from names to third-party models.
    """

    S3_C2_DETECTRON_PREFIX = "https://dl.fbaipublicfiles.com/detectron"

    # MSRA models have STRIDE_IN_1X1=True. False otherwise.
    # NOTE: all BN models here have fused BN into an affine layer.
    # As a result, you should only load them to a model with "FrozenBN".
    # Loading them to a model with regular BN or SyncBN is wrong.
    # Even when loaded to FrozenBN, it is still different from affine by an epsilon,
    # which should be negligible for training.
    # NOTE: all models here uses PIXEL_STD=[1,1,1]
    C2_IMAGENET_MODELS = {
        "MSRA/R-50": "ImageNetPretrained/MSRA/R-50.pkl",
        "MSRA/R-101": "ImageNetPretrained/MSRA/R-101.pkl",
        "FAIR/R-50-GN": "ImageNetPretrained/47261647/R-50-GN.pkl",
        "FAIR/R-101-GN": "ImageNetPretrained/47592356/R-101-GN.pkl",
        "FAIR/X-101-32x8d": "ImageNetPretrained/20171220/X-101-32x8d.pkl",
        "FAIR/X-101-64x4d": "ImageNetPretrained/FBResNeXt/X-101-64x4d.pkl",
        "FAIR/X-152-32x8d-IN5k": "ImageNetPretrained/25093814/X-152-32x8d-IN5k.pkl",
    }

    C2_DETECTRON_PATH_FORMAT = (
        "{prefix}/{url}/output/train/{dataset}/{type}/model_final.pkl"
    )  # noqa B950

    C2_DATASET_COCO = "coco_2014_train%3Acoco_2014_valminusminival"
    C2_DATASET_COCO_KEYPOINTS = "keypoints_coco_2014_train%3Akeypoints_coco_2014_valminusminival"

    # format: {model_name} -> part of the url
    C2_DETECTRON_MODELS = {
        "35857197/e2e_faster_rcnn_R-50-C4_1x": "35857197/12_2017_baselines/e2e_faster_rcnn_R-50-C4_1x.yaml.01_33_49.iAX0mXvW",  # noqa B950
        "35857345/e2e_faster_rcnn_R-50-FPN_1x": "35857345/12_2017_baselines/e2e_faster_rcnn_R-50-FPN_1x.yaml.01_36_30.cUF7QR7I",  # noqa B950
        "35857890/e2e_faster_rcnn_R-101-FPN_1x": "35857890/12_2017_baselines/e2e_faster_rcnn_R-101-FPN_1x.yaml.01_38_50.sNxI7sX7",  # noqa B950
        "36761737/e2e_faster_rcnn_X-101-32x8d-FPN_1x": "36761737/12_2017_baselines/e2e_faster_rcnn_X-101-32x8d-FPN_1x.yaml.06_31_39.5MIHi1fZ",  # noqa B950
        "35858791/e2e_mask_rcnn_R-50-C4_1x": "35858791/12_2017_baselines/e2e_mask_rcnn_R-50-C4_1x.yaml.01_45_57.ZgkA7hPB",  # noqa B950
        "35858933/e2e_mask_rcnn_R-50-FPN_1x": "35858933/12_2017_baselines/e2e_mask_rcnn_R-50-FPN_1x.yaml.01_48_14.DzEQe4wC",  # noqa B950
        "35861795/e2e_mask_rcnn_R-101-FPN_1x": "35861795/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_1x.yaml.02_31_37.KqyEK4tT",  # noqa B950
        "36761843/e2e_mask_rcnn_X-101-32x8d-FPN_1x": "36761843/12_2017_baselines/e2e_mask_rcnn_X-101-32x8d-FPN_1x.yaml.06_35_59.RZotkLKI",  # noqa B950
        "48616381/e2e_mask_rcnn_R-50-FPN_2x_gn": "GN/48616381/04_2018_gn_baselines/e2e_mask_rcnn_R-50-FPN_2x_gn_0416.13_23_38.bTlTI97Q",  # noqa B950
        "37697547/e2e_keypoint_rcnn_R-50-FPN_1x": "37697547/12_2017_baselines/e2e_keypoint_rcnn_R-50-FPN_1x.yaml.08_42_54.kdzV35ao",  # noqa B950
        "35998355/rpn_R-50-C4_1x": "35998355/12_2017_baselines/rpn_R-50-C4_1x.yaml.08_00_43.njH5oD9L",  # noqa B950
        "35998814/rpn_R-50-FPN_1x": "35998814/12_2017_baselines/rpn_R-50-FPN_1x.yaml.08_06_03.Axg0r179",  # noqa B950
        "36225147/fast_R-50-FPN_1x": "36225147/12_2017_baselines/fast_rcnn_R-50-FPN_1x.yaml.08_39_09.L3obSdQ2",  # noqa B950
    }

    @staticmethod
    def get(name):
        if name.startswith("Caffe2Detectron/COCO"):
            return ModelCatalog._get_c2_detectron_baseline(name)
        if name.startswith("ImageNetPretrained/"):
            return ModelCatalog._get_c2_imagenet_pretrained(name)
        raise RuntimeError("model not present in the catalog: {}".format(name))

    @staticmethod
    def _get_c2_imagenet_pretrained(name):
        prefix = ModelCatalog.S3_C2_DETECTRON_PREFIX
        name = name[len("ImageNetPretrained/"):]
        name = ModelCatalog.C2_IMAGENET_MODELS[name]
        url = "/".join([prefix, name])
        return url

    @staticmethod
    def _get_c2_detectron_baseline(name):
        name = name[len("Caffe2Detectron/COCO/"):]
        url = ModelCatalog.C2_DETECTRON_MODELS[name]
        if "keypoint_rcnn" in name:
            dataset = ModelCatalog.C2_DATASET_COCO_KEYPOINTS
        else:
            dataset = ModelCatalog.C2_DATASET_COCO

        if "35998355/rpn_R-50-C4_1x" in name:
            # this one model is somehow different from others ..
            type = "rpn"
        else:
            type = "generalized_rcnn"

        # Detectron C2 models are stored in the structure defined in `C2_DETECTRON_PATH_FORMAT`.
        url = ModelCatalog.C2_DETECTRON_PATH_FORMAT.format(
            prefix=ModelCatalog.S3_C2_DETECTRON_PREFIX, url=url, type=type, dataset=dataset
        )
        return url


class ModelCatalogHandler(PathHandler):
    """
    Resolve URL like catalog://.
    """

    PREFIX = "catalog://"

    def _get_supported_prefixes(self):
        return [self.PREFIX]

    def _get_local_path(self, path):
        logger = logging.getLogger(__name__)
        catalog_path = ModelCatalog.get(path[len(self.PREFIX):])
        logger.info("Catalog entry {} points to {}".format(path, catalog_path))
        return PathManager.get_local_path(catalog_path)

    def _open(self, path, mode="r", **kwargs):
        return PathManager.open(self._get_local_path(path), mode, **kwargs)


class Detectron2Handler(PathHandler):
    """
    Resolve anything that's in Detectron2 model zoo.
    """

    PREFIX = "detectron2://"
    S3_DETECTRON2_PREFIX = "https://dl.fbaipublicfiles.com/detectron2/"

    def _get_supported_prefixes(self):
        return [self.PREFIX]

    def _get_local_path(self, path):
        name = path[len(self.PREFIX):]
        return PathManager.get_local_path(self.S3_DETECTRON2_PREFIX + name)

    def _open(self, path, mode="r", **kwargs):
        return PathManager.open(self._get_local_path(path), mode, **kwargs)


PathManager.register_handler(ModelCatalogHandler())
PathManager.register_handler(Detectron2Handler())
```

### cvpods/checkpoint/checkpoint.py

```python
#!/usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.

import copy
import logging
import os
import pickle
from typing import Any, Optional

import numpy as np

import torch
import torch.nn as nn
from torch.nn.parallel import DataParallel, DistributedDataParallel

from cvpods.utils import PathManager
from cvpods.utils.distributed import comm

from .c2_model_loading import align_and_update_state_dicts
from .utils import (
    _strip_prefix_if_present,
    get_missing_parameters_message,
    get_unexpected_parameters_message
)


class Checkpointer(object):
    """
    A checkpointer that can save/load model as well as extra checkpointable
    objects.
    """

    def __init__(
        self,
        model: nn.Module,
        save_dir: str = "",
        resume: bool = False,
        *,
        save_to_disk: bool = True,
        **checkpointables: object,
    ):
        """
        Args:
            model (nn.Module): model.
            save_dir (str): a directory to save and find checkpoints.
            save_to_disk (bool): if True, save checkpoint to disk, otherwise
                disable saving for this checkpointer.
            checkpointables (object): any checkpointable objects, i.e., objects
                that have the `state_dict()` and `load_state_dict()` method. For
                example, it can be used like
                `Checkpointer(model, "dir", optimizer=optimizer)`.
        """
        if isinstance(model, (DistributedDataParallel, DataParallel)):
            model = model.module
        self.model = model
        self.checkpointables = copy.copy(checkpointables)
        self.resume = resume
        self.logger = logging.getLogger(__name__)
        self.save_dir = save_dir
        self.save_to_disk = save_to_disk

    def save(self, name: str, tag_checkpoint: bool = True, **kwargs: dict):
        """
        Dump model and checkpointables to a file.

        Args:
            name (str): name of the file.
            kwargs (dict): extra arbitrary data to save.
        """
        if not self.save_dir or not self.save_to_disk:
            return

        data = {}
        data["model"] = self.model.state_dict()
        for key, obj in self.checkpointables.items():
            data[key] = obj.state_dict()
        data.update(kwargs)

        basename = "{}.pth".format(name)
        save_file = os.path.join(self.save_dir, basename)
        assert os.path.basename(save_file) == basename, basename
        self.logger.info("Saving checkpoint to {}".format(save_file))
        with PathManager.open(save_file, "wb") as f:
            torch.save(data, f)

        if tag_checkpoint:
            self.tag_last_checkpoint(basename)

    def load(self, path: str):
        """
        Load from the given checkpoint. When path points to network file, this
        function has to be called on all ranks.

        Args:
            path (str): path or url to the checkpoint. If empty, will not load
                anything.
        Returns:
            dict:
                extra data loaded from the checkpoint that has not been
                processed. For example, those saved with
                :meth:`.save(**extra_data)`.
        """
        if not path:
            # no checkpoint provided
            self.logger.info(
                "No checkpoint found. Initializing model from scratch"
            )
            return {}
        self.logger.info("Loading checkpoint from {}".format(path))
        if not os.path.isfile(path):
            path = PathManager.get_local_path(path)
            assert PathManager.isfile(path), "Checkpoint {} not found!".format(path)

        checkpoint = self._load_file(path)
        self._load_model(checkpoint)
        if self.resume:
            for key, obj in self.checkpointables.items():
                if key in checkpoint:
                    self.logger.info("Loading {} from {}".format(key, path))
                    obj.load_state_dict(checkpoint.pop(key))
            # return any further checkpoint data
            return checkpoint
        else:
            return {}

    def has_checkpoint(self):
        """
        Returns:
            bool: whether a checkpoint exists in the target directory.
        """
        save_file = os.path.join(self.save_dir, "last_checkpoint")
        return PathManager.exists(save_file)

    def get_checkpoint_file(self):
        """
        Returns:
            str: The latest checkpoint file in target directory.
        """
        save_file = os.path.join(self.save_dir, "last_checkpoint")
        try:
            with PathManager.open(save_file, "r") as f:
                last_saved = f.read().strip()
        except IOError:
            # if file doesn't exist, maybe because it has just been
            # deleted by a separate process
            return ""
        return os.path.join(self.save_dir, last_saved)

    def get_all_checkpoint_files(self):
        """
        Returns:
            list: All available checkpoint files (.pth files) in target
                directory.
        """
        all_model_checkpoints = [
            os.path.join(self.save_dir, file)
            for file in PathManager.ls(self.save_dir)
            if PathManager.isfile(os.path.join(self.save_dir, file)) and file.endswith(".pth")
        ]
        return all_model_checkpoints

    def resume_or_load(self, path: str, *, resume: bool = True):
        """
        If `resume` is True, this method attempts to resume from the last
        checkpoint, if exists. Otherwise, load checkpoint from the given path.
        This is useful when restarting an interrupted training job.

        Args:
            path (str): path to the checkpoint.
            resume (bool): if True, resume from the last checkpoint if it exists.

        Returns:
            same as :meth:`load`.
        """
        if resume and self.has_checkpoint():
            path = self.get_checkpoint_file()
        return self.load(path)

    def tag_last_checkpoint(self, last_filename_basename: str):
        """
        Tag the last checkpoint.

        Args:
            last_filename_basename (str): the basename of the last filename.
        """
        save_file = os.path.join(self.save_dir, "last_checkpoint")
        with PathManager.open(save_file, "w") as f:
            f.write(last_filename_basename)

    def _load_file(self, f: str):
        """
        Load a checkpoint file. Can be overwritten by subclasses to support
        different formats.
        Args:
            f (str): a locally mounted file path.
        Returns:
            dict: with keys "model" and optionally others that are saved by
                the checkpointer dict["model"] must be a dict which maps strings
                to torch.Tensor or numpy arrays.
        """
        return torch.load(f, map_location=torch.device("cpu"))

    def _load_model(self, checkpoint: Any):
        """
        Load weights from a checkpoint.

        Args:
            checkpoint (Any): checkpoint contains the weights.
        """
        checkpoint_state_dict = checkpoint.pop("model")

        self._convert_ndarray_to_tensor(checkpoint_state_dict)

        # if the state_dict comes from a model that was wrapped in a
        # DataParallel or DistributedDataParallel during serialization,
        # remove the "module" prefix before performing the matching.
        _strip_prefix_if_present(checkpoint_state_dict, "module.")

        # work around https://github.com/pytorch/pytorch/issues/24139
        model_state_dict = self.model.state_dict()
        for k in list(checkpoint_state_dict.keys()):
            if k in model_state_dict:
                shape_model = tuple(model_state_dict[k].shape)
                shape_checkpoint = tuple(checkpoint_state_dict[k].shape)
                if shape_model != shape_checkpoint:
                    self.logger.warning(
                        "'{}' has shape {} in the checkpoint but {} in the "
                        "model! Skipped.".format(
                            k, shape_checkpoint, shape_model
                        )
                    )
                    checkpoint_state_dict.pop(k)

        incompatible = self.model.load_state_dict(
            checkpoint_state_dict, strict=False
        )
        if incompatible.missing_keys:
            self.logger.info(
                get_missing_parameters_message(incompatible.missing_keys)
            )
        if incompatible.unexpected_keys:
            self.logger.info(
                get_unexpected_parameters_message(incompatible.unexpected_keys)
            )

    def _convert_ndarray_to_tensor(self, state_dict: dict):
        """
        In-place convert all numpy arrays in the state_dict to torch tensor.
        Args:
            state_dict (dict): a state-dict to be loaded to the model.
        """
        # model could be an OrderedDict with _metadata attribute
        # (as returned by Pytorch's state_dict()). We should preserve these
        # properties.
        for k in list(state_dict.keys()):
            if "weight_order" in k:
                continue
            v = state_dict[k]
            if not isinstance(v, np.ndarray) and not isinstance(
                v, torch.Tensor
            ):
                raise ValueError(
                    "Unsupported type found in checkpoint! {}: {}".format(
                        k, type(v)
                    )
                )
            if not isinstance(v, torch.Tensor):
                state_dict[k] = torch.from_numpy(v)


class PeriodicCheckpointer:
    """
    Save checkpoints periodically. When `.step(iteration)` is called, it will
    execute `checkpointer.save` on the given checkpointer, if iteration is a
    multiple of period or if `max_iter` is reached.
    """

    def __init__(self,
                 checkpointer: Any,
                 period: int,
                 max_iter: int = None,
                 max_epoch: Optional[int] = None):
        """
        Args:
            checkpointer (Any): the checkpointer object used to save
            checkpoints.
            period (int): the period to save checkpoint.
            max_iter (int): maximum number of iterations. When it is reached,
                a checkpoint named "model_final" will be saved.
        """
        self.checkpointer = checkpointer
        self.period = int(period)
        self.max_iter = max_iter
        self.max_epoch = max_epoch

    def step(self, iteration: int, **kwargs: Any):
        """
        Perform the appropriate action at the given iteration.

        Args:
            iteration (int): the current iteration, ranged in [0, max_iter-1].
            kwargs (Any): extra data to save, same as in
                :meth:`Checkpointer.save`.
        """
        iteration = int(iteration)
        additional_state = {"iteration": iteration}
        additional_state.update(kwargs)
        if (iteration + 1) % self.period == 0:
            if self.max_epoch is not None:
                epoch_iters = self.max_iter // self.max_epoch
                curr_epoch = (iteration + 1) // epoch_iters
                ckpt_name = "model_epoch_{:04d}".format(curr_epoch)
            else:
                ckpt_name = "model_iter_{:07d}".format(iteration + 1)
            self.checkpointer.save(ckpt_name, **additional_state)
        if iteration >= self.max_iter - 1:
            self.checkpointer.save("model_final", **additional_state)

    def save(self, name: str, **kwargs: Any):
        """
        Same argument as :meth:`Checkpointer.save`.
        Use this method to manually save checkpoints outside the schedule.

        Args:
            name (str): file name.
            kwargs (Any): extra data to save, same as in
                :meth:`Checkpointer.save`.
        """
        self.checkpointer.save(name, **kwargs)


class DefaultCheckpointer(Checkpointer):
    """
    Same as :class:`Checkpointer`, but is able to handle models in detectron & cvpods
    model zoo, and apply conversions for legacy models.
    """

    def __init__(self, model, save_dir="", resume=False, *, save_to_disk=None, **checkpointables):
        """
        Args:
            model (nn.Module): model.
            save_dir (str): a directory to save and find checkpoints.
            resume (bool): indicate whether to resume from latest checkpoint or start from scratch.
            save_to_disk (bool): if True, save checkpoint to disk, otherwise
                disable saving for this checkpointer.
            checkpointables (object): any checkpointable objects, i.e., objects
                that have the `state_dict()` and `load_state_dict()` method. For
                example, it can be used like
                `Checkpointer(model, "dir", optimizer=optimizer)`.
        """
        is_main_process = comm.is_main_process()
        super().__init__(
            model,
            save_dir,
            resume,
            save_to_disk=is_main_process if save_to_disk is None else save_to_disk,
            **checkpointables,
        )

    def _load_file(self, filename):
        """
        Args:
            filename (str): load checkpoint file from local or oss. checkpoint can be of type
                pkl, pth
        """
        if filename.endswith(".pkl"):
            with PathManager.open(filename, "rb") as f:
                data = pickle.load(f, encoding="latin1")
            if "model" in data and "__author__" in data:
                # file is in cvpods model zoo format
                self.logger.info("Reading a file from '{}'".format(data["__author__"]))
                return data
            else:
                # assume file is from Caffe2 / Detectron1 model zoo
                if "blobs" in data:
                    # Detection models have "blobs", but ImageNet models don't
                    data = data["blobs"]
                data = {k: v for k, v in data.items() if not k.endswith("_momentum")}
                return {"model": data, "__author__": "Caffe2", "matching_heuristics": True}
        elif filename.endswith(".pth"):
            if filename.startswith("s3://"):
                with PathManager.open(filename, "rb") as f:
                    loaded = torch.load(f, map_location=torch.device("cpu"))
            else:
                loaded = super()._load_file(filename)  # load native pth checkpoint
            if "model" not in loaded:
                loaded = {"model": loaded}
            return loaded

    def _load_model(self, checkpoint):
        """
        Args:
            checkpoint (dict): model state dict.
        """
        if checkpoint.get("matching_heuristics", False):
            self._convert_ndarray_to_tensor(checkpoint["model"])
            # convert weights by name-matching heuristics
            model_state_dict = self.model.state_dict()
            align_and_update_state_dicts(
                model_state_dict,
                checkpoint["model"],
                c2_conversion=checkpoint.get("__author__", None) == "Caffe2",
            )
            checkpoint["model"] = model_state_dict
        # for non-caffe2 models, use standard ways to load it
        super()._load_model(checkpoint)
```

### cvpods/checkpoint/c2_model_loading.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import copy
import logging
import re

import torch

from .utils import (  # isort:skip
    get_missing_parameters_message,
    get_unexpected_parameters_message,
)


def convert_basic_c2_names(original_keys):
    """
    Apply some basic name conversion to names in C2 weights.
    It only deals with typical backbone models.

    Args:
        original_keys (list[str]):
    Returns:
        list[str]: The same number of strings matching those in original_keys.
    """
    layer_keys = copy.deepcopy(original_keys)
    layer_keys = [
        {"pred_b": "linear_b", "pred_w": "linear_w"}.get(k, k) for k in layer_keys
    ]  # some hard-coded mappings

    layer_keys = [k.replace("_", ".") for k in layer_keys]
    layer_keys = [re.sub("\\.b$", ".bias", k) for k in layer_keys]
    layer_keys = [re.sub("\\.w$", ".weight", k) for k in layer_keys]
    # Uniform both bn and gn names to "norm"
    layer_keys = [re.sub("bn\\.s$", "norm.weight", k) for k in layer_keys]
    layer_keys = [re.sub("bn\\.bias$", "norm.bias", k) for k in layer_keys]
    layer_keys = [re.sub("bn\\.rm", "norm.running_mean", k) for k in layer_keys]
    layer_keys = [re.sub("bn\\.running.mean$", "norm.running_mean", k) for k in layer_keys]
    layer_keys = [re.sub("bn\\.riv$", "norm.running_var", k) for k in layer_keys]
    layer_keys = [re.sub("bn\\.running.var$", "norm.running_var", k) for k in layer_keys]
    layer_keys = [re.sub("bn\\.gamma$", "norm.weight", k) for k in layer_keys]
    layer_keys = [re.sub("bn\\.beta$", "norm.bias", k) for k in layer_keys]
    layer_keys = [re.sub("gn\\.s$", "norm.weight", k) for k in layer_keys]
    layer_keys = [re.sub("gn\\.bias$", "norm.bias", k) for k in layer_keys]

    # stem
    layer_keys = [re.sub("^res\\.conv1\\.norm\\.", "conv1.norm.", k) for k in layer_keys]
    # to avoid mis-matching with "conv1" in other components (e.g. detection head)
    layer_keys = [re.sub("^conv1\\.", "stem.conv1.", k) for k in layer_keys]

    # layer1-4 is used by torchvision, however we follow the C2 naming strategy (res2-5)
    # layer_keys = [re.sub("^res2.", "layer1.", k) for k in layer_keys]
    # layer_keys = [re.sub("^res3.", "layer2.", k) for k in layer_keys]
    # layer_keys = [re.sub("^res4.", "layer3.", k) for k in layer_keys]
    # layer_keys = [re.sub("^res5.", "layer4.", k) for k in layer_keys]

    # blocks
    layer_keys = [k.replace(".branch1.", ".shortcut.") for k in layer_keys]
    layer_keys = [k.replace(".branch2a.", ".conv1.") for k in layer_keys]
    layer_keys = [k.replace(".branch2b.", ".conv2.") for k in layer_keys]
    layer_keys = [k.replace(".branch2c.", ".conv3.") for k in layer_keys]

    # DensePose substitutions
    layer_keys = [re.sub("^body.conv.fcn", "body_conv_fcn", k) for k in layer_keys]
    layer_keys = [k.replace("AnnIndex.lowres", "ann_index_lowres") for k in layer_keys]
    layer_keys = [k.replace("Index.UV.lowres", "index_uv_lowres") for k in layer_keys]
    layer_keys = [k.replace("U.lowres", "u_lowres") for k in layer_keys]
    layer_keys = [k.replace("V.lowres", "v_lowres") for k in layer_keys]
    return layer_keys


def convert_c2_detectron_names(weights):
    """
    Map Caffe2 Detectron weight names to cvpods names.

    Args:
        weights (dict): name -> tensor

    Returns:
        dict: cvpods names -> tensor
        dict: cvpods names -> C2 names
    """
    logger = logging.getLogger(__name__)
    logger.info("Remapping C2 weights ......")
    original_keys = sorted(weights.keys())
    layer_keys = copy.deepcopy(original_keys)

    layer_keys = convert_basic_c2_names(layer_keys)

    # --------------------------------------------------------------------------
    # RPN hidden representation conv
    # --------------------------------------------------------------------------
    # FPN case
    # In the C2 model, the RPN hidden layer conv is defined for FPN level 2 and then
    # shared for all other levels, hence the appearance of "fpn2"
    layer_keys = [
        k.replace("conv.rpn.fpn2", "proposal_generator.rpn_head.conv") for k in layer_keys
    ]
    # Non-FPN case
    layer_keys = [k.replace("conv.rpn", "proposal_generator.rpn_head.conv") for k in layer_keys]

    # --------------------------------------------------------------------------
    # RPN box transformation conv
    # --------------------------------------------------------------------------
    # FPN case (see note above about "fpn2")
    layer_keys = [
        k.replace("rpn.bbox.pred.fpn2", "proposal_generator.rpn_head.anchor_deltas")
        for k in layer_keys
    ]
    layer_keys = [
        k.replace("rpn.cls.logits.fpn2", "proposal_generator.rpn_head.objectness_logits")
        for k in layer_keys
    ]
    # Non-FPN case
    layer_keys = [
        k.replace("rpn.bbox.pred", "proposal_generator.rpn_head.anchor_deltas") for k in layer_keys
    ]
    layer_keys = [
        k.replace("rpn.cls.logits", "proposal_generator.rpn_head.objectness_logits")
        for k in layer_keys
    ]

    # --------------------------------------------------------------------------
    # Fast R-CNN box head
    # --------------------------------------------------------------------------
    layer_keys = [re.sub("^bbox\\.pred", "bbox_pred", k) for k in layer_keys]
    layer_keys = [re.sub("^cls\\.score", "cls_score", k) for k in layer_keys]
    layer_keys = [re.sub("^fc6\\.", "box_head.fc1.", k) for k in layer_keys]
    layer_keys = [re.sub("^fc7\\.", "box_head.fc2.", k) for k in layer_keys]
    # 4conv1fc head tensor names: head_conv1_w, head_conv1_gn_s
    layer_keys = [re.sub("^head\\.conv", "box_head.conv", k) for k in layer_keys]

    # --------------------------------------------------------------------------
    # FPN lateral and output convolutions
    # --------------------------------------------------------------------------
    def fpn_map(name):
        """
        Look for keys with the following patterns:
        1) Starts with "fpn.inner."
           Example: "fpn.inner.res2.2.sum.lateral.weight"
           Meaning: These are lateral pathway convolutions
        2) Starts with "fpn.res"
           Example: "fpn.res2.2.sum.weight"
           Meaning: These are FPN output convolutions
        """
        splits = name.split(".")
        norm = ".norm" if "norm" in splits else ""
        if name.startswith("fpn.inner."):
            # splits example: ['fpn', 'inner', 'res2', '2', 'sum', 'lateral', 'weight']
            stage = int(splits[2][len("res"):])
            return "fpn_lateral{}{}.{}".format(stage, norm, splits[-1])
        elif name.startswith("fpn.res"):
            # splits example: ['fpn', 'res2', '2', 'sum', 'weight']
            stage = int(splits[1][len("res"):])
            return "fpn_output{}{}.{}".format(stage, norm, splits[-1])
        return name

    layer_keys = [fpn_map(k) for k in layer_keys]

    # --------------------------------------------------------------------------
    # Mask R-CNN mask head
    # --------------------------------------------------------------------------
    # roi_heads.StandardROIHeads case
    layer_keys = [k.replace(".[mask].fcn", "mask_head.mask_fcn") for k in layer_keys]
    layer_keys = [re.sub("^\\.mask\\.fcn", "mask_head.mask_fcn", k) for k in layer_keys]
    layer_keys = [k.replace("mask.fcn.logits", "mask_head.predictor") for k in layer_keys]
    # roi_heads.Res5ROIHeads case
    layer_keys = [k.replace("conv5.mask", "mask_head.deconv") for k in layer_keys]

    # --------------------------------------------------------------------------
    # Keypoint R-CNN head
    # --------------------------------------------------------------------------
    # interestingly, the keypoint head convs have blob names that are simply "conv_fcnX"
    layer_keys = [k.replace("conv.fcn", "roi_heads.keypoint_head.conv_fcn") for k in layer_keys]
    layer_keys = [
        k.replace("kps.score.lowres", "roi_heads.keypoint_head.score_lowres") for k in layer_keys
    ]
    layer_keys = [k.replace("kps.score.", "roi_heads.keypoint_head.score.") for k in layer_keys]

    # --------------------------------------------------------------------------
    # Done with replacements
    # --------------------------------------------------------------------------
    assert len(set(layer_keys)) == len(layer_keys)
    assert len(original_keys) == len(layer_keys)

    new_weights = {}
    new_keys_to_original_keys = {}
    for orig, renamed in zip(original_keys, layer_keys):
        new_keys_to_original_keys[renamed] = orig
        if renamed.startswith("bbox_pred.") or renamed.startswith("mask_head.predictor."):
            # remove the meaningless prediction weight for background class
            new_start_idx = 4 if renamed.startswith("bbox_pred.") else 1
            new_weights[renamed] = weights[orig][new_start_idx:]
            logger.info(
                "Remove prediction weight for background class in {}. The shape changes from "
                "{} to {}.".format(
                    renamed, tuple(weights[orig].shape), tuple(new_weights[renamed].shape)
                )
            )
        elif renamed.startswith("cls_score."):
            # move weights of bg class from original index 0 to last index
            logger.info(
                "Move classification weights for background class in {} from index 0 to "
                "index {}.".format(renamed, weights[orig].shape[0] - 1)
            )
            new_weights[renamed] = torch.cat([weights[orig][1:], weights[orig][:1]])
        else:
            new_weights[renamed] = weights[orig]

    return new_weights, new_keys_to_original_keys


# Note the current matching is not symmetric.
# it assumes model_state_dict will have longer names.
def align_and_update_state_dicts(model_state_dict, ckpt_state_dict, c2_conversion=True):
    """
    Match names between the two state-dict, and update the values of model_state_dict in-place with
    copies of the matched tensor in ckpt_state_dict.
    If `c2_conversion==True`, `ckpt_state_dict` is assumed to be a Caffe2
    model and will be renamed at first.

    Strategy: suppose that the models that we will create will have prefixes appended
    to each of its keys, for example due to an extra level of nesting that the original
    pre-trained weights from ImageNet won't contain. For example, model.state_dict()
    might return backbone[0].body.res2.conv1.weight, while the pre-trained model contains
    res2.conv1.weight. We thus want to match both parameters together.
    For that, we look for each model weight, look among all loaded keys if there is one
    that is a suffix of the current weight name, and use it if that's the case.
    If multiple matches exist, take the one with longest size
    of the corresponding name. For example, for the same model as before, the pretrained
    weight file can contain both res2.conv1.weight, as well as conv1.weight. In this case,
    we want to match backbone[0].body.conv1.weight to conv1.weight, and
    backbone[0].body.res2.conv1.weight to res2.conv1.weight.
    """
    model_keys = sorted(model_state_dict.keys())
    if c2_conversion:
        ckpt_state_dict, original_keys = convert_c2_detectron_names(ckpt_state_dict)
        # original_keys: the name in the original dict (before renaming)
    else:
        original_keys = {x: x for x in ckpt_state_dict.keys()}
    ckpt_keys = sorted(ckpt_state_dict.keys())

    def match(a, b):
        # Matched ckpt_key should be a complete (starts with '.') suffix.
        # For example, roi_heads.mesh_head.whatever_conv1 does not match conv1,
        # but matches whatever_conv1 or mesh_head.whatever_conv1.
        return a == b or a.endswith("." + b)

    # get a matrix of string matches, where each (i, j) entry correspond to the size of the
    # ckpt_key string, if it matches
    match_matrix = [len(j) if match(i, j) else 0 for i in model_keys for j in ckpt_keys]
    match_matrix = torch.as_tensor(match_matrix).view(len(model_keys), len(ckpt_keys))
    # use the matched one with longest size in case of multiple matches
    max_match_size, idxs = match_matrix.max(1)
    # remove indices that correspond to no-match
    idxs[max_match_size == 0] = -1

    # used for logging
    max_len_model = max(len(key) for key in model_keys) if model_keys else 1
    max_len_ckpt = max(len(key) for key in ckpt_keys) if ckpt_keys else 1
    log_str_template = "{: <{}} loaded from {: <{}} of shape {}"
    logger = logging.getLogger(__name__)
    # matched_pairs (matched checkpoint key --> matched model key)
    matched_keys = {}
    for idx_model, idx_ckpt in enumerate(idxs.tolist()):
        if idx_ckpt == -1:
            continue
        key_model = model_keys[idx_model]
        key_ckpt = ckpt_keys[idx_ckpt]
        value_ckpt = ckpt_state_dict[key_ckpt]
        shape_in_model = model_state_dict[key_model].shape

        if shape_in_model != value_ckpt.shape:
            logger.warning(
                "Shape of {} in checkpoint is {}, while shape of {} in model is {}.".format(
                    key_ckpt, value_ckpt.shape, key_model, shape_in_model
                )
            )
            logger.warning(
                "{} will not be loaded. Please double check and see if this is desired.".format(
                    key_ckpt
                )
            )
            continue

        model_state_dict[key_model] = value_ckpt.clone()
        if key_ckpt in matched_keys:  # already added to matched_keys
            logger.error(
                "Ambiguity found for {} in checkpoint!"
                "It matches at least two keys in the model ({} and {}).".format(
                    key_ckpt, key_model, matched_keys[key_ckpt]
                )
            )
            raise ValueError("Cannot match one checkpoint key to multiple keys in the model.")

        matched_keys[key_ckpt] = key_model
        logger.info(
            log_str_template.format(
                key_model,
                max_len_model,
                original_keys[key_ckpt],
                max_len_ckpt,
                tuple(shape_in_model),
            )
        )
    matched_model_keys = matched_keys.values()
    matched_ckpt_keys = matched_keys.keys()
    # print warnings about unmatched keys on both side
    unmatched_model_keys = [k for k in model_keys if k not in matched_model_keys]
    if len(unmatched_model_keys):
        logger.info(get_missing_parameters_message(unmatched_model_keys))

    unmatched_ckpt_keys = [k for k in ckpt_keys if k not in matched_ckpt_keys]
    if len(unmatched_ckpt_keys):
        logger.info(
            get_unexpected_parameters_message(original_keys[x] for x in unmatched_ckpt_keys)
        )
```

### cvpods/checkpoint/__init__.py

```python
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
# File:


from . import catalog as _UNUSED  # register the handler
from .checkpoint import Checkpointer, DefaultCheckpointer, PeriodicCheckpointer

__all__ = [
    "Checkpointer",
    "PeriodicCheckpointer",
    "DefaultCheckpointer",
]
```

### cvpods/checkpoint/utils.py

```python
#!/usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.

import collections
from collections import defaultdict
from termcolor import colored


def get_missing_parameters_message(keys: list):
    """
    Get a logging-friendly message to report parameter names (keys) that are in
    the model but not found in a checkpoint.
    Args:
        keys (list[str]): List of keys that were not found in the checkpoint.
    Returns:
        str: message.
    """
    groups = _group_checkpoint_keys(keys)
    msg = "Some model parameters are not in the checkpoint:\n"
    msg += "\n".join(
        "  " + colored(k + _group_to_str(v), "blue") for k, v in groups.items()
    )
    return msg


def get_unexpected_parameters_message(keys: list):
    """
    Get a logging-friendly message to report parameter names (keys) that are in
    the checkpoint but not found in the model.
    Args:
        keys (list[str]): List of keys that were not found in the model.
    Returns:
        str: message.
    """
    groups = _group_checkpoint_keys(keys)
    msg = "The checkpoint contains parameters not used by the model:\n"
    msg += "\n".join(
        "  " + colored(k + _group_to_str(v), "magenta")
        for k, v in groups.items()
    )
    return msg


def _strip_prefix_if_present(state_dict: collections.OrderedDict, prefix: str):
    """
    Strip the prefix in metadata, if any.
    Args:
        state_dict (OrderedDict): a state-dict to be loaded to the model.
        prefix (str): prefix.
    """
    keys = sorted(state_dict.keys())
    if not all(len(key) == 0 or key.startswith(prefix) for key in keys):
        return

    for key in keys:
        newkey = key[len(prefix):]
        state_dict[newkey] = state_dict.pop(key)

    # also strip the prefix in metadata, if any..
    try:
        metadata = state_dict._metadata
    except AttributeError:
        pass
    else:
        for key in list(metadata.keys()):
            # for the metadata dict, the key can be:
            # '': for the DDP module, which we want to remove.
            # 'module': for the actual model.
            # 'module.xx.xx': for the rest.

            if len(key) == 0:
                continue
            newkey = key[len(prefix):]
            metadata[newkey] = metadata.pop(key)


def _group_checkpoint_keys(keys: list):
    """
    Group keys based on common prefixes. A prefix is the string up to the final
    "." in each key.
    Args:
        keys (list[str]): list of parameter names, i.e. keys in the model
            checkpoint dict.
    Returns:
        dict[list]: keys with common prefixes are grouped into lists.
    """
    groups = defaultdict(list)
    for key in keys:
        pos = key.rfind(".")
        if pos >= 0:
            head, tail = key[:pos], [key[pos + 1:]]
        else:
            head, tail = key, []
        groups[head].extend(tail)
    return groups


def _group_to_str(group: list):
    """
    Format a group of parameter name suffixes into a loggable string.
    Args:
        group (list[str]): list of parameter name suffixes.
    Returns:
        str: formated string.
    """
    if len(group) == 0:
        return ""

    if len(group) == 1:
        return "." + group[0]

    return ".{" + ", ".join(group) + "}"
```

### cvpods/layers/deform_conv.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import math
from functools import lru_cache

import torch
from torch import nn
from torch.autograd import Function
from torch.autograd.function import once_differentiable
from torch.nn.modules.utils import _pair

from cvpods import _C

from .wrappers import _NewEmptyTensorOp


class _DeformConv(Function):
    @staticmethod
    def forward(
        ctx,
        input,
        offset,
        weight,
        stride=1,
        padding=0,
        dilation=1,
        groups=1,
        deformable_groups=1,
        im2col_step=64,
    ):
        if input is not None and input.dim() != 4:
            raise ValueError(
                "Expected 4D tensor as input, got {}D tensor instead.".format(input.dim())
            )
        ctx.stride = _pair(stride)
        ctx.padding = _pair(padding)
        ctx.dilation = _pair(dilation)
        ctx.groups = groups
        ctx.deformable_groups = deformable_groups
        ctx.im2col_step = im2col_step

        ctx.save_for_backward(input, offset, weight)

        output = input.new_empty(
            _DeformConv._output_size(input, weight, ctx.padding, ctx.dilation, ctx.stride)
        )

        ctx.bufs_ = [input.new_empty(0), input.new_empty(0)]  # columns, ones

        if not input.is_cuda:
            raise NotImplementedError
        else:
            cur_im2col_step = _DeformConv._cal_im2col_step(input.shape[0], ctx.im2col_step)
            assert (input.shape[0] % cur_im2col_step) == 0, "im2col step must divide batchsize"

            _C.deform_conv_forward(
                input,
                weight,
                offset,
                output,
                ctx.bufs_[0],
                ctx.bufs_[1],
                weight.size(3),
                weight.size(2),
                ctx.stride[1],
                ctx.stride[0],
                ctx.padding[1],
                ctx.padding[0],
                ctx.dilation[1],
                ctx.dilation[0],
                ctx.groups,
                ctx.deformable_groups,
                cur_im2col_step,
            )
        return output

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        input, offset, weight = ctx.saved_tensors

        grad_input = grad_offset = grad_weight = None

        if not grad_output.is_cuda:
            raise NotImplementedError
        else:
            cur_im2col_step = _DeformConv._cal_im2col_step(input.shape[0], ctx.im2col_step)
            assert (input.shape[0] % cur_im2col_step) == 0, "im2col step must divide batchsize"

            if ctx.needs_input_grad[0] or ctx.needs_input_grad[1]:
                grad_input = torch.zeros_like(input)
                grad_offset = torch.zeros_like(offset)
                _C.deform_conv_backward_input(
                    input,
                    offset,
                    grad_output,
                    grad_input,
                    grad_offset,
                    weight,
                    ctx.bufs_[0],
                    weight.size(3),
                    weight.size(2),
                    ctx.stride[1],
                    ctx.stride[0],
                    ctx.padding[1],
                    ctx.padding[0],
                    ctx.dilation[1],
                    ctx.dilation[0],
                    ctx.groups,
                    ctx.deformable_groups,
                    cur_im2col_step,
                )

            if ctx.needs_input_grad[2]:
                grad_weight = torch.zeros_like(weight)
                _C.deform_conv_backward_filter(
                    input,
                    offset,
                    grad_output,
                    grad_weight,
                    ctx.bufs_[0],
                    ctx.bufs_[1],
                    weight.size(3),
                    weight.size(2),
                    ctx.stride[1],
                    ctx.stride[0],
                    ctx.padding[1],
                    ctx.padding[0],
                    ctx.dilation[1],
                    ctx.dilation[0],
                    ctx.groups,
                    ctx.deformable_groups,
                    1,
                    cur_im2col_step,
                )

        return grad_input, grad_offset, grad_weight, None, None, None, None, None, None

    @staticmethod
    def _output_size(input, weight, padding, dilation, stride):
        channels = weight.size(0)
        output_size = (input.size(0), channels)
        for d in range(input.dim() - 2):
            in_size = input.size(d + 2)
            pad = padding[d]
            kernel = dilation[d] * (weight.size(d + 2) - 1) + 1
            stride_ = stride[d]
            output_size += ((in_size + (2 * pad) - kernel) // stride_ + 1,)
        if not all(map(lambda s: s > 0, output_size)):
            raise ValueError(
                "convolution input is too small (output would be {})".format(
                    "x".join(map(str, output_size))
                )
            )
        return output_size

    @staticmethod
    @lru_cache(maxsize=128)
    def _cal_im2col_step(input_size, default_size):
        """
        Calculate proper im2col step size, which should be divisible by input_size and not larger
        than prefer_size. Meanwhile the step size should be as large as possible to be more
        efficient. So we choose the largest one among all divisors of input_size which are smaller
        than prefer_size.
        :param input_size: input batch size .
        :param default_size: default preferred im2col step size.
        :return: the largest proper step size.
        """
        if input_size <= default_size:
            return input_size
        best_step = 1
        for step in range(2, min(int(math.sqrt(input_size)) + 1, default_size)):
            if input_size % step == 0:
                if input_size // step <= default_size:
                    return input_size // step
                best_step = step

        return best_step


class _ModulatedDeformConv(Function):
    @staticmethod
    def forward(
        ctx,
        input,
        offset,
        mask,
        weight,
        bias=None,
        stride=1,
        padding=0,
        dilation=1,
        groups=1,
        deformable_groups=1,
    ):
        ctx.stride = stride
        ctx.padding = padding
        ctx.dilation = dilation
        ctx.groups = groups
        ctx.deformable_groups = deformable_groups
        ctx.with_bias = bias is not None
        if not ctx.with_bias:
            bias = input.new_empty(1)  # fake tensor
        if not input.is_cuda:
            raise NotImplementedError
        if (
            weight.requires_grad
            or mask.requires_grad
            or offset.requires_grad
            or input.requires_grad
        ):
            ctx.save_for_backward(input, offset, mask, weight, bias)
        output = input.new_empty(_ModulatedDeformConv._infer_shape(ctx, input, weight))
        ctx._bufs = [input.new_empty(0), input.new_empty(0)]
        _C.modulated_deform_conv_forward(
            input,
            weight,
            bias,
            ctx._bufs[0],
            offset,
            mask,
            output,
            ctx._bufs[1],
            weight.shape[2],
            weight.shape[3],
            ctx.stride,
            ctx.stride,
            ctx.padding,
            ctx.padding,
            ctx.dilation,
            ctx.dilation,
            ctx.groups,
            ctx.deformable_groups,
            ctx.with_bias,
        )
        return output

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        if not grad_output.is_cuda:
            raise NotImplementedError
        input, offset, mask, weight, bias = ctx.saved_tensors
        grad_input = torch.zeros_like(input)
        grad_offset = torch.zeros_like(offset)
        grad_mask = torch.zeros_like(mask)
        grad_weight = torch.zeros_like(weight)
        grad_bias = torch.zeros_like(bias)
        _C.modulated_deform_conv_backward(
            input,
            weight,
            bias,
            ctx._bufs[0],
            offset,
            mask,
            ctx._bufs[1],
            grad_input,
            grad_weight,
            grad_bias,
            grad_offset,
            grad_mask,
            grad_output,
            weight.shape[2],
            weight.shape[3],
            ctx.stride,
            ctx.stride,
            ctx.padding,
            ctx.padding,
            ctx.dilation,
            ctx.dilation,
            ctx.groups,
            ctx.deformable_groups,
            ctx.with_bias,
        )
        if not ctx.with_bias:
            grad_bias = None

        return (
            grad_input,
            grad_offset,
            grad_mask,
            grad_weight,
            grad_bias,
            None,
            None,
            None,
            None,
            None,
        )

    @staticmethod
    def _infer_shape(ctx, input, weight):
        n = input.size(0)
        channels_out = weight.size(0)
        height, width = input.shape[2:4]
        kernel_h, kernel_w = weight.shape[2:4]
        height_out = (
            height + 2 * ctx.padding - (ctx.dilation * (kernel_h - 1) + 1)
        ) // ctx.stride + 1
        width_out = (
            width + 2 * ctx.padding - (ctx.dilation * (kernel_w - 1) + 1)
        ) // ctx.stride + 1
        return n, channels_out, height_out, width_out


deform_conv = _DeformConv.apply
modulated_deform_conv = _ModulatedDeformConv.apply


class DeformConv(nn.Module):
    def __init__(
        self,
        in_channels,
        out_channels,
        kernel_size,
        stride=1,
        padding=0,
        dilation=1,
        groups=1,
        deformable_groups=1,
        bias=False,
        norm=None,
        activation=None,
    ):
        """
        Deformable convolution.

        Arguments are similar to :class:`Conv2D`. Extra arguments:

        Args:
            deformable_groups (int): number of groups used in deformable convolution.
            norm (nn.Module, optional): a normalization layer
            activation (callable(Tensor) -> Tensor): a callable activation function
        """
        super(DeformConv, self).__init__()

        assert not bias
        assert in_channels % groups == 0, "in_channels {} cannot be divisible by groups {}".format(
            in_channels, groups
        )
        assert (
            out_channels % groups == 0
        ), "out_channels {} cannot be divisible by groups {}".format(out_channels, groups)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = _pair(kernel_size)
        self.stride = _pair(stride)
        self.padding = _pair(padding)
        self.dilation = _pair(dilation)
        self.groups = groups
        self.deformable_groups = deformable_groups
        self.norm = norm
        self.activation = activation

        self.weight = nn.Parameter(
            torch.Tensor(out_channels, in_channels // self.groups, *self.kernel_size)
        )
        self.bias = None

        nn.init.kaiming_uniform_(self.weight, nonlinearity="relu")

    def forward(self, x, offset):
        if x.numel() == 0:
            # When input is empty, we want to return a empty tensor with "correct" shape,
            # So that the following operations will not panic
            # if they check for the shape of the tensor.
            # This computes the height and width of the output tensor
            output_shape = [
                (i + 2 * p - (di * (k - 1) + 1)) // s + 1
                for i, p, di, k, s in zip(
                    x.shape[-2:], self.padding, self.dilation, self.kernel_size, self.stride
                )
            ]
            output_shape = [x.shape[0], self.weight.shape[0]] + output_shape
            return _NewEmptyTensorOp.apply(x, output_shape)

        x = deform_conv(
            x,
            offset,
            self.weight,
            self.stride,
            self.padding,
            self.dilation,
            self.groups,
            self.deformable_groups,
        )
        if self.norm is not None:
            x = self.norm(x)
        if self.activation is not None:
            x = self.activation(x)
        return x

    def extra_repr(self):
        tmpstr = "in_channels=" + str(self.in_channels)
        tmpstr += ", out_channels=" + str(self.out_channels)
        tmpstr += ", kernel_size=" + str(self.kernel_size)
        tmpstr += ", stride=" + str(self.stride)
        tmpstr += ", padding=" + str(self.padding)
        tmpstr += ", dilation=" + str(self.dilation)
        tmpstr += ", groups=" + str(self.groups)
        tmpstr += ", deformable_groups=" + str(self.deformable_groups)
        tmpstr += ", bias=False"
        return tmpstr


class ModulatedDeformConv(nn.Module):
    def __init__(
        self,
        in_channels,
        out_channels,
        kernel_size,
        stride=1,
        padding=0,
        dilation=1,
        groups=1,
        deformable_groups=1,
        bias=True,
        norm=None,
        activation=None,
    ):
        """
        Modulated deformable convolution.

        Arguments are similar to :class:`Conv2D`. Extra arguments:

        Args:
            deformable_groups (int): number of groups used in deformable convolution.
            norm (nn.Module, optional): a normalization layer
            activation (callable(Tensor) -> Tensor): a callable activation function
        """
        super(ModulatedDeformConv, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = _pair(kernel_size)
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.groups = groups
        self.deformable_groups = deformable_groups
        self.with_bias = bias
        self.norm = norm
        self.activation = activation

        self.weight = nn.Parameter(
            torch.Tensor(out_channels, in_channels // groups, *self.kernel_size)
        )
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_channels))
        else:
            self.bias = None

        nn.init.kaiming_uniform_(self.weight, nonlinearity="relu")
        if self.bias is not None:
            nn.init.constant_(self.bias, 0)

    def forward(self, x, offset, mask):
        if x.numel() == 0:
            output_shape = [
                (i + 2 * p - (di * (k - 1) + 1)) // s + 1
                for i, p, di, k, s in zip(
                    x.shape[-2:], self.padding, self.dilation, self.kernel_size, self.stride
                )
            ]
            output_shape = [x.shape[0], self.weight.shape[0]] + output_shape
            return _NewEmptyTensorOp.apply(x, output_shape)

        x = modulated_deform_conv(
            x,
            offset,
            mask,
            self.weight,
            self.bias,
            self.stride,
            self.padding,
            self.dilation,
            self.groups,
            self.deformable_groups,
        )
        if self.norm is not None:
            x = self.norm(x)
        if self.activation is not None:
            x = self.activation(x)
        return x

    def extra_repr(self):
        tmpstr = "in_channels=" + str(self.in_channels)
        tmpstr += ", out_channels=" + str(self.out_channels)
        tmpstr += ", kernel_size=" + str(self.kernel_size)
        tmpstr += ", stride=" + str(self.stride)
        tmpstr += ", padding=" + str(self.padding)
        tmpstr += ", dilation=" + str(self.dilation)
        tmpstr += ", groups=" + str(self.groups)
        tmpstr += ", deformable_groups=" + str(self.deformable_groups)
        tmpstr += ", bias=" + str(self.with_bias)
        return tmpstr
```

### cvpods/layers/activation_funcs.py

```python
import torch
import torch.nn as nn


# Ref:
# https://medium.com/the-artificial-impostor/more-memory-efficient-swish-activation-function-e07c22c12a76
class SwishImplementation(torch.autograd.Function):
    """
    Swish activation function memory-efficient implementation.

    This implementation explicitly processes the gradient, it keeps a copy of the input tensor,
    and uses it to calculate the gradient during the back-propagation phase.
    """
    @staticmethod
    def forward(ctx, i):
        result = i * torch.sigmoid(i)
        ctx.save_for_backward(i)
        return result

    @staticmethod
    def backward(ctx, grad_output):
        i = ctx.saved_variables[0]
        sigmoid_i = torch.sigmoid(i)
        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))


class MemoryEfficientSwish(nn.Module):
    def forward(self, x):
        return SwishImplementation.apply(x)


class Swish(nn.Module):
    """
    Implement the Swish activation function.
    See: https://arxiv.org/abs/1710.05941 for more details.
    """
    def forward(self, x):
        return x * torch.sigmoid(x)
```

### cvpods/layers/shape_spec.py

```python
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from collections import namedtuple


class ShapeSpec(namedtuple("_ShapeSpec", ["channels", "height", "width", "stride"])):
    """
    A simple structure that contains basic shape specification about a tensor.
    It is often used as the auxiliary inputs/outputs of models,
    to obtain the shape inference ability among pytorch modules.

    Attributes:
        channels:
        height:
        width:
        stride:
    """

    def __new__(cls, *, channels=None, height=None, width=None, stride=None):
        return super().__new__(cls, channels, height, width, stride)
```

### cvpods/layers/psroi_pool.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import torch
from torch import nn
from torch.autograd import Function

from cvpods import _C


class _PSROIPool(Function):
    @staticmethod
    def forward(ctx, features, rois, output_size, spatial_scale, group_size, output_dim):
        ctx.pooled_width = int(output_size[0])
        ctx.pooled_height = int(output_size[1])
        ctx.spatial_scale = float(spatial_scale)
        ctx.group_size = int(group_size)
        ctx.output_dim = int(output_dim)

        batch_size, num_channels, data_height, data_width = features.size()
        num_rois = rois.size()[0]
        mapping_channel = torch.zeros(num_rois, ctx.output_dim,
                                      ctx.pooled_height, ctx.pooled_width).int()
        mapping_channel = mapping_channel.to(features.device)
        output = _C.psroi_pooling_forward_cuda(
            features, rois, mapping_channel,
            ctx.pooled_height, ctx.pooled_width,
            ctx.spatial_scale, ctx.group_size, ctx.output_dim
        )
        ctx.output = output
        ctx.mapping_channel = mapping_channel
        ctx.rois = rois
        ctx.feature_size = features.size()

        return output

    @staticmethod
    def backward(ctx, grad_output):
        assert(ctx.feature_size is not None and grad_output.is_cuda)

        batch_size, num_channels, data_height, data_width = ctx.feature_size

        grad_input = _C.psroi_pooling_backward_cuda(
            grad_output, ctx.rois, ctx.mapping_channel,
            batch_size, num_channels, data_height, data_width,
            ctx.spatial_scale
            # ctx.pooled_height, ctx.pooled_width, ctx.spatial_scale, ctx.output_dim
        )
        return grad_input, None, None, None, None, None


psroi_pool = _PSROIPool.apply


class PSROIPool(nn.Module):
    def __init__(self, output_size, spatial_scale, group_size, output_dim):
        super(PSROIPool, self).__init__()
        self.output_size = output_size
        self.spatial_scale = spatial_scale
        self.group_size = group_size
        self.output_dim = output_dim

    def forward(self, input, rois):
        """
        Args:
            input: NCHW images
            rois: Bx5 boxes. First column is the index into N. The other 4 columns are xyxy.
        """
        assert rois.dim() == 2 and rois.size(1) == 5
        return psroi_pool(
            input, rois, self.output_size, self.spatial_scale, self.group_size, self.output_dim
        )

    def __repr__(self):
        tmpstr = self.__class__.__name__ + "("
        tmpstr += "output_size=" + str(self.output_size)
        tmpstr += ", spatial_scale=" + str(self.spatial_scale)
        tmpstr += ", group_size=" + str(self.group_size)
        tmpstr += ", output_dim=" + str(self.output_dim)
        tmpstr += ")"
        return tmpstr
```

### cvpods/layers/swap_align2nat.py

```python
from torch import nn
from torch.autograd import Function
from torch.autograd.function import once_differentiable

from cvpods import _C


class _SwapAlign2Nat(Function):
    @staticmethod
    def forward(ctx, X, lambda_val, pad_val):
        ctx.lambda_val = lambda_val
        ctx.input_shape = X.size()

        Y = _C.swap_align2nat_forward(X, lambda_val, pad_val)
        return Y

    @staticmethod
    @once_differentiable
    def backward(ctx, gY):
        lambda_val = ctx.lambda_val
        bs, ch, h, w = ctx.input_shape

        gX = _C.swap_align2nat_backward(gY, lambda_val, bs, ch, h, w)

        return gX, None, None


swap_align2nat = _SwapAlign2Nat.apply


class SwapAlign2Nat(nn.Module):
    """
        The op `SwapAlign2Nat` described in https://arxiv.org/abs/1903.12174.
        Given an input tensor that predicts masks of shape (N, C=VxU, H, W),
        apply the op, it will return masks of shape (N, V'xU', H', W') where
        the unit lengths of (V, U) and (H, W) are swapped, and the mask representation
        is transformed from aligned to natural.
        Args:
            lambda_val (int): the relative unit length ratio between (V, U) and (H, W),
            as we always have larger unit lengths for (V, U) than (H, W),
            lambda_val is always >= 1.
            pad_val (float): padding value for the values falling outside of the input
            tensor, default set to -6 as sigmoid(-6) is ~0, indicating
            that is no masks outside of the tensor.
    """

    def __init__(self, lambda_val, pad_val=-6.0):
        super(SwapAlign2Nat, self).__init__()
        self.lambda_val = lambda_val
        self.pad_val = pad_val

    def forward(self, X):
        return swap_align2nat(X, self.lambda_val, self.pad_val)

    def __repr__(self):
        tmpstr = self.__class__.__name__ + "("
        tmpstr += "lambda_val=" + str(self.lambda_val)
        tmpstr += ", pad_val=" + str(self.pad_val)
        tmpstr += ")"
        return tmpstr
```

### cvpods/layers/roi_align.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from torch import nn
from torch.autograd import Function
from torch.autograd.function import once_differentiable
from torch.nn.modules.utils import _pair

from cvpods import _C
from cvpods.utils.apex_wrapper import float_function


class _ROIAlign(Function):
    @staticmethod
    @float_function
    def forward(ctx, input, roi, output_size, spatial_scale, sampling_ratio, aligned):
        ctx.save_for_backward(roi)
        ctx.output_size = _pair(output_size)
        ctx.spatial_scale = spatial_scale
        ctx.sampling_ratio = sampling_ratio
        ctx.input_shape = input.size()
        ctx.aligned = aligned
        output = _C.roi_align_forward(
            input, roi, spatial_scale, output_size[0], output_size[1], sampling_ratio, aligned
        )
        return output

    @staticmethod
    @once_differentiable
    @float_function
    def backward(ctx, grad_output):
        rois, = ctx.saved_tensors
        output_size = ctx.output_size
        spatial_scale = ctx.spatial_scale
        sampling_ratio = ctx.sampling_ratio
        bs, ch, h, w = ctx.input_shape
        grad_input = _C.roi_align_backward(
            grad_output,
            rois,
            spatial_scale,
            output_size[0],
            output_size[1],
            bs,
            ch,
            h,
            w,
            sampling_ratio,
            ctx.aligned,
        )
        return grad_input, None, None, None, None, None


roi_align = _ROIAlign.apply


class ROIAlign(nn.Module):
    def __init__(self, output_size, spatial_scale, sampling_ratio, aligned=True):
        """
        Args:
            output_size (tuple): h, w
            spatial_scale (float): scale the input boxes by this number
            sampling_ratio (int): number of inputs samples to take for each output
                sample. 0 to take samples densely.
            aligned (bool): if False, use the legacy implementation in
                Detectron. If True, align the results more perfectly.

        Note:
            The meaning of aligned=True:

            Given a continuous coordinate c, its two neighboring pixel indices (in our
            pixel model) are computed by floor(c - 0.5) and ceil(c - 0.5). For example,
            c=1.3 has pixel neighbors with discrete indices [0] and [1] (which are sampled
            from the underlying signal at continuous coordinates 0.5 and 1.5). But the original
            roi_align (aligned=False) does not subtract the 0.5 when computing neighboring
            pixel indices and therefore it uses pixels with a slightly incorrect alignment
            (relative to our pixel model) when performing bilinear interpolation.

            With `aligned=True`,
            we first appropriately scale the ROI and then shift it by -0.5
            prior to calling roi_align. This produces the correct neighbors; see
            cvpods/tests/test_roi_align.py for verification.

            The difference does not make a difference to the model's performance if
            ROIAlign is used together with conv layers.
        """
        super(ROIAlign, self).__init__()
        self.output_size = output_size
        self.spatial_scale = spatial_scale
        self.sampling_ratio = sampling_ratio
        self.aligned = aligned

    def forward(self, input, rois):
        """
        Args:
            input: NCHW images
            rois: Bx5 boxes. First column is the index into N. The other 4 columns are xyxy.
        """
        assert rois.dim() == 2 and rois.size(1) == 5
        return roi_align(
            input, rois, self.output_size, self.spatial_scale, self.sampling_ratio, self.aligned
        )

    def __repr__(self):
        tmpstr = self.__class__.__name__ + "("
        tmpstr += "output_size=" + str(self.output_size)
        tmpstr += ", spatial_scale=" + str(self.spatial_scale)
        tmpstr += ", sampling_ratio=" + str(self.sampling_ratio)
        tmpstr += ", aligned=" + str(self.aligned)
        tmpstr += ")"
        return tmpstr
```

### cvpods/layers/roi_align_rotated.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from torch import nn
from torch.autograd import Function
from torch.autograd.function import once_differentiable
from torch.nn.modules.utils import _pair

from cvpods import _C


class _ROIAlignRotated(Function):
    @staticmethod
    def forward(ctx, input, roi, output_size, spatial_scale, sampling_ratio):
        ctx.save_for_backward(roi)
        ctx.output_size = _pair(output_size)
        ctx.spatial_scale = spatial_scale
        ctx.sampling_ratio = sampling_ratio
        ctx.input_shape = input.size()
        output = _C.roi_align_rotated_forward(
            input, roi, spatial_scale, output_size[0], output_size[1], sampling_ratio
        )
        return output

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        rois, = ctx.saved_tensors
        output_size = ctx.output_size
        spatial_scale = ctx.spatial_scale
        sampling_ratio = ctx.sampling_ratio
        bs, ch, h, w = ctx.input_shape
        grad_input = _C.roi_align_rotated_backward(
            grad_output,
            rois,
            spatial_scale,
            output_size[0],
            output_size[1],
            bs,
            ch,
            h,
            w,
            sampling_ratio,
        )
        return grad_input, None, None, None, None, None


roi_align_rotated = _ROIAlignRotated.apply


class ROIAlignRotated(nn.Module):
    def __init__(self, output_size, spatial_scale, sampling_ratio):
        """
        Args:
            output_size (tuple): h, w
            spatial_scale (float): scale the input boxes by this number
            sampling_ratio (int): number of inputs samples to take for each output
                sample. 0 to take samples densely.

        Note:
            ROIAlignRotated supports continuous coordinate by default:
            Given a continuous coordinate c, its two neighboring pixel indices (in our
            pixel model) are computed by floor(c - 0.5) and ceil(c - 0.5). For example,
            c=1.3 has pixel neighbors with discrete indices [0] and [1] (which are sampled
            from the underlying signal at continuous coordinates 0.5 and 1.5).
        """
        super(ROIAlignRotated, self).__init__()
        self.output_size = output_size
        self.spatial_scale = spatial_scale
        self.sampling_ratio = sampling_ratio

    def forward(self, input, rois):
        """
        Args:
            input: NCHW images
            rois: Bx6 boxes. First column is the index into N.
                The other 5 columns are (x_ctr, y_ctr, width, height, angle_degrees).
        """
        assert rois.dim() == 2 and rois.size(1) == 6
        return roi_align_rotated(
            input, rois, self.output_size, self.spatial_scale, self.sampling_ratio
        )

    def __repr__(self):
        tmpstr = self.__class__.__name__ + "("
        tmpstr += "output_size=" + str(self.output_size)
        tmpstr += ", spatial_scale=" + str(self.spatial_scale)
        tmpstr += ", sampling_ratio=" + str(self.sampling_ratio)
        tmpstr += ")"
        return tmpstr
```

### cvpods/layers/position_encoding.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
"""
Various positional encodings for the transformer.
"""
import math

import torch
from torch import nn


class PositionEmbeddingSine(nn.Module):
    """
    This is a more standard version of the position embedding, very similar to the one
    used by the Attention is all you need paper, generalized to work on images.
    """

    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):
        super().__init__()
        self.num_pos_feats = num_pos_feats
        self.temperature = temperature
        self.normalize = normalize
        if scale is not None and normalize is False:
            raise ValueError("normalize should be True if scale is passed")
        if scale is None:
            scale = 2 * math.pi
        self.scale = scale

    def forward(self, x, mask):
        not_mask = ~mask
        y_embed = not_mask.cumsum(1, dtype=torch.float32)
        x_embed = not_mask.cumsum(2, dtype=torch.float32)

        if self.normalize:
            eps = 1e-6
            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale
            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale

        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)
        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)

        pos_x = x_embed[:, :, :, None] / dim_t
        pos_y = y_embed[:, :, :, None] / dim_t
        pos_x = torch.stack(
            (pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4
        ).flatten(3)
        pos_y = torch.stack(
            (pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4
        ).flatten(3)
        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)
        return pos


class PositionEmbeddingLearned(nn.Module):
    """
    Absolute pos embedding, learned.
    """

    def __init__(self, num_pos_feats=256, **kwargs):  # pylint: disable=unused-argument
        super().__init__()
        self.row_embed = nn.Embedding(50, num_pos_feats)
        self.col_embed = nn.Embedding(50, num_pos_feats)
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.uniform_(self.row_embed.weight)
        nn.init.uniform_(self.col_embed.weight)

    def forward(self, tensor_list):
        x = tensor_list.tensors
        h, w = x.shape[-2:]
        i = torch.arange(w, device=x.device)
        j = torch.arange(h, device=x.device)
        x_emb = self.col_embed(i)
        y_emb = self.row_embed(j)
        pos = (
            torch.cat(
                [x_emb.unsqueeze(0).repeat(h, 1, 1), y_emb.unsqueeze(1).repeat(1, w, 1)], dim=-1
            )
            .permute(2, 0, 1)
            .unsqueeze(0)
            .repeat(x.shape[0], 1, 1, 1)
        )
        return pos


position_encoding_dict = {"sine": PositionEmbeddingSine, "learned": PositionEmbeddingLearned}
```

### cvpods/layers/nms.py

```python
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

import torch
from torchvision.ops import boxes as box_ops
from torchvision.ops import nms  # BC-compat

from cvpods import _C
from cvpods.layers.rotated_boxes import pairwise_iou_rotated
from cvpods.utils.apex_wrapper import float_function

ml_nms = _C.ml_nms


@float_function
def batched_nms(boxes, scores, idxs, iou_threshold):
    """
    Same as torchvision.ops.boxes.batched_nms, but safer.
    """
    assert boxes.shape[-1] == 4
    # TODO may need better strategy.
    # Investigate after having a fully-cuda NMS op.
    if len(boxes) < 40000:
        return box_ops.batched_nms(boxes, scores, idxs, iou_threshold)

    result_mask = scores.new_zeros(scores.size(), dtype=torch.bool)
    for id in torch.unique(idxs).cpu().tolist():
        mask = (idxs == id).nonzero(as_tuple=False).view(-1)
        keep = nms(boxes[mask], scores[mask], iou_threshold)
        result_mask[mask[keep]] = True
    keep = result_mask.nonzero(as_tuple=False).view(-1)
    keep = keep[scores[keep].argsort(descending=True)]
    return keep


def batched_softnms(boxes, scores, idxs, iou_threshold,
                    score_threshold=0.001, soft_mode="gaussian"):
    assert soft_mode in ["linear", "gaussian"]
    assert boxes.shape[-1] == 4

    # change scores inplace
    # no need to return changed scores
    for id in torch.unique(idxs).cpu().tolist():
        mask = (idxs == id).nonzero(as_tuple=False).view(-1)
        scores[mask] = softnms(boxes[mask], scores[mask], iou_threshold,
                               score_threshold, soft_mode)

    keep = (scores > score_threshold).nonzero(as_tuple=False).view(-1)
    keep = keep[scores[keep].argsort(descending=True)]
    return keep


def batched_softnms_rotated(boxes, scores, idxs, iou_threshold,
                            score_threshold=0.001, soft_mode="gaussian"):
    assert soft_mode in ["linear", "gaussian"]
    assert boxes.shape[-1] == 5

    # change scores inplace
    # no need to return changed scores
    for id in torch.unique(idxs).cpu().tolist():
        mask = (idxs == id).nonzero(as_tuple=False).view(-1)
        scores[mask] = softnms_rotated(boxes[mask], scores[mask], iou_threshold,
                                       score_threshold, soft_mode)

    keep = (scores > score_threshold).nonzero(as_tuple=False).view(-1)
    keep = keep[scores[keep].argsort(descending=True)]
    return keep


def generalized_batched_nms(boxes, scores, idxs, iou_threshold,
                            score_threshold=0.001, nms_type="normal"):
    assert boxes.shape[-1] == 4

    if nms_type == "normal":
        keep = batched_nms(boxes, scores, idxs, iou_threshold)
    elif nms_type.startswith("softnms"):
        keep = batched_softnms(boxes, scores, idxs, iou_threshold,
                               score_threshold=score_threshold,
                               soft_mode=nms_type.lstrip("softnms-"))
    elif nms_type == "cluster":
        keep = batched_clusternms(boxes, scores, idxs, iou_threshold)
    else:
        raise NotImplementedError("NMS type not implemented: \"{}\"".format(nms_type))

    return keep


def iou(boxes, top_box):
    x1 = boxes[:, 0].clamp(min=top_box[0])
    y1 = boxes[:, 1].clamp(min=top_box[1])
    x2 = boxes[:, 2].clamp(max=top_box[2])
    y2 = boxes[:, 3].clamp(max=top_box[3])

    inters = (x2 - x1).clamp(min=0) * (y2 - y1).clamp(min=0)
    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
    unions = (top_box[2] - top_box[0]) * (top_box[3] - top_box[1]) + areas - inters

    return inters / unions


def scale_by_iou(ious, sigma, soft_mode="gaussian"):
    if soft_mode == "linear":
        scale = ious.new_ones(ious.size())
        scale[ious >= sigma] = 1 - ious[ious >= sigma]
    else:
        scale = torch.exp(-ious ** 2 / sigma)

    return scale


def softnms(boxes, scores, sigma, score_threshold, soft_mode="gaussian"):
    assert soft_mode in ["linear", "gaussian"]

    undone_mask = scores >= score_threshold
    while undone_mask.sum() > 1:
        idx = scores[undone_mask].argmax()
        idx = undone_mask.nonzero(as_tuple=False)[idx].item()
        top_box = boxes[idx]
        undone_mask[idx] = False
        _boxes = boxes[undone_mask]

        ious = iou(_boxes, top_box)
        scales = scale_by_iou(ious, sigma, soft_mode)

        scores[undone_mask] *= scales
        undone_mask[scores < score_threshold] = False
    return scores


def softnms_rotated(boxes, scores, sigma, score_threshold, soft_mode="gaussian"):
    assert soft_mode in ["linear", "gaussian"]

    iou_matrix = pairwise_iou_rotated(boxes, boxes)

    undone_mask = scores >= score_threshold
    while undone_mask.sum() > 1:
        idx = scores[undone_mask].argmax()
        idx = undone_mask.nonzero(as_tuple=False)[idx].item()
        undone_mask[idx] = False

        ious = iou_matrix[idx, undone_mask]
        scales = scale_by_iou(ious, sigma, soft_mode)

        scores[undone_mask] *= scales
        undone_mask[scores < score_threshold] = False
    return scores


def batched_clusternms(boxes, scores, idxs, iou_threshold):
    assert boxes.shape[-1] == 4

    result_mask = scores.new_zeros(scores.size(), dtype=torch.bool)
    for id in torch.unique(idxs).cpu().tolist():
        mask = (idxs == id).nonzero(as_tuple=False).view(-1)
        keep = cluster_nms(boxes[mask], scores[mask], iou_threshold)
        result_mask[mask[keep]] = True
    keep = result_mask.nonzero(as_tuple=False).view(-1)
    keep = keep[scores[keep].argsort(descending=True)]
    return keep


def cluster_nms(boxes, scores, iou_threshold):
    last_keep = torch.ones(*scores.shape).to(boxes.device)

    scores, idx = scores.sort(descending=True)
    boxes = boxes[idx]
    origin_iou_matrix = box_ops.box_iou(boxes, boxes).tril(diagonal=-1).transpose(1, 0)

    while True:
        iou_matrix = torch.mm(torch.diag(last_keep.float()), origin_iou_matrix)
        keep = (iou_matrix.max(dim=0)[0] <= iou_threshold)

        if (keep == last_keep).all():
            return idx[keep.nonzero(as_tuple=False)]

        last_keep = keep


# Note: this function (nms_rotated) might be moved into
# torchvision/ops/boxes.py in the future
def nms_rotated(boxes, scores, iou_threshold):
    r"""
    Performs non-maximum suppression (NMS) on the rotated boxes according
    to their intersection-over-union (IoU).

    Rotated NMS iteratively removes lower scoring rotated boxes which have an
    IoU greater than iou_threshold with another (higher scoring) rotated box.

    Note that RotatedBox (5, 3, 4, 2, -90) covers exactly the same region as
    RotatedBox (5, 3, 4, 2, 90) does, and their IoU will be 1. However, they
    can be representing completely different objects in certain tasks, e.g., OCR.

    As for the question of whether rotated-NMS should treat them as faraway boxes
    even though their IOU is 1, it depends on the application and/or ground truth annotation.

    As an extreme example, consider a single character v and the square box around it.

    If the angle is 0 degree, the object (text) would be read as 'v';

    If the angle is 90 degrees, the object (text) would become '>';

    If the angle is 180 degrees, the object (text) would become '^';

    If the angle is 270/-90 degrees, the object (text) would become '<'

    All of these cases have IoU of 1 to each other, and rotated NMS that only
    uses IoU as criterion would only keep one of them with the highest score -
    which, practically, still makes sense in most cases because typically
    only one of theses orientations is the correct one. Also, it does not matter
    as much if the box is only used to classify the object (instead of transcribing
    them with a sequential OCR recognition model) later.

    On the other hand, when we use IoU to filter proposals that are close to the
    ground truth during training, we should definitely take the angle into account if
    we know the ground truth is labeled with the strictly correct orientation (as in,
    upside-down words are annotated with -180 degrees even though they can be covered
    with a 0/90/-90 degree box, etc.)

    The way the original dataset is annotated also matters. For example, if the dataset
    is a 4-point polygon dataset that does not enforce ordering of vertices/orientation,
    we can estimate a minimum rotated bounding box to this polygon, but there's no way
    we can tell the correct angle with 100% confidence (as shown above, there could be 4 different
    rotated boxes, with angles differed by 90 degrees to each other, covering the exactly
    same region). In that case we have to just use IoU to determine the box
    proximity (as many detection benchmarks (even for text) do) unless there're other
    assumptions we can make (like width is always larger than height, or the object is not
    rotated by more than 90 degrees CCW/CW, etc.)

    In summary, not considering angles in rotated NMS seems to be a good option for now,
    but we should be aware of its implications.

    Args:
        boxes (Tensor[N, 5]): Rotated boxes to perform NMS on. They are expected to be in
           (x_center, y_center, width, height, angle_degrees) format.
        scores (Tensor[N]): Scores for each one of the rotated boxes
        iou_threshold (float): Discards all overlapping rotated boxes with IoU < iou_threshold

    Returns:
        keep (Tensor): int64 tensor with the indices of the elements that have been kept
        by Rotated NMS, sorted in decreasing order of scores
    """
    from cvpods import _C

    return _C.nms_rotated(boxes, scores, iou_threshold)


# Note: this function (batched_nms_rotated) might be moved into
# torchvision/ops/boxes.py in the future
def batched_nms_rotated(boxes, scores, idxs, iou_threshold):
    """
    Performs non-maximum suppression in a batched fashion.

    Each index value correspond to a category, and NMS
    will not be applied between elements of different categories.

    Args:
        boxes (Tensor[N, 5]):
           boxes where NMS will be performed. They
           are expected to be in (x_ctr, y_ctr, width, height, angle_degrees) format
        scores (Tensor[N]):
           scores for each one of the boxes
        idxs (Tensor[N]):
           indices of the categories for each one of the boxes.
        iou_threshold (float):
           discards all overlapping boxes
           with IoU < iou_threshold

    Returns:
        Tensor:
            int64 tensor with the indices of the elements that have been kept
            by NMS, sorted in decreasing order of scores
    """
    assert boxes.shape[-1] == 5

    if boxes.numel() == 0:
        return torch.empty((0,), dtype=torch.int64, device=boxes.device)
    # Strategy: in order to perform NMS independently per class,
    # we add an offset to all the boxes. The offset is dependent
    # only on the class idx, and is large enough so that boxes
    # from different classes do not overlap

    # Note that batched_nms in torchvision/ops/boxes.py only uses max_coordinate,
    # which won't handle negative coordinates correctly.
    # Here by using min_coordinate we can make sure the negative coordinates are
    # correctly handled.
    max_coordinate = (
        torch.max(boxes[:, 0], boxes[:, 1]) + torch.max(boxes[:, 2], boxes[:, 3]) / 2
    ).max()
    min_coordinate = (
        torch.min(boxes[:, 0], boxes[:, 1]) - torch.min(boxes[:, 2], boxes[:, 3]) / 2
    ).min()
    offsets = idxs.to(boxes) * (max_coordinate - min_coordinate + 1)
    boxes_for_nms = boxes.clone()  # avoid modifying the original values in boxes
    boxes_for_nms[:, :2] += offsets[:, None]
    keep = nms_rotated(boxes_for_nms, scores, iou_threshold)
    return keep


def matrix_nms(seg_masks, cate_labels, cate_scores, kernel="gaussian", sigma=2.0, sum_masks=None):
    """
    Matrix NMS for multi-class masks.
    See: https://arxiv.org/pdf/2003.10152.pdf for more details.

    Args:
        seg_masks (Tensor): shape: [N, H, W], binary masks.
        cate_labels (Tensor): shepe: [N], mask labels in descending order.
        cate_scores (Tensor): shape [N], mask scores in descending order.
        kernel (str):  'linear' or 'gaussian'.
        sigma (float): std in gaussian method.
        sum_masks (Tensor): The sum of seg_masks.

    Returns:
        Tensor: cate_scores_update, tensors of shape [N].
    """
    n_samples = len(cate_labels)
    if n_samples == 0:
        return []
    if sum_masks is None:
        sum_masks = seg_masks.sum((1, 2)).float()
    seg_masks = seg_masks.reshape(n_samples, -1).float()
    # inter.
    inter_matrix = torch.mm(seg_masks, seg_masks.transpose(1, 0))
    # union.
    sum_masks_x = sum_masks.expand(n_samples, n_samples)
    # iou.
    iou_matrix = (
        inter_matrix / (sum_masks_x + sum_masks_x.transpose(1, 0) - inter_matrix)
    ).triu(diagonal=1)
    # label_specific matrix.
    cate_labels_x = cate_labels.expand(n_samples, n_samples)
    label_matrix = (cate_labels_x == cate_labels_x.transpose(1, 0)).float().triu(diagonal=1)

    # IoU compensation
    compensate_iou, _ = (iou_matrix * label_matrix).max(0)
    compensate_iou = compensate_iou.expand(n_samples, n_samples).transpose(1, 0)

    # IoU decay
    decay_iou = iou_matrix * label_matrix

    # matrix nms
    if kernel == "gaussian":
        decay_matrix = torch.exp(-1 * sigma * (decay_iou ** 2))
        compensate_matrix = torch.exp(-1 * sigma * (compensate_iou ** 2))
        decay_coefficient, _ = (decay_matrix / compensate_matrix).min(0)
    elif kernel == "linear":
        decay_matrix = (1 - decay_iou) / (1 - compensate_iou)
        decay_coefficient, _ = decay_matrix.min(0)
    else:
        raise NotImplementedError

    # update the score.
    cate_scores_update = cate_scores * decay_coefficient
    return cate_scores_update
```

### cvpods/layers/__init__.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
# Modified by BaseDetection, Inc. and its affiliates. All Rights Reserved
from .activation_funcs import MemoryEfficientSwish, Swish
from .batch_norm import FrozenBatchNorm2d, NaiveSyncBatchNorm, get_activation, get_norm
from .deform_conv import DeformConv, ModulatedDeformConv
from .deform_conv_with_off import DeformConvWithOff, ModulatedDeformConvWithOff
from .larc import LARC
from .mask_ops import paste_masks_in_image
from .nms import (
    batched_nms,
    batched_nms_rotated,
    batched_softnms,
    batched_softnms_rotated,
    cluster_nms,
    generalized_batched_nms,
    matrix_nms,
    ml_nms,
    nms,
    nms_rotated,
    softnms,
    softnms_rotated
)
from .position_encoding import position_encoding_dict
from .roi_align import ROIAlign, roi_align
from .roi_align_rotated import ROIAlignRotated, roi_align_rotated
from .shape_spec import ShapeSpec
from .swap_align2nat import SwapAlign2Nat, swap_align2nat
from .tree_filter_v2 import TreeFilterV2
from .wrappers import (
    BatchNorm2d,
    Conv2d,
    Conv2dSamePadding,
    ConvTranspose2d,
    MaxPool2dSamePadding,
    SeparableConvBlock,
    cat,
    interpolate
)

__all__ = [k for k in globals().keys() if not k.startswith("_")]
```

### cvpods/layers/mask_ops.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import numpy as np
from PIL import Image

import torch
from torch.nn import functional as F

__all__ = ["paste_masks_in_image"]


BYTES_PER_FLOAT = 4
# TODO: This memory limit may be too much or too little. It would be better to
# determine it based on available resources.
GPU_MEM_LIMIT = 1024 ** 3  # 1 GB memory limit


def _do_paste_mask(masks, boxes, img_h, img_w, skip_empty=True):
    """
    Args:
        masks: N, 1, H, W
        boxes: N, 4
        img_h, img_w (int):
        skip_empty (bool): only paste masks within the region that
            tightly bound all boxes, and returns the results this region only.
            An important optimization for CPU.

    Returns:
        if skip_empty == False, a mask of shape (N, img_h, img_w)
        if skip_empty == True, a mask of shape (N, h', w'), and the slice
            object for the corresponding region.
    """
    # On GPU, paste all masks together (up to chunk size)
    # by using the entire image to sample the masks
    # Compared to pasting them one by one,
    # this has more operations but is faster on COCO-scale dataset.
    device = masks.device
    if skip_empty:
        x0_int, y0_int = torch.clamp(boxes.min(dim=0).values.floor()[:2] - 1, min=0).to(
            dtype=torch.int32
        )
        x1_int = torch.clamp(boxes[:, 2].max().ceil() + 1, max=img_w).to(dtype=torch.int32)
        y1_int = torch.clamp(boxes[:, 3].max().ceil() + 1, max=img_h).to(dtype=torch.int32)
    else:
        x0_int, y0_int = 0, 0
        x1_int, y1_int = img_w, img_h
    x0, y0, x1, y1 = torch.split(boxes, 1, dim=1)  # each is Nx1

    N = masks.shape[0]

    img_y = torch.arange(y0_int, y1_int, device=device, dtype=torch.float32) + 0.5
    img_x = torch.arange(x0_int, x1_int, device=device, dtype=torch.float32) + 0.5
    img_y = (img_y - y0) / (y1 - y0) * 2 - 1
    img_x = (img_x - x0) / (x1 - x0) * 2 - 1
    # img_x, img_y have shapes (N, w), (N, h)

    gx = img_x[:, None, :].expand(N, img_y.size(1), img_x.size(1))
    gy = img_y[:, :, None].expand(N, img_y.size(1), img_x.size(1))
    grid = torch.stack([gx, gy], dim=3)

    img_masks = F.grid_sample(masks.to(dtype=torch.float32), grid, align_corners=False)

    if skip_empty:
        return img_masks[:, 0], (slice(y0_int, y1_int), slice(x0_int, x1_int))
    else:
        return img_masks[:, 0], ()


def paste_masks_in_image(masks, boxes, image_shape, threshold=0.5):
    """
    Paste a set of masks that are of a fixed resolution (e.g., 28 x 28) into an image.
    The location, height, and width for pasting each mask is determined by their
    corresponding bounding boxes in boxes.

    Args:
        masks (tensor): Tensor of shape (Bimg, Hmask, Wmask), where Bimg is the number of
            detected object instances in the image and Hmask, Wmask are the mask width and mask
            height of the predicted mask (e.g., Hmask = Wmask = 28). Values are in [0, 1].
        boxes (Boxes or Tensor): A Boxes of length Bimg or Tensor of shape (Bimg, 4).
            boxes[i] and masks[i] correspond to the same object instance.
        image_shape (tuple): height, width
        threshold (float): A threshold in [0, 1] for converting the (soft) masks to
            binary masks.

    Returns:
        img_masks (Tensor): A tensor of shape (Bimg, Himage, Wimage), where Bimg is the
        number of detected object instances and Himage, Wimage are the image width
        and height. img_masks[i] is a binary mask for object instance i.
    """
    assert masks.shape[-1] == masks.shape[-2], "Only square mask predictions are supported"
    N = len(masks)
    if N == 0:
        return masks.new_empty((0,) + image_shape, dtype=torch.uint8)
    if not isinstance(boxes, torch.Tensor):
        boxes = boxes.tensor
    device = boxes.device
    assert len(boxes) == N, boxes.shape

    img_h, img_w = image_shape

    # The actual implementation split the input into chunks,
    # and paste them chunk by chunk.
    if device.type == "cpu":
        # CPU is most efficient when they are pasted one by one with skip_empty=True
        # so that it performs minimal number of operations.
        num_chunks = N
    else:
        # GPU benefits from parallelism for larger chunks, but may have memory issue
        num_chunks = int(np.ceil(N * img_h * img_w * BYTES_PER_FLOAT / GPU_MEM_LIMIT))
        assert (
            num_chunks <= N
        ), "Default GPU_MEM_LIMIT in mask_ops.py is too small; try increasing it"
    chunks = torch.chunk(torch.arange(N, device=device), num_chunks)

    img_masks = torch.zeros(
        N, img_h, img_w, device=device, dtype=torch.bool if threshold >= 0 else torch.uint8
    )
    for inds in chunks:
        masks_chunk, spatial_inds = _do_paste_mask(
            masks[inds, None, :, :], boxes[inds], img_h, img_w, skip_empty=device.type == "cpu"
        )

        if threshold >= 0:
            masks_chunk = (masks_chunk >= threshold).to(dtype=torch.bool)
        else:
            # for visualization and debugging
            masks_chunk = (masks_chunk * 255).to(dtype=torch.uint8)

        img_masks[(inds,) + spatial_inds] = masks_chunk
    return img_masks


# The below are the original paste function (from Detectron1) which has
# larger quantization error.
# It is faster on CPU, while the aligned one is faster on GPU thanks to grid_sample.


def paste_mask_in_image_old(mask, box, img_h, img_w, threshold):
    """
    Paste a single mask in an image.
    This is a per-box implementation of :func:`paste_masks_in_image`.
    This function has larger quantization error due to incorrect pixel
    modeling and is not used any more.

    Args:
        mask (Tensor): A tensor of shape (Hmask, Wmask) storing the mask of a single
            object instance. Values are in [0, 1].
        box (Tensor): A tensor of shape (4, ) storing the x0, y0, x1, y1 box corners
            of the object instance.
        img_h, img_w (int): Image height and width.
        threshold (float): Mask binarization threshold in [0, 1].

    Returns:
        im_mask (Tensor):
            The resized and binarized object mask pasted into the original
            image plane (a tensor of shape (img_h, img_w)).
    """
    # Conversion from continuous box coordinates to discrete pixel coordinates
    # via truncation (cast to int32). This determines which pixels to paste the
    # mask onto.
    box = box.to(dtype=torch.int32)  # Continuous to discrete coordinate conversion
    # An example (1D) box with continuous coordinates (x0=0.7, x1=4.3) will map to
    # a discrete coordinates (x0=0, x1=4). Note that box is mapped to 5 = x1 - x0 + 1
    # pixels (not x1 - x0 pixels).
    samples_w = box[2] - box[0] + 1  # Number of pixel samples, *not* geometric width
    samples_h = box[3] - box[1] + 1  # Number of pixel samples, *not* geometric height

    # Resample the mask from it's original grid to the new samples_w x samples_h grid
    mask = Image.fromarray(mask.cpu().numpy())
    mask = mask.resize((samples_w, samples_h), resample=Image.BILINEAR)
    mask = np.array(mask, copy=False)

    if threshold >= 0:
        mask = np.array(mask > threshold, dtype=np.uint8)
        mask = torch.from_numpy(mask)
    else:
        # for visualization and debugging, we also
        # allow it to return an unmodified mask
        mask = torch.from_numpy(mask * 255).to(torch.uint8)

    im_mask = torch.zeros((img_h, img_w), dtype=torch.uint8)
    x_0 = max(box[0], 0)
    x_1 = min(box[2] + 1, img_w)
    y_0 = max(box[1], 0)
    y_1 = min(box[3] + 1, img_h)

    im_mask[y_0:y_1, x_0:x_1] = mask[
        (y_0 - box[1]): (y_1 - box[1]), (x_0 - box[0]): (x_1 - box[0])
    ]
    return im_mask


# Our pixel modeling requires extrapolation for any continuous
# coordinate < 0.5 or > length - 0.5. When sampling pixels on the masks,
# we would like this extrapolation to be an interpolation between boundary values and zero,
# instead of using absolute zero or boundary values.
# Therefore `paste_mask_in_image_old` is often used with zero padding around the masks like this:
# masks, scale = pad_masks(masks[:, 0, :, :], 1)
# boxes = scale_boxes(boxes.tensor, scale)


def pad_masks(masks, padding):
    """
    Args:
        masks (tensor): A tensor of shape (B, M, M) representing B masks.
        padding (int): Number of cells to pad on all sides.

    Returns:
        The padded masks and the scale factor of the padding size / original size.
    """
    B = masks.shape[0]
    M = masks.shape[-1]
    pad2 = 2 * padding
    scale = float(M + pad2) / M
    padded_masks = masks.new_zeros((B, M + pad2, M + pad2))
    padded_masks[:, padding:-padding, padding:-padding] = masks
    return padded_masks, scale


def scale_boxes(boxes, scale):
    """
    Args:
        boxes (tensor): A tensor of shape (B, 4) representing B boxes with 4
            coords representing the corners x0, y0, x1, y1,
        scale (float): The box scaling factor.

    Returns:
        Scaled boxes.
    """
    w_half = (boxes[:, 2] - boxes[:, 0]) * 0.5
    h_half = (boxes[:, 3] - boxes[:, 1]) * 0.5
    x_c = (boxes[:, 2] + boxes[:, 0]) * 0.5
    y_c = (boxes[:, 3] + boxes[:, 1]) * 0.5

    w_half *= scale
    h_half *= scale

    scaled_boxes = torch.zeros_like(boxes)
    scaled_boxes[:, 0] = x_c - w_half
    scaled_boxes[:, 2] = x_c + w_half
    scaled_boxes[:, 1] = y_c - h_half
    scaled_boxes[:, 3] = y_c + h_half
    return scaled_boxes
```

### cvpods/layers/wrappers.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
"""
Wrappers around on some nn functions, mainly to support empty tensors.

Ideally, add support directly in PyTorch to empty tensors in those functions.

These can be removed once https://github.com/pytorch/pytorch/issues/12013
is implemented
"""

import math

import torch
from torch.nn import functional as F
from torch.nn.modules.utils import _ntuple

TORCH_VERSION = tuple(int(x) for x in torch.__version__.split(".")[:2])


def cat(tensors, dim=0):
    """
    Efficient version of torch.cat that avoids a copy if there is only a single element in a list
    """
    assert isinstance(tensors, (list, tuple))
    if len(tensors) == 1:
        return tensors[0]
    return torch.cat(tensors, dim)


class _NewEmptyTensorOp(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, new_shape):
        ctx.shape = x.shape
        return x.new_empty(new_shape)

    @staticmethod
    def backward(ctx, grad):
        shape = ctx.shape
        return _NewEmptyTensorOp.apply(grad, shape), None


class Conv2d(torch.nn.Conv2d):
    """
    A wrapper around :class:`torch.nn.Conv2d` to support empty inputs and more features.
    """

    def __init__(self, *args, **kwargs):
        """
        Extra keyword arguments supported in addition to those in `torch.nn.Conv2d`:

        Args:
            norm (nn.Module, optional): a normalization layer
            activation (callable(Tensor) -> Tensor): a callable activation function

        It assumes that norm layer is used before activation.
        """
        norm = kwargs.pop("norm", None)
        activation = kwargs.pop("activation", None)
        super().__init__(*args, **kwargs)

        self.norm = norm
        self.activation = activation

    def forward(self, x):
        if x.numel() == 0 and self.training:
            # https://github.com/pytorch/pytorch/issues/12013
            assert not isinstance(
                self.norm, torch.nn.SyncBatchNorm
            ), "SyncBatchNorm does not support empty inputs!"

        if x.numel() == 0 and TORCH_VERSION <= (1, 4):
            assert not isinstance(
                self.norm, torch.nn.GroupNorm
            ), "GroupNorm does not support empty inputs in PyTorch <=1.4!"
            # When input is empty, we want to return a empty tensor with "correct" shape,
            # So that the following operations will not panic
            # if they check for the shape of the tensor.
            # This computes the height and width of the output tensor
            output_shape = [
                (i + 2 * p - (di * (k - 1) + 1)) // s + 1
                for i, p, di, k, s in zip(
                    x.shape[-2:], self.padding, self.dilation, self.kernel_size, self.stride
                )
            ]
            output_shape = [x.shape[0], self.weight.shape[0]] + output_shape
            empty = _NewEmptyTensorOp.apply(x, output_shape)
            if self.training:
                # This is to make DDP happy.
                # DDP expects all workers to have gradient w.r.t the same set of parameters.
                _dummy = sum(x.view(-1)[0] for x in self.parameters()) * 0.0
                return empty + _dummy
            else:
                return empty

        x = super().forward(x)
        if self.norm is not None:
            x = self.norm(x)
        if self.activation is not None:
            x = self.activation(x)
        return x


class Conv2dSamePadding(torch.nn.Conv2d):
    """
    A wrapper around :class:`torch.nn.Conv2d` to support "SAME" padding mode and more features.
    """

    def __init__(self, *args, **kwargs):
        """
        Extra keyword arguments supported in addition to those in `torch.nn.Conv2d`:

        Args:
            norm (nn.Module, optional): a normalization layer
            activation (callable(Tensor) -> Tensor): a callable activation function

        It assumes that norm layer is used before activation.
        """
        norm = kwargs.pop("norm", None)
        activation = kwargs.pop("activation", None)

        # parse padding mode
        self.padding_method = kwargs.pop("padding", None)
        if self.padding_method is None:
            if len(args) >= 5:
                self.padding_method = args[4]
            else:
                self.padding_method = 0  # default padding number

        if isinstance(self.padding_method, str):
            if self.padding_method.upper() == "SAME":
                # If the padding mode is `SAME`, it will be manually padded
                super().__init__(*args, **kwargs, padding=0)
                # stride
                if isinstance(self.stride, int):
                    self.stride = [self.stride] * 2
                elif len(self.stride) == 1:
                    self.stride = [self.stride[0]] * 2
                # kernel size
                if isinstance(self.kernel_size, int):
                    self.kernel_size = [self.kernel_size] * 2
                elif len(self.kernel_size) == 1:
                    self.kernel_size = [self.kernel_size[0]] * 2
                # dilation
                if isinstance(self.dilation, int):
                    self.dilation = [self.dilation] * 2
                elif len(self.dilation) == 1:
                    self.dilation = [self.dilation[0]] * 2
            else:
                raise ValueError("Unknown padding method: {}".format(self.padding_method))
        else:
            super().__init__(*args, **kwargs, padding=self.padding_method)
        self.norm = norm
        self.activation = activation

    def forward(self, x):
        if isinstance(self.padding_method, str):
            if self.padding_method.upper() == "SAME":
                input_h, input_w = x.shape[-2:]
                stride_h, stride_w = self.stride
                kernel_size_h, kernel_size_w = self.kernel_size
                dilation_h, dilation_w = self.dilation

                output_h = math.ceil(input_h / stride_h)
                output_w = math.ceil(input_w / stride_w)

                padding_needed_h = max(
                    0, (output_h - 1) * stride_h + (kernel_size_h - 1) * dilation_h + 1 - input_h
                )
                padding_needed_w = max(
                    0, (output_w - 1) * stride_w + (kernel_size_w - 1) * dilation_w + 1 - input_w
                )

                left = padding_needed_w // 2
                right = padding_needed_w - left
                top = padding_needed_h // 2
                bottom = padding_needed_h - top

                x = F.pad(x, [left, right, top, bottom])
            else:
                raise ValueError("Unknown padding method: {}".format(self.padding_method))

        x = super().forward(x)
        if self.norm is not None:
            x = self.norm(x)
        if self.activation is not None:
            x = self.activation(x)
        return x


class MaxPool2dSamePadding(torch.nn.MaxPool2d):
    """
    A wrapper around :class:`torch.nn.MaxPool2d` to support "SAME" padding mode and more features.

    See: https://github.com/pytorch/pytorch/issues/3867
    """

    def __init__(self, *args, **kwargs):
        # parse padding mode
        self.padding_method = kwargs.pop("padding", None)
        if self.padding_method is None:
            if len(args) >= 3:
                self.padding_method = args[2]
            else:
                self.padding_method = 0  # default padding number

        if isinstance(self.padding_method, str):
            if self.padding_method.upper() == "SAME":
                # If the padding mode is `SAME`, it will be manually padded
                super().__init__(*args, **kwargs, padding=0)
                # stride
                if isinstance(self.stride, int):
                    self.stride = [self.stride] * 2
                elif len(self.stride) == 1:
                    self.stride = [self.stride[0]] * 2
                # kernel size
                if isinstance(self.kernel_size, int):
                    self.kernel_size = [self.kernel_size] * 2
                elif len(self.kernel_size) == 1:
                    self.kernel_size = [self.kernel_size[0]] * 2
            else:
                raise ValueError("Unknown padding method: {}".format(self.padding_method))
        else:
            super().__init__(*args, **kwargs, padding=self.padding_method)

    def forward(self, x):
        if isinstance(self.padding_method, str):
            if self.padding_method.upper() == "SAME":
                input_h, input_w = x.shape[-2:]
                stride_h, stride_w = self.stride
                kernel_size_h, kernel_size_w = self.kernel_size

                output_h = math.ceil(input_h / stride_h)
                output_w = math.ceil(input_w / stride_w)

                padding_needed_h = max(
                    0, (output_h - 1) * stride_h + (kernel_size_h - 1) + 1 - input_h
                )
                padding_needed_w = max(
                    0, (output_w - 1) * stride_w + (kernel_size_w - 1) + 1 - input_w
                )

                left = padding_needed_w // 2
                right = padding_needed_w - left
                top = padding_needed_h // 2
                bottom = padding_needed_h - top

                x = F.pad(x, [left, right, top, bottom])
            else:
                raise ValueError("Unknown padding method: {}".format(self.padding_method))

        x = super().forward(x)
        return x


class SeparableConvBlock(torch.nn.Module):
    """
    Depthwise seperable convolution block.
    """

    def __init__(self, in_channels, out_channels, kernel_size,
                 stride=1, padding=0, dilation=1, bias=True,
                 norm=None, activation=None):
        """
        Args:
            in_channels (int): the number of input tensor channels.
            out_channels (int):the number of output tensor channels.
            kernel_size (int): the kernel size.
            stride (int or tuple or list): the stride.
            bias (bool): if `True`, the pointwise conv applies bias.
            apply_bn (bool): if `True`, apply BN layer after conv layer.
            norm (nn.Module, optional): a normalization layer
            activation (callable(Tensor) -> Tensor): a callable activation function

        It assumes that norm layer is used before activation.
        """
        super(SeparableConvBlock, self).__init__()
        self.norm = norm
        self.activation = activation
        self.depthwise = Conv2dSamePadding(in_channels=in_channels,
                                           out_channels=in_channels,
                                           kernel_size=kernel_size,
                                           stride=stride,
                                           padding=padding,
                                           dilation=dilation,
                                           groups=in_channels,
                                           bias=False)
        self.pointwise = Conv2dSamePadding(in_channels=in_channels,
                                           out_channels=out_channels,
                                           kernel_size=1,
                                           stride=1,
                                           padding=0,
                                           dilation=1,
                                           groups=1,
                                           bias=bias)
        if bias:
            self.bias = self.pointwise.bias

    def forward(self, inputs):
        x = self.depthwise(inputs)
        x = self.pointwise(x)

        if self.norm is not None:
            x = self.norm(x)
        if self.activation is not None:
            x = self.activation(x)
        return x


if TORCH_VERSION > (1, 4):
    ConvTranspose2d = torch.nn.ConvTranspose2d
else:

    class ConvTranspose2d(torch.nn.ConvTranspose2d):
        """
        A wrapper around :class:`torch.nn.ConvTranspose2d` to support zero-size tensor.
        """

        def forward(self, x):
            if x.numel() > 0:
                return super(ConvTranspose2d, self).forward(x)
            # get output shape

            # When input is empty, we want to return a empty tensor with "correct" shape,
            # So that the following operations will not panic
            # if they check for the shape of the tensor.
            # This computes the height and width of the output tensor
            output_shape = [
                (i - 1) * d - 2 * p + (di * (k - 1) + 1) + op
                for i, p, di, k, d, op in zip(
                    x.shape[-2:],
                    self.padding,
                    self.dilation,
                    self.kernel_size,
                    self.stride,
                    self.output_padding,
                )
            ]
            output_shape = [x.shape[0], self.out_channels] + output_shape
            # This is to make DDP happy.
            # DDP expects all workers to have gradient w.r.t the same set of parameters.
            _dummy = sum(x.view(-1)[0] for x in self.parameters()) * 0.0
            return _NewEmptyTensorOp.apply(x, output_shape) + _dummy


if TORCH_VERSION > (1, 4):
    BatchNorm2d = torch.nn.BatchNorm2d
else:

    class BatchNorm2d(torch.nn.BatchNorm2d):
        """
        A wrapper around :class:`torch.nn.BatchNorm2d` to support zero-size tensor.
        """

        def forward(self, x):
            if x.numel() > 0:
                return super(BatchNorm2d, self).forward(x)
            # get output shape
            output_shape = x.shape
            return _NewEmptyTensorOp.apply(x, output_shape)


if TORCH_VERSION > (1, 4):
    BatchNorm1d = torch.nn.BatchNorm1d
else:

    class BatchNorm1d(torch.nn.BatchNorm1d):
        """
        A wrapper around :class:`torch.nn.BatchNorm2d` to support zero-size tensor.
        """

        def forward(self, x):
            if x.numel() > 0:
                return super(BatchNorm1d, self).forward(x)
            # get output shape
            output_shape = x.shape
            return _NewEmptyTensorOp.apply(x, output_shape)


def interpolate(input, size=None, scale_factor=None, mode="nearest", align_corners=None):
    """
    A wrapper around :func:`torch.nn.functional.interpolate` to support zero-size tensor.
    """
    if input.numel() > 0:
        return torch.nn.functional.interpolate(
            input, size, scale_factor, mode, align_corners=align_corners
        )

    def _check_size_scale_factor(dim):
        if size is None and scale_factor is None:
            raise ValueError("either size or scale_factor should be defined")
        if size is not None and scale_factor is not None:
            raise ValueError("only one of size or scale_factor should be defined")
        if (
            scale_factor is not None
            and isinstance(scale_factor, tuple)
            and len(scale_factor) != dim
        ):
            raise ValueError(
                "scale_factor shape must match input shape. "
                "Input is {}D, scale_factor size is {}".format(dim, len(scale_factor))
            )

    def _output_size(dim):
        _check_size_scale_factor(dim)
        if size is not None:
            return size
        scale_factors = _ntuple(dim)(scale_factor)
        # math.floor might return float in py2.7
        return [int(math.floor(input.size(i + 2) * scale_factors[i])) for i in range(dim)]

    output_shape = tuple(_output_size(2))
    output_shape = input.shape[:-2] + output_shape
    return _NewEmptyTensorOp.apply(input, output_shape)
```

### cvpods/layers/tree_filter_core.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import torch
from torch import nn
from torch.autograd import Function
from torch.autograd.function import once_differentiable

from cvpods import _C

# pylint: disable=W0613


class _BFS(Function):
    @staticmethod
    def forward(ctx, edge_index, max_adj_per_vertex):
        sorted_index, sorted_parent, sorted_child =\
            _C.bfs_forward(edge_index, max_adj_per_vertex)
        return sorted_index, sorted_parent, sorted_child


class _MST(Function):
    @staticmethod
    def forward(ctx, edge_index, edge_weight, vertex_index):
        edge_out = _C.mst_forward(edge_index, edge_weight, vertex_index)
        return edge_out

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        return None, None, None


class _RST(Function):
    @staticmethod
    def forward(ctx, edge_index, edge_weight, vertex_index):
        edge_out = _C.rst_forward(edge_index, edge_weight, vertex_index)
        return edge_out

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        return None, None, None


class _Refine(Function):
    @staticmethod
    def forward(ctx, feature_in, edge_weight, self_weight,
                sorted_index, sorted_parent, sorted_child):
        feature_out, feature_aggr, feature_aggr_up, =\
            _C.tree_filter_refine_forward(
                feature_in, edge_weight, self_weight,
                sorted_index, sorted_parent, sorted_child
            )

        ctx.save_for_backward(
            feature_in, edge_weight, self_weight, sorted_index,
            sorted_parent, sorted_child, feature_aggr, feature_aggr_up
        )
        return feature_out

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        feature_in, edge_weight, self_weight, sorted_index, sorted_parent,\
            sorted_child, feature_aggr, feature_aggr_up = ctx.saved_tensors

        grad_feature = _C.tree_filter_refine_backward_feature(
            feature_in, edge_weight, self_weight, sorted_index,
            sorted_parent, sorted_child, feature_aggr, feature_aggr_up,
            grad_output
        )
        grad_edge_weight = _C.tree_filter_refine_backward_edge_weight(
            feature_in, edge_weight, self_weight, sorted_index, sorted_parent,
            sorted_child, feature_aggr, feature_aggr_up, grad_output
        )
        grad_self_weight = _C.tree_filter_refine_backward_self_weight(
            feature_in, edge_weight, self_weight, sorted_index, sorted_parent,
            sorted_child, feature_aggr, feature_aggr_up, grad_output
        )

        return grad_feature, grad_edge_weight, grad_self_weight, None, None, None


bfs = _BFS.apply
mst = _MST.apply
rst = _RST.apply
refine = _Refine.apply


class MinimumSpanningTree(nn.Module):
    def __init__(self, distance_func, mapping_func=None):
        super(MinimumSpanningTree, self).__init__()
        self.distance_func = distance_func
        self.mapping_func = mapping_func

    @staticmethod
    def _build_matrix_index(fm):
        batch, height, width = (fm.shape[0], *fm.shape[2:])
        row = torch.arange(width, dtype=torch.int32, device=fm.device).unsqueeze(0)
        col = torch.arange(height, dtype=torch.int32, device=fm.device).unsqueeze(1)
        raw_index = row + col * width
        row_index = torch.stack([raw_index[:-1, :], raw_index[1:, :]], 2)
        col_index = torch.stack([raw_index[:, :-1], raw_index[:, 1:]], 2)
        index = torch.cat([row_index.reshape(1, -1, 2),
                           col_index.reshape(1, -1, 2)], 1)
        index = index.expand(batch, -1, -1)
        return index

    def _build_feature_weight(self, fm):
        batch = fm.shape[0]
        weight_row = self.distance_func(fm[:, :, :-1, :], fm[:, :, 1:, :])
        weight_col = self.distance_func(fm[:, :, :, :-1], fm[:, :, :, 1:])
        weight_row = weight_row.reshape([batch, -1])
        weight_col = weight_col.reshape([batch, -1])
        weight = torch.cat([weight_row, weight_col], dim=1)
        if self.mapping_func is not None:
            weight = self.mapping_func(weight)
        return weight

    def forward(self, guide_in):
        with torch.no_grad():
            index = self._build_matrix_index(guide_in)
            weight = self._build_feature_weight(guide_in)
            tree = mst(index, weight, guide_in.shape[2] * guide_in.shape[3])
        return tree


class RandomSpanningTree(nn.Module):
    def __init__(self, distance_func, mapping_func=None):
        super(RandomSpanningTree, self).__init__()
        self.distance_func = distance_func
        self.mapping_func = mapping_func

    @staticmethod
    def _build_matrix_index(fm):
        batch, height, width = (fm.shape[0], *fm.shape[2:])
        row = torch.arange(width, dtype=torch.int32, device=fm.device).unsqueeze(0)
        col = torch.arange(height, dtype=torch.int32, device=fm.device).unsqueeze(1)
        raw_index = row + col * width
        row_index = torch.stack([raw_index[:-1, :], raw_index[1:, :]], 2)
        col_index = torch.stack([raw_index[:, :-1], raw_index[:, 1:]], 2)
        index = torch.cat([row_index.reshape(1, -1, 2),
                           col_index.reshape(1, -1, 2)], 1)
        index = index.expand(batch, -1, -1)
        return index

    def _build_feature_weight(self, fm):
        batch = fm.shape[0]
        weight_row = self.distance_func(fm[:, :, :-1, :], fm[:, :, 1:, :])
        weight_col = self.distance_func(fm[:, :, :, :-1], fm[:, :, :, 1:])
        weight_row = weight_row.reshape([batch, -1])
        weight_col = weight_col.reshape([batch, -1])
        weight = torch.cat([weight_row, weight_col], dim=1)
        if self.mapping_func is not None:
            weight = self.mapping_func(-weight)
        return weight

    def forward(self, guide_in):
        with torch.no_grad():
            index = self._build_matrix_index(guide_in)
            weight = self._build_feature_weight(guide_in)
            tree = rst(index, weight, guide_in.shape[2] * guide_in.shape[3])
        return tree


class TreeFilter2D(nn.Module):
    def __init__(self, groups=1, distance_func=None,
                 mapping_func=torch.exp):
        super(TreeFilter2D, self).__init__()
        self.groups = groups
        self.mapping_func = mapping_func
        if distance_func is None:
            self.distance_func = self.norm2_distance
        else:
            self.distance_func = distance_func

    @staticmethod
    def norm2_distance(fm_ref, fm_tar):
        diff = fm_ref - fm_tar
        weight = (diff * diff).sum(dim=1)
        return weight

    @staticmethod
    def batch_index_opr(data, index):
        with torch.no_grad():
            channel = data.shape[1]
            index = index.unsqueeze(1).expand(-1, channel, -1).long()
        data = torch.gather(data, 2, index)
        return data

    def build_edge_weight(self, fm, sorted_index, sorted_parent):
        batch   = fm.shape[0]
        channel = fm.shape[1]
        vertex  = fm.shape[2] * fm.shape[3]

        fm = fm.reshape([batch, channel, -1])
        fm_source = self.batch_index_opr(fm, sorted_index)
        fm_target = self.batch_index_opr(fm_source, sorted_parent)
        fm_source = fm_source.reshape([-1, channel // self.groups, vertex])
        fm_target = fm_target.reshape([-1, channel // self.groups, vertex])

        edge_weight = self.distance_func(fm_source, fm_target)
        edge_weight = self.mapping_func(-edge_weight)
        return edge_weight

    def build_self_weight(self, fm, sorted_index):
        vertex = fm.shape[2] * fm.shape[3]

        fm = fm.reshape(-1, fm.shape[1] // self.groups, vertex)
        self_dist = self.distance_func(fm, 0)
        self_weight = self.mapping_func(-self_dist)
        att_weight = self_weight.reshape(-1, self.groups, vertex)
        att_weight = self.batch_index_opr(att_weight, sorted_index)
        att_weight = att_weight.reshape(-1, vertex)
        return self_weight, att_weight

    def split_group(self, feature_in, *tree_orders):
        feature_in = feature_in.reshape(
            feature_in.shape[0] * self.groups,
            feature_in.shape[1] // self.groups,
            -1
        )
        returns = [feature_in.contiguous()]
        for order in tree_orders:
            order = order.unsqueeze(1).expand(order.shape[0], self.groups, *order.shape[1:])
            order = order.reshape(-1, *order.shape[2:])
            returns.append(order.contiguous())
        return tuple(returns)

    def forward(self, feature_in, embed_in, tree, guide_in=None, self_dist_in=None):
        ori_shape = feature_in.shape
        sorted_index, sorted_parent, sorted_child = bfs(tree, 4)
        edge_weight = self.build_edge_weight(embed_in, sorted_index, sorted_parent)
        if self_dist_in is None:
            self_weight = torch.ones_like(edge_weight)
        else:
            self_weight, att_weight = self.build_self_weight(self_dist_in, sorted_index)
            edge_weight = edge_weight * att_weight

        if guide_in is not None:
            guide_weight = self.build_edge_weight(guide_in, sorted_index, sorted_parent)
            edge_weight = edge_weight * guide_weight

        feature_in, sorted_index, sorted_parent, sorted_child = \
            self.split_group(feature_in, sorted_index, sorted_parent, sorted_child)
        feature_out = refine(feature_in, edge_weight, self_weight, sorted_index,
                             sorted_parent, sorted_child)
        feature_out = feature_out.reshape(ori_shape)
        return feature_out
```

### cvpods/layers/larc.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import torch


class LARC(object):
    """
    :class:`LARC` is a pytorch implementation of both the scaling and clipping variants of LARC,
    in which the ratio between gradient and parameter magnitudes is used to calculate an adaptive
    local learning rate for each individual parameter. The algorithm is designed to improve
    convergence of large batch training.

    See https://arxiv.org/abs/1708.03888 for calculation of the local learning rate.
    In practice it modifies the gradients of parameters as a proxy for modifying the learning rate
    of the parameters. This design allows it to be used as a wrapper around any torch Optimizer.

    Args:
        optimizer: Pytorch optimizer to wrap and modify learning rate for.
        trust_coefficient: Trust coefficient for calculating the lr.
          See https://arxiv.org/abs/1708.03888
        clip: Decides between clipping or scaling mode of LARC. If `clip=True` the learning rate
          is set to `min(optimizer_lr, local_lr)` for each parameter. If `clip=False` the learning
          rate is set to `local_lr*optimizer_lr`.
        eps: epsilon kludge to help with numerical stability while calculating adaptive_lr
    """

    def __init__(self, optimizer, eps=1e-8, trust_coef=0.02, clip=True):
        self.optim = optimizer
        self.trust_coefficient = trust_coef
        self.eps = eps
        self.clip = clip

    def __getstate__(self):
        return self.optim.__getstate__()

    def __setstate__(self, state):
        self.optim.__setstate__(state)

    @property
    def state(self):
        return self.optim.state

    def __repr__(self):
        return self.optim.__repr__()

    @property
    def param_groups(self):
        return self.optim.param_groups

    @param_groups.setter
    def param_groups(self, value):
        self.optim.param_groups = value

    def state_dict(self):
        return self.optim.state_dict()

    def load_state_dict(self, state_dict):
        self.optim.load_state_dict(state_dict)

    def zero_grad(self):
        self.optim.zero_grad()

    def add_param_group(self, param_group):
        self.optim.add_param_group(param_group)

    def step(self):
        with torch.no_grad():
            weight_decays = []
            for group in self.optim.param_groups:
                # absorb weight decay control from optimizer
                weight_decay = group['weight_decay'] if 'weight_decay' in group else 0
                weight_decays.append(weight_decay)
                group['weight_decay'] = 0
                for p in group['params']:
                    if p.grad is None:
                        continue
                    param_norm = torch.norm(p.data)
                    grad_norm = torch.norm(p.grad.data)

                    if param_norm != 0 and grad_norm != 0:
                        # calculate adaptive lr + weight decay
                        adaptive_lr = self.trust_coefficient * (param_norm) / (
                            grad_norm + param_norm * weight_decay + self.eps)

                        # clip learning rate for LARC
                        if self.clip:
                            # calculation of adaptive_lr so that when multiplied by lr
                            # it equals `min(adaptive_lr, lr)`
                            adaptive_lr = min(adaptive_lr / group['lr'], 1)

                        p.grad.data += weight_decay * p.data
                        p.grad.data *= adaptive_lr

        self.optim.step()
        # return weight decay control to optimizer
        for i, group in enumerate(self.optim.param_groups):
            group['weight_decay'] = weight_decays[i]
```

### cvpods/layers/deform_conv_with_off.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import torch
import torch.nn as nn

from .deform_conv import DeformConv, ModulatedDeformConv


class DeformConvWithOff(nn.Module):

    def __init__(self, in_channels, out_channels,
                 kernel_size=3, stride=1, padding=1,
                 dilation=1, deformable_groups=1):
        super(DeformConvWithOff, self).__init__()
        self.offset_conv = nn.Conv2d(
            in_channels,
            deformable_groups * 2 * kernel_size * kernel_size,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
        )
        self.dcn = DeformConv(
            in_channels, out_channels, kernel_size=kernel_size,
            stride=stride, padding=padding, dilation=dilation,
            deformable_groups=deformable_groups,
        )

    def forward(self, input):
        offset = self.offset_conv(input)
        output = self.dcn(input, offset)
        return output


class ModulatedDeformConvWithOff(nn.Module):

    def __init__(self, in_channels, out_channels,
                 kernel_size=3, stride=1, padding=1,
                 dilation=1, deformable_groups=1):
        super(ModulatedDeformConvWithOff, self).__init__()
        self.offset_mask_conv = nn.Conv2d(
            in_channels,
            deformable_groups * 3 * kernel_size * kernel_size,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
        )
        self.dcnv2 = ModulatedDeformConv(
            in_channels, out_channels, kernel_size=kernel_size,
            stride=stride, padding=padding, dilation=dilation,
            deformable_groups=deformable_groups,
        )

    def forward(self, input):
        x = self.offset_mask_conv(input)
        o1, o2, mask = torch.chunk(x, 3, dim=1)
        offset = torch.cat((o1, o2), dim=1)
        mask = torch.sigmoid(mask)
        output = self.dcnv2(input, offset, mask)
        return output
```

### cvpods/layers/batch_norm.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import logging

import torch
import torch.distributed as dist
from torch import nn
from torch.autograd.function import Function
from torch.nn import functional as F

from cvpods.utils import comm

from .wrappers import BatchNorm1d, BatchNorm2d


class FrozenBatchNorm2d(nn.Module):
    """
    BatchNorm2d where the batch statistics and the affine parameters are fixed.

    It contains non-trainable buffers called
    "weight" and "bias", "running_mean", "running_var",
    initialized to perform identity transformation.

    The pre-trained backbone models from Caffe2 only contain "weight" and "bias",
    which are computed from the original four parameters of BN.
    The affine transform `x * weight + bias` will perform the equivalent
    computation of `(x - running_mean) / sqrt(running_var) * weight + bias`.
    When loading a backbone model from Caffe2, "running_mean" and "running_var"
    will be left unchanged as identity transformation.

    Other pre-trained backbone models may contain all 4 parameters.

    The forward is implemented by `F.batch_norm(..., training=False)`.
    """

    _version = 3

    def __init__(self, num_features, eps=1e-5):
        super().__init__()
        self.num_features = num_features
        self.eps = eps
        self.register_buffer("weight", torch.ones(num_features))
        self.register_buffer("bias", torch.zeros(num_features))
        self.register_buffer("running_mean", torch.zeros(num_features))
        self.register_buffer("running_var", torch.ones(num_features) - eps)

    def forward(self, x):
        scale = self.weight * (self.running_var + self.eps).rsqrt()
        bias = self.bias - self.running_mean * scale
        scale = scale.reshape(1, -1, 1, 1)
        bias = bias.reshape(1, -1, 1, 1)
        return x * scale + bias
        if x.requires_grad:
            # When gradients are needed, F.batch_norm will use extra memory
            # because its backward op computes gradients for weight/bias as well.
            scale = self.weight * (self.running_var + self.eps).rsqrt()
            bias = self.bias - self.running_mean * scale
            scale = scale.reshape(1, -1, 1, 1)
            bias = bias.reshape(1, -1, 1, 1)
            return x * scale + bias
        else:
            # When gradients are not needed, F.batch_norm is a single fused op
            # and provide more optimization opportunities.
            return F.batch_norm(
                x,
                self.running_mean,
                self.running_var,
                self.weight,
                self.bias,
                training=False,
                eps=self.eps,
            )

    def _load_from_state_dict(
        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs
    ):
        version = local_metadata.get("version", None)

        if version is None:
            # keep the origin key if version is None
            if prefix + "running_mean" not in state_dict:
                state_dict[prefix + "running_mean"] = self.running_mean.clone().detach()
            if prefix + "running_var" not in state_dict:
                state_dict[prefix + "running_var"] = self.running_var.clone().detach()
        else:
            if version < 2:
                # No running_mean/var in early versions
                # This will silent the warnings
                if prefix + "running_mean" not in state_dict:
                    state_dict[prefix + "running_mean"] = torch.zeros_like(self.running_mean)
                if prefix + "running_var" not in state_dict:
                    state_dict[prefix + "running_var"] = torch.ones_like(self.running_var)

            if version < 3:
                logger = logging.getLogger(__name__)
                logger.info(
                    "FrozenBatchNorm {} is upgraded to version 3.".format(prefix.rstrip("."))
                )
                # In version < 3, running_var are used without +eps.
                state_dict[prefix + "running_var"] -= self.eps

        super()._load_from_state_dict(
            state_dict, prefix, local_metadata, strict,
            missing_keys, unexpected_keys, error_msgs
        )

    def __repr__(self):
        return "FrozenBatchNorm2d(num_features={}, eps={})".format(self.num_features, self.eps)

    @classmethod
    def convert_frozen_batchnorm(cls, module):
        """
        Convert BatchNorm/SyncBatchNorm in module into FrozenBatchNorm.

        Args:
            module (torch.nn.Module):

        Returns:
            If module is BatchNorm/SyncBatchNorm, returns a new module.
            Otherwise, in-place convert module and return it.

        Similar to convert_sync_batchnorm in
        https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py
        """
        bn_module = nn.modules.batchnorm
        bn_module = (bn_module.BatchNorm2d, bn_module.SyncBatchNorm)
        res = module
        if isinstance(module, bn_module):
            res = cls(module.num_features)
            if module.affine:
                res.weight.data = module.weight.data.clone().detach()
                res.bias.data = module.bias.data.clone().detach()
            res.running_mean.data = module.running_mean.data
            res.running_var.data = module.running_var.data
            res.eps = module.eps
        else:
            for name, child in module.named_children():
                new_child = cls.convert_frozen_batchnorm(child)
                if new_child is not child:
                    res.add_module(name, new_child)
        return res


def get_norm(norm, out_channels):
    """
    Args:
        norm (str or callable):

    Returns:
        nn.Module or None: the normalization layer
    """
    if isinstance(norm, str):
        if len(norm) == 0:
            return None
        norm = {
            "BN": BatchNorm2d,
            "SyncBN": NaiveSyncBatchNorm,
            "SyncBN1d": NaiveSyncBatchNorm1d,
            "FrozenBN": FrozenBatchNorm2d,
            "GN": lambda channels: nn.GroupNorm(32, channels),
            "nnSyncBN": nn.SyncBatchNorm,  # keep for debugging
        }[norm]
    return norm(out_channels)


def get_activation(activation):
    """
    Args:
        norm (str or callable):

    Returns:
        nn.Module or None: the normalization layer
    """
    if activation is None:
        return None

    atype = activation.NAME
    inplace = activation.INPLACE
    act = {
        "ReLU": nn.ReLU,
        "ReLU6": nn.ReLU6,
    }[atype]
    return act(inplace=inplace)


class AllReduce(Function):
    @staticmethod
    def forward(ctx, input):
        input_list = [torch.zeros_like(input) for k in range(dist.get_world_size())]
        # Use allgather instead of allreduce since I don't trust in-place operations ..
        dist.all_gather(input_list, input, async_op=False)
        inputs = torch.stack(input_list, dim=0)
        return torch.sum(inputs, dim=0)

    @staticmethod
    def backward(ctx, grad_output):
        dist.all_reduce(grad_output, async_op=False)
        return grad_output


class NaiveSyncBatchNorm(BatchNorm2d):
    """
    `torch.nn.SyncBatchNorm` has known unknown bugs.
    It produces significantly worse AP (and sometimes goes NaN)
    when the batch size on each worker is quite different
    (e.g., when scale augmentation is used, or when it is applied to mask head).

    Use this implementation before `nn.SyncBatchNorm` is fixed.
    It is slower than `nn.SyncBatchNorm`.
    """

    def forward(self, input):
        if comm.get_world_size() == 1 or not self.training:
            return super().forward(input)

        assert input.shape[0] > 0, "SyncBatchNorm does not support empty inputs"
        C = input.shape[1]
        mean = torch.mean(input, dim=[0, 2, 3])
        meansqr = torch.mean(input * input, dim=[0, 2, 3])

        vec = torch.cat([mean, meansqr], dim=0)
        vec = AllReduce.apply(vec) * (1.0 / dist.get_world_size())

        mean, meansqr = torch.split(vec, C)
        var = meansqr - mean * mean
        self.running_mean += self.momentum * (mean.detach() - self.running_mean)
        self.running_var += self.momentum * (var.detach() - self.running_var)

        invstd = torch.rsqrt(var + self.eps)
        scale = self.weight * invstd
        bias = self.bias - mean * scale
        scale = scale.reshape(1, -1, 1, 1)
        bias = bias.reshape(1, -1, 1, 1)
        return input * scale + bias


class NaiveSyncBatchNorm1d(BatchNorm1d):
    """
    `torch.nn.SyncBatchNorm` has known unknown bugs.
    It produces significantly worse AP (and sometimes goes NaN)
    when the batch size on each worker is quite different
    (e.g., when scale augmentation is used, or when it is applied to mask head).

    Use this implementation before `nn.SyncBatchNorm` is fixed.
    It is slower than `nn.SyncBatchNorm`.
    """

    def forward(self, input):
        if comm.get_world_size() == 1 or not self.training:
            return super().forward(input)

        assert input.shape[0] > 0, "SyncBatchNorm does not support empty inputs"
        C = input.shape[1]

        mean = torch.mean(input, dim=[0])
        meansqr = torch.mean(input * input, dim=[0])

        vec = torch.cat([mean, meansqr], dim=0)
        vec = AllReduce.apply(vec) * (1.0 / dist.get_world_size())

        mean, meansqr = torch.split(vec, C)
        var = meansqr - mean * mean
        self.running_mean += self.momentum * (mean.detach() - self.running_mean)
        self.running_var += self.momentum * (var.detach() - self.running_var)

        invstd = torch.rsqrt(var + self.eps)
        scale = self.weight * invstd
        bias = self.bias - mean * scale
        scale = scale.reshape(1, -1)
        bias = bias.reshape(1, -1)
        return input * scale + bias
```

### cvpods/layers/tree_filter_v2.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import torch
import torch.nn as nn
import torch.nn.functional as F

from .tree_filter_core import MinimumSpanningTree, RandomSpanningTree, TreeFilter2D


class TreeFilterV2(nn.Module):
    def __init__(self, guide_channels, in_channels, embed_channels, num_groups=1, eps=1e-8):
        super(TreeFilterV2, self).__init__()
        ''' Hyper Parameters '''
        self.eps            = eps
        self.guide_channels = guide_channels
        self.in_channels    = in_channels
        self.embed_channels = embed_channels
        self.num_groups     = num_groups

        ''' Embedding Layers '''
        self.embed_layer = nn.Conv2d(in_channels, embed_channels, kernel_size=1, bias=False)
        self.conf_layer  = nn.Conv2d(in_channels, num_groups, kernel_size=1, bias=False)
        self.guide_layer = nn.Conv2d(guide_channels, self.embed_channels, kernel_size=1, bias=False)
        self.beta        = nn.Parameter(torch.zeros(num_groups))
        self.gamma       = nn.Parameter(torch.zeros(1))

        '''Core of Tree Filter'''
        self.rst_layer = RandomSpanningTree(TreeFilter2D.norm2_distance, torch.exp)
        self.mst_layer = MinimumSpanningTree(TreeFilter2D.norm2_distance, torch.exp)
        self.tree_filter_layer  = TreeFilter2D(groups=num_groups)

        ''' Parameters init '''
        self.reset_parameter()

    def reset_parameter(self):
        nn.init.constant_(self.conf_layer.weight, 0)
        nn.init.normal_(self.embed_layer.weight, std=0.01)
        nn.init.normal_(self.guide_layer.weight, std=0.01)
        nn.init.constant_(self.gamma, 0)
        nn.init.constant_(self.beta, 0)

    def split_groups(self, x):
        x = x.reshape(x.shape[0] * self.num_groups, -1, *x.shape[2:])
        return x

    def expand_groups(self, x):
        target_dim = max(self.num_groups // x.shape[1], 1)
        x = x.unsqueeze(2)
        x = x.expand(*x.shape[:2], target_dim, *x.shape[3:])
        x = x.reshape(x.shape[0], -1, *x.shape[3:])
        return x

    def forward(self, feature, guide):
        latent = feature

        ''' Compute embedding features '''
        embed = self.embed_layer(feature)

        ''' Spanning tree process '''
        guide = F.adaptive_avg_pool2d(guide, feature.shape[-2:])
        guide_embed = self.guide_layer(guide)
        if self.training:
            tree = self.rst_layer(guide_embed)
        else:
            tree = self.mst_layer(guide_embed)

        ''' Reshape beta '''
        beta = self.beta.reshape(1, -1, 1, 1)
        beta = beta.expand(embed.shape[0], self.num_groups, *embed.shape[2:])

        ''' Compute confidence '''
        conf = self.conf_layer(feature).sigmoid()
        conf = self.expand_groups(conf)
        conf_norm = self.tree_filter_layer(conf, embed, tree, guide_embed, beta)

        ''' Feature transform '''
        feature = (self.split_groups(feature) * self.split_groups(conf)).reshape_as(feature)
        feature = self.tree_filter_layer(feature, embed, tree, guide_embed, beta)
        feature_size = feature.size()
        feature = self.split_groups(feature) / (self.eps + self.split_groups(conf_norm))
        feature = feature.reshape(feature_size)

        ''' Projection '''
        feature = self.gamma * feature
        feature = feature + latent

        return feature
```

### cvpods/layers/rotated_boxes.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from __future__ import absolute_import, division, print_function, unicode_literals

# import torch
from cvpods import _C


def pairwise_iou_rotated(boxes1, boxes2):
    """
    Return intersection-over-union (Jaccard index) of boxes.

    Both sets of boxes are expected to be in
    (x_center, y_center, width, height, angle) format.

    Arguments:
        boxes1 (Tensor[N, 5])
        boxes2 (Tensor[M, 5])

    Returns:
        iou (Tensor[N, M]): the NxM matrix containing the pairwise
            IoU values for every element in boxes1 and boxes2
    """
    return _C.box_iou_rotated(boxes1, boxes2)
```

### cvpods/layers/border_align.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from torch import nn
from torch.autograd import Function
from torch.autograd.function import once_differentiable

from cvpods import _C


class BorderAlignFunc(Function):
    @staticmethod
    def forward(ctx, input, boxes, wh, pool_size):
        output = _C.border_align_forward(input, boxes, wh, pool_size)
        ctx.pool_size = pool_size
        ctx.save_for_backward(input, boxes, wh)
        return output

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        pool_size = ctx.pool_size
        input, boxes, wh = ctx.saved_tensors
        grad_input = _C.border_align_backward(
            grad_output, input, boxes, wh, pool_size)
        return grad_input, None, None, None


border_align = BorderAlignFunc.apply


class BorderAlign(nn.Module):
    def __init__(self, pool_size):
        super(BorderAlign, self).__init__()
        self.pool_size = pool_size

    def forward(self, feature, boxes):
        feature = feature.contiguous()
        boxes = boxes.contiguous()
        wh = (boxes[:, :, 2:] - boxes[:, :, :2]).contiguous()
        output = border_align(feature, boxes, wh, self.pool_size)
        return output

    def __repr__(self):
        tmpstr = self.__class__.__name__
        return tmpstr
```

#### cvpods/layers/csrc/cuda_version.cu

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.

#include <cuda_runtime_api.h>

namespace cvpods {
int get_cudart_version() {
  return CUDART_VERSION;
}
} // namespace cvpods
```

#### cvpods/layers/csrc/vision.cpp

```cpp
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

#include <torch/types.h>
#include "ROIAlign/ROIAlign.h"
#include "ROIAlignRotated/ROIAlignRotated.h"
#include "box_iou_rotated/box_iou_rotated.h"
#include "cocoeval/cocoeval.h"
#include "deformable/deform_conv.h"
#include "nms_rotated/nms_rotated.h"
#include "sigmoid_focal_loss/SigmoidFocalLoss.h"
#include "ml_nms/ml_nms.h"
#include "SwapAlign2Nat/SwapAlign2Nat.h"
#include "border_align/border_align.h"
#include "PSROIPool/psroi_pool_cuda.h"
#include "tree_filter/refine.hpp"
#include "tree_filter/mst.hpp"
#include "tree_filter/rst.hpp"
#include "tree_filter/bfs.hpp"

namespace cvpods {

#ifdef WITH_CUDA
extern int get_cudart_version();
#endif

std::string get_cuda_version() {
#ifdef WITH_CUDA
  std::ostringstream oss;

  // copied from
  // https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/detail/CUDAHooks.cpp#L231
  auto printCudaStyleVersion = [&](int v) {
    oss << (v / 1000) << "." << (v / 10 % 100);
    if (v % 10 != 0) {
      oss << "." << (v % 10);
    }
  };
  printCudaStyleVersion(get_cudart_version());
  return oss.str();
#else
  return std::string("not available");
#endif
}

// similar to
// https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/Version.cpp
std::string get_compiler_version() {
  std::ostringstream ss;
#if defined(__GNUC__)
#ifndef __clang__

#if ((__GNUC__ <= 4) && (__GNUC_MINOR__ <= 8))
#error "GCC >= 4.9 is required!"
#endif

  { ss << "GCC " << __GNUC__ << "." << __GNUC_MINOR__; }
#endif
#endif

#if defined(__clang_major__)
  {
    ss << "clang " << __clang_major__ << "." << __clang_minor__ << "."
       << __clang_patchlevel__;
  }
#endif

#if defined(_MSC_VER)
  { ss << "MSVC " << _MSC_FULL_VER; }
#endif
  return ss.str();
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
  m.def("get_compiler_version", &get_compiler_version, "get_compiler_version");
  m.def("get_cuda_version", &get_cuda_version, "get_cuda_version");

  m.def("sigmoid_focalloss_forward", &SigmoidFocalLoss_forward, "SigmoidFocalLoss_forward");
  m.def("sigmoid_focalloss_backward", &SigmoidFocalLoss_backward, "SigmoidFocalLoss_backward");

  m.def("box_iou_rotated", &box_iou_rotated, "IoU for rotated boxes");

  m.def("deform_conv_forward", &deform_conv_forward, "deform_conv_forward");
  m.def(
      "deform_conv_backward_input",
      &deform_conv_backward_input,
      "deform_conv_backward_input");
  m.def(
      "deform_conv_backward_filter",
      &deform_conv_backward_filter,
      "deform_conv_backward_filter");
  m.def(
      "modulated_deform_conv_forward",
      &modulated_deform_conv_forward,
      "modulated_deform_conv_forward");
  m.def(
      "modulated_deform_conv_backward",
      &modulated_deform_conv_backward,
      "modulated_deform_conv_backward");

  m.def("nms_rotated", &nms_rotated, "NMS for rotated boxes");
  m.def("ml_nms", &ml_nms, "multi-label non-maximum suppression");

  m.def("psroi_pooling_forward_cuda",
        &psroi_pooling_forward_cuda,
        "Forward pass for PSROI-Pooling Operator");
  m.def("psroi_pooling_backward_cuda",
        &psroi_pooling_backward_cuda,
        "Backward pass for PSROI-Pooling Operator");

  m.def("roi_align_forward", &ROIAlign_forward, "ROIAlign_forward");
  m.def("roi_align_backward", &ROIAlign_backward, "ROIAlign_backward");

  m.def(
      "roi_align_rotated_forward",
      &ROIAlignRotated_forward,
      "Forward pass for Rotated ROI-Align Operator");
  m.def(
      "roi_align_rotated_backward",
      &ROIAlignRotated_backward,
      "Backward pass for Rotated ROI-Align Operator");
  m.def(
      "swap_align2nat_forward",
      &SwapAlign2Nat_forward,
      "SwapAlign2Nat_forward");
  m.def(
      "swap_align2nat_backward",
      &SwapAlign2Nat_backward,
      "SwapAlign2Nat_backward");
  m.def(
      "border_align_forward",
      &BorderAlign_Forward,
      "BorderAlign_Forward");
  m.def(
      "border_align_backward",
      &BorderAlign_Backward,
      "BorderAlign_Backward");
  m.def("COCOevalAccumulate", &COCOeval::Accumulate, "COCOeval::Accumulate");
  m.def(
      "COCOevalEvaluateImages",
      &COCOeval::EvaluateImages,
      "COCOeval::EvaluateImages");

  m.def("rst_forward", &rst_forward, "rst forward");
  m.def("mst_forward", &mst_forward, "mst forward");
  m.def("bfs_forward", &bfs_forward, "bfs forward");
  m.def("tree_filter_refine_forward",
        &tree_filter_refine_forward,
        "tree filter refine forward");
  m.def("tree_filter_refine_backward_feature",
        &tree_filter_refine_backward_feature,
        "tree filter refine backward wrt feature");
  m.def("tree_filter_refine_backward_edge_weight",
        &tree_filter_refine_backward_edge_weight,
        "tree filter refine backward wrt edge weight");
  m.def("tree_filter_refine_backward_self_weight",
        &tree_filter_refine_backward_self_weight,
        "tree filter refine backward wrt self weight");

  pybind11::class_<COCOeval::InstanceAnnotation>(m, "InstanceAnnotation")
      .def(pybind11::init<uint64_t, double, double, bool, bool>());
  pybind11::class_<COCOeval::ImageEvaluation>(m, "ImageEvaluation")
      .def(pybind11::init<>());
}

} // namespace cvpods
```

##### cvpods/layers/csrc/cocoeval/cocoeval.h

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#pragma once

#include <pybind11/numpy.h>
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>
#include <pybind11/stl_bind.h>
#include <vector>

namespace py = pybind11;

namespace cvpods {

namespace COCOeval {

// Annotation data for a single object instance in an image
struct InstanceAnnotation {
  InstanceAnnotation(
      uint64_t id,
      double score,
      double area,
      bool is_crowd,
      bool ignore)
      : id{id}, score{score}, area{area}, is_crowd{is_crowd}, ignore{ignore} {}
  uint64_t id;
  double score = 0.;
  double area = 0.;
  bool is_crowd = false;
  bool ignore = false;
};

// Stores intermediate results for evaluating detection results for a single
// image that has D detected instances and G ground truth instances. This stores
// matches between detected and ground truth instances
struct ImageEvaluation {
  // For each of the D detected instances, the id of the matched ground truth
  // instance, or 0 if unmatched
  std::vector<uint64_t> detection_matches;

  // The detection score of each of the D detected instances
  std::vector<double> detection_scores;

  // Marks whether or not each of G instances was ignored from evaluation (e.g.,
  // because it's outside area_range)
  std::vector<bool> ground_truth_ignores;

  // Marks whether or not each of D instances was ignored from evaluation (e.g.,
  // because it's outside aRng)
  std::vector<bool> detection_ignores;
};

template <class T>
using ImageCategoryInstances = std::vector<std::vector<std::vector<T>>>;

// C++ implementation of COCO API cocoeval.py::COCOeval.evaluateImg().  For each
// combination of image, category, area range settings, and IOU thresholds to
// evaluate, it matches detected instances to ground truth instances and stores
// the results into a vector of ImageEvaluation results, which will be
// interpreted by the COCOeval::Accumulate() function to produce precion-recall
// curves.  The parameters of nested vectors have the following semantics:
//   image_category_ious[i][c][d][g] is the intersection over union of the d'th
//     detected instance and g'th ground truth instance of
//     category category_ids[c] in image image_ids[i]
//   image_category_ground_truth_instances[i][c] is a vector of ground truth
//     instances in image image_ids[i] of category category_ids[c]
//   image_category_detection_instances[i][c] is a vector of detected
//     instances in image image_ids[i] of category category_ids[c]
std::vector<ImageEvaluation> EvaluateImages(
    const std::vector<std::array<double, 2>>& area_ranges, // vector of 2-tuples
    int max_detections,
    const std::vector<double>& iou_thresholds,
    const ImageCategoryInstances<std::vector<double>>& image_category_ious,
    const ImageCategoryInstances<InstanceAnnotation>&
        image_category_ground_truth_instances,
    const ImageCategoryInstances<InstanceAnnotation>&
        image_category_detection_instances);

// C++ implementation of COCOeval.accumulate(), which generates precision
// recall curves for each set of category, IOU threshold, detection area range,
// and max number of detections parameters.  It is assumed that the parameter
// evaluations is the return value of the functon COCOeval::EvaluateImages(),
// which was called with the same parameter settings params
py::dict Accumulate(
    const py::object& params,
    const std::vector<ImageEvaluation>& evalutations);

} // namespace COCOeval
} // namespace cvpods```

##### cvpods/layers/csrc/cocoeval/cocoeval.cpp

```cpp
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#include "cocoeval.h"
#include <algorithm>
#include <cstdint>
#include <numeric>

using namespace pybind11::literals;

namespace cvpods {

namespace COCOeval {

// Sort detections from highest score to lowest, such that
// detection_instances[detection_sorted_indices[t]] >=
// detection_instances[detection_sorted_indices[t+1]].  Use stable_sort to match
// original COCO API
void SortInstancesByDetectionScore(
    const std::vector<InstanceAnnotation>& detection_instances,
    std::vector<uint64_t>* detection_sorted_indices) {
  detection_sorted_indices->resize(detection_instances.size());
  std::iota(
      detection_sorted_indices->begin(), detection_sorted_indices->end(), 0);
  std::stable_sort(
      detection_sorted_indices->begin(),
      detection_sorted_indices->end(),
      [&detection_instances](size_t j1, size_t j2) {
        return detection_instances[j1].score > detection_instances[j2].score;
      });
}

// Partition the ground truth objects based on whether or not to ignore them
// based on area
void SortInstancesByIgnore(
    const std::array<double, 2>& area_range,
    const std::vector<InstanceAnnotation>& ground_truth_instances,
    std::vector<uint64_t>* ground_truth_sorted_indices,
    std::vector<bool>* ignores) {
  ignores->clear();
  ignores->reserve(ground_truth_instances.size());
  for (auto o : ground_truth_instances) {
    ignores->push_back(
        o.ignore || o.area < area_range[0] || o.area > area_range[1]);
  }

  ground_truth_sorted_indices->resize(ground_truth_instances.size());
  std::iota(
      ground_truth_sorted_indices->begin(),
      ground_truth_sorted_indices->end(),
      0);
  std::stable_sort(
      ground_truth_sorted_indices->begin(),
      ground_truth_sorted_indices->end(),
      [&ignores](size_t j1, size_t j2) {
        return (int)(*ignores)[j1] < (int)(*ignores)[j2];
      });
}

// For each IOU threshold, greedily match each detected instance to a ground
// truth instance (if possible) and store the results
void MatchDetectionsToGroundTruth(
    const std::vector<InstanceAnnotation>& detection_instances,
    const std::vector<uint64_t>& detection_sorted_indices,
    const std::vector<InstanceAnnotation>& ground_truth_instances,
    const std::vector<uint64_t>& ground_truth_sorted_indices,
    const std::vector<bool>& ignores,
    const std::vector<std::vector<double>>& ious,
    const std::vector<double>& iou_thresholds,
    const std::array<double, 2>& area_range,
    ImageEvaluation* results) {
  // Initialize memory to store return data matches and ignore
  const uint64_t num_iou_thresholds = iou_thresholds.size();
  const uint64_t num_ground_truth = ground_truth_sorted_indices.size();
  const uint64_t num_detections = detection_sorted_indices.size();
  std::vector<uint64_t> ground_truth_matches(
      num_iou_thresholds * num_ground_truth, 0);
  std::vector<uint64_t>& detection_matches = results->detection_matches;
  std::vector<bool>& detection_ignores = results->detection_ignores;
  std::vector<bool>& ground_truth_ignores = results->ground_truth_ignores;
  detection_matches.resize(num_iou_thresholds * num_detections, 0);
  detection_ignores.resize(num_iou_thresholds * num_detections, false);
  ground_truth_ignores.resize(num_ground_truth);
  for (uint64_t g = 0; g < num_ground_truth; ++g) {
    ground_truth_ignores[g] = ignores[ground_truth_sorted_indices[g]];
  }

  for (uint64_t t = 0; t < num_iou_thresholds; ++t) {
    for (uint64_t d = 0; d < num_detections; ++d) {
      // information about best match so far (match=-1 -> unmatched)
      double best_iou = std::min(iou_thresholds[t], 1 - 1e-10);
      int64_t match = -1;
      for (uint64_t g = 0; g < num_ground_truth; ++g) {
        // if this ground truth instance is already matched and not a
        // crowd, it cannot be matched to another detection
        if (ground_truth_matches[t * num_ground_truth + g] > 0 &&
            !ground_truth_instances[ground_truth_sorted_indices[g]].is_crowd) {
          continue;
        }

        // if detected instance matched to a regular ground truth
        // instance, we can break on the first ground truth instance
        // tagged as ignore (because they are sorted by the ignore tag)
        if (match >= 0 && !ground_truth_ignores[match] &&
            ground_truth_ignores[g]) {
          break;
        }

        // if IOU overlap is the best so far, store the match appropriately
        if (ious[d][ground_truth_sorted_indices[g]] >= best_iou) {
          best_iou = ious[d][ground_truth_sorted_indices[g]];
          match = g;
        }
      }
      // if match was made, store id of match for both detection and
      // ground truth
      if (match >= 0) {
        detection_ignores[t * num_detections + d] = ground_truth_ignores[match];
        detection_matches[t * num_detections + d] =
            ground_truth_instances[ground_truth_sorted_indices[match]].id;
        ground_truth_matches[t * num_ground_truth + match] =
            detection_instances[detection_sorted_indices[d]].id;
      }

      // set unmatched detections outside of area range to ignore
      const InstanceAnnotation& detection =
          detection_instances[detection_sorted_indices[d]];
      detection_ignores[t * num_detections + d] =
          detection_ignores[t * num_detections + d] ||
          (detection_matches[t * num_detections + d] == 0 &&
           (detection.area < area_range[0] || detection.area > area_range[1]));
    }
  }

  // store detection score results
  results->detection_scores.resize(detection_sorted_indices.size());
  for (uint64_t d = 0; d < detection_sorted_indices.size(); ++d) {
    results->detection_scores[d] =
        detection_instances[detection_sorted_indices[d]].score;
  }
}

std::vector<ImageEvaluation> EvaluateImages(
    const std::vector<std::array<double, 2>>& area_ranges,
    int max_detections,
    const std::vector<double>& iou_thresholds,
    const ImageCategoryInstances<std::vector<double>>& image_category_ious,
    const ImageCategoryInstances<InstanceAnnotation>&
        image_category_ground_truth_instances,
    const ImageCategoryInstances<InstanceAnnotation>&
        image_category_detection_instances) {
  const uint64_t num_area_ranges = area_ranges.size();
  const uint64_t num_images = image_category_ground_truth_instances.size();
  const uint64_t num_categories =
      image_category_ious.size() > 0 ? image_category_ious[0].size() : 0;
  std::vector<uint64_t> detection_sorted_indices;
  std::vector<uint64_t> ground_truth_sorted_indices;
  std::vector<bool> ignores;
  std::vector<ImageEvaluation> results_all(
      num_images * num_area_ranges * num_categories);

  // Store results for each image, category, and area range combination. Results
  // for each IOU threshold are packed into the same ImageEvaluation object
  for (uint64_t i = 0; i < num_images; ++i) {
    for (uint64_t c = 0; c < num_categories; ++c) {
      const std::vector<InstanceAnnotation>& ground_truth_instances =
          image_category_ground_truth_instances[i][c];
      const std::vector<InstanceAnnotation>& detection_instances =
          image_category_detection_instances[i][c];

      SortInstancesByDetectionScore(
          detection_instances, &detection_sorted_indices);
      if (detection_sorted_indices.size() > (uint)max_detections) {
        detection_sorted_indices.resize(max_detections);
      }

      for (uint64_t a = 0; a < area_ranges.size(); ++a) {
        SortInstancesByIgnore(
            area_ranges[a],
            ground_truth_instances,
            &ground_truth_sorted_indices,
            &ignores);

        MatchDetectionsToGroundTruth(
            detection_instances,
            detection_sorted_indices,
            ground_truth_instances,
            ground_truth_sorted_indices,
            ignores,
            image_category_ious[i][c],
            iou_thresholds,
            area_ranges[a],
            &results_all
                [c * num_area_ranges * num_images + a * num_images + i]);
      }
    }
  }

  return results_all;
}

// Convert a python list to a vector
template <typename T>
std::vector<T> list_to_vec(const py::list& l) {
  std::vector<T> v(py::len(l));
  for (uint i = 0; i < py::len(l); ++i) {
    v[i] = l[i].cast<T>();
  }
  return v;
}

// Helper function to Accumulate()
// Considers the evaluation results applicable to a particular category, area
// range, and max_detections parameter setting, which begin at
// evaluations[evaluation_index].  Extracts a sorted list of length n of all
// applicable detection instances concatenated across all images in the dataset,
// which are represented by the outputs evaluation_indices, detection_scores,
// image_detection_indices, and detection_sorted_indices--all of which are
// length n. evaluation_indices[i] stores the applicable index into
// evaluations[] for instance i, which has detection score detection_score[i],
// and is the image_detection_indices[i]'th of the list of detections
// for the image containing i.  detection_sorted_indices[] defines a sorted
// permutation of the 3 other outputs
int64_t BuildSortedDetectionList(
    const std::vector<ImageEvaluation>& evaluations,
    const int64_t evaluation_index,
    const int64_t num_images,
    const int64_t max_detections,
    std::vector<uint64_t>* evaluation_indices,
    std::vector<double>* detection_scores,
    std::vector<uint64_t>* detection_sorted_indices,
    std::vector<uint64_t>* image_detection_indices) {
  assert(evaluations.size() >= evaluation_index + num_images);

  // Extract a list of object instances of the applicable category, area
  // range, and max detections requirements such that they can be sorted
  image_detection_indices->clear();
  evaluation_indices->clear();
  detection_scores->clear();
  image_detection_indices->reserve(num_images * max_detections);
  evaluation_indices->reserve(num_images * max_detections);
  detection_scores->reserve(num_images * max_detections);
  int64_t num_valid_ground_truth = 0;
  for (auto i = 0; i < num_images; ++i) {
    const ImageEvaluation& evaluation = evaluations[evaluation_index + i];

    for (int64_t d = 0;
         (uint64_t)d < evaluation.detection_scores.size() && d < max_detections;
         ++d) { // detected instances
      evaluation_indices->push_back(evaluation_index + i);
      image_detection_indices->push_back(d);
      detection_scores->push_back(evaluation.detection_scores[d]);
    }
    for (auto ground_truth_ignore : evaluation.ground_truth_ignores) {
      if (!ground_truth_ignore) {
        ++num_valid_ground_truth;
      }
    }
  }

  // Sort detections by decreasing score, using stable sort to match
  // python implementation
  detection_sorted_indices->resize(detection_scores->size());
  std::iota(
      detection_sorted_indices->begin(), detection_sorted_indices->end(), 0);
  std::stable_sort(
      detection_sorted_indices->begin(),
      detection_sorted_indices->end(),
      [&detection_scores](size_t j1, size_t j2) {
        return (*detection_scores)[j1] > (*detection_scores)[j2];
      });

  return num_valid_ground_truth;
}

// Helper function to Accumulate()
// Compute a precision recall curve given a sorted list of detected instances
// encoded in evaluations, evaluation_indices, detection_scores,
// detection_sorted_indices, image_detection_indices (see
// BuildSortedDetectionList()). Using vectors precisions and recalls
// and temporary storage, output the results into precisions_out, recalls_out,
// and scores_out, which are large buffers containing many precion/recall curves
// for all possible parameter settings, with precisions_out_index and
// recalls_out_index defining the applicable indices to store results.
void ComputePrecisionRecallCurve(
    const int64_t precisions_out_index,
    const int64_t precisions_out_stride,
    const int64_t recalls_out_index,
    const std::vector<double>& recall_thresholds,
    const int64_t iou_threshold_index,
    const int64_t num_iou_thresholds,
    const int64_t num_valid_ground_truth,
    const std::vector<ImageEvaluation>& evaluations,
    const std::vector<uint64_t>& evaluation_indices,
    const std::vector<double>& detection_scores,
    const std::vector<uint64_t>& detection_sorted_indices,
    const std::vector<uint64_t>& image_detection_indices,
    std::vector<double>* precisions,
    std::vector<double>* recalls,
    std::vector<double>* precisions_out,
    std::vector<double>* scores_out,
    std::vector<double>* recalls_out) {
  assert(recalls_out->size() > recalls_out_index);

  // Compute precision/recall for each instance in the sorted list of detections
  int64_t true_positives_sum = 0, false_positives_sum = 0;
  precisions->clear();
  recalls->clear();
  precisions->reserve(detection_sorted_indices.size());
  recalls->reserve(detection_sorted_indices.size());
  assert(!evaluations.empty() || detection_sorted_indices.empty());
  for (auto detection_sorted_index : detection_sorted_indices) {
    const ImageEvaluation& evaluation =
        evaluations[evaluation_indices[detection_sorted_index]];
    const auto num_detections =
        evaluation.detection_matches.size() / num_iou_thresholds;
    const auto detection_index = iou_threshold_index * num_detections +
        image_detection_indices[detection_sorted_index];
    assert(evaluation.detection_matches.size() > detection_index);
    assert(evaluation.detection_ignores.size() > detection_index);
    const int64_t detection_match =
        evaluation.detection_matches[detection_index];
    const bool detection_ignores =
        evaluation.detection_ignores[detection_index];
    const auto true_positive = detection_match > 0 && !detection_ignores;
    const auto false_positive = detection_match == 0 && !detection_ignores;
    if (true_positive) {
      ++true_positives_sum;
    }
    if (false_positive) {
      ++false_positives_sum;
    }

    const double recall =
        static_cast<double>(true_positives_sum) / num_valid_ground_truth;
    recalls->push_back(recall);
    const int64_t num_valid_detections =
        true_positives_sum + false_positives_sum;
    const double precision = num_valid_detections > 0
        ? static_cast<double>(true_positives_sum) / num_valid_detections
        : 0.0;
    precisions->push_back(precision);
  }

  (*recalls_out)[recalls_out_index] = !recalls->empty() ? recalls->back() : 0;

  for (int64_t i = static_cast<int64_t>(precisions->size()) - 1; i > 0; --i) {
    if ((*precisions)[i] > (*precisions)[i - 1]) {
      (*precisions)[i - 1] = (*precisions)[i];
    }
  }

  // Sample the per instance precision/recall list at each recall threshold
  for (uint64_t r = 0; r < recall_thresholds.size(); ++r) {
    // first index in recalls >= recall_thresholds[r]
    std::vector<double>::iterator low = std::lower_bound(
        recalls->begin(), recalls->end(), recall_thresholds[r]);
    const uint64_t precisions_index = low - recalls->begin();

    const auto results_ind = precisions_out_index + r * precisions_out_stride;
    assert(results_ind < precisions_out->size());
    assert(results_ind < scores_out->size());
    if (precisions_index < precisions->size()) {
      (*precisions_out)[results_ind] = (*precisions)[precisions_index];
      (*scores_out)[results_ind] =
          detection_scores[detection_sorted_indices[precisions_index]];
    } else {
      (*precisions_out)[results_ind] = 0;
      (*scores_out)[results_ind] = 0;
    }
  }
}
py::dict Accumulate(
    const py::object& params,
    const std::vector<ImageEvaluation>& evaluations) {
  const std::vector<double> recall_thresholds =
      list_to_vec<double>(params.attr("recThrs"));
  const std::vector<int64_t> max_detections =
      list_to_vec<int64_t>(params.attr("maxDets"));
  const int64_t num_iou_thresholds = py::len(params.attr("iouThrs"));
  const int64_t num_recall_thresholds = py::len(params.attr("recThrs"));
  const int64_t num_categories = params.attr("useCats").cast<int64_t>() == 1
      ? py::len(params.attr("catIds"))
      : 1;
  const int64_t num_area_ranges = py::len(params.attr("areaRng"));
  const int64_t num_max_detections = py::len(params.attr("maxDets"));
  const int64_t num_images = py::len(params.attr("imgIds"));

  std::vector<double> precisions_out(
      num_iou_thresholds * num_recall_thresholds * num_categories *
          num_area_ranges * num_max_detections,
      -1);
  std::vector<double> recalls_out(
      num_iou_thresholds * num_categories * num_area_ranges *
          num_max_detections,
      -1);
  std::vector<double> scores_out(
      num_iou_thresholds * num_recall_thresholds * num_categories *
          num_area_ranges * num_max_detections,
      -1);

  // Consider the list of all detected instances in the entire dataset in one
  // large list.  evaluation_indices, detection_scores,
  // image_detection_indices, and detection_sorted_indices all have the same
  // length as this list, such that each entry corresponds to one detected
  // instance
  std::vector<uint64_t> evaluation_indices; // indices into evaluations[]
  std::vector<double> detection_scores; // detection scores of each instance
  std::vector<uint64_t> detection_sorted_indices; // sorted indices of all
                                                  // instances in the dataset
  std::vector<uint64_t>
      image_detection_indices; // indices into the list of detected instances in
                               // the same image as each instance
  std::vector<double> precisions, recalls;

  for (auto c = 0; c < num_categories; ++c) {
    for (auto a = 0; a < num_area_ranges; ++a) {
      for (auto m = 0; m < num_max_detections; ++m) {
        // The COCO PythonAPI assumes evaluations[] (the return value of
        // COCOeval::EvaluateImages() is one long list storing results for each
        // combination of category, area range, and image id, with categories in
        // the outermost loop and images in the innermost loop.
        const int64_t evaluations_index =
            c * num_area_ranges * num_images + a * num_images;
        int64_t num_valid_ground_truth = BuildSortedDetectionList(
            evaluations,
            evaluations_index,
            num_images,
            max_detections[m],
            &evaluation_indices,
            &detection_scores,
            &detection_sorted_indices,
            &image_detection_indices);

        if (num_valid_ground_truth == 0) {
          continue;
        }

        for (auto t = 0; t < num_iou_thresholds; ++t) {
          // recalls_out is a flattened vectors representing a
          // num_iou_thresholds X num_categories X num_area_ranges X
          // num_max_detections matrix
          const int64_t recalls_out_index =
              t * num_categories * num_area_ranges * num_max_detections +
              c * num_area_ranges * num_max_detections +
              a * num_max_detections + m;

          // precisions_out and scores_out are flattened vectors
          // representing a num_iou_thresholds X num_recall_thresholds X
          // num_categories X num_area_ranges X num_max_detections matrix
          const int64_t precisions_out_stride =
              num_categories * num_area_ranges * num_max_detections;
          const int64_t precisions_out_index = t * num_recall_thresholds *
                  num_categories * num_area_ranges * num_max_detections +
              c * num_area_ranges * num_max_detections +
              a * num_max_detections + m;

          ComputePrecisionRecallCurve(
              precisions_out_index,
              precisions_out_stride,
              recalls_out_index,
              recall_thresholds,
              t,
              num_iou_thresholds,
              num_valid_ground_truth,
              evaluations,
              evaluation_indices,
              detection_scores,
              detection_sorted_indices,
              image_detection_indices,
              &precisions,
              &recalls,
              &precisions_out,
              &scores_out,
              &recalls_out);
        }
      }
    }
  }

  time_t rawtime;
  struct tm local_time;
  std::array<char, 200> buffer;
  time(&rawtime);
  localtime_r(&rawtime, &local_time);
  strftime(
      buffer.data(), 200, "%Y-%m-%d %H:%num_max_detections:%S", &local_time);
  return py::dict(
      "params"_a = params,
      "counts"_a = std::vector<int64_t>({num_iou_thresholds,
                                         num_recall_thresholds,
                                         num_categories,
                                         num_area_ranges,
                                         num_max_detections}),
      "date"_a = buffer,
      "precision"_a = precisions_out,
      "recall"_a = recalls_out,
      "scores"_a = scores_out);
}

} // namespace COCOeval

} // namespace cvpods
```

##### cvpods/layers/csrc/PSROIPool/psroi_pool_kernel.cu

```
#include <torch/types.h>
#include <torch/extension.h>
#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>
#include <ATen/cuda/CUDAApplyUtils.cuh>

#include <stdio.h>
#include <math.h>
#include <float.h>

#define CUDA_1D_KERNEL_LOOP(i, n)                            \
  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < n; \
       i += blockDim.x * gridDim.x)


template <typename T>
__global__ void PSROIPoolForward(
    const T* bottom_data,
    const T spatial_scale,
    const int num_rois,
    const int height,
    const int width,
    const int channels,
    const int pooled_height,
    const int pooled_width,
    const T* bottom_rois,
    const int group_size,
    const int output_dim,
    T* top_data,
    int* mapping_channel,
    cudaStream_t stream)
{
    const long output_size = output_dim * pooled_height * pooled_width * num_rois;
    const long nthreads = output_size;

    CUDA_1D_KERNEL_LOOP(index, nthreads)
    {
        // (n, c, ph, pw) is an element in the pooled output
        int pw = index % pooled_width;
        int ph = (index / pooled_width) % pooled_height;
        int ctop = (index / pooled_width / pooled_height) % output_dim;
        int n = index / pooled_width / pooled_height / output_dim;

        bottom_rois += n * 5;
        int roi_batch_ind = bottom_rois[0];
        T roi_start_w = bottom_rois[1] * spatial_scale;
        T roi_start_h = bottom_rois[2] * spatial_scale;
        T roi_end_w = (bottom_rois[3] + 1) * spatial_scale;
        T roi_end_h = (bottom_rois[4] + 1) * spatial_scale;
        
        T roi_width = roi_end_w - roi_start_w;
        T roi_height = roi_end_h - roi_start_h;

        // skip invalid rois
        if(roi_width <= 0 || roi_height <= 0)
        {
            continue;
        }

        T bin_size_h = roi_height / static_cast<T>(pooled_height);
        T bin_size_w = roi_width / static_cast<T>(pooled_width);

        int hstart = floor(static_cast<T>(ph) * bin_size_h + roi_start_h);
        int wstart = floor(static_cast<T>(pw) * bin_size_w + roi_start_w);
        int hend = ceil(static_cast<T>(ph + 1) * bin_size_h + roi_start_h);
        int wend = ceil(static_cast<T>(pw + 1) * bin_size_w + roi_start_w);

        // Add roi offsets and clip to input boundaries
        hstart = min(max(hstart, 0), height);
        hend = min(max(hend, 0), height);
        wstart = min(max(wstart, 0), width);
        wend = min(max(wend, 0), width);
        bool is_empty = (hend <= hstart) || (wend <= wstart);

        int gw = pw;
        int gh = ph;
        int c = (ctop * group_size + gh) * group_size + gw;

        bottom_data += (roi_batch_ind * channels + c) * height * width;
        T out_sum = 0;
        for (int h = hstart; h < hend; ++h) {
          for (int w = wstart; w < wend; ++w) {
            int bottom_index = h * width + w;
            out_sum += bottom_data[bottom_index];
          }
        }
        float bin_area = (hend - hstart) * (wend - wstart);
        //top_data[index] = nthreads;
        top_data[index] = is_empty ? 0. : out_sum / bin_area;
        mapping_channel[index] = c;
    }
}

template <typename T>
__global__ void PSROIPoolBackward(const T* top_diff,
    const int* mapping_channel,
    const int batch_size,
    const int num_rois,
    const T spatial_scale,
    const int channels,
    const int height,
    const int width,
    const int pooled_width,
    const int pooled_height,
    const int output_dim,
    T* bottom_diff,
    const T* bottom_rois,
    cudaStream_t stream)
{
    const long output_size = output_dim * pooled_height * pooled_width * num_rois;
    const long nthreads = output_size;

    CUDA_1D_KERNEL_LOOP(index, nthreads)
    {
        int pw = index % pooled_width;
        int ph = (index / pooled_width) % pooled_height;
        int n = index / pooled_width / pooled_height / output_dim;

        // [start, end) interval for spatial sampling
        bottom_rois += n * 5;
        int roi_batch_ind = bottom_rois[0];
        T roi_start_w = bottom_rois[1] * spatial_scale;
        T roi_start_h = bottom_rois[2] * spatial_scale;
        T roi_end_w = (bottom_rois[3] + 1) * spatial_scale;
        T roi_end_h = (bottom_rois[4] + 1) * spatial_scale;
        
        T roi_width = roi_end_w - roi_start_w;
        T roi_height = roi_end_h - roi_start_h;

        // skip invalid rois
        if(roi_width <= 0 || roi_height <= 0)
        {
            continue;
        }

        // Compute w and h at bottom
        T bin_size_h = roi_height / static_cast<T>(pooled_height);
        T bin_size_w = roi_width / static_cast<T>(pooled_width);

        int hstart = floor(static_cast<T>(ph) * bin_size_h + roi_start_h);
        int wstart = floor(static_cast<T>(pw) * bin_size_w + roi_start_w);
        int hend = ceil(static_cast<T>(ph + 1) * bin_size_h + roi_start_h);
        int wend = ceil(static_cast<T>(pw + 1) * bin_size_w + roi_start_w);
        // Add roi offsets and clip to input boundaries
        hstart = min(max(hstart, 0), height);
        hend = min(max(hend, 0), height);
        wstart = min(max(wstart, 0), width);
        wend = min(max(wend, 0), width);
        bool is_empty = (hend <= hstart) || (wend <= wstart);

        // Compute c at bottom
        int c = mapping_channel[index];
        T* offset_bottom_diff = bottom_diff + (roi_batch_ind * channels + c) * height * width;
        float bin_area = (hend - hstart) * (wend - wstart);
        T diff_val = is_empty ? 0. : top_diff[index] / bin_area;
        for (int h = hstart; h < hend; ++h)
        {
            for (int w = wstart; w < wend; ++w)
            {
                int bottom_index = h * width + w;
                //caffe_gpu_atomic_add(diff_val, offset_bottom_diff + bottom_index);
                atomicAdd(offset_bottom_diff + bottom_index, diff_val);
            }
        }
    }
}

namespace cvpods{

at::Tensor psroi_pooling_forward_cuda(
    at::Tensor& features,
    at::Tensor& rois,
    at::Tensor& mapping_channel,
    const int pooled_height,
    const int pooled_width,
    const float spatial_scale,
    const int group_size,
    const int output_dim)
{
    int* mapping_channel_out = mapping_channel.contiguous().data_ptr<int>();
    //Get # of Rois
    int num_rois = rois.size(0);
    int size_rois = rois.size(1);
    AT_ASSERTM(size_rois == 5, "rois channels must be 5");

    at::Tensor output = at::zeros({num_rois, output_dim, pooled_height, pooled_width}, features.options());

    int data_height = features.size(2);
    int data_width = features.size(3);
    int num_channels = features.size(1);

    const int kThreadsPerBlock = 1024;
    const long output_size = (long)num_rois * pooled_height * pooled_width * num_channels;
    cudaStream_t stream = at::cuda::getCurrentCUDAStream();

    AT_DISPATCH_FLOATING_TYPES(features.scalar_type(), "PSROIPoolForward", [&] {
        scalar_t* data_in = features.contiguous().data_ptr<scalar_t>();
        scalar_t* rois_in = rois.contiguous().data_ptr<scalar_t>();
        scalar_t* output_out = output.contiguous().data_ptr<scalar_t>();

        // call the gpu kernel for psroi_pooling
        PSROIPoolForward<scalar_t><<<(output_size + kThreadsPerBlock - 1) / kThreadsPerBlock, kThreadsPerBlock, 0, stream>>>(
            data_in, spatial_scale, num_rois,
            data_height, data_width, num_channels,
            pooled_height, pooled_width, rois_in,
            group_size, output_dim,
            output_out, mapping_channel_out, stream
        );
    });

    cudaError_t err = cudaGetLastError();
    if(cudaSuccess != err)
    {
        printf("error in psroi_pooling_forward_cuda: %s\n", cudaGetErrorString(err));
        exit(-1);
    }

    return output;
}

at::Tensor psroi_pooling_backward_cuda(
    at::Tensor& top_grad,
    at::Tensor& rois,
    at::Tensor& mapping_channel,
    const int batch_size,
    const int bottom_dim,
    const int bottom_height,
    const int bottom_width,
    const float spatial_scale)
{
    int output_dim = top_grad.size(1);
    int pooled_height = top_grad.size(2);
    int pooled_width = top_grad.size(3);
    at::Tensor bottom_grad = at::zeros({batch_size, bottom_dim, bottom_height, bottom_width}, top_grad.options());

    // Number of ROIs
    int num_rois = rois.size(0);
    int size_rois = rois.size(1);
    AT_ASSERTM(size_rois == 5, "rois channels must be 5");

    int* mapping_channel_flat = mapping_channel.contiguous().data_ptr<int>();

    const int kThreadsPerBlock = 1024;
    const long output_size = (long)output_dim * pooled_height * pooled_width * num_rois;

    cudaStream_t stream = at::cuda::getCurrentCUDAStream();

    AT_DISPATCH_FLOATING_TYPES(top_grad.scalar_type(), "PSROIPoolBackward", [&] {
        scalar_t* top_grad_flat = top_grad.contiguous().data_ptr<scalar_t>();
        scalar_t* rois_flat = rois.contiguous().data_ptr<scalar_t>();
        scalar_t* bottom_grad_flat = bottom_grad.contiguous().data_ptr<scalar_t>();

        // call the gpu kernel for psroi_pooling
        PSROIPoolBackward<scalar_t><<<(output_size + kThreadsPerBlock - 1) / kThreadsPerBlock, kThreadsPerBlock, 0, stream>>>(
            top_grad_flat, mapping_channel_flat,
            batch_size, num_rois, spatial_scale, bottom_dim,
            bottom_height, bottom_width, pooled_width,
            pooled_height, output_dim,
            bottom_grad_flat, rois_flat, stream);
    });

    cudaError_t err = cudaGetLastError();
    if(cudaSuccess != err)
    {
        printf("error in psroi_pooling_backward_cuda: %s\n", cudaGetErrorString(err));
        exit(-1);
    }

    return bottom_grad;
}

}
```

##### cvpods/layers/csrc/PSROIPool/psroi_pool_cuda.h

```
#include <torch/types.h>
#include <torch/extension.h>
#include <ATen/ATen.h>
#include <THC/THC.h>
#include <math.h>
#include <stdio.h>

namespace cvpods {
at::Tensor psroi_pooling_forward_cuda(
    at::Tensor& features,
    at::Tensor& rois,
    at::Tensor& mappingchannel,
    const int pooled_height,
    const int pooled_width,
    const float spatial_scale,
    const int group_size,
    const int output_dim);

at::Tensor psroi_pooling_backward_cuda(
    at::Tensor& top_grad,
    at::Tensor& rois,
    at::Tensor& mappingchannel,
    const int batch_size,
    const int bottom_dim,
    const int bottom_height,
    const int bottom_width,
    const float spatial_scale);
}
```

##### cvpods/layers/csrc/border_align/border_align.h

```
#pragma once
#include <torch/types.h>
#include <torch/extension.h>
#include <ATen/ATen.h>

namespace cvpods {

at::Tensor border_align_cuda_forward(
    const at::Tensor& feature,
    const at::Tensor& boxes,
    const at::Tensor& wh,
    const int pool_size);


at::Tensor border_align_cuda_backward(
    const at::Tensor& gradOutput,
    const at::Tensor& feature,
    const at::Tensor& boxes,
    const at::Tensor& wh,
    const int pool_size);


at::Tensor BorderAlign_Forward(
    const at::Tensor& feature,
    const at::Tensor& boxes,
    const at::Tensor& wh,
    const int pool_size) {
    return border_align_cuda_forward(feature, boxes, wh, pool_size);
}


at::Tensor BorderAlign_Backward(
    const at::Tensor& gradOutput,
    const at::Tensor& feature,
    const at::Tensor& boxes,
    const at::Tensor& wh,
    const int pool_size) {
    return border_align_cuda_backward(gradOutput, feature, boxes, wh, pool_size);
}

} // namespace cvpods```

##### cvpods/layers/csrc/border_align/border_align_kernel.cu

```
#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>

#include <THC/THC.h>
#include <THC/THCAtomics.cuh>
#include <THC/THCDeviceUtils.cuh>


#define CUDA_1D_KERNEL_LOOP(i, n)                            \
  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < n; \
       i += blockDim.x * gridDim.x)


template <typename T>
__device__ T bilinear_interpolate(
    const T* bottom_data,
    const int height,
    const int width,
    T y,
    T x) {

    int y_low = (int) y;
    int x_low = (int) x;
    int y_high;
    int x_high;

    if (y_low >= height - 1) {
        y_high = y_low = height - 1;
        y = (T) y_low;
    } else {
        y_high = y_low + 1;
    }

    if (x_low >= width - 1) {
        x_high = x_low = width - 1;
        x = (T) x_low;            
    } else {
        x_high = x_low + 1;
    }

    T ly = y - y_low;
    T lx = x - x_low;
    T hy = 1. - ly, hx = 1. - lx;
    // do bilinear interpolation
    T v1 = bottom_data[y_low * width + x_low];
    T v2 = bottom_data[y_low * width + x_high];
    T v3 = bottom_data[y_high * width + x_low];
    T v4 = bottom_data[y_high * width + x_high];
    T w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;

    T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);

    return val;
}

template <typename T>
__device__ void bilinear_interpolate_gradient(
    const int height,
    const int width,
    T y, T x,
    T & w1, T & w2, T & w3, T & w4,
    int & x_low, int & x_high, int & y_low, int & y_high) {

    y_low = (int) y;
    x_low = (int) x;

    if (y_low >= height - 1) {
        y_high = y_low = height - 1;
        y = (T) y_low;
    } else {
        y_high = y_low + 1;
    }

    if (x_low >= width - 1) {
        x_high = x_low = width - 1;
        x = (T) x_low;
    } else {
        x_high = x_low + 1;
    }

    T ly = y - y_low;
    T lx = x - x_low;
    T hy = 1. - ly, hx = 1. - lx;

    w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;

    return;
}


namespace cvpods {

template <typename T>
__global__ void BorderAlign_Forward(
    const int nthreads,
    T* feature,
    T* boxes,
    T* wh,
    const int channel,
    const int box_size,
    const int height,
    const int width,
    const int pool_size,
    T *output)
{
    T *feature_iter, *output_iter, *boxes_iter, *wh_iter;
    int batch_idx, box_idx, extreme_idx, fm_channel_idx;
    T stride, x_stride, y_stride;
    T x, y;
    T max, val;

    CUDA_1D_KERNEL_LOOP(index, nthreads)
    {
    extreme_idx = threadIdx.y;
    batch_idx = index / channel / box_size;
    box_idx = index % box_size + batch_idx * box_size;
    fm_channel_idx = (int)(index / box_size) % channel;

    boxes_iter = boxes + box_idx * 4 + extreme_idx / 2 * 2;
    wh_iter = wh + box_idx * 2;
    output_iter = output + index * 4 + extreme_idx;
    feature_iter = feature + (batch_idx * channel * 4 + extreme_idx * channel + fm_channel_idx) * height * width;

    x = *boxes_iter;
    y = *(boxes_iter + 1);

    switch(extreme_idx){
        case 0: stride=*wh_iter / pool_size;       x_stride=stride; y_stride=0;       break;
        case 1: stride=*(wh_iter + 1) / pool_size; x_stride=0;      y_stride=stride;  break;
        case 2: stride=*wh_iter / pool_size;       x_stride=-stride;y_stride=0;       break;
        case 3: stride=*(wh_iter + 1) / pool_size; x_stride=0;      y_stride=-stride; break;
    }

    max = bilinear_interpolate(feature_iter, height, width, y, x);
    for(int i = 1; i <= pool_size; i++) {
        x += x_stride;
        y += y_stride;
        val = bilinear_interpolate(feature_iter, height, width, y, x);
        if (val > max) {
            max = val;
        }
    }
    // Update output
    *output_iter = max;
    }
}


at::Tensor border_align_cuda_forward(
    const at::Tensor& feature,
    const at::Tensor& boxes,
    const at::Tensor& wh,
    const int pool_size)
{
    at::TensorArg feature_arg{feature, "feature", 1};
    at::TensorArg boxes_arg{boxes, "boxes", 2};
    at::TensorArg wh_arg{wh, "wh", 3};

    at::checkAllSameGPU("border_align_cuda_forward", {feature_arg, boxes_arg, wh_arg});

    AT_ASSERTM(feature.ndimension() == 4,
        "non-empty 4D(batch mode) tensor expected for feature");

    AT_ASSERTM(boxes.ndimension() == 3,
        "boxes must be 3D tensor with size of [B, H*W, 8]");

    int batch = feature.size(0);
    int fm_channel = feature.size(1);
    int out_channel = fm_channel / 4;
    int height = feature.size(2);
    int width = feature.size(3);
    int box_size = boxes.size(1);
    int output_size = batch * out_channel * box_size;

    at::Tensor pool_output = at::zeros({batch, out_channel, box_size, 4}, feature.options());

    cudaStream_t stream = at::cuda::getCurrentCUDAStream();

    dim3 grid(std::min(THCCeilDiv((long)output_size, 64L), 4096L));
    dim3 block(128, 4);

    AT_DISPATCH_FLOATING_TYPES(feature.scalar_type(), "BorderAlign_Forward", [&] {

        scalar_t *feature_data = feature.contiguous().data_ptr<scalar_t>();
        scalar_t *boxes_data = boxes.contiguous().data_ptr<scalar_t>();
        scalar_t *wh_data = wh.contiguous().data_ptr<scalar_t>();
        scalar_t *pool_data = pool_output.contiguous().data_ptr<scalar_t>();

        BorderAlign_Forward<scalar_t><<<grid, block, 0, stream>>>(
            output_size,
            feature_data,
            boxes_data,
            wh_data,
            out_channel,
            box_size,
            height,
            width,
            pool_size,
            pool_data);
        }
    );
    THCudaCheck(cudaGetLastError());
    return pool_output;
}




template <typename T>
__global__ void BorderAlign_Backward(
    const int nthreads,
    T *gradInput,
    T *gradOutput,
    T *feature,
    T *boxes,
    T *wh,
    const int channel,
    const int box_size,
    const int height,
    const int width,
    const int pool_size)
{
    T *gradinput_iter, *gradoutput_iter, *feature_iter, *boxes_iter, *wh_iter;
    int batch_idx, box_idx, extreme_idx, fm_channel_idx;
    T stride, x_stride, y_stride;
    T x, y;
    T max, val;
    int argmax;
    T w1, w2, w3, w4;
    int x_low, x_high, y_low, y_high;

    CUDA_1D_KERNEL_LOOP(index, nthreads)
    {
    extreme_idx = threadIdx.y;
    batch_idx = index / channel / box_size;
    box_idx = index % box_size + batch_idx * box_size;
    fm_channel_idx = (int)(index / box_size) % channel;

    boxes_iter = boxes + box_idx * 4 + extreme_idx / 2 * 2;
    wh_iter = wh + box_idx * 2;
    feature_iter = feature + (batch_idx * channel * 4 + extreme_idx * channel + fm_channel_idx) * height * width;
    gradinput_iter = gradInput + (batch_idx * channel * 4 + extreme_idx * channel + fm_channel_idx) * height * width;
    gradoutput_iter = gradOutput + index * 4 + extreme_idx;

    x = *boxes_iter;
    y = *(boxes_iter + 1);

    switch(extreme_idx){
        case 0: stride=*wh_iter / pool_size;       x_stride=stride; y_stride=0;       break;
        case 1: stride=*(wh_iter + 1) / pool_size; x_stride=0;      y_stride=stride;  break;
        case 2: stride=*wh_iter / pool_size;       x_stride=-stride;y_stride=0;       break;
        case 3: stride=*(wh_iter + 1) / pool_size; x_stride=0;      y_stride=-stride; break;
    }

    max = bilinear_interpolate(feature_iter, height, width, y, x);
    argmax = 0;
    for(int i = 1; i <= pool_size; i++) {
        x += x_stride;
        y += y_stride;
        val = bilinear_interpolate(feature_iter, height, width, y, x);
        if (val > max) {
            max = val;
            argmax = i;
        }
    }
    x -= x_stride * (T)(pool_size - argmax);
    y -= y_stride * (T)(pool_size - argmax);
    bilinear_interpolate_gradient(
        height, width, y, x, w1, w2, w3, w4, x_low, x_high, y_low, y_high);
    // Update gradOutput
    atomicAdd(gradinput_iter + y_low * width + x_low, *gradoutput_iter * w1);
    atomicAdd(gradinput_iter + y_low * width + x_high, *gradoutput_iter * w2);
    atomicAdd(gradinput_iter + y_high * width + x_low, *gradoutput_iter * w3);
    atomicAdd(gradinput_iter + y_high * width + x_high, *gradoutput_iter * w4);
    }
}


at::Tensor border_align_cuda_backward(
    const at::Tensor& gradOutput,
    const at::Tensor& feature,
    const at::Tensor& boxes,
    const at::Tensor& wh,
    const int pool_size)
{
    int batch = feature.size(0);
    int fm_channel = feature.size(1);
    int out_channel = fm_channel / 4;
    int height = feature.size(2);
    int width = feature.size(3);
    int box_size = boxes.size(1);
    int output_size = batch * out_channel * box_size;

    auto gradInput = at::zeros_like(feature);

    cudaStream_t stream = at::cuda::getCurrentCUDAStream();

    dim3 grid(std::min(THCCeilDiv((long)output_size, 64L), 4096L));
    dim3 block(128, 4);

    AT_DISPATCH_FLOATING_TYPES(feature.scalar_type(), "BorderAlign_Backward", [&] {

        scalar_t *gradOutput_data = gradOutput.contiguous().data_ptr<scalar_t>();
        scalar_t *gradInput_data = gradInput.contiguous().data_ptr<scalar_t>();
        scalar_t *feature_data = feature.contiguous().data_ptr<scalar_t>();
        scalar_t *boxes_data = boxes.contiguous().data_ptr<scalar_t>();
        scalar_t *wh_data = wh.contiguous().data_ptr<scalar_t>();
        
        BorderAlign_Backward<scalar_t><<<grid, block, 0, stream>>>(
            output_size,
            gradInput_data,
            gradOutput_data,
            feature_data,
            boxes_data,
            wh_data,
            out_channel,
            box_size,
            height,
            width,
            pool_size);
        }
    );
    THCudaCheck(cudaGetLastError());
    return gradInput;
}
}
```

##### cvpods/layers/csrc/ROIAlignRotated/ROIAlignRotated_cuda.cu

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAApplyUtils.cuh>

// TODO make it in a common file
#define CUDA_1D_KERNEL_LOOP(i, n)                            \
  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < n; \
       i += blockDim.x * gridDim.x)

// Note: this implementation originates from the Caffe2 ROIAlignRotated Op
// and PyTorch ROIAlign (non-rotated) Op implementations.
// The key difference between this implementation and those ones is
// we don't do "legacy offset" in this version, as there aren't many previous
// works, if any, using the "legacy" ROIAlignRotated Op.
// This would make the interface a bit cleaner.

namespace cvpods {

namespace {

template <typename T>
__device__ T bilinear_interpolate(
    const T* input,
    const int height,
    const int width,
    T y,
    T x) {
  // deal with cases that inverse elements are out of feature map boundary
  if (y < -1.0 || y > height || x < -1.0 || x > width) {
    // empty
    return 0;
  }

  if (y < 0) {
    y = 0;
  }

  if (x < 0) {
    x = 0;
  }

  int y_low = (int)y;
  int x_low = (int)x;
  int y_high;
  int x_high;

  if (y_low >= height - 1) {
    y_high = y_low = height - 1;
    y = (T)y_low;
  } else {
    y_high = y_low + 1;
  }

  if (x_low >= width - 1) {
    x_high = x_low = width - 1;
    x = (T)x_low;
  } else {
    x_high = x_low + 1;
  }

  T ly = y - y_low;
  T lx = x - x_low;
  T hy = 1. - ly, hx = 1. - lx;
  // do bilinear interpolation
  T v1 = input[y_low * width + x_low];
  T v2 = input[y_low * width + x_high];
  T v3 = input[y_high * width + x_low];
  T v4 = input[y_high * width + x_high];
  T w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;

  T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);

  return val;
}

template <typename T>
__device__ void bilinear_interpolate_gradient(
    const int height,
    const int width,
    T y,
    T x,
    T& w1,
    T& w2,
    T& w3,
    T& w4,
    int& x_low,
    int& x_high,
    int& y_low,
    int& y_high) {
  // deal with cases that inverse elements are out of feature map boundary
  if (y < -1.0 || y > height || x < -1.0 || x > width) {
    // empty
    w1 = w2 = w3 = w4 = 0.;
    x_low = x_high = y_low = y_high = -1;
    return;
  }

  if (y < 0) {
    y = 0;
  }

  if (x < 0) {
    x = 0;
  }

  y_low = (int)y;
  x_low = (int)x;

  if (y_low >= height - 1) {
    y_high = y_low = height - 1;
    y = (T)y_low;
  } else {
    y_high = y_low + 1;
  }

  if (x_low >= width - 1) {
    x_high = x_low = width - 1;
    x = (T)x_low;
  } else {
    x_high = x_low + 1;
  }

  T ly = y - y_low;
  T lx = x - x_low;
  T hy = 1. - ly, hx = 1. - lx;

  // reference in forward
  // T v1 = input[y_low * width + x_low];
  // T v2 = input[y_low * width + x_high];
  // T v3 = input[y_high * width + x_low];
  // T v4 = input[y_high * width + x_high];
  // T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);

  w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;

  return;
}

} // namespace

template <typename T>
__global__ void RoIAlignRotatedForward(
    const int nthreads,
    const T* input,
    const T spatial_scale,
    const int channels,
    const int height,
    const int width,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio,
    const T* rois,
    T* top_data) {
  CUDA_1D_KERNEL_LOOP(index, nthreads) {
    // (n, c, ph, pw) is an element in the pooled output
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;

    const T* current_roi = rois + n * 6;
    int roi_batch_ind = current_roi[0];

    // Do not use rounding; this implementation detail is critical
    // ROIAlignRotated supports align == true, i.e., continuous coordinate
    // by default, thus the 0.5 offset
    T offset = (T)0.5;
    T roi_center_w = current_roi[1] * spatial_scale - offset;
    T roi_center_h = current_roi[2] * spatial_scale - offset;
    T roi_width = current_roi[3] * spatial_scale;
    T roi_height = current_roi[4] * spatial_scale;
    T theta = current_roi[5] * M_PI / 180.0;
    T cos_theta = cos(theta);
    T sin_theta = sin(theta);

    T bin_size_h = static_cast<T>(roi_height) / static_cast<T>(pooled_height);
    T bin_size_w = static_cast<T>(roi_width) / static_cast<T>(pooled_width);

    const T* offset_input =
        input + (roi_batch_ind * channels + c) * height * width;

    // We use roi_bin_grid to sample the grid and mimic integral
    int roi_bin_grid_h = (sampling_ratio > 0)
        ? sampling_ratio
        : ceil(roi_height / pooled_height); // e.g., = 2
    int roi_bin_grid_w =
        (sampling_ratio > 0) ? sampling_ratio : ceil(roi_width / pooled_width);

    // roi_start_h and roi_start_w are computed wrt the center of RoI (x, y).
    // Appropriate translation needs to be applied after.
    T roi_start_h = -roi_height / 2.0;
    T roi_start_w = -roi_width / 2.0;

    // We do average (inte  gral) pooling inside a bin
    const T count = max(roi_bin_grid_h * roi_bin_grid_w, 1); // e.g. = 4

    T output_val = 0.;
    for (int iy = 0; iy < roi_bin_grid_h; iy++) // e.g., iy = 0, 1
    {
      const T yy = roi_start_h + ph * bin_size_h +
          static_cast<T>(iy + .5f) * bin_size_h /
              static_cast<T>(roi_bin_grid_h); // e.g., 0.5, 1.5
      for (int ix = 0; ix < roi_bin_grid_w; ix++) {
        const T xx = roi_start_w + pw * bin_size_w +
            static_cast<T>(ix + .5f) * bin_size_w /
                static_cast<T>(roi_bin_grid_w);

        // Rotate by theta around the center and translate
        T y = yy * cos_theta - xx * sin_theta + roi_center_h;
        T x = yy * sin_theta + xx * cos_theta + roi_center_w;

        T val = bilinear_interpolate(offset_input, height, width, y, x);
        output_val += val;
      }
    }
    output_val /= count;

    top_data[index] = output_val;
  }
}

template <typename T>
__global__ void RoIAlignRotatedBackwardFeature(
    const int nthreads,
    const T* top_diff,
    const int num_rois,
    const T spatial_scale,
    const int channels,
    const int height,
    const int width,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio,
    T* bottom_diff,
    const T* rois) {
  CUDA_1D_KERNEL_LOOP(index, nthreads) {
    // (n, c, ph, pw) is an element in the pooled output
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;

    const T* current_roi = rois + n * 6;
    int roi_batch_ind = current_roi[0];

    // Do not use rounding; this implementation detail is critical
    // ROIAlignRotated supports align == true, i.e., continuous coordinate
    // by default, thus the 0.5 offset
    T offset = (T)0.5;
    T roi_center_w = current_roi[1] * spatial_scale - offset;
    T roi_center_h = current_roi[2] * spatial_scale - offset;
    T roi_width = current_roi[3] * spatial_scale;
    T roi_height = current_roi[4] * spatial_scale;
    T theta = current_roi[5] * M_PI / 180.0;
    T cos_theta = cos(theta);
    T sin_theta = sin(theta);

    T bin_size_h = static_cast<T>(roi_height) / static_cast<T>(pooled_height);
    T bin_size_w = static_cast<T>(roi_width) / static_cast<T>(pooled_width);

    T* offset_bottom_diff =
        bottom_diff + (roi_batch_ind * channels + c) * height * width;

    int top_offset = (n * channels + c) * pooled_height * pooled_width;
    const T* offset_top_diff = top_diff + top_offset;
    const T top_diff_this_bin = offset_top_diff[ph * pooled_width + pw];

    // We use roi_bin_grid to sample the grid and mimic integral
    int roi_bin_grid_h = (sampling_ratio > 0)
        ? sampling_ratio
        : ceil(roi_height / pooled_height); // e.g., = 2
    int roi_bin_grid_w =
        (sampling_ratio > 0) ? sampling_ratio : ceil(roi_width / pooled_width);

    // roi_start_h and roi_start_w are computed wrt the center of RoI (x, y).
    // Appropriate translation needs to be applied after.
    T roi_start_h = -roi_height / 2.0;
    T roi_start_w = -roi_width / 2.0;

    // We do average (integral) pooling inside a bin
    const T count = roi_bin_grid_h * roi_bin_grid_w; // e.g. = 4

    for (int iy = 0; iy < roi_bin_grid_h; iy++) // e.g., iy = 0, 1
    {
      const T yy = roi_start_h + ph * bin_size_h +
          static_cast<T>(iy + .5f) * bin_size_h /
              static_cast<T>(roi_bin_grid_h); // e.g., 0.5, 1.5
      for (int ix = 0; ix < roi_bin_grid_w; ix++) {
        const T xx = roi_start_w + pw * bin_size_w +
            static_cast<T>(ix + .5f) * bin_size_w /
                static_cast<T>(roi_bin_grid_w);

        // Rotate by theta around the center and translate
        T y = yy * cos_theta - xx * sin_theta + roi_center_h;
        T x = yy * sin_theta + xx * cos_theta + roi_center_w;

        T w1, w2, w3, w4;
        int x_low, x_high, y_low, y_high;

        bilinear_interpolate_gradient(
            height, width, y, x, w1, w2, w3, w4, x_low, x_high, y_low, y_high);

        T g1 = top_diff_this_bin * w1 / count;
        T g2 = top_diff_this_bin * w2 / count;
        T g3 = top_diff_this_bin * w3 / count;
        T g4 = top_diff_this_bin * w4 / count;

        if (x_low >= 0 && x_high >= 0 && y_low >= 0 && y_high >= 0) {
          atomicAdd(
              offset_bottom_diff + y_low * width + x_low, static_cast<T>(g1));
          atomicAdd(
              offset_bottom_diff + y_low * width + x_high, static_cast<T>(g2));
          atomicAdd(
              offset_bottom_diff + y_high * width + x_low, static_cast<T>(g3));
          atomicAdd(
              offset_bottom_diff + y_high * width + x_high, static_cast<T>(g4));
        } // if
      } // ix
    } // iy
  } // CUDA_1D_KERNEL_LOOP
} // RoIAlignRotatedBackward

at::Tensor ROIAlignRotated_forward_cuda(
    const at::Tensor& input,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio) {
  AT_ASSERTM(input.device().is_cuda(), "input must be a CUDA tensor");
  AT_ASSERTM(rois.device().is_cuda(), "rois must be a CUDA tensor");
  at::TensorArg input_t{input, "input", 1}, rois_t{rois, "rois", 2};

  at::CheckedFrom c = "ROIAlignRotated_forward_cuda";
  at::checkAllSameGPU(c, {input_t, rois_t});
  at::checkAllSameType(c, {input_t, rois_t});
  at::cuda::CUDAGuard device_guard(input.device());

  auto num_rois = rois.size(0);
  auto channels = input.size(1);
  auto height = input.size(2);
  auto width = input.size(3);

  auto output = at::empty(
      {num_rois, channels, pooled_height, pooled_width}, input.options());
  auto output_size = num_rois * pooled_height * pooled_width * channels;
  cudaStream_t stream = at::cuda::getCurrentCUDAStream();

  dim3 grid(std::min(
      at::cuda::ATenCeilDiv(
          static_cast<int64_t>(output_size), static_cast<int64_t>(512)),
      static_cast<int64_t>(4096)));
  dim3 block(512);

  if (output.numel() == 0) {
    AT_CUDA_CHECK(cudaGetLastError());
    return output;
  }

  AT_DISPATCH_FLOATING_TYPES(
      input.scalar_type(), "ROIAlignRotated_forward", [&] {
        RoIAlignRotatedForward<scalar_t><<<grid, block, 0, stream>>>(
            output_size,
            input.contiguous().data_ptr<scalar_t>(),
            spatial_scale,
            channels,
            height,
            width,
            pooled_height,
            pooled_width,
            sampling_ratio,
            rois.contiguous().data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>());
      });
  cudaDeviceSynchronize();
  AT_CUDA_CHECK(cudaGetLastError());
  return output;
}

// TODO remove the dependency on input and use instead its sizes -> save memory
at::Tensor ROIAlignRotated_backward_cuda(
    const at::Tensor& grad,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int batch_size,
    const int channels,
    const int height,
    const int width,
    const int sampling_ratio) {
  AT_ASSERTM(grad.device().is_cuda(), "grad must be a CUDA tensor");
  AT_ASSERTM(rois.device().is_cuda(), "rois must be a CUDA tensor");

  at::TensorArg grad_t{grad, "grad", 1}, rois_t{rois, "rois", 2};
  at::CheckedFrom c = "ROIAlign_backward_cuda";
  at::checkAllSameGPU(c, {grad_t, rois_t});
  at::checkAllSameType(c, {grad_t, rois_t});
  at::cuda::CUDAGuard device_guard(grad.device());

  auto num_rois = rois.size(0);
  auto grad_input =
      at::zeros({batch_size, channels, height, width}, grad.options());

  cudaStream_t stream = at::cuda::getCurrentCUDAStream();

  dim3 grid(std::min(
      at::cuda::ATenCeilDiv(
          static_cast<int64_t>(grad.numel()), static_cast<int64_t>(512)),
      static_cast<int64_t>(4096)));
  dim3 block(512);

  // handle possibly empty gradients
  if (grad.numel() == 0) {
    AT_CUDA_CHECK(cudaGetLastError());
    return grad_input;
  }

  AT_DISPATCH_FLOATING_TYPES(
      grad.scalar_type(), "ROIAlignRotated_backward", [&] {
        RoIAlignRotatedBackwardFeature<scalar_t><<<grid, block, 0, stream>>>(
            grad.numel(),
            grad.contiguous().data_ptr<scalar_t>(),
            num_rois,
            spatial_scale,
            channels,
            height,
            width,
            pooled_height,
            pooled_width,
            sampling_ratio,
            grad_input.data_ptr<scalar_t>(),
            rois.contiguous().data_ptr<scalar_t>());
      });
  AT_CUDA_CHECK(cudaGetLastError());
  return grad_input;
}

} // namespace cvpods
```

##### cvpods/layers/csrc/ROIAlignRotated/ROIAlignRotated_cpu.cpp

```cpp
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#include <ATen/TensorUtils.h>
#include "ROIAlignRotated.h"

// Note: this implementation originates from the Caffe2 ROIAlignRotated Op
// and PyTorch ROIAlign (non-rotated) Op implementations.
// The key difference between this implementation and those ones is
// we don't do "legacy offset" in this version, as there aren't many previous
// works, if any, using the "legacy" ROIAlignRotated Op.
// This would make the interface a bit cleaner.

namespace cvpods {

namespace {
template <typename T>
struct PreCalc {
  int pos1;
  int pos2;
  int pos3;
  int pos4;
  T w1;
  T w2;
  T w3;
  T w4;
};

template <typename T>
void pre_calc_for_bilinear_interpolate(
    const int height,
    const int width,
    const int pooled_height,
    const int pooled_width,
    const int iy_upper,
    const int ix_upper,
    T roi_start_h,
    T roi_start_w,
    T bin_size_h,
    T bin_size_w,
    int roi_bin_grid_h,
    int roi_bin_grid_w,
    T roi_center_h,
    T roi_center_w,
    T cos_theta,
    T sin_theta,
    std::vector<PreCalc<T>>& pre_calc) {
  int pre_calc_index = 0;
  for (int ph = 0; ph < pooled_height; ph++) {
    for (int pw = 0; pw < pooled_width; pw++) {
      for (int iy = 0; iy < iy_upper; iy++) {
        const T yy = roi_start_h + ph * bin_size_h +
            static_cast<T>(iy + .5f) * bin_size_h /
                static_cast<T>(roi_bin_grid_h); // e.g., 0.5, 1.5
        for (int ix = 0; ix < ix_upper; ix++) {
          const T xx = roi_start_w + pw * bin_size_w +
              static_cast<T>(ix + .5f) * bin_size_w /
                  static_cast<T>(roi_bin_grid_w);

          // Rotate by theta around the center and translate
          // In image space, (y, x) is the order for Right Handed System,
          // and this is essentially multiplying the point by a rotation matrix
          // to rotate it counterclockwise through angle theta.
          T y = yy * cos_theta - xx * sin_theta + roi_center_h;
          T x = yy * sin_theta + xx * cos_theta + roi_center_w;
          // deal with: inverse elements are out of feature map boundary
          if (y < -1.0 || y > height || x < -1.0 || x > width) {
            // empty
            PreCalc<T> pc;
            pc.pos1 = 0;
            pc.pos2 = 0;
            pc.pos3 = 0;
            pc.pos4 = 0;
            pc.w1 = 0;
            pc.w2 = 0;
            pc.w3 = 0;
            pc.w4 = 0;
            pre_calc[pre_calc_index] = pc;
            pre_calc_index += 1;
            continue;
          }

          if (y < 0) {
            y = 0;
          }
          if (x < 0) {
            x = 0;
          }

          int y_low = (int)y;
          int x_low = (int)x;
          int y_high;
          int x_high;

          if (y_low >= height - 1) {
            y_high = y_low = height - 1;
            y = (T)y_low;
          } else {
            y_high = y_low + 1;
          }

          if (x_low >= width - 1) {
            x_high = x_low = width - 1;
            x = (T)x_low;
          } else {
            x_high = x_low + 1;
          }

          T ly = y - y_low;
          T lx = x - x_low;
          T hy = 1. - ly, hx = 1. - lx;
          T w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;

          // save weights and indices
          PreCalc<T> pc;
          pc.pos1 = y_low * width + x_low;
          pc.pos2 = y_low * width + x_high;
          pc.pos3 = y_high * width + x_low;
          pc.pos4 = y_high * width + x_high;
          pc.w1 = w1;
          pc.w2 = w2;
          pc.w3 = w3;
          pc.w4 = w4;
          pre_calc[pre_calc_index] = pc;

          pre_calc_index += 1;
        }
      }
    }
  }
}

template <typename T>
void bilinear_interpolate_gradient(
    const int height,
    const int width,
    T y,
    T x,
    T& w1,
    T& w2,
    T& w3,
    T& w4,
    int& x_low,
    int& x_high,
    int& y_low,
    int& y_high) {
  // deal with cases that inverse elements are out of feature map boundary
  if (y < -1.0 || y > height || x < -1.0 || x > width) {
    // empty
    w1 = w2 = w3 = w4 = 0.;
    x_low = x_high = y_low = y_high = -1;
    return;
  }

  if (y < 0) {
    y = 0;
  }

  if (x < 0) {
    x = 0;
  }

  y_low = (int)y;
  x_low = (int)x;

  if (y_low >= height - 1) {
    y_high = y_low = height - 1;
    y = (T)y_low;
  } else {
    y_high = y_low + 1;
  }

  if (x_low >= width - 1) {
    x_high = x_low = width - 1;
    x = (T)x_low;
  } else {
    x_high = x_low + 1;
  }

  T ly = y - y_low;
  T lx = x - x_low;
  T hy = 1. - ly, hx = 1. - lx;

  // reference in forward
  // T v1 = input[y_low * width + x_low];
  // T v2 = input[y_low * width + x_high];
  // T v3 = input[y_high * width + x_low];
  // T v4 = input[y_high * width + x_high];
  // T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);

  w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;

  return;
}

template <class T>
inline void add(T* address, const T& val) {
  *address += val;
}

} // namespace

template <typename T>
void ROIAlignRotatedForward(
    const int nthreads,
    const T* input,
    const T& spatial_scale,
    const int channels,
    const int height,
    const int width,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio,
    const T* rois,
    T* output) {
  int n_rois = nthreads / channels / pooled_width / pooled_height;
  // (n, c, ph, pw) is an element in the pooled output
  // can be parallelized using omp
  // #pragma omp parallel for num_threads(32)
  for (int n = 0; n < n_rois; n++) {
    int index_n = n * channels * pooled_width * pooled_height;

    const T* current_roi = rois + n * 6;
    int roi_batch_ind = current_roi[0];

    // Do not use rounding; this implementation detail is critical
    // ROIAlignRotated supports align == true, i.e., continuous coordinate
    // by default, thus the 0.5 offset
    T offset = (T)0.5;
    T roi_center_w = current_roi[1] * spatial_scale - offset;
    T roi_center_h = current_roi[2] * spatial_scale - offset;
    T roi_width = current_roi[3] * spatial_scale;
    T roi_height = current_roi[4] * spatial_scale;
    T theta = current_roi[5] * M_PI / 180.0;
    T cos_theta = cos(theta);
    T sin_theta = sin(theta);

    AT_ASSERTM(
        roi_width >= 0 && roi_height >= 0,
        "ROIs in ROIAlignRotated do not have non-negative size!");

    T bin_size_h = static_cast<T>(roi_height) / static_cast<T>(pooled_height);
    T bin_size_w = static_cast<T>(roi_width) / static_cast<T>(pooled_width);

    // We use roi_bin_grid to sample the grid and mimic integral
    int roi_bin_grid_h = (sampling_ratio > 0)
        ? sampling_ratio
        : ceil(roi_height / pooled_height); // e.g., = 2
    int roi_bin_grid_w =
        (sampling_ratio > 0) ? sampling_ratio : ceil(roi_width / pooled_width);

    // We do average (integral) pooling inside a bin
    const T count = std::max(roi_bin_grid_h * roi_bin_grid_w, 1); // e.g. = 4

    // we want to precalculate indices and weights shared by all channels,
    // this is the key point of optimization
    std::vector<PreCalc<T>> pre_calc(
        roi_bin_grid_h * roi_bin_grid_w * pooled_width * pooled_height);

    // roi_start_h and roi_start_w are computed wrt the center of RoI (x, y).
    // Appropriate translation needs to be applied after.
    T roi_start_h = -roi_height / 2.0;
    T roi_start_w = -roi_width / 2.0;

    pre_calc_for_bilinear_interpolate(
        height,
        width,
        pooled_height,
        pooled_width,
        roi_bin_grid_h,
        roi_bin_grid_w,
        roi_start_h,
        roi_start_w,
        bin_size_h,
        bin_size_w,
        roi_bin_grid_h,
        roi_bin_grid_w,
        roi_center_h,
        roi_center_w,
        cos_theta,
        sin_theta,
        pre_calc);

    for (int c = 0; c < channels; c++) {
      int index_n_c = index_n + c * pooled_width * pooled_height;
      const T* offset_input =
          input + (roi_batch_ind * channels + c) * height * width;
      int pre_calc_index = 0;

      for (int ph = 0; ph < pooled_height; ph++) {
        for (int pw = 0; pw < pooled_width; pw++) {
          int index = index_n_c + ph * pooled_width + pw;

          T output_val = 0.;
          for (int iy = 0; iy < roi_bin_grid_h; iy++) {
            for (int ix = 0; ix < roi_bin_grid_w; ix++) {
              PreCalc<T> pc = pre_calc[pre_calc_index];
              output_val += pc.w1 * offset_input[pc.pos1] +
                  pc.w2 * offset_input[pc.pos2] +
                  pc.w3 * offset_input[pc.pos3] + pc.w4 * offset_input[pc.pos4];

              pre_calc_index += 1;
            }
          }
          output_val /= count;

          output[index] = output_val;
        } // for pw
      } // for ph
    } // for c
  } // for n
}

template <typename T>
void ROIAlignRotatedBackward(
    const int nthreads,
    const T* grad_output,
    const T& spatial_scale,
    const int channels,
    const int height,
    const int width,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio,
    T* grad_input,
    const T* rois,
    const int n_stride,
    const int c_stride,
    const int h_stride,
    const int w_stride) {
  for (int index = 0; index < nthreads; index++) {
    // (n, c, ph, pw) is an element in the pooled output
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;

    const T* current_roi = rois + n * 6;
    int roi_batch_ind = current_roi[0];

    // Do not use rounding; this implementation detail is critical
    // ROIAlignRotated supports align == true, i.e., continuous coordinate
    // by default, thus the 0.5 offset
    T offset = (T)0.5;
    T roi_center_w = current_roi[1] * spatial_scale - offset;
    T roi_center_h = current_roi[2] * spatial_scale - offset;
    T roi_width = current_roi[3] * spatial_scale;
    T roi_height = current_roi[4] * spatial_scale;
    T theta = current_roi[5] * M_PI / 180.0;
    T cos_theta = cos(theta);
    T sin_theta = sin(theta);

    AT_ASSERTM(
        roi_width >= 0 && roi_height >= 0,
        "ROIs in ROIAlignRotated do not have non-negative size!");

    T bin_size_h = static_cast<T>(roi_height) / static_cast<T>(pooled_height);
    T bin_size_w = static_cast<T>(roi_width) / static_cast<T>(pooled_width);

    T* offset_grad_input =
        grad_input + ((roi_batch_ind * channels + c) * height * width);

    int output_offset = n * n_stride + c * c_stride;
    const T* offset_grad_output = grad_output + output_offset;
    const T grad_output_this_bin =
        offset_grad_output[ph * h_stride + pw * w_stride];

    // We use roi_bin_grid to sample the grid and mimic integral
    int roi_bin_grid_h = (sampling_ratio > 0)
        ? sampling_ratio
        : ceil(roi_height / pooled_height); // e.g., = 2
    int roi_bin_grid_w =
        (sampling_ratio > 0) ? sampling_ratio : ceil(roi_width / pooled_width);

    // roi_start_h and roi_start_w are computed wrt the center of RoI (x, y).
    // Appropriate translation needs to be applied after.
    T roi_start_h = -roi_height / 2.0;
    T roi_start_w = -roi_width / 2.0;

    // We do average (integral) pooling inside a bin
    const T count = roi_bin_grid_h * roi_bin_grid_w; // e.g. = 4

    for (int iy = 0; iy < roi_bin_grid_h; iy++) {
      const T yy = roi_start_h + ph * bin_size_h +
          static_cast<T>(iy + .5f) * bin_size_h /
              static_cast<T>(roi_bin_grid_h); // e.g., 0.5, 1.5
      for (int ix = 0; ix < roi_bin_grid_w; ix++) {
        const T xx = roi_start_w + pw * bin_size_w +
            static_cast<T>(ix + .5f) * bin_size_w /
                static_cast<T>(roi_bin_grid_w);

        // Rotate by theta around the center and translate
        T y = yy * cos_theta - xx * sin_theta + roi_center_h;
        T x = yy * sin_theta + xx * cos_theta + roi_center_w;

        T w1, w2, w3, w4;
        int x_low, x_high, y_low, y_high;

        bilinear_interpolate_gradient(
            height, width, y, x, w1, w2, w3, w4, x_low, x_high, y_low, y_high);

        T g1 = grad_output_this_bin * w1 / count;
        T g2 = grad_output_this_bin * w2 / count;
        T g3 = grad_output_this_bin * w3 / count;
        T g4 = grad_output_this_bin * w4 / count;

        if (x_low >= 0 && x_high >= 0 && y_low >= 0 && y_high >= 0) {
          // atomic add is not needed for now since it is single threaded
          add(offset_grad_input + y_low * width + x_low, static_cast<T>(g1));
          add(offset_grad_input + y_low * width + x_high, static_cast<T>(g2));
          add(offset_grad_input + y_high * width + x_low, static_cast<T>(g3));
          add(offset_grad_input + y_high * width + x_high, static_cast<T>(g4));
        } // if
      } // ix
    } // iy
  } // for
} // ROIAlignRotatedBackward

at::Tensor ROIAlignRotated_forward_cpu(
    const at::Tensor& input,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio) {
  AT_ASSERTM(input.device().is_cpu(), "input must be a CPU tensor");
  AT_ASSERTM(rois.device().is_cpu(), "rois must be a CPU tensor");

  at::TensorArg input_t{input, "input", 1}, rois_t{rois, "rois", 2};

  at::CheckedFrom c = "ROIAlign_forward_cpu";
  at::checkAllSameType(c, {input_t, rois_t});

  auto num_rois = rois.size(0);
  auto channels = input.size(1);
  auto height = input.size(2);
  auto width = input.size(3);

  at::Tensor output = at::zeros(
      {num_rois, channels, pooled_height, pooled_width}, input.options());

  auto output_size = num_rois * pooled_height * pooled_width * channels;

  if (output.numel() == 0) {
    return output;
  }

  AT_DISPATCH_FLOATING_TYPES_AND_HALF(
      input.scalar_type(), "ROIAlignRotated_forward", [&] {
        ROIAlignRotatedForward<scalar_t>(
            output_size,
            input.contiguous().data_ptr<scalar_t>(),
            spatial_scale,
            channels,
            height,
            width,
            pooled_height,
            pooled_width,
            sampling_ratio,
            rois.contiguous().data_ptr<scalar_t>(),
            output.data_ptr<scalar_t>());
      });
  return output;
}

at::Tensor ROIAlignRotated_backward_cpu(
    const at::Tensor& grad,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int batch_size,
    const int channels,
    const int height,
    const int width,
    const int sampling_ratio) {
  AT_ASSERTM(grad.device().is_cpu(), "grad must be a CPU tensor");
  AT_ASSERTM(rois.device().is_cpu(), "rois must be a CPU tensor");

  at::TensorArg grad_t{grad, "grad", 1}, rois_t{rois, "rois", 2};

  at::CheckedFrom c = "ROIAlignRotated_backward_cpu";
  at::checkAllSameType(c, {grad_t, rois_t});

  at::Tensor grad_input =
      at::zeros({batch_size, channels, height, width}, grad.options());

  // handle possibly empty gradients
  if (grad.numel() == 0) {
    return grad_input;
  }

  // get stride values to ensure indexing into gradients is correct.
  int n_stride = grad.stride(0);
  int c_stride = grad.stride(1);
  int h_stride = grad.stride(2);
  int w_stride = grad.stride(3);

  AT_DISPATCH_FLOATING_TYPES_AND_HALF(
      grad.scalar_type(), "ROIAlignRotated_forward", [&] {
        ROIAlignRotatedBackward<scalar_t>(
            grad.numel(),
            grad.contiguous().data_ptr<scalar_t>(),
            spatial_scale,
            channels,
            height,
            width,
            pooled_height,
            pooled_width,
            sampling_ratio,
            grad_input.data_ptr<scalar_t>(),
            rois.contiguous().data_ptr<scalar_t>(),
            n_stride,
            c_stride,
            h_stride,
            w_stride);
      });
  return grad_input;
}

} // namespace cvpods
```

##### cvpods/layers/csrc/ROIAlignRotated/ROIAlignRotated.h

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#pragma once
#include <torch/types.h>

namespace cvpods {

at::Tensor ROIAlignRotated_forward_cpu(
    const at::Tensor& input,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio);

at::Tensor ROIAlignRotated_backward_cpu(
    const at::Tensor& grad,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int batch_size,
    const int channels,
    const int height,
    const int width,
    const int sampling_ratio);

#ifdef WITH_CUDA
at::Tensor ROIAlignRotated_forward_cuda(
    const at::Tensor& input,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio);

at::Tensor ROIAlignRotated_backward_cuda(
    const at::Tensor& grad,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int batch_size,
    const int channels,
    const int height,
    const int width,
    const int sampling_ratio);
#endif

// Interface for Python
inline at::Tensor ROIAlignRotated_forward(
    const at::Tensor& input,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio) {
  if (input.device().is_cuda()) {
#ifdef WITH_CUDA
    return ROIAlignRotated_forward_cuda(
        input,
        rois,
        spatial_scale,
        pooled_height,
        pooled_width,
        sampling_ratio);
#else
    AT_ERROR("Not compiled with GPU support");
#endif
  }
  return ROIAlignRotated_forward_cpu(
      input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio);
}

inline at::Tensor ROIAlignRotated_backward(
    const at::Tensor& grad,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int batch_size,
    const int channels,
    const int height,
    const int width,
    const int sampling_ratio) {
  if (grad.device().is_cuda()) {
#ifdef WITH_CUDA
    return ROIAlignRotated_backward_cuda(
        grad,
        rois,
        spatial_scale,
        pooled_height,
        pooled_width,
        batch_size,
        channels,
        height,
        width,
        sampling_ratio);
#else
    AT_ERROR("Not compiled with GPU support");
#endif
  }
  return ROIAlignRotated_backward_cpu(
      grad,
      rois,
      spatial_scale,
      pooled_height,
      pooled_width,
      batch_size,
      channels,
      height,
      width,
      sampling_ratio);
}

} // namespace cvpods
```

##### cvpods/layers/csrc/sigmoid_focal_loss/SigmoidFocalLoss.h

```
#pragma once
#include <torch/extension.h>

namespace cvpods {
#ifdef WITH_CUDA
at::Tensor SigmoidFocalLoss_forward_cuda(
		const at::Tensor& logits,
        const at::Tensor& targets,
		const int num_classes, 
		const float gamma, 
		const float alpha); 

at::Tensor SigmoidFocalLoss_backward_cuda(
			     const at::Tensor& logits,
                 const at::Tensor& targets,
			     const at::Tensor& d_losses,
			     const int num_classes,
			     const float gamma,
			     const float alpha);
#endif

//
// Interface for Python
inline at::Tensor SigmoidFocalLoss_forward(
		const at::Tensor& logits,
        const at::Tensor& targets,
		const int num_classes, 
		const float gamma, 
		const float alpha) {
  if (logits.device().is_cuda()) {
#ifdef WITH_CUDA
    return SigmoidFocalLoss_forward_cuda(logits, targets, num_classes, gamma, alpha);
#else
    AT_ERROR("Not compiled with GPU support");
#endif
  }
  AT_ERROR("Not implemented on the CPU");
}

inline at::Tensor SigmoidFocalLoss_backward(
			     const at::Tensor& logits,
                 const at::Tensor& targets,
			     const at::Tensor& d_losses,
			     const int num_classes,
			     const float gamma,
			     const float alpha) {
  if (logits.device().is_cuda()) {
#ifdef WITH_CUDA
    return SigmoidFocalLoss_backward_cuda(logits, targets, d_losses, num_classes, gamma, alpha);
#else
    AT_ERROR("Not compiled with GPU support");
#endif
  }
  AT_ERROR("Not implemented on the CPU");
}

}
```

##### cvpods/layers/csrc/sigmoid_focal_loss/SigmoidFocalLoss_cuda.cu

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
// This file is modified from  https://github.com/pytorch/pytorch/blob/master/modules/detectron/sigmoid_focal_loss_op.cu
// Cheng-Yang Fu
// cyfu@cs.unc.edu
#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>

#include <THC/THC.h>
#include <THC/THCAtomics.cuh>
#include <THC/THCDeviceUtils.cuh>

#include <cfloat>

// TODO make it in a common file
#define CUDA_1D_KERNEL_LOOP(i, n)                            \
  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < n; \
       i += blockDim.x * gridDim.x)


template <typename T>
__global__ void SigmoidFocalLossForward(const int nthreads, 
    const T* logits,
    const int* targets,
    const int num_classes,
    const float gamma, 
    const float alpha,
    const int num, 
    T* losses) {
  CUDA_1D_KERNEL_LOOP(i, nthreads) {

    int n = i / num_classes;
    int d = i % num_classes; // current class[0~79]; 
    int t = targets[n]; // target class [1~80];

    // Decide it is positive or negative case. 
    T c1 = (t == (d+1)); 
    T c2 = (t>=0 & t != (d+1));

    T zn = (1.0 - alpha);
    T zp = (alpha);

    // p = 1. / 1. + expf(-x); p = sigmoid(x)
    T  p = 1. / (1. + expf(-logits[i]));

    // (1-p)**gamma * log(p) where
    T term1 = powf((1. - p), gamma) * logf(max(p, FLT_MIN));

    // p**gamma * log(1-p)
    T term2 = powf(p, gamma) *
            (-1. * logits[i] * (logits[i] >= 0) -   
             logf(1. + expf(logits[i] - 2. * logits[i] * (logits[i] >= 0))));

    losses[i] = 0.0;
    losses[i] += -c1 * term1 * zp;
    losses[i] += -c2 * term2 * zn;

  } // CUDA_1D_KERNEL_LOOP
} // SigmoidFocalLossForward


template <typename T>
__global__ void SigmoidFocalLossBackward(const int nthreads,
                const T* logits,
                const int* targets,
                const T* d_losses,
                const int num_classes,
                const float gamma,
                const float alpha,
                const int num,
                T* d_logits) {
  CUDA_1D_KERNEL_LOOP(i, nthreads) {

    int n = i / num_classes;
    int d = i % num_classes; // current class[0~79]; 
    int t = targets[n]; // target class [1~80], 0 is background;

    // Decide it is positive or negative case. 
    T c1 = (t == (d+1));
    T c2 = (t>=0 & t != (d+1));

    T zn = (1.0 - alpha);
    T zp = (alpha);
    // p = 1. / 1. + expf(-x); p = sigmoid(x)
    T  p = 1. / (1. + expf(-logits[i]));

    // (1-p)**g * (1 - p - g*p*log(p)
    T term1 = powf((1. - p), gamma) *
                      (1. - p - (p * gamma * logf(max(p, FLT_MIN))));

    // (p**g) * (g*(1-p)*log(1-p) - p)
    T term2 = powf(p, gamma) *
                  ((-1. * logits[i] * (logits[i] >= 0) -
                      logf(1. + expf(logits[i] - 2. * logits[i] * (logits[i] >= 0)))) *
                      (1. - p) * gamma - p);
    d_logits[i] = 0.0;
    d_logits[i] += -c1 * term1 * zp;
    d_logits[i] += -c2 * term2 * zn;
    d_logits[i] = d_logits[i] * d_losses[i];

  } // CUDA_1D_KERNEL_LOOP
} // SigmoidFocalLossBackward


namespace cvpods {

at::Tensor SigmoidFocalLoss_forward_cuda(
		const at::Tensor& logits,
                const at::Tensor& targets,
		const int num_classes, 
		const float gamma, 
		const float alpha) {
  AT_ASSERTM(logits.device().is_cuda(), "logits must be a CUDA tensor");
  AT_ASSERTM(targets.device().is_cuda(), "targets must be a CUDA tensor");
  AT_ASSERTM(logits.dim() == 2, "logits should be NxClass");

  const int num_samples = logits.size(0);
	
  auto losses = at::empty({num_samples, logits.size(1)}, logits.options());
  auto losses_size = num_samples * logits.size(1);
  cudaStream_t stream = at::cuda::getCurrentCUDAStream();

  dim3 grid(std::min(THCCeilDiv((long)losses_size, 512L), 4096L));
  dim3 block(512);

  if (losses.numel() == 0) {
    THCudaCheck(cudaGetLastError());
    return losses;
  }

  AT_DISPATCH_FLOATING_TYPES(logits.scalar_type(), "SigmoidFocalLoss_forward", [&] {
    SigmoidFocalLossForward<scalar_t><<<grid, block, 0, stream>>>(
         losses_size,
         logits.contiguous().data_ptr<scalar_t>(),
	 targets.contiguous().data_ptr<int>(),
         num_classes,
	 gamma,
	 alpha,
	 num_samples,
         losses.data_ptr<scalar_t>());
  });
  THCudaCheck(cudaGetLastError());
  return losses;   
}	


at::Tensor SigmoidFocalLoss_backward_cuda(
		const at::Tensor& logits,
                const at::Tensor& targets,
		const at::Tensor& d_losses,
		const int num_classes, 
		const float gamma, 
		const float alpha) {
  AT_ASSERTM(logits.device().is_cuda(), "logits must be a CUDA tensor");
  AT_ASSERTM(targets.device().is_cuda(), "targets must be a CUDA tensor");
  AT_ASSERTM(d_losses.device().is_cuda(), "d_losses must be a CUDA tensor");

  AT_ASSERTM(logits.dim() == 2, "logits should be NxClass");

  const int num_samples = logits.size(0);
  AT_ASSERTM(logits.size(1) == num_classes, "logits.size(1) should be num_classes");
	
  auto d_logits = at::zeros({num_samples, num_classes}, logits.options());
  auto d_logits_size = num_samples * logits.size(1);
  cudaStream_t stream = at::cuda::getCurrentCUDAStream();

  dim3 grid(std::min(THCCeilDiv((long)d_logits_size, 512L), 4096L));
  dim3 block(512);

  if (d_logits.numel() == 0) {
    THCudaCheck(cudaGetLastError());
    return d_logits;
  }

  AT_DISPATCH_FLOATING_TYPES(logits.scalar_type(), "SigmoidFocalLoss_backward", [&] {
    SigmoidFocalLossBackward<scalar_t><<<grid, block, 0, stream>>>(
         d_logits_size,
         logits.contiguous().data_ptr<scalar_t>(),
	 targets.contiguous().data_ptr<int>(),
	 d_losses.contiguous().data_ptr<scalar_t>(),
         num_classes,
	 gamma,
	 alpha,
	 num_samples,
         d_logits.data_ptr<scalar_t>());
  });

  THCudaCheck(cudaGetLastError());
  return d_logits;   
}	

}
```

##### cvpods/layers/csrc/SwapAlign2Nat/SwapAlign2Nat_cuda.cu

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAApplyUtils.cuh>

// TODO make it in a common file
#define CUDA_1D_KERNEL_LOOP(i, n)                            \
  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < n; \
       i += blockDim.x * gridDim.x)

template <typename T>
__device__ inline T get_pixel_val(
    const T* tensor,
    const int idx,
    const int H,
    const int W,
    const int y,
    const int x,
    const int V,
    const int U,
    const int v,
    const int u,
    const T pad_val) {
  if ((y < 0) || (y >= H) || (x < 0) || (x >= W) || (v < 0) || (v >= V) ||
      (u < 0) || (u >= U)) {
    return pad_val;
  } else {
    return tensor[(((idx * V + v) * U + u) * H + y) * W + x];
  }
}

template <typename T>
__device__ inline void add_pixel_val(
    T* tensor,
    const T val,
    const int idx,
    const int H,
    const int W,
    const int y,
    const int x,
    const int V,
    const int U,
    const int v,
    const int u) {
  if ((val == 0.) || (y < 0) || (y >= H) || (x < 0) || (x >= W) || (v < 0) ||
      (v >= V) || (u < 0) || (u >= U)) {
    return;
  } else {
    atomicAdd(tensor + ((((idx * V + v) * U + u) * H + y) * W + x), val);
  }
}

template <typename T>
__global__ void SwapAlign2NatForwardFeat(
    const int nthreads,
    const T* bottom_data,
    const int Vout,
    const int Uout,
    const float hVout,
    const float hUout,
    const int Vin,
    const int Uin,
    const float lambda,
    const int Hin,
    const int Win,
    const int Hout,
    const int Wout,
    const T pad_val,
    T* top_data) {
  CUDA_1D_KERNEL_LOOP(index, nthreads) {
    int idx = index;
    const int x = idx % Wout;
    idx /= Wout;
    const int y = idx % Hout;
    idx /= Hout;
    const int u = idx % Uout;
    idx /= Uout;
    const int v = idx % Vout;
    idx /= Vout;

    const float ox = x * lambda + u - hUout + 0.5;
    const int xf = static_cast<int>(floor(ox));
    const int xc = static_cast<int>(ceil(ox));
    const float xwc = ox - xf;
    const float xwf = 1. - xwc;

    const float oy = y * lambda + v - hVout + 0.5;
    const int yf = static_cast<int>(floor(oy));
    const int yc = static_cast<int>(ceil(oy));
    const float ywc = oy - yf;
    const float ywf = 1. - ywc;

    const float ou = (u + 0.5) / lambda - 0.5;
    const int uf = static_cast<int>(floor(ou));
    const int uc = static_cast<int>(ceil(ou));
    const float uwc = ou - uf;
    const float uwf = 1. - uwc;

    const float ov = (v + 0.5) / lambda - 0.5;
    const int vf = static_cast<int>(floor(ov));
    const int vc = static_cast<int>(ceil(ov));
    const float vwc = ov - vf;
    const float vwf = 1. - vwc;

    T val = ywf * xwf * vwf * uwf *
            get_pixel_val(
                bottom_data, idx, Hin, Win, yf, xf, Vin, Uin, vf, uf, pad_val) +
        ywf * xwf * vwf * uwc *
            get_pixel_val(
                bottom_data, idx, Hin, Win, yf, xf, Vin, Uin, vf, uc, pad_val) +
        ywf * xwf * vwc * uwf *
            get_pixel_val(
                bottom_data, idx, Hin, Win, yf, xf, Vin, Uin, vc, uf, pad_val) +
        ywf * xwf * vwc * uwc *
            get_pixel_val(
                bottom_data, idx, Hin, Win, yf, xf, Vin, Uin, vc, uc, pad_val) +
        ywf * xwc * vwf * uwf *
            get_pixel_val(
                bottom_data, idx, Hin, Win, yf, xc, Vin, Uin, vf, uf, pad_val) +
        ywf * xwc * vwf * uwc *
            get_pixel_val(
                bottom_data, idx, Hin, Win, yf, xc, Vin, Uin, vf, uc, pad_val) +
        ywf * xwc * vwc * uwf *
            get_pixel_val(
                bottom_data, idx, Hin, Win, yf, xc, Vin, Uin, vc, uf, pad_val) +
        ywf * xwc * vwc * uwc *
            get_pixel_val(
                bottom_data, idx, Hin, Win, yf, xc, Vin, Uin, vc, uc, pad_val) +
        ywc * xwf * vwf * uwf *
            get_pixel_val(
                bottom_data, idx, Hin, Win, yc, xf, Vin, Uin, vf, uf, pad_val) +
        ywc * xwf * vwf * uwc *
            get_pixel_val(
                bottom_data, idx, Hin, Win, yc, xf, Vin, Uin, vf, uc, pad_val) +
        ywc * xwf * vwc * uwf *
            get_pixel_val(
                bottom_data, idx, Hin, Win, yc, xf, Vin, Uin, vc, uf, pad_val) +
        ywc * xwf * vwc * uwc *
            get_pixel_val(
                bottom_data, idx, Hin, Win, yc, xf, Vin, Uin, vc, uc, pad_val) +
        ywc * xwc * vwf * uwf *
            get_pixel_val(
                bottom_data, idx, Hin, Win, yc, xc, Vin, Uin, vf, uf, pad_val) +
        ywc * xwc * vwf * uwc *
            get_pixel_val(
                bottom_data, idx, Hin, Win, yc, xc, Vin, Uin, vf, uc, pad_val) +
        ywc * xwc * vwc * uwf *
            get_pixel_val(
                bottom_data, idx, Hin, Win, yc, xc, Vin, Uin, vc, uf, pad_val) +
        ywc * xwc * vwc * uwc *
            get_pixel_val(
                bottom_data, idx, Hin, Win, yc, xc, Vin, Uin, vc, uc, pad_val);

    top_data[index] = val;
  }
}

template <typename T>
__global__ void SwapAlign2NatBackwardFeat(
    const int nthreads,
    const T* top_diff,
    const int Vout,
    const int Uout,
    const float hVout,
    const float hUout,
    const int Vin,
    const int Uin,
    const float lambda,
    const int Hin,
    const int Win,
    const int Hout,
    const int Wout,
    T* bottom_diff) {
  CUDA_1D_KERNEL_LOOP(index, nthreads) {
    int idx = index;
    const int x = idx % Wout;
    idx /= Wout;
    const int y = idx % Hout;
    idx /= Hout;
    const int u = idx % Uout;
    idx /= Uout;
    const int v = idx % Vout;
    idx /= Vout;

    const float ox = x * lambda + u - hUout + 0.5;
    const int xf = static_cast<int>(floor(ox));
    const int xc = static_cast<int>(ceil(ox));
    const float xwc = ox - xf;
    const float xwf = 1. - xwc;

    const float oy = y * lambda + v - hVout + 0.5;
    const int yf = static_cast<int>(floor(oy));
    const int yc = static_cast<int>(ceil(oy));
    const float ywc = oy - yf;
    const float ywf = 1. - ywc;

    const float ou = (u + 0.5) / lambda - 0.5;
    const int uf = static_cast<int>(floor(ou));
    const int uc = static_cast<int>(ceil(ou));
    const float uwc = ou - uf;
    const float uwf = 1. - uwc;

    const float ov = (v + 0.5) / lambda - 0.5;
    const int vf = static_cast<int>(floor(ov));
    const int vc = static_cast<int>(ceil(ov));
    const float vwc = ov - vf;
    const float vwf = 1. - vwc;

    const T grad = top_diff[index];

    add_pixel_val(
        bottom_diff,
        ywf * xwf * vwf * uwf * grad,
        idx,
        Hin,
        Win,
        yf,
        xf,
        Vin,
        Uin,
        vf,
        uf);
    add_pixel_val(
        bottom_diff,
        ywf * xwf * vwf * uwc * grad,
        idx,
        Hin,
        Win,
        yf,
        xf,
        Vin,
        Uin,
        vf,
        uc);
    add_pixel_val(
        bottom_diff,
        ywf * xwf * vwc * uwf * grad,
        idx,
        Hin,
        Win,
        yf,
        xf,
        Vin,
        Uin,
        vc,
        uf);
    add_pixel_val(
        bottom_diff,
        ywf * xwf * vwc * uwc * grad,
        idx,
        Hin,
        Win,
        yf,
        xf,
        Vin,
        Uin,
        vc,
        uc);
    add_pixel_val(
        bottom_diff,
        ywf * xwc * vwf * uwf * grad,
        idx,
        Hin,
        Win,
        yf,
        xc,
        Vin,
        Uin,
        vf,
        uf);
    add_pixel_val(
        bottom_diff,
        ywf * xwc * vwf * uwc * grad,
        idx,
        Hin,
        Win,
        yf,
        xc,
        Vin,
        Uin,
        vf,
        uc);
    add_pixel_val(
        bottom_diff,
        ywf * xwc * vwc * uwf * grad,
        idx,
        Hin,
        Win,
        yf,
        xc,
        Vin,
        Uin,
        vc,
        uf);
    add_pixel_val(
        bottom_diff,
        ywf * xwc * vwc * uwc * grad,
        idx,
        Hin,
        Win,
        yf,
        xc,
        Vin,
        Uin,
        vc,
        uc);
    add_pixel_val(
        bottom_diff,
        ywc * xwf * vwf * uwf * grad,
        idx,
        Hin,
        Win,
        yc,
        xf,
        Vin,
        Uin,
        vf,
        uf);
    add_pixel_val(
        bottom_diff,
        ywc * xwf * vwf * uwc * grad,
        idx,
        Hin,
        Win,
        yc,
        xf,
        Vin,
        Uin,
        vf,
        uc);
    add_pixel_val(
        bottom_diff,
        ywc * xwf * vwc * uwf * grad,
        idx,
        Hin,
        Win,
        yc,
        xf,
        Vin,
        Uin,
        vc,
        uf);
    add_pixel_val(
        bottom_diff,
        ywc * xwf * vwc * uwc * grad,
        idx,
        Hin,
        Win,
        yc,
        xf,
        Vin,
        Uin,
        vc,
        uc);
    add_pixel_val(
        bottom_diff,
        ywc * xwc * vwf * uwf * grad,
        idx,
        Hin,
        Win,
        yc,
        xc,
        Vin,
        Uin,
        vf,
        uf);
    add_pixel_val(
        bottom_diff,
        ywc * xwc * vwf * uwc * grad,
        idx,
        Hin,
        Win,
        yc,
        xc,
        Vin,
        Uin,
        vf,
        uc);
    add_pixel_val(
        bottom_diff,
        ywc * xwc * vwc * uwf * grad,
        idx,
        Hin,
        Win,
        yc,
        xc,
        Vin,
        Uin,
        vc,
        uf);
    add_pixel_val(
        bottom_diff,
        ywc * xwc * vwc * uwc * grad,
        idx,
        Hin,
        Win,
        yc,
        xc,
        Vin,
        Uin,
        vc,
        uc);
  }
}

namespace cvpods {

at::Tensor SwapAlign2Nat_forward_cuda(
    const at::Tensor& X,
    const int lambda_val,
    const float pad_val) {
  AT_ASSERTM(X.device().is_cuda(), "input must be a CUDA tensor");
  AT_ASSERTM(X.ndimension() == 4, "input must be a 4D tensor");
  AT_ASSERTM(lambda_val >= 1, "lambda should be greater or equal to 1");
  const int N = X.size(0);
  const int C = X.size(1);
  const int Vin = static_cast<int>(sqrt(static_cast<float>(C)));
  const int Uin = C / Vin;
  AT_ASSERTM(
      C == Vin * Uin && Vin == Uin, "#channels should be a square number");
  const int Vout = lambda_val * Vin;
  const int Uout = lambda_val * Uin;
  const int Hin = X.size(2);
  const int Win = X.size(3);
  const float lambda = static_cast<float>(lambda_val);
  const int Hout = static_cast<int>(ceil(Hin / lambda));
  const int Wout = static_cast<int>(ceil(Win / lambda));
  const float hVout = Vout / 2.;
  const float hUout = Uout / 2.;

  at::cuda::CUDAGuard device_guard(X.device());

  at::Tensor Y = at::empty({N, Vout * Uout, Hout, Wout}, X.options());

  cudaStream_t stream = at::cuda::getCurrentCUDAStream();

  dim3 grid(std::min(at::cuda::ATenCeilDiv(Y.numel(), 512L), 4096L));
  dim3 block(512);

  if (Y.numel() == 0) {
    AT_CUDA_CHECK(cudaGetLastError());
    return Y;
  }

  AT_DISPATCH_FLOATING_TYPES(X.scalar_type(), "SwapAlign2Nat_forward", [&] {
    SwapAlign2NatForwardFeat<scalar_t><<<grid, block, 0, stream>>>(
        Y.numel(),
        X.contiguous().data_ptr<scalar_t>(),
        Vout,
        Uout,
        hVout,
        hUout,
        Vin,
        Uin,
        lambda,
        Hin,
        Win,
        Hout,
        Wout,
        pad_val,
        Y.data_ptr<scalar_t>());
  });
  cudaDeviceSynchronize();
  AT_CUDA_CHECK(cudaGetLastError());
  return Y;
}

at::Tensor SwapAlign2Nat_backward_cuda(
    const at::Tensor& gY,
    const int lambda_val,
    const int batch_size,
    const int channel,
    const int height,
    const int width) {
  AT_ASSERTM(gY.device().is_cuda(), "input gradient must be a CUDA tensor");
  AT_ASSERTM(gY.ndimension() == 4, "input gradient must be a 4D tensor");
  AT_ASSERTM(lambda_val >= 1, "lambda should be greater or equal to 1");
  const int Vin = static_cast<int>(sqrt(static_cast<float>(channel)));
  const int Uin = channel / Vin;
  const int Vout = lambda_val * Vin;
  const int Uout = lambda_val * Uin;
  const float hVout = Vout / 2.;
  const float hUout = Uout / 2.;
  const int Hout = gY.size(2);
  const int Wout = gY.size(3);

  at::cuda::CUDAGuard device_guard(gY.device());

  at::Tensor gX = at::zeros({batch_size, channel, height, width}, gY.options());

  cudaStream_t stream = at::cuda::getCurrentCUDAStream();

  dim3 grid(std::min(at::cuda::ATenCeilDiv(gY.numel(), 512L), 4096L));
  dim3 block(512);

  // handle possibly empty gradients
  if (gY.numel() == 0) {
    AT_CUDA_CHECK(cudaGetLastError());
    return gX;
  }

  AT_DISPATCH_FLOATING_TYPES(gY.scalar_type(), "SwapAlign2Nat_backward", [&] {
    SwapAlign2NatBackwardFeat<scalar_t><<<grid, block, 0, stream>>>(
        gY.numel(),
        gY.contiguous().data_ptr<scalar_t>(),
        Vout,
        Uout,
        hVout,
        hUout,
        Vin,
        Uin,
        static_cast<float>(lambda_val),
        height,
        width,
        Hout,
        Wout,
        gX.data_ptr<scalar_t>());
  });
  AT_CUDA_CHECK(cudaGetLastError());
  return gX;
}

} // namespace cvpods 
```

##### cvpods/layers/csrc/SwapAlign2Nat/SwapAlign2Nat.h

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#pragma once
#include <torch/types.h>

namespace cvpods {

#ifdef WITH_CUDA
at::Tensor SwapAlign2Nat_forward_cuda(
    const at::Tensor& X,
    const int lambda_val,
    const float pad_val);

at::Tensor SwapAlign2Nat_backward_cuda(
    const at::Tensor& gY,
    const int lambda_val,
    const int batch_size,
    const int channel,
    const int height,
    const int width);
#endif

inline at::Tensor SwapAlign2Nat_forward(
    const at::Tensor& X,
    const int lambda_val,
    const float pad_val) {
  if (X.device().is_cuda()) {
#ifdef WITH_CUDA
    return SwapAlign2Nat_forward_cuda(X, lambda_val, pad_val);
#else
    AT_ERROR("Not compiled with GPU support");
#endif
  }
  AT_ERROR("Not implemented on the CPU");
}

inline at::Tensor SwapAlign2Nat_backward(
    const at::Tensor& gY,
    const int lambda_val,
    const int batch_size,
    const int channel,
    const int height,
    const int width) {
  if (gY.device().is_cuda()) {
#ifdef WITH_CUDA
    return SwapAlign2Nat_backward_cuda(
        gY, lambda_val, batch_size, channel, height, width);
#else
    AT_ERROR("Not compiled with GPU support");
#endif
  }
  AT_ERROR("Not implemented on the CPU");
}

} // namespace cvpods 
```

##### cvpods/layers/csrc/ml_nms/ml_nms.h

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
#pragma once
#include <torch/extension.h>

namespace cvpods {
#ifdef WITH_CUDA
at::Tensor ml_nms_cuda(const at::Tensor boxes, float nms_overlap_thresh);
#endif


inline at::Tensor ml_nms(const at::Tensor& dets,
                  const at::Tensor& scores,
                  const at::Tensor& labels,
                  const float threshold) {

  if (dets.device().is_cuda()) {
#ifdef WITH_CUDA
    // TODO raise error if not compiled with CUDA
    if (dets.numel() == 0)
      return at::empty({0}, dets.options().dtype(at::kLong).device(at::kCPU));
    auto b = at::cat({dets, scores.unsqueeze(1), labels.unsqueeze(1)}, 1);
    return ml_nms_cuda(b, threshold);
#else
    AT_ERROR("Not compiled with GPU support");
#endif
  }
  AT_ERROR("CPU version not implemented");
}

}
```

##### cvpods/layers/csrc/ml_nms/ml_nms.cu

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>

#include <THC/THC.h>
#include <THC/THCDeviceUtils.cuh>

#include <vector>
#include <iostream>

int const threadsPerBlock = sizeof(unsigned long long) * 8;

__device__ inline float devIoU(float const * const a, float const * const b) {
  if (a[5] != b[5]) {
    return 0.0;
  }
  float left = max(a[0], b[0]), right = min(a[2], b[2]);
  float top = max(a[1], b[1]), bottom = min(a[3], b[3]);
  float width = max(right - left + 1, 0.f), height = max(bottom - top + 1, 0.f);
  float interS = width * height;
  float Sa = (a[2] - a[0] + 1) * (a[3] - a[1] + 1);
  float Sb = (b[2] - b[0] + 1) * (b[3] - b[1] + 1);
  return interS / (Sa + Sb - interS);
}

__global__ void ml_nms_kernel(const int n_boxes, const float nms_overlap_thresh,
                           const float *dev_boxes, unsigned long long *dev_mask) {
  const int row_start = blockIdx.y;
  const int col_start = blockIdx.x;

  // if (row_start > col_start) return;

  const int row_size =
        min(n_boxes - row_start * threadsPerBlock, threadsPerBlock);
  const int col_size =
        min(n_boxes - col_start * threadsPerBlock, threadsPerBlock);

  __shared__ float block_boxes[threadsPerBlock * 6];
  if (threadIdx.x < col_size) {
    block_boxes[threadIdx.x * 6 + 0] =
        dev_boxes[(threadsPerBlock * col_start + threadIdx.x) * 6 + 0];
    block_boxes[threadIdx.x * 6 + 1] =
        dev_boxes[(threadsPerBlock * col_start + threadIdx.x) * 6 + 1];
    block_boxes[threadIdx.x * 6 + 2] =
        dev_boxes[(threadsPerBlock * col_start + threadIdx.x) * 6 + 2];
    block_boxes[threadIdx.x * 6 + 3] =
        dev_boxes[(threadsPerBlock * col_start + threadIdx.x) * 6 + 3];
    block_boxes[threadIdx.x * 6 + 4] =
        dev_boxes[(threadsPerBlock * col_start + threadIdx.x) * 6 + 4];
    block_boxes[threadIdx.x * 6 + 5] =
        dev_boxes[(threadsPerBlock * col_start + threadIdx.x) * 6 + 5];
  }
  __syncthreads();

  if (threadIdx.x < row_size) {
    const int cur_box_idx = threadsPerBlock * row_start + threadIdx.x;
    const float *cur_box = dev_boxes + cur_box_idx * 6;
    int i = 0;
    unsigned long long t = 0;
    int start = 0;
    if (row_start == col_start) {
      start = threadIdx.x + 1;
    }
    for (i = start; i < col_size; i++) {
      if (devIoU(cur_box, block_boxes + i * 6) > nms_overlap_thresh) {
        t |= 1ULL << i;
      }
    }
    const int col_blocks = THCCeilDiv(n_boxes, threadsPerBlock);
    dev_mask[cur_box_idx * col_blocks + col_start] = t;
  }
}

namespace cvpods {
// boxes is a N x 6 tensor
at::Tensor ml_nms_cuda(const at::Tensor boxes, float nms_overlap_thresh) {
  using scalar_t = float;
  AT_ASSERTM(boxes.device().is_cuda(), "boxes must be a CUDA tensor");
  auto scores = boxes.select(1, 4);
  auto order_t = std::get<1>(scores.sort(0, /* descending=*/true));
  auto boxes_sorted = boxes.index_select(0, order_t);

  int boxes_num = boxes.size(0);

  const int col_blocks = THCCeilDiv(boxes_num, threadsPerBlock);

  scalar_t* boxes_dev = boxes_sorted.data_ptr<scalar_t>();

  THCState *state = at::globalContext().lazyInitCUDA(); // TODO replace with getTHCState

  unsigned long long* mask_dev = NULL;
  //THCudaCheck(THCudaMalloc(state, (void**) &mask_dev,
  //                      boxes_num * col_blocks * sizeof(unsigned long long)));

  mask_dev = (unsigned long long*) THCudaMalloc(state, boxes_num * col_blocks * sizeof(unsigned long long));

  dim3 blocks(THCCeilDiv(boxes_num, threadsPerBlock),
              THCCeilDiv(boxes_num, threadsPerBlock));
  dim3 threads(threadsPerBlock);
  ml_nms_kernel<<<blocks, threads>>>(boxes_num,
                                  nms_overlap_thresh,
                                  boxes_dev,
                                  mask_dev);

  std::vector<unsigned long long> mask_host(boxes_num * col_blocks);
  THCudaCheck(cudaMemcpy(&mask_host[0],
                        mask_dev,
                        sizeof(unsigned long long) * boxes_num * col_blocks,
                        cudaMemcpyDeviceToHost));

  std::vector<unsigned long long> remv(col_blocks);
  memset(&remv[0], 0, sizeof(unsigned long long) * col_blocks);

  at::Tensor keep = at::empty({boxes_num}, boxes.options().dtype(at::kLong).device(at::kCPU));
  int64_t* keep_out = keep.data_ptr<int64_t>();

  int num_to_keep = 0;
  for (int i = 0; i < boxes_num; i++) {
    int nblock = i / threadsPerBlock;
    int inblock = i % threadsPerBlock;

    if (!(remv[nblock] & (1ULL << inblock))) {
      keep_out[num_to_keep++] = i;
      unsigned long long *p = &mask_host[0] + i * col_blocks;
      for (int j = nblock; j < col_blocks; j++) {
        remv[j] |= p[j];
      }
    }
  }

  THCudaFree(state, mask_dev);
  // TODO improve this part
  return std::get<0>(order_t.index({
                       keep.narrow(/*dim=*/0, /*start=*/0, /*length=*/num_to_keep).to(
                         order_t.device(), keep.scalar_type())
                     }).sort(0, false));
}

}
```

##### cvpods/layers/csrc/tree_filter/boruvka.cpp

```cpp
// Boruvka's algorithm to find Minimum Spanning 
// Tree of a given connected, undirected and 
// weighted graph 
#include <stdio.h> 
#include "boruvka.hpp"

// A structure to represent a subset for union-find 
struct subset 
{ 
    int parent;
    int rank;
}; 

// Function prototypes for union-find (These functions are defined 
// after boruvkaMST() ) 
int find(struct subset subsets[], int i); 
void Union(struct subset subsets[], int x, int y); 

// The main function for MST using Boruvka's algorithm 
void boruvkaMST(struct Graph* graph, int * edge_out) 
{ 
    // Get data of given graph
    int V = graph->V, E = graph->E;
    Edge *edge = graph->edge;

    // Allocate memory for creating V subsets.
    struct subset *subsets = new subset[V];

    // An array to store index of the cheapest edge of
    // subset. The stored index for indexing array 'edge[]'
    int *cheapest = new int[V];

    // Create V subsets with single elements
    for (int v = 0; v < V; ++v)
    {
        subsets[v].parent = v;
        subsets[v].rank = 0;
        cheapest[v] = -1;
    }

    // Initially there are V different trees.
    // Finally there will be one tree that will be MST
    int numTrees = V;
    int MSTweight = 0;

    // Keep combining components (or sets) until all
    // compnentes are not combined into single MST.
    while (numTrees > 1)
    {
        // Everytime initialize cheapest array
        for (int v = 0; v < V; ++v)
        {
            cheapest[v] = -1;
        }

        // Traverse through all edges and update
        // cheapest of every component
        for (int i=0; i<E; i++)
        {
            // Find components (or sets) of two corners
            // of current edge
            int set1 = find(subsets, edge[i].src);
            int set2 = find(subsets, edge[i].dest);

            // If two corners of current edge belong to
            // same set, ignore current edge
            if (set1 == set2)
                continue;

            // Else check if current edge is closer to previous
            // cheapest edges of set1 and set2
            else
            {
            if (cheapest[set1] == -1 ||
                edge[cheapest[set1]].weight > edge[i].weight)
                cheapest[set1] = i;

            if (cheapest[set2] == -1 ||
                edge[cheapest[set2]].weight > edge[i].weight)
                cheapest[set2] = i;
            }
        }

        // Consider the above picked cheapest edges and add them
        // to MST
        for (int i=0; i<V; i++)
        {
            // Check if cheapest for current set exists
            if (cheapest[i] != -1)
            {
                int set1 = find(subsets, edge[cheapest[i]].src);
                int set2 = find(subsets, edge[cheapest[i]].dest);

                if (set1 == set2)
                    continue;
                MSTweight += edge[cheapest[i]].weight;
                *(edge_out++) = edge[cheapest[i]].src;
                *(edge_out++) = edge[cheapest[i]].dest;
                //printf("Edge %d-%d included in MST\n",
                    //edge[cheapest[i]].src, edge[cheapest[i]].dest);

                // Do a union of set1 and set2 and decrease number
                // of trees
                Union(subsets, set1, set2);
                numTrees--;
            }
        }
    }

    delete[] subsets;
    delete[] cheapest;
}

// Creates a graph with V vertices and E edges 
struct Graph* createGraph(int V, int E) 
{ 
    Graph* graph = new Graph;
    graph->V = V;
    graph->E = E;
    graph->edge = new Edge[E];
    return graph;
} 

// A utility function to find set of an element i 
// (uses path compression technique) 
int find(struct subset subsets[], int i) 
{ 
    // find root and make root as parent of i
    // (path compression)
    if (subsets[i].parent != i)
    subsets[i].parent =
            find(subsets, subsets[i].parent);

    return subsets[i].parent;
} 

// A function that does union of two sets of x and y 
// (uses union by rank) 
void Union(struct subset subsets[], int x, int y) 
{ 
    int xroot = find(subsets, x);
    int yroot = find(subsets, y);

    // Attach smaller rank tree under root of high
    // rank tree (Union by Rank)
    if (subsets[xroot].rank < subsets[yroot].rank)
        subsets[xroot].parent = yroot;
    else if (subsets[xroot].rank > subsets[yroot].rank)
        subsets[yroot].parent = xroot;

    // If ranks are same, then make one as root and
    // increment its rank by one
    else
    {
        subsets[yroot].parent = xroot;
        subsets[xroot].rank++;
    }
} 

```

##### cvpods/layers/csrc/tree_filter/mst.hpp

```
#pragma once
#include <torch/extension.h>

extern at::Tensor mst_forward(
            const at::Tensor & edge_index_tensor,
            const at::Tensor & edge_weight_tensor,
            int vertex_count);

```

##### cvpods/layers/csrc/tree_filter/refine.cu

```
#include <math.h>
#include <thread>
#include <vector>
#include <deque>
#include <iostream>
#include <stdlib.h>
#include <fstream>
#include <cuda_runtime.h>
#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>

#include <THC/THC.h>
#include <THC/THCAtomics.cuh>
#include <THC/THCDeviceUtils.cuh>

#define CUDA_NUM_THREADS         64
#define GET_CUDA_CHANNEL(N)      ceil(512.0f / N)

__global__ void root_leaf_prop_kernel(
        float * in_data, 
        float * out_data, 
        float * weight,
        int * sorted_index, 
        int * sorted_parent_index, 
        int batch_size, 
        int channel_size, 
        int vertex_count){

    const int thread_idx    = threadIdx.x;
    const int batch_idx     = blockIdx.x;
    const int channel_idx   = blockIdx.y;
    const int thread_count  = blockDim.x;
    const int channel_step  = gridDim.y;

    in_data             += batch_idx * vertex_count * channel_size;
    out_data            += batch_idx * vertex_count * channel_size;
    weight              += batch_idx * vertex_count;
    sorted_index        += batch_idx * vertex_count;
    sorted_parent_index += batch_idx * vertex_count;

    __shared__ int node_per_thread[CUDA_NUM_THREADS];
    node_per_thread[thread_idx] = -1;
    if (thread_idx == 0){
        weight[0]              = 0;
        sorted_parent_index[0] = 0;
    }
    __syncthreads();

    int i = thread_idx;
    while (i < vertex_count){
        int par = sorted_parent_index[i];
        int par_thread = par % thread_count;
        if ((node_per_thread[par_thread] >= par) || (i == 0)){
            int cur_pos = sorted_index[i];
            int par_pos = sorted_index[par];
            for (int k = channel_idx * vertex_count; k < channel_size * vertex_count;
                       k += channel_step * vertex_count){
                float edge_weight = weight[i];
                out_data[cur_pos + k] = in_data[i + k] * (1 - edge_weight * edge_weight) +
                                        out_data[par_pos + k] * edge_weight;
                __threadfence_block();
            }
            node_per_thread[thread_idx] = i;
            i += thread_count;
        }
        __syncthreads();
    }
}

__global__ void leaf_root_aggr_kernel(
        float * in_data, 
        float * out_data, 
        float * weight,
        int * sorted_index, 
        int * sorted_child_index, 
        int batch_size, 
        int channel_size, 
        int vertex_count,
        int max_adj_per_node){

    const int thread_idx    = threadIdx.x;
    const int batch_idx     = blockIdx.x;
    const int channel_idx   = blockIdx.y;
    const int thread_count  = blockDim.x;
    const int channel_step  = gridDim.y;
    
    if (in_data != NULL){
        in_data    += batch_idx * vertex_count * channel_size;
    }    
    out_data             += batch_idx * vertex_count * channel_size;
    weight               += batch_idx * vertex_count;
    sorted_index         += batch_idx * vertex_count;
    sorted_child_index   += batch_idx * vertex_count * max_adj_per_node;

    __shared__ int node_per_thread[CUDA_NUM_THREADS];
    node_per_thread[thread_idx] = vertex_count;
    __syncthreads();

    int i = vertex_count - thread_idx - 1;
    while (i >= 0){
        int child_len = 0;
        bool valid = true;
        for (int j = 0; j < max_adj_per_node; j++){
            int child        = sorted_child_index[i * max_adj_per_node + j];
            int child_thread = (vertex_count - child - 1) % thread_count;

            if (child <= 0) break;
            if (node_per_thread[child_thread] > child){
                valid = false;
                break;
            }
            child_len++;
        }
        if (valid){
            int cur_pos = sorted_index[i];
            for (int k = channel_idx * vertex_count; k < channel_size * vertex_count; 
                    k += channel_step * vertex_count){
                float aggr_sum;
                if (in_data != NULL)    
                    aggr_sum = in_data[cur_pos + k];
                else
                    aggr_sum = 1;
                for (int j = 0; j < child_len; j++){
                    int child = sorted_child_index[i * max_adj_per_node + j];
                    aggr_sum += out_data[child + k] * weight[child];
                }
                out_data[i + k] = aggr_sum;
            }
            node_per_thread[thread_idx] = i;
            i -= thread_count;
        }
        __syncthreads();
    }
}

__global__ void root_leaf_grad_kernel(
        float * in_data,
        float * in_grad,
        float * out_data,
        float * out_grad, 
        float * weight,
        float * grad,
        int * sorted_index, 
        int * sorted_parent_index, 
        int batch_size, 
        int data_channel_size,
        int grad_channel_size,
        int vertex_count){

    const int thread_idx    = threadIdx.x;
    const int batch_idx     = blockIdx.x;
    const int channel_idx   = blockIdx.y;
    const int thread_count  = blockDim.x;
    const int channel_step  = gridDim.y;
    const int channel_size  = data_channel_size > grad_channel_size ? data_channel_size : grad_channel_size;

    in_data             += batch_idx * vertex_count * data_channel_size;
    in_grad             += batch_idx * vertex_count * grad_channel_size;
    out_data            += batch_idx * vertex_count * data_channel_size;
    out_grad            += batch_idx * vertex_count * grad_channel_size;
    weight              += batch_idx * vertex_count;
    grad                += batch_idx * vertex_count * channel_size;
    sorted_index        += batch_idx * vertex_count;
    sorted_parent_index += batch_idx * vertex_count;

    __shared__ int node_per_thread[CUDA_NUM_THREADS];
    node_per_thread[thread_idx] = -1;

    int i = thread_idx;
    while (i < vertex_count){
        int cur         = i;
        int par         = sorted_parent_index[i];
        int par_pos     = sorted_index[par];
        int par_thread  = par % thread_count;
        if ((cur == 0) || (node_per_thread[par_thread] >= par)){
            for (int k = channel_idx; k < channel_size; k += channel_step){
                float edge_weight   = weight[i];
                int data_offset     = (k % data_channel_size) * vertex_count;
                int grad_offset     = (k % grad_channel_size) * vertex_count;
                int out_offset      = k * vertex_count;
                
                if (cur > 0){
                    float left  = in_grad[cur + grad_offset] * (out_data[par_pos + data_offset] - edge_weight * in_data[cur + data_offset]);
                    float right = in_data[cur + data_offset] * (out_grad[par + grad_offset] - edge_weight * in_grad[cur + grad_offset]);

                    grad[cur + out_offset]      = left + right;
                    out_grad[cur + grad_offset] = in_grad[cur + grad_offset] * (1 - edge_weight * edge_weight) +
                                                  out_grad[par + grad_offset] * edge_weight;
                    __threadfence_block();
                }
                else
                    grad[cur + out_offset] = 0;
            }
            node_per_thread[thread_idx] = i;
            i += thread_count;
        }
        __syncthreads();
    }
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
tree_filter_refine_forward(
        const at::Tensor & feature_in_tensor, 
        const at::Tensor & edge_weight_tensor, 
        const at::Tensor & self_weight_tensor,
        const at::Tensor & sorted_index_tensor, 
        const at::Tensor & sorted_parent_tensor, 
        const at::Tensor & sorted_child_tensor 
    ){
    
    const int batch_size        = feature_in_tensor.size(0);
    const int channel_size      = feature_in_tensor.size(1); 
    const int vertex_size       = feature_in_tensor.size(2);
    const int max_adj_per_node  = sorted_child_tensor.size(2);

    auto options                  = feature_in_tensor.options();
    auto feature_aggr_tensor      = at::zeros_like(feature_in_tensor, options);
    auto feature_aggr_up_tensor   = at::zeros_like(feature_in_tensor, options);

    cudaStream_t stream = at::cuda::getCurrentCUDAStream();

    float * feature_in          = feature_in_tensor.contiguous().data_ptr<float>();
    float * edge_weight         = edge_weight_tensor.contiguous().data_ptr<float>();
    int * sorted_index          = sorted_index_tensor.contiguous().data_ptr<int>();
    int * sorted_parent_index   = sorted_parent_tensor.contiguous().data_ptr<int>();
    int * sorted_child_index    = sorted_child_tensor.contiguous().data_ptr<int>();
    float * feature_aggr        = feature_aggr_tensor.contiguous().data_ptr<float>();
    float * feature_aggr_sum    = feature_aggr_up_tensor.contiguous().data_ptr<float>();

    dim3 feature_block_dims(CUDA_NUM_THREADS, 1, 1), feature_grid_dims(batch_size, channel_size, 1);
    leaf_root_aggr_kernel <<< feature_grid_dims, feature_block_dims, sizeof(int) * CUDA_NUM_THREADS, stream >>>(
            feature_in, feature_aggr_sum, edge_weight, sorted_index, sorted_child_index, batch_size, channel_size, vertex_size, max_adj_per_node);
    root_leaf_prop_kernel <<< feature_grid_dims, feature_block_dims, sizeof(int) * CUDA_NUM_THREADS, stream >>>(
            feature_aggr_sum, feature_aggr, edge_weight, sorted_index, sorted_parent_index, batch_size, channel_size, vertex_size);

    auto feature_out_tensor = feature_aggr_tensor + (self_weight_tensor - 1).unsqueeze(1) * feature_in_tensor;

    auto result = std::make_tuple(feature_out_tensor, feature_aggr_tensor, feature_aggr_up_tensor);

    return result;
}

at::Tensor tree_filter_refine_backward_feature(
        const at::Tensor & feature_in_tensor, 
        const at::Tensor & edge_weight_tensor, 
        const at::Tensor & self_weight_tensor,
        const at::Tensor & sorted_index_tensor, 
        const at::Tensor & sorted_parent_tensor, 
        const at::Tensor & sorted_child_tensor,
        const at::Tensor & feature_aggr_tensor,
        const at::Tensor & feature_aggr_up_tensor,
        const at::Tensor & grad_out_tensor
    ){

    auto options                        = feature_in_tensor.options();
    auto grad_feature_tensor            = at::zeros_like(feature_in_tensor, options);
    auto grad_feature_aggr_sum_tensor   = at::zeros_like(feature_in_tensor, options);

    const int batch_size        = feature_in_tensor.size(0);
    const int channel_size      = feature_in_tensor.size(1); 
    const int vertex_size       = feature_in_tensor.size(2);
    const int max_adj_per_node  = sorted_child_tensor.size(2);

    float * feature_in          = feature_in_tensor.contiguous().data_ptr<float>();
    float * edge_weight         = edge_weight_tensor.contiguous().data_ptr<float>();
    int * sorted_index          = sorted_index_tensor.contiguous().data_ptr<int>();
    int * sorted_parent_index   = sorted_parent_tensor.contiguous().data_ptr<int>();
    int * sorted_child_index    = sorted_child_tensor.contiguous().data_ptr<int>();
    float * feature_aggr        = feature_aggr_tensor.contiguous().data_ptr<float>();
    float * feature_aggr_sum    = feature_aggr_up_tensor.contiguous().data_ptr<float>();
    float * grad_out            = grad_out_tensor.contiguous().data_ptr<float>();
    float * grad_feature        = grad_feature_tensor.contiguous().data_ptr<float>();

    float * grad_feature_aggr_sum   = grad_feature_aggr_sum_tensor.contiguous().data_ptr<float>();

    cudaStream_t stream = at::cuda::getCurrentCUDAStream();

    dim3 feature_block_dims(CUDA_NUM_THREADS, 1, 1), feature_grid_dims(batch_size, channel_size, 1);
    leaf_root_aggr_kernel <<< feature_grid_dims, feature_block_dims, sizeof(int) * CUDA_NUM_THREADS, stream >>>(
            grad_out, grad_feature_aggr_sum, edge_weight, sorted_index, sorted_child_index, batch_size, channel_size, vertex_size, max_adj_per_node);
    root_leaf_prop_kernel <<< feature_grid_dims, feature_block_dims, sizeof(int) * CUDA_NUM_THREADS, stream >>>(
            grad_feature_aggr_sum, grad_feature, edge_weight, sorted_index, sorted_parent_index, batch_size, channel_size, vertex_size);

    grad_feature_tensor += (self_weight_tensor - 1).unsqueeze(1) * grad_out_tensor;
    
    return grad_feature_tensor;
}

at::Tensor tree_filter_refine_backward_edge_weight(
        const at::Tensor & feature_in_tensor, 
        const at::Tensor & edge_weight_tensor, 
        const at::Tensor & self_weight_tensor, 
        const at::Tensor & sorted_index_tensor, 
        const at::Tensor & sorted_parent_tensor, 
        const at::Tensor & sorted_child_tensor,
        const at::Tensor & feature_aggr_tensor,
        const at::Tensor & feature_aggr_up_tensor,
        const at::Tensor & grad_out_tensor
    ){

    auto options            = feature_in_tensor.options();
    auto grad_weight_tensor = at::zeros_like(edge_weight_tensor, options);

    const int batch_size        = feature_in_tensor.size(0);
    const int channel_size      = feature_in_tensor.size(1); 
    const int vertex_size       = feature_in_tensor.size(2);
    const int max_adj_per_node  = sorted_child_tensor.size(2);

    float * feature_in          = feature_in_tensor.contiguous().data_ptr<float>();
    float * edge_weight         = edge_weight_tensor.contiguous().data_ptr<float>();
    int * sorted_index          = sorted_index_tensor.contiguous().data_ptr<int>();
    int * sorted_parent_index   = sorted_parent_tensor.contiguous().data_ptr<int>();
    int * sorted_child_index    = sorted_child_tensor.contiguous().data_ptr<int>();
    float * feature_aggr        = feature_aggr_tensor.contiguous().data_ptr<float>();
    float * feature_aggr_sum    = feature_aggr_up_tensor.contiguous().data_ptr<float>();
    float * grad_out            = grad_out_tensor.contiguous().data_ptr<float>();
    float * grad_weight         = grad_weight_tensor.contiguous().data_ptr<float>();
    
    auto grad_all_channel_tensor        = at::zeros_like(feature_in_tensor, options);
    auto grad_norm_all_channel_tensor   = at::zeros_like(feature_in_tensor, options);
    auto grad_out_norm_aggr_sum_tensor  = at::zeros_like(feature_in_tensor, options);
    auto feature_grad_aggr_sum_tensor   = at::zeros_like(feature_in_tensor, options);
    
    float * grad_all_channel            = grad_all_channel_tensor.contiguous().data_ptr<float>();
    float * grad_norm_all_channel       = grad_norm_all_channel_tensor.contiguous().data_ptr<float>();
    float * grad_out_norm_aggr_sum      = grad_out_norm_aggr_sum_tensor.contiguous().data_ptr<float>();
    float * feature_grad_aggr_sum       = feature_grad_aggr_sum_tensor.contiguous().data_ptr<float>();

    auto grad_out_norm_tensor = grad_out_tensor;
    float * grad_out_norm     = grad_out_norm_tensor.contiguous().data_ptr<float>();

    cudaStream_t stream = at::cuda::getCurrentCUDAStream();

    dim3 feature_block_dims(CUDA_NUM_THREADS, 1, 1), feature_grid_dims(batch_size, channel_size, 1);
    leaf_root_aggr_kernel <<< feature_grid_dims, feature_block_dims, sizeof(int) * CUDA_NUM_THREADS, stream >>>(
            grad_out_norm, grad_out_norm_aggr_sum, edge_weight, sorted_index, sorted_child_index, batch_size, channel_size, vertex_size, max_adj_per_node);

    root_leaf_grad_kernel <<< feature_grid_dims, feature_block_dims, sizeof(int) * CUDA_NUM_THREADS, stream >>>(
            feature_aggr_sum, grad_out_norm_aggr_sum, feature_aggr, grad_out_norm_aggr_sum, edge_weight, grad_all_channel, 
            sorted_index, sorted_parent_index, batch_size, channel_size, channel_size, vertex_size);

    grad_weight_tensor = grad_all_channel_tensor.sum(1);

    return grad_weight_tensor;
}

at::Tensor tree_filter_refine_backward_self_weight(
        const at::Tensor & feature_in_tensor, 
        const at::Tensor & edge_weight_tensor, 
        const at::Tensor & self_weight_tensor,
        const at::Tensor & sorted_index_tensor, 
        const at::Tensor & sorted_parent_tensor, 
        const at::Tensor & sorted_child_tensor,
        const at::Tensor & feature_aggr_tensor,
        const at::Tensor & feature_aggr_up_tensor,
        const at::Tensor & grad_out_tensor
    ){

    auto grad_self_weight_tensor = (grad_out_tensor * feature_in_tensor).sum(1);

    return grad_self_weight_tensor;
}

```

##### cvpods/layers/csrc/tree_filter/rst.hpp

```
#pragma once
#include <torch/extension.h>

extern at::Tensor rst_forward(
            const at::Tensor & edge_index_tensor,
            const at::Tensor & edge_weight_tensor,
            int vertex_count);

```

##### cvpods/layers/csrc/tree_filter/boruvka_rst.cpp

```cpp
// Boruvka's algorithm to find Minimum Spanning 
// Tree of a given connected, undirected and 
// weighted graph 
#include <stdio.h> 
#include <stdlib.h>
#include <time.h>
#include "boruvka_rst.hpp"

// A structure to represent a subset for union-find 
struct subset 
{ 
    int parent;
    int rank;
}; 

// Function prototypes for union-find (These functions are defined 
// after boruvka_rst() ) 
static bool random_lt(float a, float b);
static int find(struct subset subsets[], int i); 
static void Union(struct subset subsets[], int x, int y); 

// The main function for MST using Boruvka's algorithm 
void boruvka_rst(struct Graph* graph, int * edge_out) 
{ 
    // Get data of given graph
    int V = graph->V, E = graph->E;
    Edge *edge = graph->edge;

    // Allocate memory for creating V subsets.
    struct subset *subsets = new subset[V];

    // An array to store index of the cheapest edge of
    // subset. The stored index for indexing array 'edge[]'
    int *cheapest = new int[V];

    // Create V subsets with single elements
    for (int v = 0; v < V; ++v)
    {
        subsets[v].parent = v;
        subsets[v].rank = 0;
        cheapest[v] = -1;
    }

    // Initially there are V different trees.
    // Finally there will be one tree that will be MST
    int numTrees = V;
    int MSTweight = 0;

    srand(time(0));

    // Keep combining components (or sets) until all
    // compnentes are not combined into single MST.
    while (numTrees > 1)
    {
        // Everytime initialize cheapest array
        for (int v = 0; v < V; ++v)
        {
            cheapest[v] = -1;
        }

        // Traverse through all edges and update
        // cheapest of every component
        for (int i=0; i<E; i++)
        {
            // Find components (or sets) of two corners
            // of current edge
            int set1 = find(subsets, edge[i].src);
            int set2 = find(subsets, edge[i].dest);

            // If two corners of current edge belong to
            // same set, ignore current edge
            if (set1 == set2)
                continue;

            // Else check if current edge is closer to previous
            // cheapest edges of set1 and set2
            else
            {
            if (cheapest[set1] == -1 ||
                random_lt(edge[cheapest[set1]].weight, edge[i].weight))
                cheapest[set1] = i;

            if (cheapest[set2] == -1 ||
                random_lt(edge[cheapest[set2]].weight, edge[i].weight))
                cheapest[set2] = i;
            }
        }

        // Consider the above picked cheapest edges and add them
        // to MST
        for (int i=0; i<V; i++)
        {
            // Check if cheapest for current set exists
            if (cheapest[i] != -1)
            {
                int set1 = find(subsets, edge[cheapest[i]].src);
                int set2 = find(subsets, edge[cheapest[i]].dest);

                if (set1 == set2)
                    continue;
                MSTweight += edge[cheapest[i]].weight;
                *(edge_out++) = edge[cheapest[i]].src;
                *(edge_out++) = edge[cheapest[i]].dest;
                //printf("Edge %d-%d included in MST\n",
                    //edge[cheapest[i]].src, edge[cheapest[i]].dest);

                // Do a union of set1 and set2 and decrease number
                // of trees
                Union(subsets, set1, set2);
                numTrees--;
            }
        }
    }

    delete[] subsets;
    delete[] cheapest;
}

// Creates a graph with V vertices and E edges 
struct Graph* create_graph(int V, int E) 
{ 
    Graph* graph = new Graph;
    graph->V = V;
    graph->E = E;
    graph->edge = new Edge[E];
    return graph;
} 

static bool random_lt(float a, float b)
{
    float prob = (float)rand() / (float)RAND_MAX;
    if (a + b == 0) return a < b;
    if ((a / (a + b)) < prob)
        return true;
    else
        return false;
}

// A utility function to find set of an element i 
// (uses path compression technique) 
static int find(struct subset subsets[], int i) 
{ 
    // find root and make root as parent of i
    // (path compression)
    if (subsets[i].parent != i)
    subsets[i].parent =
            find(subsets, subsets[i].parent);

    return subsets[i].parent;
} 

// A function that does union of two sets of x and y 
// (uses union by rank) 
static void Union(struct subset subsets[], int x, int y) 
{ 
    int xroot = find(subsets, x);
    int yroot = find(subsets, y);

    // Attach smaller rank tree under root of high
    // rank tree (Union by Rank)
    if (subsets[xroot].rank < subsets[yroot].rank)
        subsets[xroot].parent = yroot;
    else if (subsets[xroot].rank > subsets[yroot].rank)
        subsets[yroot].parent = xroot;

    // If ranks are same, then make one as root and
    // increment its rank by one
    else
    {
        subsets[yroot].parent = xroot;
        subsets[xroot].rank++;
    }
} 

```

##### cvpods/layers/csrc/tree_filter/refine.hpp

```
#pragma once
#include <torch/extension.h>

extern std::tuple<at::Tensor, at::Tensor, at::Tensor>
    tree_filter_refine_forward(
        const at::Tensor & feature_in_tensor, 
        const at::Tensor & edge_weight_tensor, 
        const at::Tensor & self_weight_tensor,
        const at::Tensor & sorted_index_tensor, 
        const at::Tensor & sorted_parent_index_tensor, 
        const at::Tensor & sorted_child_index_tensor 
    );

extern at::Tensor tree_filter_refine_backward_feature(
        const at::Tensor & feature_in_tensor, 
        const at::Tensor & edge_weight_tensor, 
        const at::Tensor & self_weight_tensor,
        const at::Tensor & sorted_index_tensor, 
        const at::Tensor & sorted_parent_tensor, 
        const at::Tensor & sorted_child_tensor,
        const at::Tensor & feature_aggr_tensor,
        const at::Tensor & feature_aggr_up_tensor,
        const at::Tensor & grad_out_tensor
    );

extern at::Tensor tree_filter_refine_backward_edge_weight(
        const at::Tensor & feature_in_tensor, 
        const at::Tensor & edge_weight_tensor, 
        const at::Tensor & self_weight_tensor,
        const at::Tensor & sorted_index_tensor, 
        const at::Tensor & sorted_parent_tensor, 
        const at::Tensor & sorted_child_tensor,
        const at::Tensor & feature_aggr_tensor,
        const at::Tensor & feature_aggr_up_tensor,
        const at::Tensor & grad_out_tensor
    );

extern at::Tensor tree_filter_refine_backward_self_weight(
        const at::Tensor & feature_in_tensor, 
        const at::Tensor & edge_weight_tensor, 
        const at::Tensor & self_weight_tensor,
        const at::Tensor & sorted_index_tensor, 
        const at::Tensor & sorted_parent_tensor, 
        const at::Tensor & sorted_child_tensor,
        const at::Tensor & feature_aggr_tensor,
        const at::Tensor & feature_aggr_up_tensor,
        const at::Tensor & grad_out_tensor
    );

```

##### cvpods/layers/csrc/tree_filter/bfs.hpp

```
#pragma once
#include <torch/extension.h>

extern std::tuple<at::Tensor, at::Tensor, at::Tensor>
    bfs_forward(
        const at::Tensor & edge_index_tensor,
        int max_adj_per_node
    );

```

##### cvpods/layers/csrc/tree_filter/boruvka_rst.hpp

```
#pragma once

// a structure to represent a weighted edge in graph 
struct Edge 
{ 
    int src, dest;
    float weight; 
}; 

// a structure to represent a connected, undirected 
// and weighted graph as a collection of edges. 
struct Graph 
{ 
    // V-> Number of vertices, E-> Number of edges
    int V, E;

    // graph is represented as an array of edges.
    // Since the graph is undirected, the edge
    // from src to dest is also edge from dest
    // to src. Both are counted as 1 edge here.
    Edge* edge;
}; 

extern struct Graph* create_graph(int V, int E); 
extern void boruvka_rst(struct Graph* graph, int * edge_out);

```

##### cvpods/layers/csrc/tree_filter/rst.cu

```
#include <thread>
#include <iostream>
#include <stdlib.h>
#include <fstream>

#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>

#include <THC/THC.h>
#include <THC/THCAtomics.cuh>
#include <THC/THCDeviceUtils.cuh>

#include "boruvka_rst.hpp"

static void forward_kernel(int * edge_index, float * edge_weight, int * edge_out, int vertex_count, int edge_count){
    struct Graph * g = create_graph(vertex_count, edge_count);
    for (int i = 0; i < edge_count; ++i){
        g->edge[i].src = edge_index[i * 2];
        g->edge[i].dest = edge_index[i * 2 + 1];
        g->edge[i].weight = edge_weight[i];
    }
    
    boruvka_rst(g, edge_out);

    delete[] g->edge;
    delete[] g;
}
    
at::Tensor rst_forward(
            const at::Tensor & edge_index_tensor,
            const at::Tensor & edge_weight_tensor,
            int vertex_count){
    unsigned batch_size = edge_index_tensor.size(0);
    unsigned edge_count = edge_index_tensor.size(1);
    
    auto edge_index_cpu   = edge_index_tensor.cpu();
    auto edge_weight_cpu  = edge_weight_tensor.cpu(); 
    auto edge_out_cpu     = at::empty({batch_size, vertex_count - 1, 2}, edge_index_cpu.options());
    
    int * edge_out      = edge_out_cpu.contiguous().data_ptr<int>();
    int * edge_index    = edge_index_cpu.contiguous().data_ptr<int>();
    float * edge_weight = edge_weight_cpu.contiguous().data_ptr<float>(); 

    // Loop for batch
    std::thread pids[batch_size];
    for (unsigned i = 0; i < batch_size; i++){
        auto edge_index_iter  = edge_index + i * edge_count * 2;
        auto edge_weight_iter = edge_weight + i * edge_count;
        auto edge_out_iter    = edge_out + i * (vertex_count - 1) * 2;
        pids[i] = std::thread(forward_kernel, edge_index_iter, edge_weight_iter, edge_out_iter, vertex_count, edge_count);
    }
    
    for (unsigned i = 0; i < batch_size; i++){
        pids[i].join();
    }
    
    auto edge_out_tensor = edge_out_cpu.to(edge_index_tensor.device());
    
    return edge_out_tensor;
}

```

##### cvpods/layers/csrc/tree_filter/boruvka.hpp

```
#pragma once

// a structure to represent a weighted edge in graph 
struct Edge 
{ 
    int src, dest;
    float weight; 
}; 

// a structure to represent a connected, undirected 
// and weighted graph as a collection of edges. 
struct Graph 
{ 
    // V-> Number of vertices, E-> Number of edges
    int V, E;

    // graph is represented as an array of edges.
    // Since the graph is undirected, the edge
    // from src to dest is also edge from dest
    // to src. Both are counted as 1 edge here.
    Edge* edge;
}; 

extern struct Graph* createGraph(int V, int E); 
extern void boruvkaMST(struct Graph* graph, int * edge_out);

```

##### cvpods/layers/csrc/tree_filter/mst.cu

```
#include <thread>
#include <iostream>
#include <stdlib.h>
#include <fstream>

#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>

#include <THC/THC.h>
#include <THC/THCAtomics.cuh>
#include <THC/THCDeviceUtils.cuh>

/* Switch of minimal spanning tree algorithms */
/* Note: we will migrate the cuda implementaion to PyTorch in the next version */
//#define MST_PRIM
//#define MST_KRUSKAL
#define MST_BORUVKA

#ifdef MST_PRIM
    #include <boost/graph/adjacency_list.hpp>
    #include <boost/graph/prim_minimum_spanning_tree.hpp>
#endif
#ifdef MST_KRUSKAL 
    #include <boost/graph/adjacency_list.hpp>
    #include <boost/graph/kruskal_min_spanning_tree.hpp>
#endif
#ifdef MST_BORUVKA 
    #include "boruvka.hpp"
#endif


#ifndef MST_BORUVKA
    using namespace boost;
    typedef adjacency_list <vecS, vecS, undirectedS, no_property,
            property < edge_weight_t, float > > Graph;
    typedef graph_traits < Graph >::edge_descriptor Edge;
    typedef graph_traits < Graph >::vertex_descriptor Vertex;
    typedef std::pair<int, int> E;
#endif

static void forward_kernel(int * edge_index, float * edge_weight, int * edge_out, int vertex_count, int edge_count){
#ifdef MST_BORUVKA
    struct Graph * g = createGraph(vertex_count, edge_count);
    for (int i = 0; i < edge_count; ++i){
        g->edge[i].src = edge_index[i * 2];
        g->edge[i].dest = edge_index[i * 2 + 1];
        g->edge[i].weight = edge_weight[i];
    }
#else
    Graph g(vertex_count);
    for (int i = 0; i < edge_count; ++i)
        boost::add_edge((int)edge_index[i * 2], (int)edge_index[i * 2 + 1],
                edge_weight[i], g);
#endif

#ifdef MST_PRIM
    std::vector < graph_traits < Graph >::vertex_descriptor > p(num_vertices(g));
    prim_minimum_spanning_tree(g, &(p[0]));
    int * edge_out_ptr = edge_out;
    for (std::size_t i = 0; i != p.size(); ++i)
        if (p[i] != i) {
            *(edge_out_ptr++) = i;
            *(edge_out_ptr++) = p[i];
        }
#endif
    
#ifdef MST_KRUSKAL
    std::vector < Edge > spanning_tree;
    kruskal_minimum_spanning_tree(g, std::back_inserter(spanning_tree));
    float * edge_out_ptr = edge_out;
    for (std::vector < Edge >::iterator ei = spanning_tree.begin();
            ei != spanning_tree.end(); ++ei){
        *(edge_out_ptr++) = source(*ei, g);
        *(edge_out_ptr++) = target(*ei, g);
    }
#endif

#ifdef MST_BORUVKA
    boruvkaMST(g, edge_out);
    delete[] g->edge;
    delete[] g;
#endif

}
    
at::Tensor mst_forward(
            const at::Tensor & edge_index_tensor,
            const at::Tensor & edge_weight_tensor,
            int vertex_count){
    unsigned batch_size = edge_index_tensor.size(0);
    unsigned edge_count = edge_index_tensor.size(1);
    
    auto edge_index_cpu   = edge_index_tensor.cpu();
    auto edge_weight_cpu  = edge_weight_tensor.cpu(); 
    auto edge_out_cpu     = at::empty({batch_size, vertex_count - 1, 2}, edge_index_cpu.options());
    
    int * edge_out      = edge_out_cpu.contiguous().data_ptr<int>();
    int * edge_index    = edge_index_cpu.contiguous().data_ptr<int>();
    float * edge_weight = edge_weight_cpu.contiguous().data_ptr<float>(); 

    // Loop for batch
    std::thread pids[batch_size];
    for (unsigned i = 0; i < batch_size; i++){
        auto edge_index_iter  = edge_index + i * edge_count * 2;
        auto edge_weight_iter = edge_weight + i * edge_count;
        auto edge_out_iter    = edge_out + i * (vertex_count - 1) * 2;
        pids[i] = std::thread(forward_kernel, edge_index_iter, edge_weight_iter, edge_out_iter, vertex_count, edge_count);
    }
    
    for (unsigned i = 0; i < batch_size; i++){
        pids[i].join();
    }
    
    auto edge_out_tensor = edge_out_cpu.to(edge_index_tensor.device());
    
    return edge_out_tensor;
}

```

##### cvpods/layers/csrc/tree_filter/bfs.cu

```
#include <math.h>
#include <thread>
#include <vector>
#include <deque>
#include <iostream>
#include <stdlib.h>
#include <fstream>
#include <cuda_runtime.h>
#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>

#include <THC/THC.h>
#include <THC/THCAtomics.cuh>
#include <THC/THCDeviceUtils.cuh>

#define CUDA_NUM_THREADS 64
#define GET_CUDA_BLOCKS(N) ceil((float)N / CUDA_NUM_THREADS)

__global__ void adj_vec_kernel(
        int batch_size, 
        int * edge_index, 
        int vertex_count,
        int * adj_vec,
        int * adj_vec_len,
        int max_adj_per_node){

    const int edge_count    = vertex_count - 1;
    const int batch_idx     = blockIdx.x;
    const int thread_idx    = threadIdx.x;
    const int thread_count  = blockDim.x;

    edge_index  += batch_idx * edge_count * 2;
    adj_vec     += batch_idx * vertex_count * max_adj_per_node;
    adj_vec_len += batch_idx * vertex_count;

    for (int i = thread_idx; i < edge_count; i += thread_count){
        int source = edge_index[2 * i];
        int target = edge_index[2 * i + 1];
        int source_len = atomicAdd(&(adj_vec_len[source]), 1);
        adj_vec[source * max_adj_per_node + source_len] = target;
        int target_len = atomicAdd(&(adj_vec_len[target]), 1);
        adj_vec[target * max_adj_per_node + target_len] = source;
    }
}

__global__ void breadth_first_sort_kernel(
        int * sorted_index,
        int * sorted_parent_index,
        int * sorted_child_index,
        int * adj_vec,
        int * adj_vec_len,
        int * parent_index,
        int batch_size,
        int vertex_count,
        int max_adj_per_node){

    const int batch_idx     = blockIdx.x;
    const int thread_idx    = threadIdx.x;
    const int thread_count  = blockDim.x;

    adj_vec              += batch_idx * vertex_count * max_adj_per_node;
    adj_vec_len          += batch_idx * vertex_count;
    parent_index         += batch_idx * vertex_count;
    sorted_index         += batch_idx * vertex_count;
    sorted_parent_index  += batch_idx * vertex_count;
    sorted_child_index   += batch_idx * vertex_count * max_adj_per_node;

    __shared__ int sorted_len;
    if (thread_idx == 0) {
        sorted_len = 1;
        parent_index[0] = 0;
        sorted_index[0] = 0;
        sorted_parent_index[0] = 0;
    }
    __syncthreads();

    int i = thread_idx;
    while (i < vertex_count){
        if ((sorted_index[i] > 0) || (i == 0)){
            int child_index = 0;
            int par         = parent_index[i];
            int cur         = sorted_index[i];
            for (int j = 0; j < adj_vec_len[cur]; j++){
                int child = adj_vec[cur * max_adj_per_node + j];
                if (child != par){
                    int pos = atomicAdd(&(sorted_len), 1);
                    sorted_index[pos]        = child;
                    parent_index[pos]        = cur;
                    sorted_parent_index[pos] = i;
                    sorted_child_index[i * max_adj_per_node + child_index] = pos;
                    child_index++;
                }
            }
            i += thread_count;
        }
        __syncthreads();
    }
}

std::tuple<at::Tensor, at::Tensor, at::Tensor>
bfs_forward(
    const at::Tensor & edge_index_tensor,
    int max_adj_per_node){

    int batch_size   = edge_index_tensor.size(0);
    int vertex_count = edge_index_tensor.size(1) + 1;

    auto options = edge_index_tensor.options();
    auto sorted_index_tensor    = at::zeros({batch_size, vertex_count}, options);
    auto sorted_parent_tensor   = at::zeros({batch_size, vertex_count}, options);
    auto sorted_child_tensor    = at::zeros({batch_size, vertex_count, max_adj_per_node}, options);
    auto adj_vec_tensor         = at::zeros({batch_size, vertex_count, max_adj_per_node}, options);
    auto adj_vec_len_tensor     = at::zeros({batch_size, vertex_count}, options);
    auto parent_index_tensor    = at::zeros({batch_size, vertex_count}, options);

    int * edge_index      = edge_index_tensor.contiguous().data_ptr<int>();
    int * sorted_index    = sorted_index_tensor.contiguous().data_ptr<int>();
    int * sorted_parent   = sorted_parent_tensor.contiguous().data_ptr<int>();
    int * sorted_child    = sorted_child_tensor.contiguous().data_ptr<int>();
    int * adj_vec         = adj_vec_tensor.contiguous().data_ptr<int>();
    int * adj_vec_len     = adj_vec_len_tensor.contiguous().data_ptr<int>();
    int * parent_index    = parent_index_tensor.contiguous().data_ptr<int>();
    
    cudaStream_t stream = at::cuda::getCurrentCUDAStream();        

    dim3 block_dims(CUDA_NUM_THREADS, 1, 1), grid_dims(batch_size, 1, 1);
    adj_vec_kernel <<< grid_dims, block_dims, 0, stream >>>(
            batch_size, edge_index, vertex_count, adj_vec, adj_vec_len, max_adj_per_node);

    breadth_first_sort_kernel <<< grid_dims, block_dims, 1, stream >>>(
            sorted_index, sorted_parent, sorted_child, adj_vec, adj_vec_len, parent_index,
            batch_size, vertex_count, max_adj_per_node);

    return std::make_tuple(sorted_index_tensor, sorted_parent_tensor, sorted_child_tensor);
}
```

##### cvpods/layers/csrc/box_iou_rotated/box_iou_rotated.h

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#pragma once
#include <torch/types.h>

namespace cvpods {

at::Tensor box_iou_rotated_cpu(
    const at::Tensor& boxes1,
    const at::Tensor& boxes2);

#ifdef WITH_CUDA
at::Tensor box_iou_rotated_cuda(
    const at::Tensor& boxes1,
    const at::Tensor& boxes2);
#endif

// Interface for Python
// inline is needed to prevent multiple function definitions when this header is
// included by different cpps
inline at::Tensor box_iou_rotated(
    const at::Tensor& boxes1,
    const at::Tensor& boxes2) {
  assert(boxes1.device().is_cuda() == boxes2.device().is_cuda());
  if (boxes1.device().is_cuda()) {
#ifdef WITH_CUDA
    return box_iou_rotated_cuda(boxes1, boxes2);
#else
    AT_ERROR("Not compiled with GPU support");
#endif
  }

  return box_iou_rotated_cpu(boxes1, boxes2);
}

} // namespace cvpods
```

##### cvpods/layers/csrc/box_iou_rotated/box_iou_rotated_utils.h

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#pragma once

#include <cassert>
#include <cmath>

#ifdef __CUDACC__
// Designates functions callable from the host (CPU) and the device (GPU)
#define HOST_DEVICE __host__ __device__
#define HOST_DEVICE_INLINE HOST_DEVICE __forceinline__
#else
#include <algorithm>
#define HOST_DEVICE
#define HOST_DEVICE_INLINE HOST_DEVICE inline
#endif

namespace cvpods {

namespace {

template <typename T>
struct RotatedBox {
  T x_ctr, y_ctr, w, h, a;
};

template <typename T>
struct Point {
  T x, y;
  HOST_DEVICE_INLINE Point(const T& px = 0, const T& py = 0) : x(px), y(py) {}
  HOST_DEVICE_INLINE Point operator+(const Point& p) const {
    return Point(x + p.x, y + p.y);
  }
  HOST_DEVICE_INLINE Point& operator+=(const Point& p) {
    x += p.x;
    y += p.y;
    return *this;
  }
  HOST_DEVICE_INLINE Point operator-(const Point& p) const {
    return Point(x - p.x, y - p.y);
  }
  HOST_DEVICE_INLINE Point operator*(const T coeff) const {
    return Point(x * coeff, y * coeff);
  }
};

template <typename T>
HOST_DEVICE_INLINE T dot_2d(const Point<T>& A, const Point<T>& B) {
  return A.x * B.x + A.y * B.y;
}

template <typename T>
HOST_DEVICE_INLINE T cross_2d(const Point<T>& A, const Point<T>& B) {
  return A.x * B.y - B.x * A.y;
}

template <typename T>
HOST_DEVICE_INLINE void get_rotated_vertices(
    const RotatedBox<T>& box,
    Point<T> (&pts)[4]) {
  // M_PI / 180. == 0.01745329251
  double theta = box.a * 0.01745329251;
  T cosTheta2 = (T)cos(theta) * 0.5f;
  T sinTheta2 = (T)sin(theta) * 0.5f;

  // y: top --> down; x: left --> right
  pts[0].x = box.x_ctr + sinTheta2 * box.h + cosTheta2 * box.w;
  pts[0].y = box.y_ctr + cosTheta2 * box.h - sinTheta2 * box.w;
  pts[1].x = box.x_ctr - sinTheta2 * box.h + cosTheta2 * box.w;
  pts[1].y = box.y_ctr - cosTheta2 * box.h - sinTheta2 * box.w;
  pts[2].x = 2 * box.x_ctr - pts[0].x;
  pts[2].y = 2 * box.y_ctr - pts[0].y;
  pts[3].x = 2 * box.x_ctr - pts[1].x;
  pts[3].y = 2 * box.y_ctr - pts[1].y;
}

template <typename T>
HOST_DEVICE_INLINE int get_intersection_points(
    const Point<T> (&pts1)[4],
    const Point<T> (&pts2)[4],
    Point<T> (&intersections)[24]) {
  // Line vector
  // A line from p1 to p2 is: p1 + (p2-p1)*t, t=[0,1]
  Point<T> vec1[4], vec2[4];
  for (int i = 0; i < 4; i++) {
    vec1[i] = pts1[(i + 1) % 4] - pts1[i];
    vec2[i] = pts2[(i + 1) % 4] - pts2[i];
  }

  // Line test - test all line combos for intersection
  int num = 0; // number of intersections
  for (int i = 0; i < 4; i++) {
    for (int j = 0; j < 4; j++) {
      // Solve for 2x2 Ax=b
      T det = cross_2d<T>(vec2[j], vec1[i]);

      // This takes care of parallel lines
      if (fabs(det) <= 1e-14) {
        continue;
      }

      auto vec12 = pts2[j] - pts1[i];

      T t1 = cross_2d<T>(vec2[j], vec12) / det;
      T t2 = cross_2d<T>(vec1[i], vec12) / det;

      if (t1 >= 0.0f && t1 <= 1.0f && t2 >= 0.0f && t2 <= 1.0f) {
        intersections[num++] = pts1[i] + vec1[i] * t1;
      }
    }
  }

  // Check for vertices of rect1 inside rect2
  {
    const auto& AB = vec2[0];
    const auto& DA = vec2[3];
    auto ABdotAB = dot_2d<T>(AB, AB);
    auto ADdotAD = dot_2d<T>(DA, DA);
    for (int i = 0; i < 4; i++) {
      // assume ABCD is the rectangle, and P is the point to be judged
      // P is inside ABCD iff. P's projection on AB lies within AB
      // and P's projection on AD lies within AD

      auto AP = pts1[i] - pts2[0];

      auto APdotAB = dot_2d<T>(AP, AB);
      auto APdotAD = -dot_2d<T>(AP, DA);

      if ((APdotAB >= 0) && (APdotAD >= 0) && (APdotAB <= ABdotAB) &&
          (APdotAD <= ADdotAD)) {
        intersections[num++] = pts1[i];
      }
    }
  }

  // Reverse the check - check for vertices of rect2 inside rect1
  {
    const auto& AB = vec1[0];
    const auto& DA = vec1[3];
    auto ABdotAB = dot_2d<T>(AB, AB);
    auto ADdotAD = dot_2d<T>(DA, DA);
    for (int i = 0; i < 4; i++) {
      auto AP = pts2[i] - pts1[0];

      auto APdotAB = dot_2d<T>(AP, AB);
      auto APdotAD = -dot_2d<T>(AP, DA);

      if ((APdotAB >= 0) && (APdotAD >= 0) && (APdotAB <= ABdotAB) &&
          (APdotAD <= ADdotAD)) {
        intersections[num++] = pts2[i];
      }
    }
  }

  return num;
}

template <typename T>
HOST_DEVICE_INLINE int convex_hull_graham(
    const Point<T> (&p)[24],
    const int& num_in,
    Point<T> (&q)[24],
    bool shift_to_zero = false) {
  assert(num_in >= 2);

  // Step 1:
  // Find point with minimum y
  // if more than 1 points have the same minimum y,
  // pick the one with the minimum x.
  int t = 0;
  for (int i = 1; i < num_in; i++) {
    if (p[i].y < p[t].y || (p[i].y == p[t].y && p[i].x < p[t].x)) {
      t = i;
    }
  }
  auto& start = p[t]; // starting point

  // Step 2:
  // Subtract starting point from every points (for sorting in the next step)
  for (int i = 0; i < num_in; i++) {
    q[i] = p[i] - start;
  }

  // Swap the starting point to position 0
  auto tmp = q[0];
  q[0] = q[t];
  q[t] = tmp;

  // Step 3:
  // Sort point 1 ~ num_in according to their relative cross-product values
  // (essentially sorting according to angles)
  // If the angles are the same, sort according to their distance to origin
  T dist[24];
  for (int i = 0; i < num_in; i++) {
    dist[i] = dot_2d<T>(q[i], q[i]);
  }

#ifdef __CUDACC__
  // CUDA version
  // In the future, we can potentially use thrust
  // for sorting here to improve speed (though not guaranteed)
  for (int i = 1; i < num_in - 1; i++) {
    for (int j = i + 1; j < num_in; j++) {
      T crossProduct = cross_2d<T>(q[i], q[j]);
      if ((crossProduct < -1e-6) ||
          (fabs(crossProduct) < 1e-6 && dist[i] > dist[j])) {
        auto q_tmp = q[i];
        q[i] = q[j];
        q[j] = q_tmp;
        auto dist_tmp = dist[i];
        dist[i] = dist[j];
        dist[j] = dist_tmp;
      }
    }
  }
#else
  // CPU version
  std::sort(
      q + 1, q + num_in, [](const Point<T>& A, const Point<T>& B) -> bool {
        T temp = cross_2d<T>(A, B);
        if (fabs(temp) < 1e-6) {
          return dot_2d<T>(A, A) < dot_2d<T>(B, B);
        } else {
          return temp > 0;
        }
      });
#endif

  // Step 4:
  // Make sure there are at least 2 points (that don't overlap with each other)
  // in the stack
  int k; // index of the non-overlapped second point
  for (k = 1; k < num_in; k++) {
    if (dist[k] > 1e-8) {
      break;
    }
  }
  if (k == num_in) {
    // We reach the end, which means the convex hull is just one point
    q[0] = p[t];
    return 1;
  }
  q[1] = q[k];
  int m = 2; // 2 points in the stack
  // Step 5:
  // Finally we can start the scanning process.
  // When a non-convex relationship between the 3 points is found
  // (either concave shape or duplicated points),
  // we pop the previous point from the stack
  // until the 3-point relationship is convex again, or
  // until the stack only contains two points
  for (int i = k + 1; i < num_in; i++) {
    while (m > 1 && cross_2d<T>(q[i] - q[m - 2], q[m - 1] - q[m - 2]) >= 0) {
      m--;
    }
    q[m++] = q[i];
  }

  // Step 6 (Optional):
  // In general sense we need the original coordinates, so we
  // need to shift the points back (reverting Step 2)
  // But if we're only interested in getting the area/perimeter of the shape
  // We can simply return.
  if (!shift_to_zero) {
    for (int i = 0; i < m; i++) {
      q[i] += start;
    }
  }

  return m;
}

template <typename T>
HOST_DEVICE_INLINE T polygon_area(const Point<T> (&q)[24], const int& m) {
  if (m <= 2) {
    return 0;
  }

  T area = 0;
  for (int i = 1; i < m - 1; i++) {
    area += fabs(cross_2d<T>(q[i] - q[0], q[i + 1] - q[0]));
  }

  return area / 2.0;
}

template <typename T>
HOST_DEVICE_INLINE T rotated_boxes_intersection(
    const RotatedBox<T>& box1,
    const RotatedBox<T>& box2) {
  // There are up to 4 x 4 + 4 + 4 = 24 intersections (including dups) returned
  // from rotated_rect_intersection_pts
  Point<T> intersectPts[24], orderedPts[24];

  Point<T> pts1[4];
  Point<T> pts2[4];
  get_rotated_vertices<T>(box1, pts1);
  get_rotated_vertices<T>(box2, pts2);

  int num = get_intersection_points<T>(pts1, pts2, intersectPts);

  if (num <= 2) {
    return 0.0;
  }

  // Convex Hull to order the intersection points in clockwise order and find
  // the contour area.
  int num_convex = convex_hull_graham<T>(intersectPts, num, orderedPts, true);
  return polygon_area<T>(orderedPts, num_convex);
}

} // namespace

template <typename T>
HOST_DEVICE_INLINE T
single_box_iou_rotated(T const* const box1_raw, T const* const box2_raw) {
  // shift center to the middle point to achieve higher precision in result
  RotatedBox<T> box1, box2;
  auto center_shift_x = (box1_raw[0] + box2_raw[0]) / 2.0;
  auto center_shift_y = (box1_raw[1] + box2_raw[1]) / 2.0;
  box1.x_ctr = box1_raw[0] - center_shift_x;
  box1.y_ctr = box1_raw[1] - center_shift_y;
  box1.w = box1_raw[2];
  box1.h = box1_raw[3];
  box1.a = box1_raw[4];
  box2.x_ctr = box2_raw[0] - center_shift_x;
  box2.y_ctr = box2_raw[1] - center_shift_y;
  box2.w = box2_raw[2];
  box2.h = box2_raw[3];
  box2.a = box2_raw[4];

  const T area1 = box1.w * box1.h;
  const T area2 = box2.w * box2.h;
  if (area1 < 1e-14 || area2 < 1e-14) {
    return 0.f;
  }

  const T intersection = rotated_boxes_intersection<T>(box1, box2);
  const T iou = intersection / (area1 + area2 - intersection);
  return iou;
}

} // namespace cvpods
```

##### cvpods/layers/csrc/box_iou_rotated/box_iou_rotated_cpu.cpp

```cpp
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#include "box_iou_rotated.h"
#include "box_iou_rotated_utils.h"

namespace cvpods {

template <typename T>
void box_iou_rotated_cpu_kernel(
    const at::Tensor& boxes1,
    const at::Tensor& boxes2,
    at::Tensor& ious) {
  auto widths1 = boxes1.select(1, 2).contiguous();
  auto heights1 = boxes1.select(1, 3).contiguous();
  auto widths2 = boxes2.select(1, 2).contiguous();
  auto heights2 = boxes2.select(1, 3).contiguous();

  at::Tensor areas1 = widths1 * heights1;
  at::Tensor areas2 = widths2 * heights2;

  auto num_boxes1 = boxes1.size(0);
  auto num_boxes2 = boxes2.size(0);

  for (int i = 0; i < num_boxes1; i++) {
    for (int j = 0; j < num_boxes2; j++) {
      ious[i * num_boxes2 + j] = single_box_iou_rotated<T>(
          boxes1[i].data_ptr<T>(), boxes2[j].data_ptr<T>());
    }
  }
}

at::Tensor box_iou_rotated_cpu(
    const at::Tensor& boxes1,
    const at::Tensor& boxes2) {
  auto num_boxes1 = boxes1.size(0);
  auto num_boxes2 = boxes2.size(0);
  at::Tensor ious =
      at::empty({num_boxes1 * num_boxes2}, boxes1.options().dtype(at::kFloat));

  box_iou_rotated_cpu_kernel<float>(boxes1, boxes2, ious);

  // reshape from 1d array to 2d array
  auto shape = std::vector<int64_t>{num_boxes1, num_boxes2};
  return ious.reshape(shape);
}

} // namespace cvpods
```

##### cvpods/layers/csrc/box_iou_rotated/box_iou_rotated_cuda.cu

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAApplyUtils.cuh>
#include "box_iou_rotated_utils.h"

namespace cvpods {

// 2D block with 32 * 16 = 512 threads per block
const int BLOCK_DIM_X = 32;
const int BLOCK_DIM_Y = 16;

template <typename T>
__global__ void box_iou_rotated_cuda_kernel(
    const int n_boxes1,
    const int n_boxes2,
    const T* dev_boxes1,
    const T* dev_boxes2,
    T* dev_ious) {
  const int row_start = blockIdx.x * blockDim.x;
  const int col_start = blockIdx.y * blockDim.y;

  const int row_size = min(n_boxes1 - row_start, blockDim.x);
  const int col_size = min(n_boxes2 - col_start, blockDim.y);

  __shared__ float block_boxes1[BLOCK_DIM_X * 5];
  __shared__ float block_boxes2[BLOCK_DIM_Y * 5];

  // It's safe to copy using threadIdx.x since BLOCK_DIM_X >= BLOCK_DIM_Y
  if (threadIdx.x < row_size && threadIdx.y == 0) {
    block_boxes1[threadIdx.x * 5 + 0] =
        dev_boxes1[(row_start + threadIdx.x) * 5 + 0];
    block_boxes1[threadIdx.x * 5 + 1] =
        dev_boxes1[(row_start + threadIdx.x) * 5 + 1];
    block_boxes1[threadIdx.x * 5 + 2] =
        dev_boxes1[(row_start + threadIdx.x) * 5 + 2];
    block_boxes1[threadIdx.x * 5 + 3] =
        dev_boxes1[(row_start + threadIdx.x) * 5 + 3];
    block_boxes1[threadIdx.x * 5 + 4] =
        dev_boxes1[(row_start + threadIdx.x) * 5 + 4];
  }

  if (threadIdx.x < col_size && threadIdx.y == 0) {
    block_boxes2[threadIdx.x * 5 + 0] =
        dev_boxes2[(col_start + threadIdx.x) * 5 + 0];
    block_boxes2[threadIdx.x * 5 + 1] =
        dev_boxes2[(col_start + threadIdx.x) * 5 + 1];
    block_boxes2[threadIdx.x * 5 + 2] =
        dev_boxes2[(col_start + threadIdx.x) * 5 + 2];
    block_boxes2[threadIdx.x * 5 + 3] =
        dev_boxes2[(col_start + threadIdx.x) * 5 + 3];
    block_boxes2[threadIdx.x * 5 + 4] =
        dev_boxes2[(col_start + threadIdx.x) * 5 + 4];
  }
  __syncthreads();

  if (threadIdx.x < row_size && threadIdx.y < col_size) {
    int offset = (row_start + threadIdx.x) * n_boxes2 + col_start + threadIdx.y;
    dev_ious[offset] = single_box_iou_rotated<T>(
        block_boxes1 + threadIdx.x * 5, block_boxes2 + threadIdx.y * 5);
  }
}

at::Tensor box_iou_rotated_cuda(
    const at::Tensor& boxes1,
    const at::Tensor& boxes2) {
  using scalar_t = float;
  AT_ASSERTM(boxes1.device().is_cuda(), "boxes1 must be a CUDA tensor");
  AT_ASSERTM(boxes2.device().is_cuda(), "boxes2 must be a CUDA tensor");
  at::cuda::CUDAGuard device_guard(boxes1.device());

  int num_boxes1 = boxes1.size(0);
  int num_boxes2 = boxes2.size(0);

  at::Tensor ious =
      at::empty({num_boxes1 * num_boxes2}, boxes1.options().dtype(at::kFloat));

  bool transpose = false;
  if (num_boxes1 > 0 && num_boxes2 > 0) {
    scalar_t *data1 = boxes1.data_ptr<scalar_t>(),
             *data2 = boxes2.data_ptr<scalar_t>();

    if (num_boxes2 > 65535 * BLOCK_DIM_Y) {
      AT_ASSERTM(
          num_boxes1 <= 65535 * BLOCK_DIM_Y,
          "Too many boxes for box_iou_rotated_cuda!");
      // x dim is allowed to be large, but y dim cannot,
      // so we transpose the two to avoid "invalid configuration argument"
      // error. We assume one of them is small. Otherwise the result is hard to
      // fit in memory anyway.
      std::swap(num_boxes1, num_boxes2);
      std::swap(data1, data2);
      transpose = true;
    }

    const int blocks_x = at::cuda::ATenCeilDiv(num_boxes1, BLOCK_DIM_X);
    const int blocks_y = at::cuda::ATenCeilDiv(num_boxes2, BLOCK_DIM_Y);

    dim3 blocks(blocks_x, blocks_y);
    dim3 threads(BLOCK_DIM_X, BLOCK_DIM_Y);
    cudaStream_t stream = at::cuda::getCurrentCUDAStream();

    box_iou_rotated_cuda_kernel<scalar_t><<<blocks, threads, 0, stream>>>(
        num_boxes1,
        num_boxes2,
		data1,
		data2,
        (scalar_t*)ious.data_ptr<scalar_t>());

    AT_CUDA_CHECK(cudaGetLastError());
  }

  // reshape from 1d array to 2d array
  auto shape = std::vector<int64_t>{num_boxes1, num_boxes2};
  if (transpose) {
    return ious.view(shape).t();
  } else {
    return ious.view(shape);
  }
}

} // namespace cvpods
```

##### cvpods/layers/csrc/ROIAlign/ROIAlign.h

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#pragma once
#include <torch/types.h>

namespace cvpods {

at::Tensor ROIAlign_forward_cpu(
    const at::Tensor& input,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio,
    bool aligned);

at::Tensor ROIAlign_backward_cpu(
    const at::Tensor& grad,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int batch_size,
    const int channels,
    const int height,
    const int width,
    const int sampling_ratio,
    bool aligned);

#ifdef WITH_CUDA
at::Tensor ROIAlign_forward_cuda(
    const at::Tensor& input,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio,
    bool aligned);

at::Tensor ROIAlign_backward_cuda(
    const at::Tensor& grad,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int batch_size,
    const int channels,
    const int height,
    const int width,
    const int sampling_ratio,
    bool aligned);
#endif

// Interface for Python
inline at::Tensor ROIAlign_forward(
    const at::Tensor& input,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio,
    bool aligned) {
  if (input.device().is_cuda()) {
#ifdef WITH_CUDA
    return ROIAlign_forward_cuda(
        input,
        rois,
        spatial_scale,
        pooled_height,
        pooled_width,
        sampling_ratio,
        aligned);
#else
    AT_ERROR("Not compiled with GPU support");
#endif
  }
  return ROIAlign_forward_cpu(
      input,
      rois,
      spatial_scale,
      pooled_height,
      pooled_width,
      sampling_ratio,
      aligned);
}

inline at::Tensor ROIAlign_backward(
    const at::Tensor& grad,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int batch_size,
    const int channels,
    const int height,
    const int width,
    const int sampling_ratio,
    bool aligned) {
  if (grad.device().is_cuda()) {
#ifdef WITH_CUDA
    return ROIAlign_backward_cuda(
        grad,
        rois,
        spatial_scale,
        pooled_height,
        pooled_width,
        batch_size,
        channels,
        height,
        width,
        sampling_ratio,
        aligned);
#else
    AT_ERROR("Not compiled with GPU support");
#endif
  }
  return ROIAlign_backward_cpu(
      grad,
      rois,
      spatial_scale,
      pooled_height,
      pooled_width,
      batch_size,
      channels,
      height,
      width,
      sampling_ratio,
      aligned);
}

} // namespace cvpods
```

##### cvpods/layers/csrc/ROIAlign/ROIAlign_cpu.cpp

```cpp
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#include <ATen/TensorUtils.h>
#include "ROIAlign.h"

namespace {

// implementation taken from Caffe2
template <typename T>
struct PreCalc {
  int pos1;
  int pos2;
  int pos3;
  int pos4;
  T w1;
  T w2;
  T w3;
  T w4;
};

template <typename T>
void pre_calc_for_bilinear_interpolate(
    const int height,
    const int width,
    const int pooled_height,
    const int pooled_width,
    const int iy_upper,
    const int ix_upper,
    T roi_start_h,
    T roi_start_w,
    T bin_size_h,
    T bin_size_w,
    int roi_bin_grid_h,
    int roi_bin_grid_w,
    std::vector<PreCalc<T>>& pre_calc) {
  int pre_calc_index = 0;
  for (int ph = 0; ph < pooled_height; ph++) {
    for (int pw = 0; pw < pooled_width; pw++) {
      for (int iy = 0; iy < iy_upper; iy++) {
        const T yy = roi_start_h + ph * bin_size_h +
            static_cast<T>(iy + .5f) * bin_size_h /
                static_cast<T>(roi_bin_grid_h); // e.g., 0.5, 1.5
        for (int ix = 0; ix < ix_upper; ix++) {
          const T xx = roi_start_w + pw * bin_size_w +
              static_cast<T>(ix + .5f) * bin_size_w /
                  static_cast<T>(roi_bin_grid_w);

          T x = xx;
          T y = yy;
          // deal with: inverse elements are out of feature map boundary
          if (y < -1.0 || y > height || x < -1.0 || x > width) {
            // empty
            PreCalc<T> pc;
            pc.pos1 = 0;
            pc.pos2 = 0;
            pc.pos3 = 0;
            pc.pos4 = 0;
            pc.w1 = 0;
            pc.w2 = 0;
            pc.w3 = 0;
            pc.w4 = 0;
            pre_calc[pre_calc_index] = pc;
            pre_calc_index += 1;
            continue;
          }

          if (y <= 0) {
            y = 0;
          }
          if (x <= 0) {
            x = 0;
          }

          int y_low = (int)y;
          int x_low = (int)x;
          int y_high;
          int x_high;

          if (y_low >= height - 1) {
            y_high = y_low = height - 1;
            y = (T)y_low;
          } else {
            y_high = y_low + 1;
          }

          if (x_low >= width - 1) {
            x_high = x_low = width - 1;
            x = (T)x_low;
          } else {
            x_high = x_low + 1;
          }

          T ly = y - y_low;
          T lx = x - x_low;
          T hy = 1. - ly, hx = 1. - lx;
          T w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;

          // save weights and indices
          PreCalc<T> pc;
          pc.pos1 = y_low * width + x_low;
          pc.pos2 = y_low * width + x_high;
          pc.pos3 = y_high * width + x_low;
          pc.pos4 = y_high * width + x_high;
          pc.w1 = w1;
          pc.w2 = w2;
          pc.w3 = w3;
          pc.w4 = w4;
          pre_calc[pre_calc_index] = pc;

          pre_calc_index += 1;
        }
      }
    }
  }
}

template <typename T>
void ROIAlignForward(
    const int nthreads,
    const T* input,
    const T& spatial_scale,
    const int channels,
    const int height,
    const int width,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio,
    const T* rois,
    T* output,
    bool aligned) {
  int n_rois = nthreads / channels / pooled_width / pooled_height;
  // (n, c, ph, pw) is an element in the pooled output
  // can be parallelized using omp
  // #pragma omp parallel for num_threads(32)
  for (int n = 0; n < n_rois; n++) {
    int index_n = n * channels * pooled_width * pooled_height;

    const T* offset_rois = rois + n * 5;
    int roi_batch_ind = offset_rois[0];

    // Do not use rounding; this implementation detail is critical
    T offset = aligned ? (T)0.5 : (T)0.0;
    T roi_start_w = offset_rois[1] * spatial_scale - offset;
    T roi_start_h = offset_rois[2] * spatial_scale - offset;
    T roi_end_w = offset_rois[3] * spatial_scale - offset;
    T roi_end_h = offset_rois[4] * spatial_scale - offset;

    T roi_width = roi_end_w - roi_start_w;
    T roi_height = roi_end_h - roi_start_h;
    if (aligned) {
      AT_ASSERTM(
          roi_width >= 0 && roi_height >= 0,
          "ROIs in ROIAlign cannot have non-negative size!");
    } else { // for backward-compatibility only
      roi_width = std::max(roi_width, (T)1.);
      roi_height = std::max(roi_height, (T)1.);
    }
    T bin_size_h = static_cast<T>(roi_height) / static_cast<T>(pooled_height);
    T bin_size_w = static_cast<T>(roi_width) / static_cast<T>(pooled_width);

    // We use roi_bin_grid to sample the grid and mimic integral
    int roi_bin_grid_h = (sampling_ratio > 0)
        ? sampling_ratio
        : ceil(roi_height / pooled_height); // e.g., = 2
    int roi_bin_grid_w =
        (sampling_ratio > 0) ? sampling_ratio : ceil(roi_width / pooled_width);

    // We do average (integral) pooling inside a bin
    // When the grid is empty, output zeros == 0/1, instead of NaN.
    const T count = std::max(roi_bin_grid_h * roi_bin_grid_w, 1); // e.g. = 4

    // we want to precalculate indices and weights shared by all channels,
    // this is the key point of optimization
    std::vector<PreCalc<T>> pre_calc(
        roi_bin_grid_h * roi_bin_grid_w * pooled_width * pooled_height);
    pre_calc_for_bilinear_interpolate(
        height,
        width,
        pooled_height,
        pooled_width,
        roi_bin_grid_h,
        roi_bin_grid_w,
        roi_start_h,
        roi_start_w,
        bin_size_h,
        bin_size_w,
        roi_bin_grid_h,
        roi_bin_grid_w,
        pre_calc);

    for (int c = 0; c < channels; c++) {
      int index_n_c = index_n + c * pooled_width * pooled_height;
      const T* offset_input =
          input + (roi_batch_ind * channels + c) * height * width;
      int pre_calc_index = 0;

      for (int ph = 0; ph < pooled_height; ph++) {
        for (int pw = 0; pw < pooled_width; pw++) {
          int index = index_n_c + ph * pooled_width + pw;

          T output_val = 0.;
          for (int iy = 0; iy < roi_bin_grid_h; iy++) {
            for (int ix = 0; ix < roi_bin_grid_w; ix++) {
              PreCalc<T> pc = pre_calc[pre_calc_index];
              output_val += pc.w1 * offset_input[pc.pos1] +
                  pc.w2 * offset_input[pc.pos2] +
                  pc.w3 * offset_input[pc.pos3] + pc.w4 * offset_input[pc.pos4];

              pre_calc_index += 1;
            }
          }
          output_val /= count;

          output[index] = output_val;
        } // for pw
      } // for ph
    } // for c
  } // for n
}

template <typename T>
void bilinear_interpolate_gradient(
    const int height,
    const int width,
    T y,
    T x,
    T& w1,
    T& w2,
    T& w3,
    T& w4,
    int& x_low,
    int& x_high,
    int& y_low,
    int& y_high,
    const int index /* index for debug only*/) {
  // deal with cases that inverse elements are out of feature map boundary
  if (y < -1.0 || y > height || x < -1.0 || x > width) {
    // empty
    w1 = w2 = w3 = w4 = 0.;
    x_low = x_high = y_low = y_high = -1;
    return;
  }

  if (y <= 0)
    y = 0;
  if (x <= 0)
    x = 0;

  y_low = (int)y;
  x_low = (int)x;

  if (y_low >= height - 1) {
    y_high = y_low = height - 1;
    y = (T)y_low;
  } else {
    y_high = y_low + 1;
  }

  if (x_low >= width - 1) {
    x_high = x_low = width - 1;
    x = (T)x_low;
  } else {
    x_high = x_low + 1;
  }

  T ly = y - y_low;
  T lx = x - x_low;
  T hy = 1. - ly, hx = 1. - lx;

  // reference in forward
  // T v1 = input[y_low * width + x_low];
  // T v2 = input[y_low * width + x_high];
  // T v3 = input[y_high * width + x_low];
  // T v4 = input[y_high * width + x_high];
  // T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);

  w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;

  return;
}

template <class T>
inline void add(T* address, const T& val) {
  *address += val;
}

template <typename T>
void ROIAlignBackward(
    const int nthreads,
    const T* grad_output,
    const T& spatial_scale,
    const int channels,
    const int height,
    const int width,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio,
    T* grad_input,
    const T* rois,
    const int n_stride,
    const int c_stride,
    const int h_stride,
    const int w_stride,
    bool aligned) {
  for (int index = 0; index < nthreads; index++) {
    // (n, c, ph, pw) is an element in the pooled output
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;

    const T* offset_rois = rois + n * 5;
    int roi_batch_ind = offset_rois[0];

    // Do not use rounding; this implementation detail is critical
    T offset = aligned ? (T)0.5 : (T)0.0;
    T roi_start_w = offset_rois[1] * spatial_scale - offset;
    T roi_start_h = offset_rois[2] * spatial_scale - offset;
    T roi_end_w = offset_rois[3] * spatial_scale - offset;
    T roi_end_h = offset_rois[4] * spatial_scale - offset;

    T roi_width = roi_end_w - roi_start_w;
    T roi_height = roi_end_h - roi_start_h;
    if (aligned) {
      AT_ASSERTM(
          roi_width >= 0 && roi_height >= 0,
          "ROIs in ROIAlign do not have non-negative size!");
    } else { // for backward-compatibility only
      roi_width = std::max(roi_width, (T)1.);
      roi_height = std::max(roi_height, (T)1.);
    }
    T bin_size_h = static_cast<T>(roi_height) / static_cast<T>(pooled_height);
    T bin_size_w = static_cast<T>(roi_width) / static_cast<T>(pooled_width);

    T* offset_grad_input =
        grad_input + ((roi_batch_ind * channels + c) * height * width);

    int output_offset = n * n_stride + c * c_stride;
    const T* offset_grad_output = grad_output + output_offset;
    const T grad_output_this_bin =
        offset_grad_output[ph * h_stride + pw * w_stride];

    // We use roi_bin_grid to sample the grid and mimic integral
    int roi_bin_grid_h = (sampling_ratio > 0)
        ? sampling_ratio
        : ceil(roi_height / pooled_height); // e.g., = 2
    int roi_bin_grid_w =
        (sampling_ratio > 0) ? sampling_ratio : ceil(roi_width / pooled_width);

    // We do average (integral) pooling inside a bin
    const T count = roi_bin_grid_h * roi_bin_grid_w; // e.g. = 4

    for (int iy = 0; iy < roi_bin_grid_h; iy++) {
      const T y = roi_start_h + ph * bin_size_h +
          static_cast<T>(iy + .5f) * bin_size_h /
              static_cast<T>(roi_bin_grid_h); // e.g., 0.5, 1.5
      for (int ix = 0; ix < roi_bin_grid_w; ix++) {
        const T x = roi_start_w + pw * bin_size_w +
            static_cast<T>(ix + .5f) * bin_size_w /
                static_cast<T>(roi_bin_grid_w);

        T w1, w2, w3, w4;
        int x_low, x_high, y_low, y_high;

        bilinear_interpolate_gradient(
            height,
            width,
            y,
            x,
            w1,
            w2,
            w3,
            w4,
            x_low,
            x_high,
            y_low,
            y_high,
            index);

        T g1 = grad_output_this_bin * w1 / count;
        T g2 = grad_output_this_bin * w2 / count;
        T g3 = grad_output_this_bin * w3 / count;
        T g4 = grad_output_this_bin * w4 / count;

        if (x_low >= 0 && x_high >= 0 && y_low >= 0 && y_high >= 0) {
          // atomic add is not needed for now since it is single threaded
          add(offset_grad_input + y_low * width + x_low, static_cast<T>(g1));
          add(offset_grad_input + y_low * width + x_high, static_cast<T>(g2));
          add(offset_grad_input + y_high * width + x_low, static_cast<T>(g3));
          add(offset_grad_input + y_high * width + x_high, static_cast<T>(g4));
        } // if
      } // ix
    } // iy
  } // for
} // ROIAlignBackward

} // namespace

namespace cvpods {

at::Tensor ROIAlign_forward_cpu(
    const at::Tensor& input,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio,
    bool aligned) {
  AT_ASSERTM(input.device().is_cpu(), "input must be a CPU tensor");
  AT_ASSERTM(rois.device().is_cpu(), "rois must be a CPU tensor");

  at::TensorArg input_t{input, "input", 1}, rois_t{rois, "rois", 2};

  at::CheckedFrom c = "ROIAlign_forward_cpu";
  at::checkAllSameType(c, {input_t, rois_t});

  auto num_rois = rois.size(0);
  auto channels = input.size(1);
  auto height = input.size(2);
  auto width = input.size(3);

  at::Tensor output = at::zeros(
      {num_rois, channels, pooled_height, pooled_width}, input.options());

  auto output_size = num_rois * pooled_height * pooled_width * channels;

  if (output.numel() == 0)
    return output;

  AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), "ROIAlign_forward", [&] {
    ROIAlignForward<scalar_t>(
        output_size,
        input.contiguous().data_ptr<scalar_t>(),
        spatial_scale,
        channels,
        height,
        width,
        pooled_height,
        pooled_width,
        sampling_ratio,
        rois.contiguous().data_ptr<scalar_t>(),
        output.data_ptr<scalar_t>(),
        aligned);
  });
  return output;
}

at::Tensor ROIAlign_backward_cpu(
    const at::Tensor& grad,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int batch_size,
    const int channels,
    const int height,
    const int width,
    const int sampling_ratio,
    bool aligned) {
  AT_ASSERTM(grad.device().is_cpu(), "grad must be a CPU tensor");
  AT_ASSERTM(rois.device().is_cpu(), "rois must be a CPU tensor");

  at::TensorArg grad_t{grad, "grad", 1}, rois_t{rois, "rois", 2};

  at::CheckedFrom c = "ROIAlign_backward_cpu";
  at::checkAllSameType(c, {grad_t, rois_t});

  at::Tensor grad_input =
      at::zeros({batch_size, channels, height, width}, grad.options());

  // handle possibly empty gradients
  if (grad.numel() == 0) {
    return grad_input;
  }

  // get stride values to ensure indexing into gradients is correct.
  int n_stride = grad.stride(0);
  int c_stride = grad.stride(1);
  int h_stride = grad.stride(2);
  int w_stride = grad.stride(3);

  AT_DISPATCH_FLOATING_TYPES_AND_HALF(grad.scalar_type(), "ROIAlign_forward", [&] {
    ROIAlignBackward<scalar_t>(
        grad.numel(),
        grad.contiguous().data_ptr<scalar_t>(),
        spatial_scale,
        channels,
        height,
        width,
        pooled_height,
        pooled_width,
        sampling_ratio,
        grad_input.data_ptr<scalar_t>(),
        rois.contiguous().data_ptr<scalar_t>(),
        n_stride,
        c_stride,
        h_stride,
        w_stride,
        aligned);
  });
  return grad_input;
}

} // namespace cvpods
```

##### cvpods/layers/csrc/ROIAlign/ROIAlign_cuda.cu

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAApplyUtils.cuh>

// TODO make it in a common file
#define CUDA_1D_KERNEL_LOOP(i, n)                            \
  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < n; \
       i += blockDim.x * gridDim.x)

template <typename T>
__device__ T bilinear_interpolate(
    const T* bottom_data,
    const int height,
    const int width,
    T y,
    T x,
    const int index /* index for debug only*/) {
  // deal with cases that inverse elements are out of feature map boundary
  if (y < -1.0 || y > height || x < -1.0 || x > width) {
    // empty
    return 0;
  }

  if (y <= 0)
    y = 0;
  if (x <= 0)
    x = 0;

  int y_low = (int)y;
  int x_low = (int)x;
  int y_high;
  int x_high;

  if (y_low >= height - 1) {
    y_high = y_low = height - 1;
    y = (T)y_low;
  } else {
    y_high = y_low + 1;
  }

  if (x_low >= width - 1) {
    x_high = x_low = width - 1;
    x = (T)x_low;
  } else {
    x_high = x_low + 1;
  }

  T ly = y - y_low;
  T lx = x - x_low;
  T hy = 1. - ly, hx = 1. - lx;
  // do bilinear interpolation
  T v1 = bottom_data[y_low * width + x_low];
  T v2 = bottom_data[y_low * width + x_high];
  T v3 = bottom_data[y_high * width + x_low];
  T v4 = bottom_data[y_high * width + x_high];
  T w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;

  T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);

  return val;
}

template <typename T>
__global__ void RoIAlignForward(
    const int nthreads,
    const T* bottom_data,
    const T spatial_scale,
    const int channels,
    const int height,
    const int width,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio,
    const T* bottom_rois,
    T* top_data,
    bool aligned) {
  CUDA_1D_KERNEL_LOOP(index, nthreads) {
    // (n, c, ph, pw) is an element in the pooled output
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;

    const T* offset_bottom_rois = bottom_rois + n * 5;
    int roi_batch_ind = offset_bottom_rois[0];

    // Do not use rounding; this implementation detail is critical
    T offset = aligned ? (T)0.5 : (T)0.0;
    T roi_start_w = offset_bottom_rois[1] * spatial_scale - offset;
    T roi_start_h = offset_bottom_rois[2] * spatial_scale - offset;
    T roi_end_w = offset_bottom_rois[3] * spatial_scale - offset;
    T roi_end_h = offset_bottom_rois[4] * spatial_scale - offset;

    T roi_width = roi_end_w - roi_start_w;
    T roi_height = roi_end_h - roi_start_h;
    if (!aligned) { // for backward-compatibility only
      roi_width = max(roi_width, (T)1.);
      roi_height = max(roi_height, (T)1.);
    }
    T bin_size_h = static_cast<T>(roi_height) / static_cast<T>(pooled_height);
    T bin_size_w = static_cast<T>(roi_width) / static_cast<T>(pooled_width);

    const T* offset_bottom_data =
        bottom_data + (roi_batch_ind * channels + c) * height * width;

    // We use roi_bin_grid to sample the grid and mimic integral
    int roi_bin_grid_h = (sampling_ratio > 0)
        ? sampling_ratio
        : ceil(roi_height / pooled_height); // e.g., = 2
    int roi_bin_grid_w =
        (sampling_ratio > 0) ? sampling_ratio : ceil(roi_width / pooled_width);

    // We do average (integral) pooling inside a bin
    // When the grid is empty, output zeros == 0/1, instead of NaN.
    const T count = max(roi_bin_grid_h * roi_bin_grid_w, 1); // e.g. = 4

    T output_val = 0.;
    for (int iy = 0; iy < roi_bin_grid_h; iy++) // e.g., iy = 0, 1
    {
      const T y = roi_start_h + ph * bin_size_h +
          static_cast<T>(iy + .5f) * bin_size_h /
              static_cast<T>(roi_bin_grid_h); // e.g., 0.5, 1.5
      for (int ix = 0; ix < roi_bin_grid_w; ix++) {
        const T x = roi_start_w + pw * bin_size_w +
            static_cast<T>(ix + .5f) * bin_size_w /
                static_cast<T>(roi_bin_grid_w);

        T val = bilinear_interpolate(
            offset_bottom_data, height, width, y, x, index);
        output_val += val;
      }
    }
    output_val /= count;

    top_data[index] = output_val;
  }
}

template <typename T>
__device__ void bilinear_interpolate_gradient(
    const int height,
    const int width,
    T y,
    T x,
    T& w1,
    T& w2,
    T& w3,
    T& w4,
    int& x_low,
    int& x_high,
    int& y_low,
    int& y_high,
    const int index /* index for debug only*/) {
  // deal with cases that inverse elements are out of feature map boundary
  if (y < -1.0 || y > height || x < -1.0 || x > width) {
    // empty
    w1 = w2 = w3 = w4 = 0.;
    x_low = x_high = y_low = y_high = -1;
    return;
  }

  if (y <= 0)
    y = 0;
  if (x <= 0)
    x = 0;

  y_low = (int)y;
  x_low = (int)x;

  if (y_low >= height - 1) {
    y_high = y_low = height - 1;
    y = (T)y_low;
  } else {
    y_high = y_low + 1;
  }

  if (x_low >= width - 1) {
    x_high = x_low = width - 1;
    x = (T)x_low;
  } else {
    x_high = x_low + 1;
  }

  T ly = y - y_low;
  T lx = x - x_low;
  T hy = 1. - ly, hx = 1. - lx;

  // reference in forward
  // T v1 = bottom_data[y_low * width + x_low];
  // T v2 = bottom_data[y_low * width + x_high];
  // T v3 = bottom_data[y_high * width + x_low];
  // T v4 = bottom_data[y_high * width + x_high];
  // T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);

  w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;

  return;
}

template <typename T>
__global__ void RoIAlignBackwardFeature(
    const int nthreads,
    const T* top_diff,
    const int num_rois,
    const T spatial_scale,
    const int channels,
    const int height,
    const int width,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio,
    T* bottom_diff,
    const T* bottom_rois,
    bool aligned) {
  CUDA_1D_KERNEL_LOOP(index, nthreads) {
    // (n, c, ph, pw) is an element in the pooled output
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;

    const T* offset_bottom_rois = bottom_rois + n * 5;
    int roi_batch_ind = offset_bottom_rois[0];

    // Do not use rounding; this implementation detail is critical
    T offset = aligned ? (T)0.5 : (T)0.0;
    T roi_start_w = offset_bottom_rois[1] * spatial_scale - offset;
    T roi_start_h = offset_bottom_rois[2] * spatial_scale - offset;
    T roi_end_w = offset_bottom_rois[3] * spatial_scale - offset;
    T roi_end_h = offset_bottom_rois[4] * spatial_scale - offset;

    T roi_width = roi_end_w - roi_start_w;
    T roi_height = roi_end_h - roi_start_h;
    if (!aligned) { // for backward-compatibility only
      roi_width = max(roi_width, (T)1.);
      roi_height = max(roi_height, (T)1.);
    }
    T bin_size_h = static_cast<T>(roi_height) / static_cast<T>(pooled_height);
    T bin_size_w = static_cast<T>(roi_width) / static_cast<T>(pooled_width);

    T* offset_bottom_diff =
        bottom_diff + (roi_batch_ind * channels + c) * height * width;

    int top_offset = (n * channels + c) * pooled_height * pooled_width;
    const T* offset_top_diff = top_diff + top_offset;
    const T top_diff_this_bin = offset_top_diff[ph * pooled_width + pw];

    // We use roi_bin_grid to sample the grid and mimic integral
    int roi_bin_grid_h = (sampling_ratio > 0)
        ? sampling_ratio
        : ceil(roi_height / pooled_height); // e.g., = 2
    int roi_bin_grid_w =
        (sampling_ratio > 0) ? sampling_ratio : ceil(roi_width / pooled_width);

    // We do average (integral) pooling inside a bin
    const T count = roi_bin_grid_h * roi_bin_grid_w; // e.g. = 4

    for (int iy = 0; iy < roi_bin_grid_h; iy++) // e.g., iy = 0, 1
    {
      const T y = roi_start_h + ph * bin_size_h +
          static_cast<T>(iy + .5f) * bin_size_h /
              static_cast<T>(roi_bin_grid_h); // e.g., 0.5, 1.5
      for (int ix = 0; ix < roi_bin_grid_w; ix++) {
        const T x = roi_start_w + pw * bin_size_w +
            static_cast<T>(ix + .5f) * bin_size_w /
                static_cast<T>(roi_bin_grid_w);

        T w1, w2, w3, w4;
        int x_low, x_high, y_low, y_high;

        bilinear_interpolate_gradient(
            height,
            width,
            y,
            x,
            w1,
            w2,
            w3,
            w4,
            x_low,
            x_high,
            y_low,
            y_high,
            index);

        T g1 = top_diff_this_bin * w1 / count;
        T g2 = top_diff_this_bin * w2 / count;
        T g3 = top_diff_this_bin * w3 / count;
        T g4 = top_diff_this_bin * w4 / count;

        if (x_low >= 0 && x_high >= 0 && y_low >= 0 && y_high >= 0) {
          atomicAdd(
              offset_bottom_diff + y_low * width + x_low, static_cast<T>(g1));
          atomicAdd(
              offset_bottom_diff + y_low * width + x_high, static_cast<T>(g2));
          atomicAdd(
              offset_bottom_diff + y_high * width + x_low, static_cast<T>(g3));
          atomicAdd(
              offset_bottom_diff + y_high * width + x_high, static_cast<T>(g4));
        } // if
      } // ix
    } // iy
  } // CUDA_1D_KERNEL_LOOP
} // RoIAlignBackward

namespace cvpods {

at::Tensor ROIAlign_forward_cuda(
    const at::Tensor& input,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int sampling_ratio,
    bool aligned) {
  AT_ASSERTM(input.device().is_cuda(), "input must be a CUDA tensor");
  AT_ASSERTM(rois.device().is_cuda(), "rois must be a CUDA tensor");
  at::TensorArg input_t{input, "input", 1}, rois_t{rois, "rois", 2};

  at::CheckedFrom c = "ROIAlign_forward_cuda";
  at::checkAllSameGPU(c, {input_t, rois_t});
  at::checkAllSameType(c, {input_t, rois_t});
  at::cuda::CUDAGuard device_guard(input.device());

  auto num_rois = rois.size(0);
  auto channels = input.size(1);
  auto height = input.size(2);
  auto width = input.size(3);

  auto output = at::empty(
      {num_rois, channels, pooled_height, pooled_width}, input.options());
  auto output_size = num_rois * pooled_height * pooled_width * channels;
  cudaStream_t stream = at::cuda::getCurrentCUDAStream();

  dim3 grid(std::min(
      at::cuda::ATenCeilDiv(
          static_cast<int64_t>(output_size), static_cast<int64_t>(512)),
      static_cast<int64_t>(4096)));
  dim3 block(512);

  if (output.numel() == 0) {
    AT_CUDA_CHECK(cudaGetLastError());
    return output;
  }

  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "ROIAlign_forward", [&] {
    RoIAlignForward<scalar_t><<<grid, block, 0, stream>>>(
        output_size,
        input.contiguous().data_ptr<scalar_t>(),
        spatial_scale,
        channels,
        height,
        width,
        pooled_height,
        pooled_width,
        sampling_ratio,
        rois.contiguous().data_ptr<scalar_t>(),
        output.data_ptr<scalar_t>(),
        aligned);
  });
  cudaDeviceSynchronize();
  AT_CUDA_CHECK(cudaGetLastError());
  return output;
}

// TODO remove the dependency on input and use instead its sizes -> save memory
at::Tensor ROIAlign_backward_cuda(
    const at::Tensor& grad,
    const at::Tensor& rois,
    const float spatial_scale,
    const int pooled_height,
    const int pooled_width,
    const int batch_size,
    const int channels,
    const int height,
    const int width,
    const int sampling_ratio,
    bool aligned) {
  AT_ASSERTM(grad.device().is_cuda(), "grad must be a CUDA tensor");
  AT_ASSERTM(rois.device().is_cuda(), "rois must be a CUDA tensor");

  at::TensorArg grad_t{grad, "grad", 1}, rois_t{rois, "rois", 2};
  at::CheckedFrom c = "ROIAlign_backward_cuda";
  at::checkAllSameGPU(c, {grad_t, rois_t});
  at::checkAllSameType(c, {grad_t, rois_t});
  at::cuda::CUDAGuard device_guard(grad.device());

  auto num_rois = rois.size(0);
  auto grad_input =
      at::zeros({batch_size, channels, height, width}, grad.options());

  cudaStream_t stream = at::cuda::getCurrentCUDAStream();

  dim3 grid(std::min(
      at::cuda::ATenCeilDiv(
          static_cast<int64_t>(grad.numel()), static_cast<int64_t>(512)),
      static_cast<int64_t>(4096)));
  dim3 block(512);

  // handle possibly empty gradients
  if (grad.numel() == 0) {
    AT_CUDA_CHECK(cudaGetLastError());
    return grad_input;
  }

  AT_DISPATCH_FLOATING_TYPES(grad.scalar_type(), "ROIAlign_backward", [&] {
    RoIAlignBackwardFeature<scalar_t><<<grid, block, 0, stream>>>(
        grad.numel(),
        grad.contiguous().data_ptr<scalar_t>(),
        num_rois,
        spatial_scale,
        channels,
        height,
        width,
        pooled_height,
        pooled_width,
        sampling_ratio,
        grad_input.data_ptr<scalar_t>(),
        rois.contiguous().data_ptr<scalar_t>(),
        aligned);
  });
  AT_CUDA_CHECK(cudaGetLastError());
  return grad_input;
}

} // namespace cvpods
```

##### cvpods/layers/csrc/deformable/deform_conv_cuda_kernel.cu

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

// modified from
// https://github.com/open-mmlab/mmdetection/blob/master/mmdet/ops/dcn/src/deform_conv_cuda_kernel.cu
// Original license: Apache 2.0
// clang-format off

// modify from
// https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/blob/mmdetection/mmdet/ops/dcn/src/deform_conv_cuda_kernel.cu

/*!
 ******************* BEGIN Caffe Copyright Notice and Disclaimer *****************
 *
 * COPYRIGHT
 *
 * All contributions by the University of California:
 * Copyright (c) 2014-2017 The Regents of the University of California (Regents)
 * All rights reserved.
 *
 * All other contributions:
 * Copyright (c) 2014-2017, the respective contributors
 * All rights reserved.
 *
 * Caffe uses a shared copyright model: each contributor holds copyright over
 * their contributions to Caffe. The project versioning records all such
 * contribution and copyright details. If a contributor wants to further mark
 * their specific copyright on a particular contribution, they should indicate
 * their copyright solely in the commit message of the change when it is
 * committed.
 *
 * LICENSE
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * 1. Redistributions of source code must retain the above copyright notice, this
 * list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright notice,
 * this list of conditions and the following disclaimer in the documentation
 * and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 *AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 *IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
 *FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 *DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 *SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 *CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 *OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 *OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 * CONTRIBUTION AGREEMENT
 *
 * By contributing to the BVLC/caffe repository through pull-request, comment,
 * or otherwise, the contributor releases their content to the
 * license and copyright terms herein.
 *
 ***************** END Caffe Copyright Notice and Disclaimer *********************
 *
 * Copyright (c) 2018 Microsoft
 * Licensed under The MIT License [see LICENSE for details]
 * \file modulated_deformable_im2col.cuh
 * \brief Function definitions of converting an image to
 * column matrix based on kernel, padding, dilation, and offset.
 * These functions are mainly used in deformable convolution operators.
 * \ref: https://arxiv.org/abs/1703.06211
 * \author Yuwen Xiong, Haozhi Qi, Jifeng Dai, Xizhou Zhu, Han Hu, Dazhi Cheng
 */

#include <ATen/ATen.h>
#include <c10/cuda/CUDAGuard.h>
#include <float.h>
#include <math.h>
#include <stdio.h>
#include <THC/THCAtomics.cuh>

using namespace at;

#define CUDA_KERNEL_LOOP(i, n)                                 \
  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < (n); \
       i += blockDim.x * gridDim.x)


namespace {

const int CUDA_NUM_THREADS = 1024;
const int kMaxGridNum = 65535;

inline int GET_BLOCKS(const int N) {
  return std::min(kMaxGridNum, (N + CUDA_NUM_THREADS - 1) / CUDA_NUM_THREADS);
}

}

template <typename scalar_t>
__device__ scalar_t deformable_im2col_bilinear(
    const scalar_t* bottom_data,
    const int data_width,
    const int height,
    const int width,
    scalar_t h,
    scalar_t w) {
  int h_low = floor(h);
  int w_low = floor(w);
  int h_high = h_low + 1;
  int w_high = w_low + 1;

  scalar_t lh = h - h_low;
  scalar_t lw = w - w_low;
  scalar_t hh = 1 - lh, hw = 1 - lw;

  scalar_t v1 = 0;
  if (h_low >= 0 && w_low >= 0)
    v1 = bottom_data[h_low * data_width + w_low];
  scalar_t v2 = 0;
  if (h_low >= 0 && w_high <= width - 1)
    v2 = bottom_data[h_low * data_width + w_high];
  scalar_t v3 = 0;
  if (h_high <= height - 1 && w_low >= 0)
    v3 = bottom_data[h_high * data_width + w_low];
  scalar_t v4 = 0;
  if (h_high <= height - 1 && w_high <= width - 1)
    v4 = bottom_data[h_high * data_width + w_high];

  scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;

  scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);
  return val;
}

template <typename scalar_t>
__device__ scalar_t get_gradient_weight(
    scalar_t argmax_h,
    scalar_t argmax_w,
    const int h,
    const int w,
    const int height,
    const int width) {
  if (argmax_h <= -1 || argmax_h >= height || argmax_w <= -1 ||
      argmax_w >= width) {
    // empty
    return 0;
  }

  int argmax_h_low = floor(argmax_h);
  int argmax_w_low = floor(argmax_w);
  int argmax_h_high = argmax_h_low + 1;
  int argmax_w_high = argmax_w_low + 1;

  scalar_t weight = 0;
  if (h == argmax_h_low && w == argmax_w_low)
    weight = (h + 1 - argmax_h) * (w + 1 - argmax_w);
  if (h == argmax_h_low && w == argmax_w_high)
    weight = (h + 1 - argmax_h) * (argmax_w + 1 - w);
  if (h == argmax_h_high && w == argmax_w_low)
    weight = (argmax_h + 1 - h) * (w + 1 - argmax_w);
  if (h == argmax_h_high && w == argmax_w_high)
    weight = (argmax_h + 1 - h) * (argmax_w + 1 - w);
  return weight;
}

template <typename scalar_t>
__device__ scalar_t get_coordinate_weight(
    scalar_t argmax_h,
    scalar_t argmax_w,
    const int height,
    const int width,
    const scalar_t* im_data,
    const int data_width,
    const int bp_dir) {
  if (argmax_h <= -1 || argmax_h >= height || argmax_w <= -1 ||
      argmax_w >= width) {
    // empty
    return 0;
  }

  int argmax_h_low = floor(argmax_h);
  int argmax_w_low = floor(argmax_w);
  int argmax_h_high = argmax_h_low + 1;
  int argmax_w_high = argmax_w_low + 1;

  scalar_t weight = 0;

  if (bp_dir == 0) {
    if (argmax_h_low >= 0 && argmax_w_low >= 0)
      weight += -1 * (argmax_w_low + 1 - argmax_w) *
          im_data[argmax_h_low * data_width + argmax_w_low];
    if (argmax_h_low >= 0 && argmax_w_high <= width - 1)
      weight += -1 * (argmax_w - argmax_w_low) *
          im_data[argmax_h_low * data_width + argmax_w_high];
    if (argmax_h_high <= height - 1 && argmax_w_low >= 0)
      weight += (argmax_w_low + 1 - argmax_w) *
          im_data[argmax_h_high * data_width + argmax_w_low];
    if (argmax_h_high <= height - 1 && argmax_w_high <= width - 1)
      weight += (argmax_w - argmax_w_low) *
          im_data[argmax_h_high * data_width + argmax_w_high];
  } else if (bp_dir == 1) {
    if (argmax_h_low >= 0 && argmax_w_low >= 0)
      weight += -1 * (argmax_h_low + 1 - argmax_h) *
          im_data[argmax_h_low * data_width + argmax_w_low];
    if (argmax_h_low >= 0 && argmax_w_high <= width - 1)
      weight += (argmax_h_low + 1 - argmax_h) *
          im_data[argmax_h_low * data_width + argmax_w_high];
    if (argmax_h_high <= height - 1 && argmax_w_low >= 0)
      weight += -1 * (argmax_h - argmax_h_low) *
          im_data[argmax_h_high * data_width + argmax_w_low];
    if (argmax_h_high <= height - 1 && argmax_w_high <= width - 1)
      weight += (argmax_h - argmax_h_low) *
          im_data[argmax_h_high * data_width + argmax_w_high];
  }

  return weight;
}

template <typename scalar_t>
__global__ void deformable_im2col_gpu_kernel(
    const int n,
    const scalar_t* data_im,
    const scalar_t* data_offset,
    const int height,
    const int width,
    const int kernel_h,
    const int kernel_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int channel_per_deformable_group,
    const int batch_size,
    const int num_channels,
    const int deformable_group,
    const int height_col,
    const int width_col,
    scalar_t* data_col) {
  CUDA_KERNEL_LOOP(index, n) {
    // index index of output matrix
    const int w_col = index % width_col;
    const int h_col = (index / width_col) % height_col;
    const int b_col = (index / width_col / height_col) % batch_size;
    const int c_im = (index / width_col / height_col) / batch_size;
    const int c_col = c_im * kernel_h * kernel_w;

    // compute deformable group index
    const int deformable_group_index = c_im / channel_per_deformable_group;

    const int h_in = h_col * stride_h - pad_h;
    const int w_in = w_col * stride_w - pad_w;
    scalar_t* data_col_ptr = data_col +
        ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;
    // const scalar_t* data_im_ptr = data_im + ((b_col * num_channels + c_im) *
    // height + h_in) * width + w_in;
    const scalar_t* data_im_ptr =
        data_im + (b_col * num_channels + c_im) * height * width;
    const scalar_t* data_offset_ptr = data_offset +
        (b_col * deformable_group + deformable_group_index) * 2 * kernel_h *
            kernel_w * height_col * width_col;

    for (int i = 0; i < kernel_h; ++i) {
      for (int j = 0; j < kernel_w; ++j) {
        const int data_offset_h_ptr =
            ((2 * (i * kernel_w + j)) * height_col + h_col) * width_col + w_col;
        const int data_offset_w_ptr =
            ((2 * (i * kernel_w + j) + 1) * height_col + h_col) * width_col +
            w_col;
        const scalar_t offset_h = data_offset_ptr[data_offset_h_ptr];
        const scalar_t offset_w = data_offset_ptr[data_offset_w_ptr];
        scalar_t val = static_cast<scalar_t>(0);
        const scalar_t h_im = h_in + i * dilation_h + offset_h;
        const scalar_t w_im = w_in + j * dilation_w + offset_w;
        if (h_im > -1 && w_im > -1 && h_im < height && w_im < width) {
          // const scalar_t map_h = i * dilation_h + offset_h;
          // const scalar_t map_w = j * dilation_w + offset_w;
          // const int cur_height = height - h_in;
          // const int cur_width = width - w_in;
          // val = deformable_im2col_bilinear(data_im_ptr, width, cur_height,
          // cur_width, map_h, map_w);
          val = deformable_im2col_bilinear(
              data_im_ptr, width, height, width, h_im, w_im);
        }
        *data_col_ptr = val;
        data_col_ptr += batch_size * height_col * width_col;
      }
    }
  }
}


template <typename scalar_t>
__global__ void deformable_col2im_gpu_kernel(
    const int n,
    const scalar_t* data_col,
    const scalar_t* data_offset,
    const int channels,
    const int height,
    const int width,
    const int kernel_h,
    const int kernel_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int channel_per_deformable_group,
    const int batch_size,
    const int deformable_group,
    const int height_col,
    const int width_col,
    scalar_t* grad_im) {
  CUDA_KERNEL_LOOP(index, n) {
    const int j = (index / width_col / height_col / batch_size) % kernel_w;
    const int i =
        (index / width_col / height_col / batch_size / kernel_w) % kernel_h;
    const int c =
        index / width_col / height_col / batch_size / kernel_w / kernel_h;
    // compute the start and end of the output

    const int deformable_group_index = c / channel_per_deformable_group;

    int w_out = index % width_col;
    int h_out = (index / width_col) % height_col;
    int b = (index / width_col / height_col) % batch_size;
    int w_in = w_out * stride_w - pad_w;
    int h_in = h_out * stride_h - pad_h;

    const scalar_t* data_offset_ptr = data_offset +
        (b * deformable_group + deformable_group_index) * 2 * kernel_h *
            kernel_w * height_col * width_col;
    const int data_offset_h_ptr =
        ((2 * (i * kernel_w + j)) * height_col + h_out) * width_col + w_out;
    const int data_offset_w_ptr =
        ((2 * (i * kernel_w + j) + 1) * height_col + h_out) * width_col + w_out;
    const scalar_t offset_h = data_offset_ptr[data_offset_h_ptr];
    const scalar_t offset_w = data_offset_ptr[data_offset_w_ptr];
    const scalar_t cur_inv_h_data = h_in + i * dilation_h + offset_h;
    const scalar_t cur_inv_w_data = w_in + j * dilation_w + offset_w;

    const scalar_t cur_top_grad = data_col[index];
    const int cur_h = (int)cur_inv_h_data;
    const int cur_w = (int)cur_inv_w_data;
    for (int dy = -2; dy <= 2; dy++) {
      for (int dx = -2; dx <= 2; dx++) {
        if (cur_h + dy >= 0 && cur_h + dy < height && cur_w + dx >= 0 &&
            cur_w + dx < width && abs(cur_inv_h_data - (cur_h + dy)) < 1 &&
            abs(cur_inv_w_data - (cur_w + dx)) < 1) {
          int cur_bottom_grad_pos =
              ((b * channels + c) * height + cur_h + dy) * width + cur_w + dx;
          scalar_t weight = get_gradient_weight(
              cur_inv_h_data,
              cur_inv_w_data,
              cur_h + dy,
              cur_w + dx,
              height,
              width);
          atomicAdd(grad_im + cur_bottom_grad_pos, weight * cur_top_grad);
        }
      }
    }
  }
}


template <typename scalar_t>
__global__ void deformable_col2im_coord_gpu_kernel(
    const int n,
    const scalar_t* data_col,
    const scalar_t* data_im,
    const scalar_t* data_offset,
    const int channels,
    const int height,
    const int width,
    const int kernel_h,
    const int kernel_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int channel_per_deformable_group,
    const int batch_size,
    const int offset_channels,
    const int deformable_group,
    const int height_col,
    const int width_col,
    scalar_t* grad_offset) {
  CUDA_KERNEL_LOOP(index, n) {
    scalar_t val = 0;
    int w = index % width_col;
    int h = (index / width_col) % height_col;
    int c = (index / width_col / height_col) % offset_channels;
    int b = (index / width_col / height_col) / offset_channels;
    // compute the start and end of the output

    const int deformable_group_index = c / (2 * kernel_h * kernel_w);
    const int col_step = kernel_h * kernel_w;
    int cnt = 0;
    const scalar_t* data_col_ptr = data_col +
        deformable_group_index * channel_per_deformable_group * batch_size *
            width_col * height_col;
    const scalar_t* data_im_ptr = data_im +
        (b * deformable_group + deformable_group_index) *
            channel_per_deformable_group / kernel_h / kernel_w * height * width;
    const scalar_t* data_offset_ptr = data_offset +
        (b * deformable_group + deformable_group_index) * 2 * kernel_h *
            kernel_w * height_col * width_col;

    const int offset_c = c - deformable_group_index * 2 * kernel_h * kernel_w;

    for (int col_c = (offset_c / 2); col_c < channel_per_deformable_group;
         col_c += col_step) {
      const int col_pos =
          (((col_c * batch_size + b) * height_col) + h) * width_col + w;
      const int bp_dir = offset_c % 2;

      int j = (col_pos / width_col / height_col / batch_size) % kernel_w;
      int i =
          (col_pos / width_col / height_col / batch_size / kernel_w) % kernel_h;
      int w_out = col_pos % width_col;
      int h_out = (col_pos / width_col) % height_col;
      int w_in = w_out * stride_w - pad_w;
      int h_in = h_out * stride_h - pad_h;
      const int data_offset_h_ptr =
          (((2 * (i * kernel_w + j)) * height_col + h_out) * width_col + w_out);
      const int data_offset_w_ptr =
          (((2 * (i * kernel_w + j) + 1) * height_col + h_out) * width_col +
           w_out);
      const scalar_t offset_h = data_offset_ptr[data_offset_h_ptr];
      const scalar_t offset_w = data_offset_ptr[data_offset_w_ptr];
      scalar_t inv_h = h_in + i * dilation_h + offset_h;
      scalar_t inv_w = w_in + j * dilation_w + offset_w;
      if (inv_h <= -1 || inv_w <= -1 || inv_h >= height || inv_w >= width) {
        inv_h = inv_w = -2;
      }
      const scalar_t weight = get_coordinate_weight(
          inv_h,
          inv_w,
          height,
          width,
          data_im_ptr + cnt * height * width,
          width,
          bp_dir);
      val += weight * data_col_ptr[col_pos];
      cnt += 1;
    }

    grad_offset[index] = val;
  }
}


namespace cvpods {

void deformable_im2col(
    const at::Tensor data_im,
    const at::Tensor data_offset,
    const int channels,
    const int height,
    const int width,
    const int ksize_h,
    const int ksize_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int parallel_imgs,
    const int deformable_group,
    at::Tensor data_col) {
  // num_axes should be smaller than block size
  // todo: check parallel_imgs is correctly passed in
  int height_col =
      (height + 2 * pad_h - (dilation_h * (ksize_h - 1) + 1)) / stride_h + 1;
  int width_col =
      (width + 2 * pad_w - (dilation_w * (ksize_w - 1) + 1)) / stride_w + 1;
  int num_kernels = channels * height_col * width_col * parallel_imgs;
  int channel_per_deformable_group = channels / deformable_group;

  at::cuda::CUDAGuard device_guard(data_im.device());
  cudaStream_t stream = at::cuda::getCurrentCUDAStream();

  AT_DISPATCH_FLOATING_TYPES_AND_HALF(
      data_im.scalar_type(), "deformable_im2col_gpu", ([&] {
        const scalar_t* data_im_ = data_im.data_ptr<scalar_t>();
        const scalar_t* data_offset_ = data_offset.data_ptr<scalar_t>();
        scalar_t* data_col_ = data_col.data_ptr<scalar_t>();

        deformable_im2col_gpu_kernel<<<
            GET_BLOCKS(num_kernels),
            CUDA_NUM_THREADS,
            0,
            stream>>>(
            num_kernels,
            data_im_,
            data_offset_,
            height,
            width,
            ksize_h,
            ksize_w,
            pad_h,
            pad_w,
            stride_h,
            stride_w,
            dilation_h,
            dilation_w,
            channel_per_deformable_group,
            parallel_imgs,
            channels,
            deformable_group,
            height_col,
            width_col,
            data_col_);
      }));

  cudaError_t err = cudaGetLastError();
  if (err != cudaSuccess) {
    printf("error in deformable_im2col: %s\n", cudaGetErrorString(err));
  }
}


void deformable_col2im(
    const at::Tensor data_col,
    const at::Tensor data_offset,
    const int channels,
    const int height,
    const int width,
    const int ksize_h,
    const int ksize_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int parallel_imgs,
    const int deformable_group,
    at::Tensor grad_im) {
  // todo: make sure parallel_imgs is passed in correctly
  int height_col =
      (height + 2 * pad_h - (dilation_h * (ksize_h - 1) + 1)) / stride_h + 1;
  int width_col =
      (width + 2 * pad_w - (dilation_w * (ksize_w - 1) + 1)) / stride_w + 1;
  int num_kernels =
      channels * ksize_h * ksize_w * height_col * width_col * parallel_imgs;
  int channel_per_deformable_group = channels / deformable_group;

  at::cuda::CUDAGuard device_guard(data_col.device());
  cudaStream_t stream = at::cuda::getCurrentCUDAStream();

  AT_DISPATCH_FLOATING_TYPES_AND_HALF(
      data_col.scalar_type(), "deformable_col2im_gpu", ([&] {
        const scalar_t* data_col_ = data_col.data_ptr<scalar_t>();
        const scalar_t* data_offset_ = data_offset.data_ptr<scalar_t>();
        scalar_t* grad_im_ = grad_im.data_ptr<scalar_t>();

        deformable_col2im_gpu_kernel<<<
            GET_BLOCKS(num_kernels),
            CUDA_NUM_THREADS,
            0,
            stream>>>(
            num_kernels,
            data_col_,
            data_offset_,
            channels,
            height,
            width,
            ksize_h,
            ksize_w,
            pad_h,
            pad_w,
            stride_h,
            stride_w,
            dilation_h,
            dilation_w,
            channel_per_deformable_group,
            parallel_imgs,
            deformable_group,
            height_col,
            width_col,
            grad_im_);
      }));

  cudaError_t err = cudaGetLastError();
  if (err != cudaSuccess) {
    printf("error in deformable_col2im: %s\n", cudaGetErrorString(err));
  }
}


void deformable_col2im_coord(
    const at::Tensor data_col,
    const at::Tensor data_im,
    const at::Tensor data_offset,
    const int channels,
    const int height,
    const int width,
    const int ksize_h,
    const int ksize_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int parallel_imgs,
    const int deformable_group,
    at::Tensor grad_offset) {
  int height_col =
      (height + 2 * pad_h - (dilation_h * (ksize_h - 1) + 1)) / stride_h + 1;
  int width_col =
      (width + 2 * pad_w - (dilation_w * (ksize_w - 1) + 1)) / stride_w + 1;
  int num_kernels = height_col * width_col * 2 * ksize_h * ksize_w *
      deformable_group * parallel_imgs;
  int channel_per_deformable_group =
      channels * ksize_h * ksize_w / deformable_group;

  at::cuda::CUDAGuard device_guard(data_col.device());
  cudaStream_t stream = at::cuda::getCurrentCUDAStream();

  AT_DISPATCH_FLOATING_TYPES_AND_HALF(
      data_col.scalar_type(), "deformable_col2im_coord_gpu", ([&] {
        const scalar_t* data_col_ = data_col.data_ptr<scalar_t>();
        const scalar_t* data_im_ = data_im.data_ptr<scalar_t>();
        const scalar_t* data_offset_ = data_offset.data_ptr<scalar_t>();
        scalar_t* grad_offset_ = grad_offset.data_ptr<scalar_t>();

        deformable_col2im_coord_gpu_kernel<<<
            GET_BLOCKS(num_kernels),
            CUDA_NUM_THREADS,
            0,
            stream>>>(
            num_kernels,
            data_col_,
            data_im_,
            data_offset_,
            channels,
            height,
            width,
            ksize_h,
            ksize_w,
            pad_h,
            pad_w,
            stride_h,
            stride_w,
            dilation_h,
            dilation_w,
            channel_per_deformable_group,
            parallel_imgs,
            2 * ksize_h * ksize_w * deformable_group,
            deformable_group,
            height_col,
            width_col,
            grad_offset_);
      }));
}

} // namespace cvpods


template <typename scalar_t>
__device__ scalar_t dmcn_im2col_bilinear(
    const scalar_t* bottom_data,
    const int data_width,
    const int height,
    const int width,
    scalar_t h,
    scalar_t w) {
  int h_low = floor(h);
  int w_low = floor(w);
  int h_high = h_low + 1;
  int w_high = w_low + 1;

  scalar_t lh = h - h_low;
  scalar_t lw = w - w_low;
  scalar_t hh = 1 - lh, hw = 1 - lw;

  scalar_t v1 = 0;
  if (h_low >= 0 && w_low >= 0)
    v1 = bottom_data[h_low * data_width + w_low];
  scalar_t v2 = 0;
  if (h_low >= 0 && w_high <= width - 1)
    v2 = bottom_data[h_low * data_width + w_high];
  scalar_t v3 = 0;
  if (h_high <= height - 1 && w_low >= 0)
    v3 = bottom_data[h_high * data_width + w_low];
  scalar_t v4 = 0;
  if (h_high <= height - 1 && w_high <= width - 1)
    v4 = bottom_data[h_high * data_width + w_high];

  scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;

  scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);
  return val;
}

template <typename scalar_t>
__device__ scalar_t dmcn_get_gradient_weight(
    scalar_t argmax_h,
    scalar_t argmax_w,
    const int h,
    const int w,
    const int height,
    const int width) {
  if (argmax_h <= -1 || argmax_h >= height || argmax_w <= -1 ||
      argmax_w >= width) {
    // empty
    return 0;
  }

  int argmax_h_low = floor(argmax_h);
  int argmax_w_low = floor(argmax_w);
  int argmax_h_high = argmax_h_low + 1;
  int argmax_w_high = argmax_w_low + 1;

  scalar_t weight = 0;
  if (h == argmax_h_low && w == argmax_w_low)
    weight = (h + 1 - argmax_h) * (w + 1 - argmax_w);
  if (h == argmax_h_low && w == argmax_w_high)
    weight = (h + 1 - argmax_h) * (argmax_w + 1 - w);
  if (h == argmax_h_high && w == argmax_w_low)
    weight = (argmax_h + 1 - h) * (w + 1 - argmax_w);
  if (h == argmax_h_high && w == argmax_w_high)
    weight = (argmax_h + 1 - h) * (argmax_w + 1 - w);
  return weight;
}

template <typename scalar_t>
__device__ scalar_t dmcn_get_coordinate_weight(
    scalar_t argmax_h,
    scalar_t argmax_w,
    const int height,
    const int width,
    const scalar_t* im_data,
    const int data_width,
    const int bp_dir) {
  if (argmax_h <= -1 || argmax_h >= height || argmax_w <= -1 ||
      argmax_w >= width) {
    // empty
    return 0;
  }

  int argmax_h_low = floor(argmax_h);
  int argmax_w_low = floor(argmax_w);
  int argmax_h_high = argmax_h_low + 1;
  int argmax_w_high = argmax_w_low + 1;

  scalar_t weight = 0;

  if (bp_dir == 0) {
    if (argmax_h_low >= 0 && argmax_w_low >= 0)
      weight += -1 * (argmax_w_low + 1 - argmax_w) *
          im_data[argmax_h_low * data_width + argmax_w_low];
    if (argmax_h_low >= 0 && argmax_w_high <= width - 1)
      weight += -1 * (argmax_w - argmax_w_low) *
          im_data[argmax_h_low * data_width + argmax_w_high];
    if (argmax_h_high <= height - 1 && argmax_w_low >= 0)
      weight += (argmax_w_low + 1 - argmax_w) *
          im_data[argmax_h_high * data_width + argmax_w_low];
    if (argmax_h_high <= height - 1 && argmax_w_high <= width - 1)
      weight += (argmax_w - argmax_w_low) *
          im_data[argmax_h_high * data_width + argmax_w_high];
  } else if (bp_dir == 1) {
    if (argmax_h_low >= 0 && argmax_w_low >= 0)
      weight += -1 * (argmax_h_low + 1 - argmax_h) *
          im_data[argmax_h_low * data_width + argmax_w_low];
    if (argmax_h_low >= 0 && argmax_w_high <= width - 1)
      weight += (argmax_h_low + 1 - argmax_h) *
          im_data[argmax_h_low * data_width + argmax_w_high];
    if (argmax_h_high <= height - 1 && argmax_w_low >= 0)
      weight += -1 * (argmax_h - argmax_h_low) *
          im_data[argmax_h_high * data_width + argmax_w_low];
    if (argmax_h_high <= height - 1 && argmax_w_high <= width - 1)
      weight += (argmax_h - argmax_h_low) *
          im_data[argmax_h_high * data_width + argmax_w_high];
  }

  return weight;
}

template <typename scalar_t>
__global__ void modulated_deformable_im2col_gpu_kernel(
    const int n,
    const scalar_t* data_im,
    const scalar_t* data_offset,
    const scalar_t* data_mask,
    const int height,
    const int width,
    const int kernel_h,
    const int kernel_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int channel_per_deformable_group,
    const int batch_size,
    const int num_channels,
    const int deformable_group,
    const int height_col,
    const int width_col,
    scalar_t* data_col) {
  CUDA_KERNEL_LOOP(index, n) {
    // index index of output matrix
    const int w_col = index % width_col;
    const int h_col = (index / width_col) % height_col;
    const int b_col = (index / width_col / height_col) % batch_size;
    const int c_im = (index / width_col / height_col) / batch_size;
    const int c_col = c_im * kernel_h * kernel_w;

    // compute deformable group index
    const int deformable_group_index = c_im / channel_per_deformable_group;

    const int h_in = h_col * stride_h - pad_h;
    const int w_in = w_col * stride_w - pad_w;

    scalar_t* data_col_ptr = data_col +
        ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;
    // const float* data_im_ptr = data_im + ((b_col * num_channels + c_im) *
    // height + h_in) * width + w_in;
    const scalar_t* data_im_ptr =
        data_im + (b_col * num_channels + c_im) * height * width;
    const scalar_t* data_offset_ptr = data_offset +
        (b_col * deformable_group + deformable_group_index) * 2 * kernel_h *
            kernel_w * height_col * width_col;

    const scalar_t* data_mask_ptr = data_mask +
        (b_col * deformable_group + deformable_group_index) * kernel_h *
            kernel_w * height_col * width_col;

    for (int i = 0; i < kernel_h; ++i) {
      for (int j = 0; j < kernel_w; ++j) {
        const int data_offset_h_ptr =
            ((2 * (i * kernel_w + j)) * height_col + h_col) * width_col + w_col;
        const int data_offset_w_ptr =
            ((2 * (i * kernel_w + j) + 1) * height_col + h_col) * width_col +
            w_col;
        const int data_mask_hw_ptr =
            ((i * kernel_w + j) * height_col + h_col) * width_col + w_col;
        const scalar_t offset_h = data_offset_ptr[data_offset_h_ptr];
        const scalar_t offset_w = data_offset_ptr[data_offset_w_ptr];
        const scalar_t mask = data_mask_ptr[data_mask_hw_ptr];
        scalar_t val = static_cast<scalar_t>(0);
        const scalar_t h_im = h_in + i * dilation_h + offset_h;
        const scalar_t w_im = w_in + j * dilation_w + offset_w;
        // if (h_im >= 0 && w_im >= 0 && h_im < height && w_im < width) {
        if (h_im > -1 && w_im > -1 && h_im < height && w_im < width) {
          // const float map_h = i * dilation_h + offset_h;
          // const float map_w = j * dilation_w + offset_w;
          // const int cur_height = height - h_in;
          // const int cur_width = width - w_in;
          // val = dmcn_im2col_bilinear(data_im_ptr, width, cur_height,
          // cur_width, map_h, map_w);
          val = dmcn_im2col_bilinear(
              data_im_ptr, width, height, width, h_im, w_im);
        }
        *data_col_ptr = val * mask;
        data_col_ptr += batch_size * height_col * width_col;
        // data_col_ptr += height_col * width_col;
      }
    }
  }
}

template <typename scalar_t>
__global__ void modulated_deformable_col2im_gpu_kernel(
    const int n,
    const scalar_t* data_col,
    const scalar_t* data_offset,
    const scalar_t* data_mask,
    const int channels,
    const int height,
    const int width,
    const int kernel_h,
    const int kernel_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int channel_per_deformable_group,
    const int batch_size,
    const int deformable_group,
    const int height_col,
    const int width_col,
    scalar_t* grad_im) {
  CUDA_KERNEL_LOOP(index, n) {
    const int j = (index / width_col / height_col / batch_size) % kernel_w;
    const int i =
        (index / width_col / height_col / batch_size / kernel_w) % kernel_h;
    const int c =
        index / width_col / height_col / batch_size / kernel_w / kernel_h;
    // compute the start and end of the output

    const int deformable_group_index = c / channel_per_deformable_group;

    int w_out = index % width_col;
    int h_out = (index / width_col) % height_col;
    int b = (index / width_col / height_col) % batch_size;
    int w_in = w_out * stride_w - pad_w;
    int h_in = h_out * stride_h - pad_h;

    const scalar_t* data_offset_ptr = data_offset +
        (b * deformable_group + deformable_group_index) * 2 * kernel_h *
            kernel_w * height_col * width_col;
    const scalar_t* data_mask_ptr = data_mask +
        (b * deformable_group + deformable_group_index) * kernel_h * kernel_w *
            height_col * width_col;
    const int data_offset_h_ptr =
        ((2 * (i * kernel_w + j)) * height_col + h_out) * width_col + w_out;
    const int data_offset_w_ptr =
        ((2 * (i * kernel_w + j) + 1) * height_col + h_out) * width_col + w_out;
    const int data_mask_hw_ptr =
        ((i * kernel_w + j) * height_col + h_out) * width_col + w_out;
    const scalar_t offset_h = data_offset_ptr[data_offset_h_ptr];
    const scalar_t offset_w = data_offset_ptr[data_offset_w_ptr];
    const scalar_t mask = data_mask_ptr[data_mask_hw_ptr];
    const scalar_t cur_inv_h_data = h_in + i * dilation_h + offset_h;
    const scalar_t cur_inv_w_data = w_in + j * dilation_w + offset_w;

    const scalar_t cur_top_grad = data_col[index] * mask;
    const int cur_h = (int)cur_inv_h_data;
    const int cur_w = (int)cur_inv_w_data;
    for (int dy = -2; dy <= 2; dy++) {
      for (int dx = -2; dx <= 2; dx++) {
        if (cur_h + dy >= 0 && cur_h + dy < height && cur_w + dx >= 0 &&
            cur_w + dx < width && abs(cur_inv_h_data - (cur_h + dy)) < 1 &&
            abs(cur_inv_w_data - (cur_w + dx)) < 1) {
          int cur_bottom_grad_pos =
              ((b * channels + c) * height + cur_h + dy) * width + cur_w + dx;
          scalar_t weight = dmcn_get_gradient_weight(
              cur_inv_h_data,
              cur_inv_w_data,
              cur_h + dy,
              cur_w + dx,
              height,
              width);
          atomicAdd(grad_im + cur_bottom_grad_pos, weight * cur_top_grad);
        }
      }
    }
  }
}

template <typename scalar_t>
__global__ void modulated_deformable_col2im_coord_gpu_kernel(
    const int n,
    const scalar_t* data_col,
    const scalar_t* data_im,
    const scalar_t* data_offset,
    const scalar_t* data_mask,
    const int channels,
    const int height,
    const int width,
    const int kernel_h,
    const int kernel_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int channel_per_deformable_group,
    const int batch_size,
    const int offset_channels,
    const int deformable_group,
    const int height_col,
    const int width_col,
    scalar_t* grad_offset,
    scalar_t* grad_mask) {
  CUDA_KERNEL_LOOP(index, n) {
    scalar_t val = 0, mval = 0;
    int w = index % width_col;
    int h = (index / width_col) % height_col;
    int c = (index / width_col / height_col) % offset_channels;
    int b = (index / width_col / height_col) / offset_channels;
    // compute the start and end of the output

    const int deformable_group_index = c / (2 * kernel_h * kernel_w);
    const int col_step = kernel_h * kernel_w;
    int cnt = 0;
    const scalar_t* data_col_ptr = data_col +
        deformable_group_index * channel_per_deformable_group * batch_size *
            width_col * height_col;
    const scalar_t* data_im_ptr = data_im +
        (b * deformable_group + deformable_group_index) *
            channel_per_deformable_group / kernel_h / kernel_w * height * width;
    const scalar_t* data_offset_ptr = data_offset +
        (b * deformable_group + deformable_group_index) * 2 * kernel_h *
            kernel_w * height_col * width_col;
    const scalar_t* data_mask_ptr = data_mask +
        (b * deformable_group + deformable_group_index) * kernel_h * kernel_w *
            height_col * width_col;

    const int offset_c = c - deformable_group_index * 2 * kernel_h * kernel_w;

    for (int col_c = (offset_c / 2); col_c < channel_per_deformable_group;
         col_c += col_step) {
      const int col_pos =
          (((col_c * batch_size + b) * height_col) + h) * width_col + w;
      const int bp_dir = offset_c % 2;

      int j = (col_pos / width_col / height_col / batch_size) % kernel_w;
      int i =
          (col_pos / width_col / height_col / batch_size / kernel_w) % kernel_h;
      int w_out = col_pos % width_col;
      int h_out = (col_pos / width_col) % height_col;
      int w_in = w_out * stride_w - pad_w;
      int h_in = h_out * stride_h - pad_h;
      const int data_offset_h_ptr =
          (((2 * (i * kernel_w + j)) * height_col + h_out) * width_col + w_out);
      const int data_offset_w_ptr =
          (((2 * (i * kernel_w + j) + 1) * height_col + h_out) * width_col +
           w_out);
      const int data_mask_hw_ptr =
          (((i * kernel_w + j) * height_col + h_out) * width_col + w_out);
      const scalar_t offset_h = data_offset_ptr[data_offset_h_ptr];
      const scalar_t offset_w = data_offset_ptr[data_offset_w_ptr];
      const scalar_t mask = data_mask_ptr[data_mask_hw_ptr];
      scalar_t inv_h = h_in + i * dilation_h + offset_h;
      scalar_t inv_w = w_in + j * dilation_w + offset_w;
      if (inv_h <= -1 || inv_w <= -1 || inv_h >= height || inv_w >= width) {
        inv_h = inv_w = -2;
      } else {
        mval += data_col_ptr[col_pos] *
            dmcn_im2col_bilinear(
                    data_im_ptr + cnt * height * width,
                    width,
                    height,
                    width,
                    inv_h,
                    inv_w);
      }
      const scalar_t weight = dmcn_get_coordinate_weight(
          inv_h,
          inv_w,
          height,
          width,
          data_im_ptr + cnt * height * width,
          width,
          bp_dir);
      val += weight * data_col_ptr[col_pos] * mask;
      cnt += 1;
    }
    // KERNEL_ASSIGN(grad_offset[index], offset_req, val);
    grad_offset[index] = val;
    if (offset_c % 2 == 0)
      // KERNEL_ASSIGN(grad_mask[(((b * deformable_group +
      // deformable_group_index) * kernel_h * kernel_w + offset_c / 2) *
      // height_col + h) * width_col + w], mask_req, mval);
      grad_mask
          [(((b * deformable_group + deformable_group_index) * kernel_h *
                 kernel_w +
             offset_c / 2) *
                height_col +
            h) *
               width_col +
           w] = mval;
  }
}


namespace cvpods {

void modulated_deformable_im2col_cuda(
    const at::Tensor data_im,
    const at::Tensor data_offset,
    const at::Tensor data_mask,
    const int batch_size,
    const int channels,
    const int height_im,
    const int width_im,
    const int height_col,
    const int width_col,
    const int kernel_h,
    const int kenerl_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int deformable_group,
    at::Tensor data_col) {
  // num_axes should be smaller than block size
  const int channel_per_deformable_group = channels / deformable_group;
  const int num_kernels = channels * batch_size * height_col * width_col;

  at::cuda::CUDAGuard device_guard(data_im.device());
  cudaStream_t stream = at::cuda::getCurrentCUDAStream();

  AT_DISPATCH_FLOATING_TYPES_AND_HALF(
      data_im.scalar_type(), "modulated_deformable_im2col_gpu", ([&] {
        const scalar_t* data_im_ = data_im.data_ptr<scalar_t>();
        const scalar_t* data_offset_ = data_offset.data_ptr<scalar_t>();
        const scalar_t* data_mask_ = data_mask.data_ptr<scalar_t>();
        scalar_t* data_col_ = data_col.data_ptr<scalar_t>();

        modulated_deformable_im2col_gpu_kernel<<<
            GET_BLOCKS(num_kernels),
            CUDA_NUM_THREADS,
            0,
            stream>>>(
            num_kernels,
            data_im_,
            data_offset_,
            data_mask_,
            height_im,
            width_im,
            kernel_h,
            kenerl_w,
            pad_h,
            pad_w,
            stride_h,
            stride_w,
            dilation_h,
            dilation_w,
            channel_per_deformable_group,
            batch_size,
            channels,
            deformable_group,
            height_col,
            width_col,
            data_col_);
      }));

  cudaError_t err = cudaGetLastError();
  if (err != cudaSuccess) {
    printf(
        "error in modulated_deformable_im2col_cuda: %s\n",
        cudaGetErrorString(err));
  }
}

void modulated_deformable_col2im_cuda(
    const at::Tensor data_col,
    const at::Tensor data_offset,
    const at::Tensor data_mask,
    const int batch_size,
    const int channels,
    const int height_im,
    const int width_im,
    const int height_col,
    const int width_col,
    const int kernel_h,
    const int kernel_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int deformable_group,
    at::Tensor grad_im) {
  const int channel_per_deformable_group = channels / deformable_group;
  const int num_kernels =
      channels * kernel_h * kernel_w * batch_size * height_col * width_col;

  at::cuda::CUDAGuard device_guard(data_col.device());
  cudaStream_t stream = at::cuda::getCurrentCUDAStream();

  AT_DISPATCH_FLOATING_TYPES_AND_HALF(
      data_col.scalar_type(), "modulated_deformable_col2im_gpu", ([&] {
        const scalar_t* data_col_ = data_col.data_ptr<scalar_t>();
        const scalar_t* data_offset_ = data_offset.data_ptr<scalar_t>();
        const scalar_t* data_mask_ = data_mask.data_ptr<scalar_t>();
        scalar_t* grad_im_ = grad_im.data_ptr<scalar_t>();

        modulated_deformable_col2im_gpu_kernel<<<
            GET_BLOCKS(num_kernels),
            CUDA_NUM_THREADS,
            0,
            stream>>>(
            num_kernels,
            data_col_,
            data_offset_,
            data_mask_,
            channels,
            height_im,
            width_im,
            kernel_h,
            kernel_w,
            pad_h,
            pad_w,
            stride_h,
            stride_w,
            dilation_h,
            dilation_w,
            channel_per_deformable_group,
            batch_size,
            deformable_group,
            height_col,
            width_col,
            grad_im_);
      }));

  cudaError_t err = cudaGetLastError();
  if (err != cudaSuccess) {
    printf(
        "error in modulated_deformable_col2im_cuda: %s\n",
        cudaGetErrorString(err));
  }
}

void modulated_deformable_col2im_coord_cuda(
    const at::Tensor data_col,
    const at::Tensor data_im,
    const at::Tensor data_offset,
    const at::Tensor data_mask,
    const int batch_size,
    const int channels,
    const int height_im,
    const int width_im,
    const int height_col,
    const int width_col,
    const int kernel_h,
    const int kernel_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int deformable_group,
    at::Tensor grad_offset,
    at::Tensor grad_mask) {
  const int num_kernels = batch_size * height_col * width_col * 2 * kernel_h *
      kernel_w * deformable_group;
  const int channel_per_deformable_group =
      channels * kernel_h * kernel_w / deformable_group;

  at::cuda::CUDAGuard device_guard(data_col.device());
  cudaStream_t stream = at::cuda::getCurrentCUDAStream();

  AT_DISPATCH_FLOATING_TYPES_AND_HALF(
      data_col.scalar_type(), "modulated_deformable_col2im_coord_gpu", ([&] {
        const scalar_t* data_col_ = data_col.data_ptr<scalar_t>();
        const scalar_t* data_im_ = data_im.data_ptr<scalar_t>();
        const scalar_t* data_offset_ = data_offset.data_ptr<scalar_t>();
        const scalar_t* data_mask_ = data_mask.data_ptr<scalar_t>();
        scalar_t* grad_offset_ = grad_offset.data_ptr<scalar_t>();
        scalar_t* grad_mask_ = grad_mask.data_ptr<scalar_t>();

        modulated_deformable_col2im_coord_gpu_kernel<<<
            GET_BLOCKS(num_kernels),
            CUDA_NUM_THREADS,
            0,
            stream>>>(
            num_kernels,
            data_col_,
            data_im_,
            data_offset_,
            data_mask_,
            channels,
            height_im,
            width_im,
            kernel_h,
            kernel_w,
            pad_h,
            pad_w,
            stride_h,
            stride_w,
            dilation_h,
            dilation_w,
            channel_per_deformable_group,
            batch_size,
            2 * kernel_h * kernel_w * deformable_group,
            deformable_group,
            height_col,
            width_col,
            grad_offset_,
            grad_mask_);
      }));
  cudaError_t err = cudaGetLastError();
  if (err != cudaSuccess) {
    printf(
        "error in modulated_deformable_col2im_coord_cuda: %s\n",
        cudaGetErrorString(err));
  }
}

} // namespace cvpods
```

##### cvpods/layers/csrc/deformable/deform_conv_cuda.cu

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

// modified from
// https://github.com/open-mmlab/mmdetection/blob/master/mmdet/ops/dcn/src/deform_conv_cuda.cpp
// Original license: Apache 2.0

// modify from
// https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/blob/mmdetection/mmdet/ops/dcn/src/deform_conv_cuda.c
// Original license: Apache 2.0

#include <torch/types.h>

#include "deform_conv.h"

#include <cmath>
#include <vector>

namespace cvpods {

void deformable_im2col(
    const at::Tensor data_im,
    const at::Tensor data_offset,
    const int channels,
    const int height,
    const int width,
    const int ksize_h,
    const int ksize_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int parallel_imgs,
    const int deformable_group,
    at::Tensor data_col);

void deformable_col2im(
    const at::Tensor data_col,
    const at::Tensor data_offset,
    const int channels,
    const int height,
    const int width,
    const int ksize_h,
    const int ksize_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int parallel_imgs,
    const int deformable_group,
    at::Tensor grad_im);

void deformable_col2im_coord(
    const at::Tensor data_col,
    const at::Tensor data_im,
    const at::Tensor data_offset,
    const int channels,
    const int height,
    const int width,
    const int ksize_h,
    const int ksize_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int parallel_imgs,
    const int deformable_group,
    at::Tensor grad_offset);

void modulated_deformable_im2col_cuda(
    const at::Tensor data_im,
    const at::Tensor data_offset,
    const at::Tensor data_mask,
    const int batch_size,
    const int channels,
    const int height_im,
    const int width_im,
    const int height_col,
    const int width_col,
    const int kernel_h,
    const int kenerl_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int deformable_group,
    at::Tensor data_col);

void modulated_deformable_col2im_cuda(
    const at::Tensor data_col,
    const at::Tensor data_offset,
    const at::Tensor data_mask,
    const int batch_size,
    const int channels,
    const int height_im,
    const int width_im,
    const int height_col,
    const int width_col,
    const int kernel_h,
    const int kenerl_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int deformable_group,
    at::Tensor grad_im);

void modulated_deformable_col2im_coord_cuda(
    const at::Tensor data_col,
    const at::Tensor data_im,
    const at::Tensor data_offset,
    const at::Tensor data_mask,
    const int batch_size,
    const int channels,
    const int height_im,
    const int width_im,
    const int height_col,
    const int width_col,
    const int kernel_h,
    const int kenerl_w,
    const int pad_h,
    const int pad_w,
    const int stride_h,
    const int stride_w,
    const int dilation_h,
    const int dilation_w,
    const int deformable_group,
    at::Tensor grad_offset,
    at::Tensor grad_mask);

void shape_check(
    at::Tensor input,
    at::Tensor offset,
    at::Tensor* gradOutput,
    at::Tensor weight,
    int kH,
    int kW,
    int dH,
    int dW,
    int padH,
    int padW,
    int dilationH,
    int dilationW,
    int group,
    int deformable_group) {
  TORCH_CHECK(
      weight.ndimension() == 4,
      "4D weight tensor (nOutputPlane,nInputPlane,kH,kW) expected, "
      "but got: %s",
      weight.ndimension());

  TORCH_CHECK(weight.is_contiguous(), "weight tensor has to be contiguous");

  TORCH_CHECK(
      kW > 0 && kH > 0,
      "kernel size should be greater than zero, but got kH: %d kW: %d",
      kH,
      kW);

  TORCH_CHECK(
      (weight.size(2) == kH && weight.size(3) == kW),
      "kernel size should be consistent with weight, ",
      "but got kH: %d kW: %d weight.size(2): %d, weight.size(3): %d",
      kH,
      kW,
      weight.size(2),
      weight.size(3));

  TORCH_CHECK(
      dW > 0 && dH > 0,
      "stride should be greater than zero, but got dH: %d dW: %d",
      dH,
      dW);

  TORCH_CHECK(
      dilationW > 0 && dilationH > 0,
      "dilation should be greater than 0, but got dilationH: %d dilationW: %d",
      dilationH,
      dilationW);

  int ndim = input.ndimension();
  int dimf = 0;
  int dimh = 1;
  int dimw = 2;

  if (ndim == 4) {
    dimf++;
    dimh++;
    dimw++;
  }

  TORCH_CHECK(
      ndim == 3 || ndim == 4,
      "3D or 4D input tensor expected but got: %s",
      ndim);

  long nInputPlane = weight.size(1) * group;
  long inputHeight = input.size(dimh);
  long inputWidth = input.size(dimw);
  long nOutputPlane = weight.size(0);
  long outputHeight =
      (inputHeight + 2 * padH - (dilationH * (kH - 1) + 1)) / dH + 1;
  long outputWidth =
      (inputWidth + 2 * padW - (dilationW * (kW - 1) + 1)) / dW + 1;

  TORCH_CHECK(
      nInputPlane % deformable_group == 0,
      "input channels must divide deformable group size");

  if (outputWidth < 1 || outputHeight < 1)
    AT_ERROR(
        "Given input size: (%ld x %ld x %ld). "
        "Calculated output size: (%ld x %ld x %ld). Output size is too small",
        nInputPlane,
        inputHeight,
        inputWidth,
        nOutputPlane,
        outputHeight,
        outputWidth);

  TORCH_CHECK(
      input.size(1) == nInputPlane,
      "invalid number of input planes, expected: %d, but got: %d",
      nInputPlane,
      input.size(1));

  TORCH_CHECK(
      (inputHeight + 2 * padH >= kH && inputWidth + 2 * padW>= kW),
      "input image is smaller than kernel");

  TORCH_CHECK(
      (offset.size(2) == outputHeight && offset.size(3) == outputWidth),
      "invalid spatial size of offset, expected height: %d width: %d, but "
      "got height: %d width: %d",
      outputHeight,
      outputWidth,
      offset.size(2),
      offset.size(3));

  TORCH_CHECK(
      (offset.size(1) == deformable_group * 2 * kH * kW),
      "invalid number of channels of offset");

  if (gradOutput != NULL) {
    TORCH_CHECK(
        gradOutput->size(dimf) == nOutputPlane,
        "invalid number of gradOutput planes, expected: %d, but got: %d",
        nOutputPlane,
        gradOutput->size(dimf));

    TORCH_CHECK(
        (gradOutput->size(dimh) == outputHeight &&
         gradOutput->size(dimw) == outputWidth),
        "invalid size of gradOutput, expected height: %d width: %d , but "
        "got height: %d width: %d",
        outputHeight,
        outputWidth,
        gradOutput->size(dimh),
        gradOutput->size(dimw));
  }
}

int deform_conv_forward_cuda(
    at::Tensor input,
    at::Tensor weight,
    at::Tensor offset,
    at::Tensor output,
    at::Tensor columns,
    at::Tensor ones,
    int kW,
    int kH,
    int dW,
    int dH,
    int padW,
    int padH,
    int dilationW,
    int dilationH,
    int group,
    int deformable_group,
    int im2col_step) {
  // todo: resize columns to include im2col: done
  // todo: add im2col_step as input
  // todo: add new output buffer and transpose it to output (or directly
  // transpose output) todo: possibly change data indexing because of
  // parallel_imgs

  shape_check(
      input,
      offset,
      NULL,
      weight,
      kH,
      kW,
      dH,
      dW,
      padH,
      padW,
      dilationH,
      dilationW,
      group,
      deformable_group);

  input = input.contiguous();
  offset = offset.contiguous();
  weight = weight.contiguous();

  int batch = 1;
  if (input.ndimension() == 3) {
    // Force batch
    batch = 0;
    input.unsqueeze_(0);
    offset.unsqueeze_(0);
  }

  // todo: assert batchsize dividable by im2col_step

  long batchSize = input.size(0);
  long nInputPlane = input.size(1);
  long inputHeight = input.size(2);
  long inputWidth = input.size(3);

  long nOutputPlane = weight.size(0);

  long outputWidth =
      (inputWidth + 2 * padW - (dilationW * (kW - 1) + 1)) / dW + 1;
  long outputHeight =
      (inputHeight + 2 * padH - (dilationH * (kH - 1) + 1)) / dH + 1;

  TORCH_CHECK((offset.size(0) == batchSize), "invalid batch size of offset");

  output = output.view({batchSize / im2col_step,
                        im2col_step,
                        nOutputPlane,
                        outputHeight,
                        outputWidth});
  columns = at::zeros(
      {nInputPlane * kW * kH, im2col_step * outputHeight * outputWidth},
      input.options());

  if (ones.ndimension() != 2 ||
      ones.size(0) * ones.size(1) < outputHeight * outputWidth) {
    ones = at::ones({outputHeight, outputWidth}, input.options());
  }

  input = input.view({batchSize / im2col_step,
                      im2col_step,
                      nInputPlane,
                      inputHeight,
                      inputWidth});
  offset = offset.view({batchSize / im2col_step,
                        im2col_step,
                        deformable_group * 2 * kH * kW,
                        outputHeight,
                        outputWidth});

  at::Tensor output_buffer = at::zeros(
      {batchSize / im2col_step,
       nOutputPlane,
       im2col_step * outputHeight,
       outputWidth},
      output.options());

  output_buffer = output_buffer.view({output_buffer.size(0),
                                      group,
                                      output_buffer.size(1) / group,
                                      output_buffer.size(2),
                                      output_buffer.size(3)});

  for (int elt = 0; elt < batchSize / im2col_step; elt++) {
    deformable_im2col(
        input[elt],
        offset[elt],
        nInputPlane,
        inputHeight,
        inputWidth,
        kH,
        kW,
        padH,
        padW,
        dH,
        dW,
        dilationH,
        dilationW,
        im2col_step,
        deformable_group,
        columns);

    columns = columns.view({group, columns.size(0) / group, columns.size(1)});
    weight = weight.view({group,
                          weight.size(0) / group,
                          weight.size(1),
                          weight.size(2),
                          weight.size(3)});

    for (int g = 0; g < group; g++) {
      output_buffer[elt][g] = output_buffer[elt][g]
                                  .flatten(1)
                                  .addmm_(weight[g].flatten(1), columns[g])
                                  .view_as(output_buffer[elt][g]);
    }
  }

  output_buffer =
      output_buffer.view({output_buffer.size(0),
                          output_buffer.size(1) * output_buffer.size(2),
                          output_buffer.size(3),
                          output_buffer.size(4)});

  output_buffer = output_buffer.view({batchSize / im2col_step,
                                      nOutputPlane,
                                      im2col_step,
                                      outputHeight,
                                      outputWidth});
  output_buffer.transpose_(1, 2);
  output.copy_(output_buffer);
  output = output.view({batchSize, nOutputPlane, outputHeight, outputWidth});

  input = input.view({batchSize, nInputPlane, inputHeight, inputWidth});
  offset = offset.view(
      {batchSize, deformable_group * 2 * kH * kW, outputHeight, outputWidth});

  if (batch == 0) {
    output = output.view({nOutputPlane, outputHeight, outputWidth});
    input = input.view({nInputPlane, inputHeight, inputWidth});
    offset = offset.view({offset.size(1), offset.size(2), offset.size(3)});
  }

  return 1;
}

int deform_conv_backward_input_cuda(
    at::Tensor input,
    at::Tensor offset,
    at::Tensor gradOutput,
    at::Tensor gradInput,
    at::Tensor gradOffset,
    at::Tensor weight,
    at::Tensor columns,
    int kW,
    int kH,
    int dW,
    int dH,
    int padW,
    int padH,
    int dilationW,
    int dilationH,
    int group,
    int deformable_group,
    int im2col_step) {
  shape_check(
      input,
      offset,
      &gradOutput,
      weight,
      kH,
      kW,
      dH,
      dW,
      padH,
      padW,
      dilationH,
      dilationW,
      group,
      deformable_group);

  input = input.contiguous();
  offset = offset.contiguous();
  gradOutput = gradOutput.contiguous();
  weight = weight.contiguous();

  int batch = 1;

  if (input.ndimension() == 3) {
    // Force batch
    batch = 0;
    input = input.view({1, input.size(0), input.size(1), input.size(2)});
    offset = offset.view({1, offset.size(0), offset.size(1), offset.size(2)});
    gradOutput = gradOutput.view(
        {1, gradOutput.size(0), gradOutput.size(1), gradOutput.size(2)});
  }

  long batchSize = input.size(0);
  long nInputPlane = input.size(1);
  long inputHeight = input.size(2);
  long inputWidth = input.size(3);

  long nOutputPlane = weight.size(0);

  long outputWidth =
      (inputWidth + 2 * padW - (dilationW * (kW - 1) + 1)) / dW + 1;
  long outputHeight =
      (inputHeight + 2 * padH - (dilationH * (kH - 1) + 1)) / dH + 1;

  TORCH_CHECK((offset.size(0) == batchSize), 3, "invalid batch size of offset");
  gradInput = gradInput.view({batchSize, nInputPlane, inputHeight, inputWidth});
  columns = at::zeros(
      {nInputPlane * kW * kH, im2col_step * outputHeight * outputWidth},
      input.options());

  // change order of grad output
  gradOutput = gradOutput.view({batchSize / im2col_step,
                                im2col_step,
                                nOutputPlane,
                                outputHeight,
                                outputWidth});
  gradOutput.transpose_(1, 2);

  gradInput = gradInput.view({batchSize / im2col_step,
                              im2col_step,
                              nInputPlane,
                              inputHeight,
                              inputWidth});
  input = input.view({batchSize / im2col_step,
                      im2col_step,
                      nInputPlane,
                      inputHeight,
                      inputWidth});
  gradOffset = gradOffset.view({batchSize / im2col_step,
                                im2col_step,
                                deformable_group * 2 * kH * kW,
                                outputHeight,
                                outputWidth});
  offset = offset.view({batchSize / im2col_step,
                        im2col_step,
                        deformable_group * 2 * kH * kW,
                        outputHeight,
                        outputWidth});

  for (int elt = 0; elt < batchSize / im2col_step; elt++) {
    // divide into groups
    columns = columns.view({group, columns.size(0) / group, columns.size(1)});
    weight = weight.view({group,
                          weight.size(0) / group,
                          weight.size(1),
                          weight.size(2),
                          weight.size(3)});
    gradOutput = gradOutput.view({gradOutput.size(0),
                                  group,
                                  gradOutput.size(1) / group,
                                  gradOutput.size(2),
                                  gradOutput.size(3),
                                  gradOutput.size(4)});

    for (int g = 0; g < group; g++) {
      columns[g] = columns[g].addmm_(
          weight[g].flatten(1).transpose(0, 1),
          gradOutput[elt][g].flatten(1),
          0.0f,
          1.0f);
    }

    columns =
        columns.view({columns.size(0) * columns.size(1), columns.size(2)});
    gradOutput = gradOutput.view({gradOutput.size(0),
                                  gradOutput.size(1) * gradOutput.size(2),
                                  gradOutput.size(3),
                                  gradOutput.size(4),
                                  gradOutput.size(5)});

    deformable_col2im_coord(
        columns,
        input[elt],
        offset[elt],
        nInputPlane,
        inputHeight,
        inputWidth,
        kH,
        kW,
        padH,
        padW,
        dH,
        dW,
        dilationH,
        dilationW,
        im2col_step,
        deformable_group,
        gradOffset[elt]);

    deformable_col2im(
        columns,
        offset[elt],
        nInputPlane,
        inputHeight,
        inputWidth,
        kH,
        kW,
        padH,
        padW,
        dH,
        dW,
        dilationH,
        dilationW,
        im2col_step,
        deformable_group,
        gradInput[elt]);
  }

  gradOutput.transpose_(1, 2);
  gradOutput =
      gradOutput.view({batchSize, nOutputPlane, outputHeight, outputWidth});

  gradInput = gradInput.view({batchSize, nInputPlane, inputHeight, inputWidth});
  input = input.view({batchSize, nInputPlane, inputHeight, inputWidth});
  gradOffset = gradOffset.view(
      {batchSize, deformable_group * 2 * kH * kW, outputHeight, outputWidth});
  offset = offset.view(
      {batchSize, deformable_group * 2 * kH * kW, outputHeight, outputWidth});

  if (batch == 0) {
    gradOutput = gradOutput.view({nOutputPlane, outputHeight, outputWidth});
    input = input.view({nInputPlane, inputHeight, inputWidth});
    gradInput = gradInput.view({nInputPlane, inputHeight, inputWidth});
    offset = offset.view({offset.size(1), offset.size(2), offset.size(3)});
    gradOffset =
        gradOffset.view({offset.size(1), offset.size(2), offset.size(3)});
  }

  return 1;
}

int deform_conv_backward_parameters_cuda(
    at::Tensor input,
    at::Tensor offset,
    at::Tensor gradOutput,
    at::Tensor gradWeight, // at::Tensor gradBias,
    at::Tensor columns,
    at::Tensor ones,
    int kW,
    int kH,
    int dW,
    int dH,
    int padW,
    int padH,
    int dilationW,
    int dilationH,
    int group,
    int deformable_group,
    float scale,
    int im2col_step) {
  // todo: transpose and reshape outGrad
  // todo: reshape columns
  // todo: add im2col_step as input

  shape_check(
      input,
      offset,
      &gradOutput,
      gradWeight,
      kH,
      kW,
      dH,
      dW,
      padH,
      padW,
      dilationH,
      dilationW,
      group,
      deformable_group);

  input = input.contiguous();
  offset = offset.contiguous();
  gradOutput = gradOutput.contiguous();

  int batch = 1;

  if (input.ndimension() == 3) {
    // Force batch
    batch = 0;
    input = input.view(
        at::IntList({1, input.size(0), input.size(1), input.size(2)}));
    gradOutput = gradOutput.view(
        {1, gradOutput.size(0), gradOutput.size(1), gradOutput.size(2)});
  }

  long batchSize = input.size(0);
  long nInputPlane = input.size(1);
  long inputHeight = input.size(2);
  long inputWidth = input.size(3);

  long nOutputPlane = gradWeight.size(0);

  long outputWidth =
      (inputWidth + 2 * padW - (dilationW * (kW - 1) + 1)) / dW + 1;
  long outputHeight =
      (inputHeight + 2 * padH - (dilationH * (kH - 1) + 1)) / dH + 1;

  TORCH_CHECK((offset.size(0) == batchSize), "invalid batch size of offset");

  columns = at::zeros(
      {nInputPlane * kW * kH, im2col_step * outputHeight * outputWidth},
      input.options());

  gradOutput = gradOutput.view({batchSize / im2col_step,
                                im2col_step,
                                nOutputPlane,
                                outputHeight,
                                outputWidth});
  gradOutput.transpose_(1, 2);

  at::Tensor gradOutputBuffer = at::zeros_like(gradOutput);
  gradOutputBuffer = gradOutputBuffer.view({batchSize / im2col_step,
                                            nOutputPlane,
                                            im2col_step,
                                            outputHeight,
                                            outputWidth});
  gradOutputBuffer.copy_(gradOutput);
  // gradOutput is not contiguous, so we do reshape (instead of view) next
  gradOutputBuffer = gradOutputBuffer.reshape({batchSize / im2col_step,
                                               nOutputPlane,
                                               im2col_step * outputHeight,
                                               outputWidth});

  gradOutput.transpose_(1, 2);
  gradOutput =
      gradOutput.view({batchSize, nOutputPlane, outputHeight, outputWidth});

  input = input.view({batchSize / im2col_step,
                      im2col_step,
                      nInputPlane,
                      inputHeight,
                      inputWidth});
  offset = offset.view({batchSize / im2col_step,
                        im2col_step,
                        deformable_group * 2 * kH * kW,
                        outputHeight,
                        outputWidth});

  for (int elt = 0; elt < batchSize / im2col_step; elt++) {
    deformable_im2col(
        input[elt],
        offset[elt],
        nInputPlane,
        inputHeight,
        inputWidth,
        kH,
        kW,
        padH,
        padW,
        dH,
        dW,
        dilationH,
        dilationW,
        im2col_step,
        deformable_group,
        columns);

    // divide into group
    gradOutputBuffer = gradOutputBuffer.view({gradOutputBuffer.size(0),
                                              group,
                                              gradOutputBuffer.size(1) / group,
                                              gradOutputBuffer.size(2),
                                              gradOutputBuffer.size(3)});
    columns = columns.view({group, columns.size(0) / group, columns.size(1)});
    gradWeight = gradWeight.view({group,
                                  gradWeight.size(0) / group,
                                  gradWeight.size(1),
                                  gradWeight.size(2),
                                  gradWeight.size(3)});

    for (int g = 0; g < group; g++) {
      gradWeight[g] = gradWeight[g]
                          .flatten(1)
                          .addmm_(
                              gradOutputBuffer[elt][g].flatten(1),
                              columns[g].transpose(1, 0),
                              1.0,
                              scale)
                          .view_as(gradWeight[g]);
    }
    gradOutputBuffer = gradOutputBuffer.view(
        {gradOutputBuffer.size(0),
         gradOutputBuffer.size(1) * gradOutputBuffer.size(2),
         gradOutputBuffer.size(3),
         gradOutputBuffer.size(4)});
    columns =
        columns.view({columns.size(0) * columns.size(1), columns.size(2)});
    gradWeight = gradWeight.view({gradWeight.size(0) * gradWeight.size(1),
                                  gradWeight.size(2),
                                  gradWeight.size(3),
                                  gradWeight.size(4)});
  }

  input = input.view({batchSize, nInputPlane, inputHeight, inputWidth});
  offset = offset.view(
      {batchSize, deformable_group * 2 * kH * kW, outputHeight, outputWidth});

  if (batch == 0) {
    gradOutput = gradOutput.view({nOutputPlane, outputHeight, outputWidth});
    input = input.view({nInputPlane, inputHeight, inputWidth});
  }

  return 1;
}

void modulated_deform_conv_cuda_forward(
    at::Tensor input,
    at::Tensor weight,
    at::Tensor bias,
    at::Tensor ones,
    at::Tensor offset,
    at::Tensor mask,
    at::Tensor output,
    at::Tensor columns,
    int kernel_h,
    int kernel_w,
    const int stride_h,
    const int stride_w,
    const int pad_h,
    const int pad_w,
    const int dilation_h,
    const int dilation_w,
    const int group,
    const int deformable_group,
    const bool with_bias) {
  TORCH_CHECK(input.is_contiguous(), "input tensor has to be contiguous");
  TORCH_CHECK(weight.is_contiguous(), "weight tensor has to be contiguous");

  const int batch = input.size(0);
  const int channels = input.size(1);
  const int height = input.size(2);
  const int width = input.size(3);

  const int channels_out = weight.size(0);
  const int channels_kernel = weight.size(1);
  const int kernel_h_ = weight.size(2);
  const int kernel_w_ = weight.size(3);

  if (kernel_h_ != kernel_h || kernel_w_ != kernel_w)
    AT_ERROR(
        "Input shape and kernel shape wont match: (%d x %d vs %d x %d).",
        kernel_h_,
        kernel_w,
        kernel_h_,
        kernel_w_);
  if (channels != channels_kernel * group)
    AT_ERROR(
        "Input shape and kernel channels wont match: (%d vs %d).",
        channels,
        channels_kernel * group);

  const int height_out =
      (height + 2 * pad_h - (dilation_h * (kernel_h - 1) + 1)) / stride_h + 1;
  const int width_out =
      (width + 2 * pad_w - (dilation_w * (kernel_w - 1) + 1)) / stride_w + 1;

  if (ones.ndimension() != 2 ||
      ones.size(0) * ones.size(1) < height_out * width_out) {
    // Resize plane and fill with ones...
    ones = at::ones({height_out, width_out}, input.options());
  }

  // resize output
  output = output.view({batch, channels_out, height_out, width_out}).zero_();
  // resize temporary columns
  columns = at::zeros(
      {channels * kernel_h * kernel_w, 1 * height_out * width_out},
      input.options());

  output = output.view({output.size(0),
                        group,
                        output.size(1) / group,
                        output.size(2),
                        output.size(3)});

  for (int b = 0; b < batch; b++) {
    modulated_deformable_im2col_cuda(
        input[b],
        offset[b],
        mask[b],
        1,
        channels,
        height,
        width,
        height_out,
        width_out,
        kernel_h,
        kernel_w,
        pad_h,
        pad_w,
        stride_h,
        stride_w,
        dilation_h,
        dilation_w,
        deformable_group,
        columns);

    // divide into group
    weight = weight.view({group,
                          weight.size(0) / group,
                          weight.size(1),
                          weight.size(2),
                          weight.size(3)});
    columns = columns.view({group, columns.size(0) / group, columns.size(1)});

    for (int g = 0; g < group; g++) {
      output[b][g] = output[b][g]
                         .flatten(1)
                         .addmm_(weight[g].flatten(1), columns[g])
                         .view_as(output[b][g]);
    }

    weight = weight.view({weight.size(0) * weight.size(1),
                          weight.size(2),
                          weight.size(3),
                          weight.size(4)});
    columns =
        columns.view({columns.size(0) * columns.size(1), columns.size(2)});
  }

  output = output.view({output.size(0),
                        output.size(1) * output.size(2),
                        output.size(3),
                        output.size(4)});

  if (with_bias) {
    output += bias.view({1, bias.size(0), 1, 1});
  }
}

void modulated_deform_conv_cuda_backward(
    at::Tensor input,
    at::Tensor weight,
    at::Tensor bias,
    at::Tensor ones,
    at::Tensor offset,
    at::Tensor mask,
    at::Tensor columns,
    at::Tensor grad_input,
    at::Tensor grad_weight,
    at::Tensor grad_bias,
    at::Tensor grad_offset,
    at::Tensor grad_mask,
    at::Tensor grad_output,
    int kernel_h,
    int kernel_w,
    int stride_h,
    int stride_w,
    int pad_h,
    int pad_w,
    int dilation_h,
    int dilation_w,
    int group,
    int deformable_group,
    const bool with_bias) {
  TORCH_CHECK(input.is_contiguous(), "input tensor has to be contiguous");
  TORCH_CHECK(weight.is_contiguous(), "weight tensor has to be contiguous");

  const int batch = input.size(0);
  const int channels = input.size(1);
  const int height = input.size(2);
  const int width = input.size(3);

  const int channels_kernel = weight.size(1);
  const int kernel_h_ = weight.size(2);
  const int kernel_w_ = weight.size(3);
  if (kernel_h_ != kernel_h || kernel_w_ != kernel_w)
    AT_ERROR(
        "Input shape and kernel shape wont match: (%d x %d vs %d x %d).",
        kernel_h_,
        kernel_w,
        kernel_h_,
        kernel_w_);
  if (channels != channels_kernel * group)
    AT_ERROR(
        "Input shape and kernel channels wont match: (%d vs %d).",
        channels,
        channels_kernel * group);

  const int height_out =
      (height + 2 * pad_h - (dilation_h * (kernel_h - 1) + 1)) / stride_h + 1;
  const int width_out =
      (width + 2 * pad_w - (dilation_w * (kernel_w - 1) + 1)) / stride_w + 1;

  if (ones.ndimension() != 2 ||
      ones.size(0) * ones.size(1) < height_out * width_out) {
    // Resize plane and fill with ones...
    ones = at::ones({height_out, width_out}, input.options());
  }

  grad_input = grad_input.view({batch, channels, height, width});
  columns = at::zeros(
      {channels * kernel_h * kernel_w, height_out * width_out},
      input.options());

  grad_output = grad_output.view({grad_output.size(0),
                                  group,
                                  grad_output.size(1) / group,
                                  grad_output.size(2),
                                  grad_output.size(3)});

  for (int b = 0; b < batch; b++) {
    // divide int group
    columns = columns.view({group, columns.size(0) / group, columns.size(1)});
    weight = weight.view({group,
                          weight.size(0) / group,
                          weight.size(1),
                          weight.size(2),
                          weight.size(3)});

    for (int g = 0; g < group; g++) {
      columns[g].addmm_(
          weight[g].flatten(1).transpose(0, 1),
          grad_output[b][g].flatten(1),
          0.0f,
          1.0f);
    }

    columns =
        columns.view({columns.size(0) * columns.size(1), columns.size(2)});
    weight = weight.view({weight.size(0) * weight.size(1),
                          weight.size(2),
                          weight.size(3),
                          weight.size(4)});

    // gradient w.r.t. input coordinate data
    modulated_deformable_col2im_coord_cuda(
        columns,
        input[b],
        offset[b],
        mask[b],
        1,
        channels,
        height,
        width,
        height_out,
        width_out,
        kernel_h,
        kernel_w,
        pad_h,
        pad_w,
        stride_h,
        stride_w,
        dilation_h,
        dilation_w,
        deformable_group,
        grad_offset[b],
        grad_mask[b]);
    // gradient w.r.t. input data
    modulated_deformable_col2im_cuda(
        columns,
        offset[b],
        mask[b],
        1,
        channels,
        height,
        width,
        height_out,
        width_out,
        kernel_h,
        kernel_w,
        pad_h,
        pad_w,
        stride_h,
        stride_w,
        dilation_h,
        dilation_w,
        deformable_group,
        grad_input[b]);

    // gradient w.r.t. weight, dWeight should accumulate across the batch and
    // group
    modulated_deformable_im2col_cuda(
        input[b],
        offset[b],
        mask[b],
        1,
        channels,
        height,
        width,
        height_out,
        width_out,
        kernel_h,
        kernel_w,
        pad_h,
        pad_w,
        stride_h,
        stride_w,
        dilation_h,
        dilation_w,
        deformable_group,
        columns);

    columns = columns.view({group, columns.size(0) / group, columns.size(1)});
    grad_weight = grad_weight.view({group,
                                    grad_weight.size(0) / group,
                                    grad_weight.size(1),
                                    grad_weight.size(2),
                                    grad_weight.size(3)});
    if (with_bias)
      grad_bias = grad_bias.view({group, grad_bias.size(0) / group});

    for (int g = 0; g < group; g++) {
      grad_weight[g] =
          grad_weight[g]
              .flatten(1)
              .addmm_(grad_output[b][g].flatten(1), columns[g].transpose(0, 1))
              .view_as(grad_weight[g]);
      if (with_bias) {
        grad_bias[g] =
            grad_bias[g]
                .view({-1, 1})
                .addmm_(grad_output[b][g].flatten(1), ones.view({-1, 1}))
                .view(-1);
      }
    }

    columns =
        columns.view({columns.size(0) * columns.size(1), columns.size(2)});
    grad_weight = grad_weight.view({grad_weight.size(0) * grad_weight.size(1),
                                    grad_weight.size(2),
                                    grad_weight.size(3),
                                    grad_weight.size(4)});
    if (with_bias)
      grad_bias = grad_bias.view({grad_bias.size(0) * grad_bias.size(1)});
  }
  grad_output = grad_output.view({grad_output.size(0) * grad_output.size(1),
                                  grad_output.size(2),
                                  grad_output.size(3),
                                  grad_output.size(4)});
}

} // namespace cvpods
```

##### cvpods/layers/csrc/deformable/deform_conv.h

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#pragma once
#include <torch/types.h>

namespace cvpods {

#ifdef WITH_CUDA
int deform_conv_forward_cuda(
    at::Tensor input,
    at::Tensor weight,
    at::Tensor offset,
    at::Tensor output,
    at::Tensor columns,
    at::Tensor ones,
    int kW,
    int kH,
    int dW,
    int dH,
    int padW,
    int padH,
    int dilationW,
    int dilationH,
    int group,
    int deformable_group,
    int im2col_step);

int deform_conv_backward_input_cuda(
    at::Tensor input,
    at::Tensor offset,
    at::Tensor gradOutput,
    at::Tensor gradInput,
    at::Tensor gradOffset,
    at::Tensor weight,
    at::Tensor columns,
    int kW,
    int kH,
    int dW,
    int dH,
    int padW,
    int padH,
    int dilationW,
    int dilationH,
    int group,
    int deformable_group,
    int im2col_step);

int deform_conv_backward_parameters_cuda(
    at::Tensor input,
    at::Tensor offset,
    at::Tensor gradOutput,
    at::Tensor gradWeight, // at::Tensor gradBias,
    at::Tensor columns,
    at::Tensor ones,
    int kW,
    int kH,
    int dW,
    int dH,
    int padW,
    int padH,
    int dilationW,
    int dilationH,
    int group,
    int deformable_group,
    float scale,
    int im2col_step);

void modulated_deform_conv_cuda_forward(
    at::Tensor input,
    at::Tensor weight,
    at::Tensor bias,
    at::Tensor ones,
    at::Tensor offset,
    at::Tensor mask,
    at::Tensor output,
    at::Tensor columns,
    int kernel_h,
    int kernel_w,
    const int stride_h,
    const int stride_w,
    const int pad_h,
    const int pad_w,
    const int dilation_h,
    const int dilation_w,
    const int group,
    const int deformable_group,
    const bool with_bias);

void modulated_deform_conv_cuda_backward(
    at::Tensor input,
    at::Tensor weight,
    at::Tensor bias,
    at::Tensor ones,
    at::Tensor offset,
    at::Tensor mask,
    at::Tensor columns,
    at::Tensor grad_input,
    at::Tensor grad_weight,
    at::Tensor grad_bias,
    at::Tensor grad_offset,
    at::Tensor grad_mask,
    at::Tensor grad_output,
    int kernel_h,
    int kernel_w,
    int stride_h,
    int stride_w,
    int pad_h,
    int pad_w,
    int dilation_h,
    int dilation_w,
    int group,
    int deformable_group,
    const bool with_bias);

#endif

inline int deform_conv_forward(
    at::Tensor input,
    at::Tensor weight,
    at::Tensor offset,
    at::Tensor output,
    at::Tensor columns,
    at::Tensor ones,
    int kW,
    int kH,
    int dW,
    int dH,
    int padW,
    int padH,
    int dilationW,
    int dilationH,
    int group,
    int deformable_group,
    int im2col_step) {
  if (input.device().is_cuda()) {
#ifdef WITH_CUDA
    TORCH_CHECK(weight.device().is_cuda(), "weight tensor is not on GPU!");
    TORCH_CHECK(offset.device().is_cuda(), "offset tensor is not on GPU!");
    return deform_conv_forward_cuda(
        input,
        weight,
        offset,
        output,
        columns,
        ones,
        kW,
        kH,
        dW,
        dH,
        padW,
        padH,
        dilationW,
        dilationH,
        group,
        deformable_group,
        im2col_step);
#else
    AT_ERROR("Not compiled with GPU support");
#endif
  }
  AT_ERROR("Not implemented on the CPU");
}

inline int deform_conv_backward_input(
    at::Tensor input,
    at::Tensor offset,
    at::Tensor gradOutput,
    at::Tensor gradInput,
    at::Tensor gradOffset,
    at::Tensor weight,
    at::Tensor columns,
    int kW,
    int kH,
    int dW,
    int dH,
    int padW,
    int padH,
    int dilationW,
    int dilationH,
    int group,
    int deformable_group,
    int im2col_step) {
  if (gradOutput.device().is_cuda()) {
#ifdef WITH_CUDA
    TORCH_CHECK(input.device().is_cuda(), "input tensor is not on GPU!");
    TORCH_CHECK(weight.device().is_cuda(), "weight tensor is not on GPU!");
    TORCH_CHECK(offset.device().is_cuda(), "offset tensor is not on GPU!");
    return deform_conv_backward_input_cuda(
        input,
        offset,
        gradOutput,
        gradInput,
        gradOffset,
        weight,
        columns,
        kW,
        kH,
        dW,
        dH,
        padW,
        padH,
        dilationW,
        dilationH,
        group,
        deformable_group,
        im2col_step);
#else
    AT_ERROR("Not compiled with GPU support");
#endif
  }
  AT_ERROR("Not implemented on the CPU");
}

inline int deform_conv_backward_filter(
    at::Tensor input,
    at::Tensor offset,
    at::Tensor gradOutput,
    at::Tensor gradWeight, // at::Tensor gradBias,
    at::Tensor columns,
    at::Tensor ones,
    int kW,
    int kH,
    int dW,
    int dH,
    int padW,
    int padH,
    int dilationW,
    int dilationH,
    int group,
    int deformable_group,
    float scale,
    int im2col_step) {
  if (gradOutput.device().is_cuda()) {
#ifdef WITH_CUDA
    TORCH_CHECK(input.device().is_cuda(), "input tensor is not on GPU!");
    TORCH_CHECK(offset.device().is_cuda(), "offset tensor is not on GPU!");
    return deform_conv_backward_parameters_cuda(
        input,
        offset,
        gradOutput,
        gradWeight,
        columns,
        ones,
        kW,
        kH,
        dW,
        dH,
        padW,
        padH,
        dilationW,
        dilationH,
        group,
        deformable_group,
        scale,
        im2col_step);
#else
    AT_ERROR("Not compiled with GPU support");
#endif
  }
  AT_ERROR("Not implemented on the CPU");
}

inline void modulated_deform_conv_forward(
    at::Tensor input,
    at::Tensor weight,
    at::Tensor bias,
    at::Tensor ones,
    at::Tensor offset,
    at::Tensor mask,
    at::Tensor output,
    at::Tensor columns,
    int kernel_h,
    int kernel_w,
    const int stride_h,
    const int stride_w,
    const int pad_h,
    const int pad_w,
    const int dilation_h,
    const int dilation_w,
    const int group,
    const int deformable_group,
    const bool with_bias) {
  if (input.device().is_cuda()) {
#ifdef WITH_CUDA
    TORCH_CHECK(weight.device().is_cuda(), "weight tensor is not on GPU!");
    TORCH_CHECK(bias.device().is_cuda(), "bias tensor is not on GPU!");
    TORCH_CHECK(offset.device().is_cuda(), "offset tensor is not on GPU!");
    return modulated_deform_conv_cuda_forward(
        input,
        weight,
        bias,
        ones,
        offset,
        mask,
        output,
        columns,
        kernel_h,
        kernel_w,
        stride_h,
        stride_w,
        pad_h,
        pad_w,
        dilation_h,
        dilation_w,
        group,
        deformable_group,
        with_bias);
#else
    AT_ERROR("Not compiled with GPU support");
#endif
  }
  AT_ERROR("Not implemented on the CPU");
}

inline void modulated_deform_conv_backward(
    at::Tensor input,
    at::Tensor weight,
    at::Tensor bias,
    at::Tensor ones,
    at::Tensor offset,
    at::Tensor mask,
    at::Tensor columns,
    at::Tensor grad_input,
    at::Tensor grad_weight,
    at::Tensor grad_bias,
    at::Tensor grad_offset,
    at::Tensor grad_mask,
    at::Tensor grad_output,
    int kernel_h,
    int kernel_w,
    int stride_h,
    int stride_w,
    int pad_h,
    int pad_w,
    int dilation_h,
    int dilation_w,
    int group,
    int deformable_group,
    const bool with_bias) {
  if (grad_output.device().is_cuda()) {
#ifdef WITH_CUDA
    TORCH_CHECK(input.device().is_cuda(), "input tensor is not on GPU!");
    TORCH_CHECK(weight.device().is_cuda(), "weight tensor is not on GPU!");
    TORCH_CHECK(bias.device().is_cuda(), "bias tensor is not on GPU!");
    TORCH_CHECK(offset.device().is_cuda(), "offset tensor is not on GPU!");
    return modulated_deform_conv_cuda_backward(
        input,
        weight,
        bias,
        ones,
        offset,
        mask,
        columns,
        grad_input,
        grad_weight,
        grad_bias,
        grad_offset,
        grad_mask,
        grad_output,
        kernel_h,
        kernel_w,
        stride_h,
        stride_w,
        pad_h,
        pad_w,
        dilation_h,
        dilation_w,
        group,
        deformable_group,
        with_bias);
#else
    AT_ERROR("Not compiled with GPU support");
#endif
  }
  AT_ERROR("Not implemented on the CPU");
}

} // namespace cvpods
```

##### cvpods/layers/csrc/nms_rotated/nms_rotated_cuda.cu

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAApplyUtils.cuh>
#include "../box_iou_rotated/box_iou_rotated_utils.h"

using namespace cvpods;

namespace {
int const threadsPerBlock = sizeof(unsigned long long) * 8;
}

template <typename T>
__global__ void nms_rotated_cuda_kernel(
    const int n_boxes,
    const float iou_threshold,
    const T* dev_boxes,
    unsigned long long* dev_mask) {
  // nms_rotated_cuda_kernel is modified from torchvision's nms_cuda_kernel

  const int row_start = blockIdx.y;
  const int col_start = blockIdx.x;

  // if (row_start > col_start) return;

  const int row_size =
      min(n_boxes - row_start * threadsPerBlock, threadsPerBlock);
  const int col_size =
      min(n_boxes - col_start * threadsPerBlock, threadsPerBlock);

  // Compared to nms_cuda_kernel, where each box is represented with 4 values
  // (x1, y1, x2, y2), each rotated box is represented with 5 values
  // (x_center, y_center, width, height, angle_degrees) here.
  __shared__ T block_boxes[threadsPerBlock * 5];
  if (threadIdx.x < col_size) {
    block_boxes[threadIdx.x * 5 + 0] =
        dev_boxes[(threadsPerBlock * col_start + threadIdx.x) * 5 + 0];
    block_boxes[threadIdx.x * 5 + 1] =
        dev_boxes[(threadsPerBlock * col_start + threadIdx.x) * 5 + 1];
    block_boxes[threadIdx.x * 5 + 2] =
        dev_boxes[(threadsPerBlock * col_start + threadIdx.x) * 5 + 2];
    block_boxes[threadIdx.x * 5 + 3] =
        dev_boxes[(threadsPerBlock * col_start + threadIdx.x) * 5 + 3];
    block_boxes[threadIdx.x * 5 + 4] =
        dev_boxes[(threadsPerBlock * col_start + threadIdx.x) * 5 + 4];
  }
  __syncthreads();

  if (threadIdx.x < row_size) {
    const int cur_box_idx = threadsPerBlock * row_start + threadIdx.x;
    const T* cur_box = dev_boxes + cur_box_idx * 5;
    int i = 0;
    unsigned long long t = 0;
    int start = 0;
    if (row_start == col_start) {
      start = threadIdx.x + 1;
    }
    for (i = start; i < col_size; i++) {
      // Instead of devIoU used by original horizontal nms, here
      // we use the single_box_iou_rotated function from box_iou_rotated_utils.h
      if (single_box_iou_rotated<T>(cur_box, block_boxes + i * 5) >
          iou_threshold) {
        t |= 1ULL << i;
      }
    }
    const int col_blocks = at::cuda::ATenCeilDiv(n_boxes, threadsPerBlock);
    dev_mask[cur_box_idx * col_blocks + col_start] = t;
  }
}

namespace cvpods {

at::Tensor nms_rotated_cuda(
    const at::Tensor& dets,
    const at::Tensor& scores,
    float iou_threshold) {
  // using scalar_t = float;
  AT_ASSERTM(dets.device().is_cuda(), "dets must be a CUDA tensor");
  AT_ASSERTM(scores.device().is_cuda(), "scores must be a CUDA tensor");
  at::cuda::CUDAGuard device_guard(dets.device());

  auto order_t = std::get<1>(scores.sort(0, /* descending=*/true));
  auto dets_sorted = dets.index_select(0, order_t);

  int dets_num = dets.size(0);

  const int col_blocks = at::cuda::ATenCeilDiv(dets_num, threadsPerBlock);

  at::Tensor mask =
      at::empty({dets_num * col_blocks}, dets.options().dtype(at::kLong));

  dim3 blocks(col_blocks, col_blocks);
  dim3 threads(threadsPerBlock);
  cudaStream_t stream = at::cuda::getCurrentCUDAStream();

  AT_DISPATCH_FLOATING_TYPES_AND_HALF(
      dets_sorted.scalar_type(), "nms_rotated_kernel_cuda", [&] {
        nms_rotated_cuda_kernel<scalar_t><<<blocks, threads, 0, stream>>>(
            dets_num,
            iou_threshold,
            dets_sorted.data_ptr<scalar_t>(),
            (unsigned long long*)mask.data_ptr<int64_t>());
      });

  at::Tensor mask_cpu = mask.to(at::kCPU);
  unsigned long long* mask_host = (unsigned long long*)mask_cpu.data_ptr<int64_t>();

  std::vector<unsigned long long> remv(col_blocks);
  memset(&remv[0], 0, sizeof(unsigned long long) * col_blocks);

  at::Tensor keep =
      at::empty({dets_num}, dets.options().dtype(at::kLong).device(at::kCPU));
  int64_t* keep_out = keep.data_ptr<int64_t>();

  int num_to_keep = 0;
  for (int i = 0; i < dets_num; i++) {
    int nblock = i / threadsPerBlock;
    int inblock = i % threadsPerBlock;

    if (!(remv[nblock] & (1ULL << inblock))) {
      keep_out[num_to_keep++] = i;
      unsigned long long* p = mask_host + i * col_blocks;
      for (int j = nblock; j < col_blocks; j++) {
        remv[j] |= p[j];
      }
    }
  }

  AT_CUDA_CHECK(cudaGetLastError());
  return order_t.index(
      {keep.narrow(/*dim=*/0, /*start=*/0, /*length=*/num_to_keep)
           .to(order_t.device(), keep.scalar_type())});
}

} // namespace cvpods
```

##### cvpods/layers/csrc/nms_rotated/nms_rotated_cpu.cpp

```cpp
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#include "../box_iou_rotated/box_iou_rotated_utils.h"
#include "nms_rotated.h"

namespace cvpods {

template <typename scalar_t>
at::Tensor nms_rotated_cpu_kernel(
    const at::Tensor& dets,
    const at::Tensor& scores,
    const float iou_threshold) {
  // nms_rotated_cpu_kernel is modified from torchvision's nms_cpu_kernel,
  // however, the code in this function is much shorter because
  // we delegate the IoU computation for rotated boxes to
  // the single_box_iou_rotated function in box_iou_rotated_utils.h
  AT_ASSERTM(!dets.device().is_cuda(), "dets must be a CPU tensor");
  AT_ASSERTM(!scores.device().is_cuda(), "scores must be a CPU tensor");
  AT_ASSERTM(
      dets.dtype() == scores.dtype(), "dets should have the same type as scores");

  if (dets.numel() == 0) {
    return at::empty({0}, dets.options().dtype(at::kLong));
  }

  auto order_t = std::get<1>(scores.sort(0, /* descending=*/true));

  auto ndets = dets.size(0);
  at::Tensor suppressed_t = at::zeros({ndets}, dets.options().dtype(at::kByte));
  at::Tensor keep_t = at::zeros({ndets}, dets.options().dtype(at::kLong));

  auto suppressed = suppressed_t.data_ptr<uint8_t>();
  auto keep = keep_t.data_ptr<int64_t>();
  auto order = order_t.data_ptr<int64_t>();

  int64_t num_to_keep = 0;

  for (int64_t _i = 0; _i < ndets; _i++) {
    auto i = order[_i];
    if (suppressed[i] == 1) {
      continue;
    }

    keep[num_to_keep++] = i;

    for (int64_t _j = _i + 1; _j < ndets; _j++) {
      auto j = order[_j];
      if (suppressed[j] == 1) {
        continue;
      }

      auto ovr = single_box_iou_rotated<scalar_t>(
          dets[i].data_ptr<scalar_t>(), dets[j].data_ptr<scalar_t>());
      if (ovr >= iou_threshold) {
        suppressed[j] = 1;
      }
    }
  }
  return keep_t.narrow(/*dim=*/0, /*start=*/0, /*length=*/num_to_keep);
}

at::Tensor nms_rotated_cpu(
    const at::Tensor& dets,
    const at::Tensor& scores,
    const float iou_threshold) {
  auto result = at::empty({0}, dets.options());

  AT_DISPATCH_FLOATING_TYPES(dets.scalar_type(), "nms_rotated", [&] {
    result = nms_rotated_cpu_kernel<scalar_t>(dets, scores, iou_threshold);
  });
  return result;
}

} // namespace cvpods
```

##### cvpods/layers/csrc/nms_rotated/nms_rotated.h

```
// Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
#pragma once
#include <torch/types.h>

namespace cvpods {

at::Tensor nms_rotated_cpu(
    const at::Tensor& dets,
    const at::Tensor& scores,
    const float iou_threshold);

#ifdef WITH_CUDA
at::Tensor nms_rotated_cuda(
    const at::Tensor& dets,
    const at::Tensor& scores,
    const float iou_threshold);
#endif

// Interface for Python
// inline is needed to prevent multiple function definitions when this header is
// included by different cpps
inline at::Tensor nms_rotated(
    const at::Tensor& dets,
    const at::Tensor& scores,
    const float iou_threshold) {
  assert(dets.device().is_cuda() == scores.device().is_cuda());
  if (dets.device().is_cuda()) {
#ifdef WITH_CUDA
    return nms_rotated_cuda(dets, scores, iou_threshold);
#else
    AT_ERROR("Not compiled with GPU support");
#endif
  }

  return nms_rotated_cpu(dets, scores, iou_threshold);
}

} // namespace cvpods
```

### cvpods/structures/instances.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import itertools
from typing import Any, Dict, List, Tuple, Union

import torch

from cvpods.layers import cat


class Instances:
    """
    This class represents a list of instances in an image.
    It stores the attributes of instances (e.g., boxes, masks, labels, scores) as "fields".
    All fields must have the same ``__len__`` which is the number of instances.

    All other (non-field) attributes of this class are considered private:
    they must start with '_' and are not modifiable by a user.

    Some basic usage:

    1. Set/Get a field:
       .. code-block:: python
          instances.gt_boxes = Boxes(...)
          print(instances.pred_masks)  # a tensor of shape (N, H, W)
          print('gt_masks' in instances)
    2. ``len(instances)`` returns the number of instances
    3. Indexing: ``instances[indices]`` will apply the indexing on all the fields
       and returns a new :class:`Instances`.
       Typically, ``indices`` is a integer vector of indices,
       or a binary mask of length ``num_instances``,
    """

    def __init__(self, image_size: Tuple[int, int], **kwargs: Any):
        """
        Args:
            image_size (height, width): the spatial size of the image.
            kwargs: fields to add to this `Instances`.
        """
        self._image_size = image_size
        self._fields: Dict[str, Any] = {}
        for k, v in kwargs.items():
            self.set(k, v)

    @property
    def image_size(self) -> Tuple[int, int]:
        """
        Returns:
            tuple: height, width
        """
        return self._image_size

    def __setattr__(self, name: str, val: Any) -> None:
        if name.startswith("_"):
            super().__setattr__(name, val)
        else:
            self.set(name, val)

    def __getattr__(self, name: str) -> Any:
        if name == "_fields" or name not in self._fields:
            raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
        return self._fields[name]

    def set(self, name: str, value: Any) -> None:
        """
        Set the field named `name` to `value`.
        The length of `value` must be the number of instances,
        and must agree with other existing fields in this object.
        """
        data_len = len(value)
        if len(self._fields):
            assert (
                len(self) == data_len
            ), "Adding a field of length {} to a Instances of length {}".format(data_len, len(self))
        self._fields[name] = value

    def has(self, name: str) -> bool:
        """
        Returns:
            bool: whether the field called `name` exists.
        """
        return name in self._fields

    def remove(self, name: str) -> None:
        """
        Remove the field called `name`.
        """
        del self._fields[name]

    def get(self, name: str) -> Any:
        """
        Returns the field called `name`.
        """
        return self._fields[name]

    def get_fields(self) -> Dict[str, Any]:
        """
        Returns:
            dict: a dict which maps names (str) to data of the fields

        Modifying the returned dict will modify this instance.
        """
        return self._fields

    # Tensor-like methods
    def to(self, device: str) -> "Instances":
        """
        Returns:
            Instances: all fields are called with a `to(device)`, if the field has this method.
        """
        ret = Instances(self._image_size)
        for k, v in self._fields.items():
            if hasattr(v, "to"):
                v = v.to(device)
            ret.set(k, v)
        return ret

    def __getitem__(self, item: Union[int, slice, torch.BoolTensor]) -> "Instances":
        """
        Args:
            item: an index-like object and will be used to index all the fields.

        Returns:
            If `item` is a string, return the data in the corresponding field.
            Otherwise, returns an `Instances` where all fields are indexed by `item`.
        """
        if type(item) == int:
            if item >= len(self) or item < -len(self):
                raise IndexError("Instances index out of range!")
            else:
                item = slice(item, None, len(self))

        ret = Instances(self._image_size)
        for k, v in self._fields.items():
            ret.set(k, v[item])
        return ret

    def __len__(self) -> int:
        for v in self._fields.values():
            return len(v)
        return 0

    def __iter__(self):
        raise NotImplementedError("`Instances` object is not iterable!")

    @staticmethod
    def cat(instance_lists: List["Instances"]) -> "Instances":
        """
        Args:
            instance_lists (list[Instances])

        Returns:
            Instances
        """
        assert all(isinstance(i, Instances) for i in instance_lists)
        assert len(instance_lists) > 0
        if len(instance_lists) == 1:
            return instance_lists[0]

        image_size = instance_lists[0].image_size
        for i in instance_lists[1:]:
            assert i.image_size == image_size
        ret = Instances(image_size)
        for k in instance_lists[0]._fields.keys():
            values = [i.get(k) for i in instance_lists]
            v0 = values[0]
            if isinstance(v0, torch.Tensor):
                values = cat(values, dim=0)
            elif isinstance(v0, list):
                values = list(itertools.chain(*values))
            elif hasattr(type(v0), "cat"):
                values = type(v0).cat(values)
            else:
                raise ValueError("Unsupported type {} for concatenation".format(type(v0)))
            ret.set(k, values)
        return ret

    def __str__(self) -> str:
        s = self.__class__.__name__ + "("
        s += "num_instances={}, ".format(len(self))
        s += "image_height={}, ".format(self._image_size[0])
        s += "image_width={}, ".format(self._image_size[1])
        s += "fields=[{}])".format(", ".join(self._fields.keys()))
        return s

    def __repr__(self) -> str:
        s = self.__class__.__name__ + "("
        s += "num_instances={}, ".format(len(self))
        s += "image_height={}, ".format(self._image_size[0])
        s += "image_width={}, ".format(self._image_size[1])
        s += "fields=["
        for k, v in self._fields.items():
            s += "{} = {}, ".format(k, v)
        s += "])"
        return s
```

### cvpods/structures/__init__.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from .boxes import Boxes, BoxMode, pairwise_ioa, pairwise_iou
from .image_list import ImageList
from .instances import Instances
from .keypoints import Keypoints, heatmaps_to_keypoints
from .masks import BitMasks, PolygonMasks, polygons_to_bitmask, rasterize_polygons_within_box
from .rotated_boxes import RotatedBoxes
from .rotated_boxes import pairwise_iou as pairwise_iou_rotated

__all__ = [k for k in globals().keys() if not k.startswith("_")]
```

### cvpods/structures/boxes.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import math
from enum import IntEnum, unique
from typing import Iterator, List, Tuple, Union

import numpy as np

import torch
from torchvision.ops.boxes import box_area

from cvpods.layers import cat

_RawBoxType = Union[List[float], Tuple[float, ...], torch.Tensor, np.ndarray]


@unique
class BoxMode(IntEnum):
    """
    Enum of different ways to represent a box.

    Attributes:

        XYXY_ABS: (x0, y0, x1, y1) in absolute floating points coordinates.
            The coordinates in range [0, width or height].
        XYWH_ABS: (x0, y0, w, h) in absolute floating points coordinates.
        XYXY_REL: (x0, y0, x1, y1) in range [0, 1]. They are relative to the size of the image.
        XYWH_REL: (x0, y0, w, h) in range [0, 1]. They are relative to the size of the image.
        XYWHA_ABS: (xc, yc, w, h, a) in absolute floating points coordinates.
            (xc, yc) is the center of the rotated box, and the angle a is in degrees ccw.
    """

    XYXY_ABS = 0
    XYWH_ABS = 1
    XYXY_REL = 2
    XYWH_REL = 3
    XYWHA_ABS = 4

    @staticmethod
    def convert(box: _RawBoxType, from_mode: "BoxMode", to_mode: "BoxMode") -> _RawBoxType:
        """
        Args:
            box: can be a k-tuple, k-list or an Nxk array/tensor, where k = 4 or 5
            from_mode, to_mode (BoxMode)

        Returns:
            The converted box of the same type.
        """
        if from_mode == to_mode:
            return box

        original_type = type(box)
        is_numpy = isinstance(box, np.ndarray)
        single_box = isinstance(box, (list, tuple))
        if single_box:
            assert len(box) == 4 or len(box) == 5, (
                "BoxMode.convert takes either a k-tuple/list or an Nxk array/tensor,"
                " where k == 4 or 5"
            )
            arr = torch.tensor(box)[None, :]
        else:
            # avoid modifying the input box
            if is_numpy:
                arr = torch.from_numpy(np.asarray(box)).clone()
            else:
                arr = box.clone()

        assert to_mode.value not in [
            BoxMode.XYXY_REL,
            BoxMode.XYWH_REL,
        ] and from_mode.value not in [
            BoxMode.XYXY_REL,
            BoxMode.XYWH_REL,
        ], "Relative mode not yet supported!"

        if from_mode == BoxMode.XYWHA_ABS and to_mode == BoxMode.XYXY_ABS:
            assert (
                arr.shape[-1] == 5
            ), "The last dimension of input shape must be 5 for XYWHA format"

            original_dtype = arr.dtype
            arr = arr.double()

            w = arr[:, 2]
            h = arr[:, 3]
            a = arr[:, 4]
            c = torch.abs(torch.cos(a * math.pi / 180.0))
            s = torch.abs(torch.sin(a * math.pi / 180.0))
            # This basically computes the horizontal bounding rectangle of the rotated box
            new_w = c * w + s * h
            new_h = c * h + s * w

            # convert center to top-left corner
            arr[:, 0] -= new_w / 2.0
            arr[:, 1] -= new_h / 2.0
            # bottom-right corner
            arr[:, 2] = arr[:, 0] + new_w
            arr[:, 3] = arr[:, 1] + new_h

            arr = arr[:, :4].to(dtype=original_dtype)
        elif from_mode == BoxMode.XYWH_ABS and to_mode == BoxMode.XYWHA_ABS:
            original_dtype = arr.dtype
            arr = arr.double()
            arr[:, 0] += arr[:, 2] / 2.0
            arr[:, 1] += arr[:, 3] / 2.0
            angles = torch.zeros((arr.shape[0], 1), dtype=arr.dtype)
            arr = torch.cat((arr, angles), axis=1).to(dtype=original_dtype)
        else:
            if to_mode == BoxMode.XYXY_ABS and from_mode == BoxMode.XYWH_ABS:
                arr[:, 2] += arr[:, 0]
                arr[:, 3] += arr[:, 1]
            elif from_mode == BoxMode.XYXY_ABS and to_mode == BoxMode.XYWH_ABS:
                arr[:, 2] -= arr[:, 0]
                arr[:, 3] -= arr[:, 1]
            else:
                raise NotImplementedError(
                    "Conversion from BoxMode {} to {} is not supported yet".format(
                        from_mode, to_mode
                    )
                )

        if single_box:
            return original_type(arr.flatten())

        if is_numpy:
            return arr.numpy()
        else:
            return arr


class Boxes:
    """
    This structure stores a list of boxes as a Nx4 torch.Tensor.
    It supports some common methods about boxes
    (`area`, `clip`, `nonempty`, etc),
    and also behaves like a Tensor
    (support indexing, `to(device)`, `.device`, and iteration over all boxes)

    Attributes:
        tensor(torch.Tensor): float matrix of Nx4.
    """

    BoxSizeType = Union[List[int], Tuple[int, int]]

    def __init__(self, tensor: torch.Tensor):
        """
        Args:
            tensor (Tensor[float]): a Nx4 matrix.  Each row is (x1, y1, x2, y2).
        """
        device = tensor.device if isinstance(tensor, torch.Tensor) else torch.device("cpu")
        tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)
        if tensor.numel() == 0:
            tensor = torch.zeros(0, 4, dtype=torch.float32, device=device)
        assert tensor.dim() == 2 and tensor.size(-1) == 4, tensor.size()

        self.tensor = tensor

    def clone(self) -> "Boxes":
        """
        Clone the Boxes.

        Returns:
            Boxes
        """
        return Boxes(self.tensor.clone())

    def to(self, device: str) -> "Boxes":
        return Boxes(self.tensor.to(device))

    def area(self) -> torch.Tensor:
        """
        Computes the area of all the boxes.

        Returns:
            torch.Tensor: a vector with areas of each box.
        """
        box = self.tensor
        area = (box[:, 2] - box[:, 0]) * (box[:, 3] - box[:, 1])
        return area

    def clip(self, box_size: BoxSizeType) -> None:
        """
        Clip (in place) the boxes by limiting x coordinates to the range [0, width]
        and y coordinates to the range [0, height].

        Args:
            box_size (height, width): The clipping box's size.
        """
        assert torch.isfinite(self.tensor).all(), "Box tensor contains infinite or NaN!"
        h, w = box_size
        self.tensor[:, 0].clamp_(min=0, max=w)
        self.tensor[:, 1].clamp_(min=0, max=h)
        self.tensor[:, 2].clamp_(min=0, max=w)
        self.tensor[:, 3].clamp_(min=0, max=h)

    def nonempty(self, threshold: int = 0) -> torch.Tensor:
        """
        Find boxes that are non-empty.
        A box is considered empty, if either of its side is no larger than threshold.

        Returns:
            Tensor:
                a binary vector which represents whether each box is empty
                (False) or non-empty (True).
        """
        box = self.tensor
        widths = box[:, 2] - box[:, 0]
        heights = box[:, 3] - box[:, 1]
        keep = (widths > threshold) & (heights > threshold)
        return keep

    def __getitem__(self, item: Union[int, slice, torch.BoolTensor]) -> "Boxes":
        """
        Returns:
            Boxes: Create a new :class:`Boxes` by indexing.

        The following usage are allowed:

        1. `new_boxes = boxes[3]`: return a `Boxes` which contains only one box.
        2. `new_boxes = boxes[2:10]`: return a slice of boxes.
        3. `new_boxes = boxes[vector]`, where vector is a torch.BoolTensor
        with `length = len(boxes)`. Nonzero elements in the vector will be selected.

        Note that the returned Boxes might share storage with this Boxes,
        subject to Pytorch's indexing semantics.
        """
        if isinstance(item, int):
            return Boxes(self.tensor[item].view(1, -1))
        b = self.tensor[item]
        assert b.dim() == 2, "Indexing on Boxes with {} failed to return a matrix!".format(item)
        return Boxes(b)

    def __len__(self) -> int:
        return self.tensor.shape[0]

    def __repr__(self) -> str:
        return "Boxes(" + str(self.tensor) + ")"

    def inside_box(self, box_size: BoxSizeType, boundary_threshold: int = 0) -> torch.Tensor:
        """
        Args:
            box_size (height, width): Size of the reference box.
            boundary_threshold (int): Boxes that extend beyond the reference box
                boundary by more than boundary_threshold are considered "outside".

        Returns:
            a binary vector, indicating whether each box is inside the reference box.
        """
        height, width = box_size
        inds_inside = (
            (self.tensor[..., 0] >= -boundary_threshold)
            & (self.tensor[..., 1] >= -boundary_threshold)
            & (self.tensor[..., 2] < width + boundary_threshold)
            & (self.tensor[..., 3] < height + boundary_threshold)
        )
        return inds_inside

    def get_centers(self) -> torch.Tensor:
        """
        Returns:
            The box centers in a Nx2 array of (x, y).
        """
        return (self.tensor[:, :2] + self.tensor[:, 2:]) / 2

    def scale(self, scale_x: float, scale_y: float) -> None:
        """
        Scale the box with horizontal and vertical scaling factors
        """
        self.tensor[:, 0::2] *= scale_x
        self.tensor[:, 1::2] *= scale_y

    @classmethod
    def cat(cls, boxes_list: List["Boxes"]) -> "Boxes":
        """
        Concatenates a list of Boxes into a single Boxes

        Arguments:
            boxes_list (list[Boxes])

        Returns:
            Boxes: the concatenated Boxes
        """
        assert isinstance(boxes_list, (list, tuple))
        assert all(isinstance(box, Boxes) for box in boxes_list)

        if len(boxes_list) == 0:
            return cls(torch.empty(0))

        cat_boxes = type(boxes_list[0])(cat([b.tensor for b in boxes_list], dim=0))
        return cat_boxes

    @property
    def device(self) -> torch.device:
        return self.tensor.device

    def __iter__(self) -> Iterator[torch.Tensor]:
        """
        Yield a box as a Tensor of shape (4,) at a time.
        """
        yield from self.tensor


# implementation from https://github.com/kuangliu/torchcv/blob/master/torchcv/utils/box.py
# with slight modifications
def pairwise_iou(boxes1: Boxes, boxes2: Boxes) -> torch.Tensor:
    """
    Given two lists of boxes of size N and M,
    compute the IoU (intersection over union)
    between __all__ N x M pairs of boxes.
    The box order must be (xmin, ymin, xmax, ymax).

    Args:
        boxes1,boxes2 (Boxes): two `Boxes`. Contains N & M boxes, respectively.

    Returns:
        Tensor: IoU, sized [N,M].
    """
    area1 = boxes1.area()
    area2 = boxes2.area()

    boxes1, boxes2 = boxes1.tensor, boxes2.tensor

    width_height = torch.min(boxes1[:, None, 2:], boxes2[:, 2:]) - torch.max(
        boxes1[:, None, :2], boxes2[:, :2]
    )  # [N,M,2]

    width_height.clamp_(min=0)  # [N,M,2]
    inter = width_height.prod(dim=2)  # [N,M]
    del width_height

    # handle empty boxes
    iou = torch.where(
        inter > 0,
        inter / (area1[:, None] + area2 - inter),
        torch.zeros(1, dtype=inter.dtype, device=inter.device),
    )
    return iou


def pairwise_ioa(gt: Boxes, boxes: Boxes, labels, ignore_label=-1) -> torch.Tensor:
    """
    Given two lists of boxes of size N and M,
    compute the IoU (intersection over union)
    between __all__ N x M pairs of boxes.
    The box order must be (xmin, ymin, xmax, ymax).

    Args:
        boxes1,boxes2 (Boxes): two `Boxes`. Contains N & M boxes, respectively.

    Returns:
        Tensor: IoU, sized [N,M].
    """
    area_boxes = boxes.area()

    gt, boxes = gt.tensor, boxes.tensor

    width_height = torch.min(gt[:, None, 2:], boxes[:, 2:]) - torch.max(
        gt[:, None, :2], boxes[:, :2]
    )  # [N,M,2]

    width_height.clamp_(min=0)  # [N,M,2]
    inter = width_height.prod(dim=2)  # [N,M]
    del width_height

    # handle empty boxes
    ioa = torch.where(
        inter > 0,
        inter / (area_boxes),
        torch.zeros(1, dtype=inter.dtype, device=inter.device),
    )
    gt_ignore_mask = labels.eq(ignore_label).unsqueeze(1)
    ioa *= gt_ignore_mask
    return ioa


def matched_boxlist_iou(boxes1: Boxes, boxes2: Boxes) -> torch.Tensor:
    """
    Compute pairwise intersection over union (IOU) of two sets of matched
    boxes. The box order must be (xmin, ymin, xmax, ymax).
    Similar to boxlist_iou, but computes only diagonal elements of the matrix
    Arguments:
        boxes1: (Boxes) bounding boxes, sized [N,4].
        boxes2: (Boxes) bounding boxes, sized [N,4].
    Returns:
        (tensor) iou, sized [N].
    """
    assert len(boxes1) == len(boxes2), (
        "boxlists should have the same"
        "number of entries, got {}, {}".format(len(boxes1), len(boxes2))
    )
    area1 = boxes1.area()  # [N]
    area2 = boxes2.area()  # [N]
    box1, box2 = boxes1.tensor, boxes2.tensor
    lt = torch.max(box1[:, :2], box2[:, :2])  # [N,2]
    rb = torch.min(box1[:, 2:], box2[:, 2:])  # [N,2]
    wh = (rb - lt).clamp(min=0)  # [N,2]
    inter = wh[:, 0] * wh[:, 1]  # [N]
    iou = inter / (area1 + area2 - inter)  # [N]
    return iou


# added for DETR
# TODO @wangfeng02, use BoxMode instead and provide a better func
def box_cxcywh_to_xyxy(x):
    x_c, y_c, w, h = x.unbind(-1)
    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]
    return torch.stack(b, dim=-1)


def box_xyxy_to_cxcywh(x):
    x0, y0, x1, y1 = x.unbind(-1)
    b = [(x0 + x1) / 2, (y0 + y1) / 2, (x1 - x0), (y1 - y0)]
    return torch.stack(b, dim=-1)


def generalized_box_iou(boxes1, boxes2):
    """
    Generalized IoU from https://giou.stanford.edu/
    The boxes should be in [x0, y0, x1, y1] format
    Returns a [N, M] pairwise matrix, where N = len(boxes1)
    and M = len(boxes2)
    """
    # degenerate boxes gives inf / nan results
    # so do an early check
    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()
    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()

    # vallina box iou
    # modified from torchvision to also return the union
    area1 = box_area(boxes1)
    area2 = box_area(boxes2)

    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]
    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]

    wh = (rb - lt).clamp(min=0)  # [N,M,2]
    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]

    union = area1[:, None] + area2 - inter
    iou = inter / union

    # iou, union = box_iou(boxes1, boxes2)

    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])
    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])

    wh = (rb - lt).clamp(min=0)  # [N,M,2]
    area = wh[:, :, 0] * wh[:, :, 1]

    return iou - (area - union) / area


def masks_to_boxes(masks):
    """
    Compute the bounding boxes around the provided masks
    The masks should be in format [N, H, W] where N is the number of masks,
    (H, W) are the spatial dimensions.
    Returns a [N, 4] tensors, with the boxes in xyxy format
    """
    if masks.numel() == 0:
        return torch.zeros((0, 4), device=masks.device)

    h, w = masks.shape[-2:]

    y = torch.arange(0, h, dtype=torch.float)
    x = torch.arange(0, w, dtype=torch.float)
    y, x = torch.meshgrid(y, x)

    x_mask = masks * x.unsqueeze(0)
    x_max = x_mask.flatten(1).max(-1)[0]
    x_min = x_mask.masked_fill(~(masks.bool()), 1e8).flatten(1).min(-1)[0]

    y_mask = masks * y.unsqueeze(0)
    y_max = y_mask.flatten(1).max(-1)[0]
    y_min = y_mask.masked_fill(~(masks.bool()), 1e8).flatten(1).min(-1)[0]

    return torch.stack([x_min, y_min, x_max, y_max], 1)
```

### cvpods/structures/keypoints.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from typing import Any, List, Tuple, Union

import numpy as np

import torch

from cvpods.layers import interpolate


class Keypoints:
    """
    Stores keypoint annotation data. GT Instances have a `gt_keypoints` property
    containing the x,y location and visibility flag of each keypoint. This tensor has shape
    (N, K, 3) where N is the number of instances and K is the number of keypoints per instance.

    The visibility flag follows the COCO format and must be one of three integers:
    * v=0: not labeled (in which case x=y=0)
    * v=1: labeled but not visible
    * v=2: labeled and visible
    """

    def __init__(self, keypoints: Union[torch.Tensor, np.ndarray, List[List[float]]]):
        """
        Arguments:
            keypoints: A Tensor, numpy array, or list of the x, y, and visibility of each keypoint.
            The shape should be (N, K, 3) where N is the number of
            instances, and K is the number of keypoints per instance.
        """
        device = keypoints.device if isinstance(keypoints, torch.Tensor) else torch.device("cpu")
        keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)
        assert keypoints.dim() == 3 and keypoints.shape[2] == 3, keypoints.shape
        self.tensor = keypoints

    def __len__(self) -> int:
        return self.tensor.size(0)

    def to(self, *args: Any, **kwargs: Any) -> "Keypoints":
        return type(self)(self.tensor.to(*args, **kwargs))

    @property
    def device(self) -> torch.device:
        return self.tensor.device

    def to_heatmap(self, boxes: torch.Tensor, heatmap_size: int) -> torch.Tensor:
        """
        Arguments:
            boxes: Nx4 tensor, the boxes to draw the keypoints to

        Returns:
            heatmaps:
                A tensor of shape (N, K) containing an integer spatial label
                in the range [0, heatmap_size**2 - 1] for each keypoint in the input.
            valid:
                A tensor of shape (N, K) containing whether each keypoint is in the roi or not.
        """
        return _keypoints_to_heatmap(self.tensor, boxes, heatmap_size)

    def __getitem__(self, item: Union[int, slice, torch.BoolTensor]) -> "Keypoints":
        """
        Create a new `Keypoints` by indexing on this `Keypoints`.

        The following usage are allowed:

        1. `new_kpts = kpts[3]`: return a `Keypoints` which contains only one instance.
        2. `new_kpts = kpts[2:10]`: return a slice of key points.
        3. `new_kpts = kpts[vector]`, where vector is a torch.ByteTensor
           with `length = len(kpts)`. Nonzero elements in the vector will be selected.

        Note that the returned Keypoints might share storage with this Keypoints,
        subject to Pytorch's indexing semantics.
        """
        if isinstance(item, int):
            return Keypoints([self.tensor[item]])
        return Keypoints(self.tensor[item])

    def __repr__(self) -> str:
        s = self.__class__.__name__ + "("
        s += "num_instances={})".format(len(self.tensor))
        return s


# TODO make this nicer, this is a direct translation from C2 (but removing the inner loop)
def _keypoints_to_heatmap(
    keypoints: torch.Tensor, rois: torch.Tensor, heatmap_size: int
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Encode keypoint locations into a target heatmap for use in SoftmaxWithLoss across space.

    Maps keypoints from the half-open interval [x1, x2) on continuous image coordinates to the
    closed interval [0, heatmap_size - 1] on discrete image coordinates. We use the
    continuous-discrete conversion from Heckbert 1990 ("What is the coordinate of a pixel?"):
    d = floor(c) and c = d + 0.5, where d is a discrete coordinate and c is a continuous coordinate.

    Arguments:
        keypoints: tensor of keypoint locations in of shape (N, K, 3).
        rois: Nx4 tensor of rois in xyxy format
        heatmap_size: integer side length of square heatmap.

    Returns:
        heatmaps: A tensor of shape (N, K) containing an integer spatial label
            in the range [0, heatmap_size**2 - 1] for each keypoint in the input.
        valid: A tensor of shape (N, K) containing whether each keypoint is in
            the roi or not.
    """

    if rois.numel() == 0:
        return rois.new().long(), rois.new().long()
    offset_x = rois[:, 0]
    offset_y = rois[:, 1]
    scale_x = heatmap_size / (rois[:, 2] - rois[:, 0])
    scale_y = heatmap_size / (rois[:, 3] - rois[:, 1])

    offset_x = offset_x[:, None]
    offset_y = offset_y[:, None]
    scale_x = scale_x[:, None]
    scale_y = scale_y[:, None]

    x = keypoints[..., 0]
    y = keypoints[..., 1]

    x_boundary_inds = x == rois[:, 2][:, None]
    y_boundary_inds = y == rois[:, 3][:, None]

    x = (x - offset_x) * scale_x
    x = x.floor().long()
    y = (y - offset_y) * scale_y
    y = y.floor().long()

    x[x_boundary_inds] = heatmap_size - 1
    y[y_boundary_inds] = heatmap_size - 1

    valid_loc = (x >= 0) & (y >= 0) & (x < heatmap_size) & (y < heatmap_size)
    vis = keypoints[..., 2] > 0
    valid = (valid_loc & vis).long()

    lin_ind = y * heatmap_size + x
    heatmaps = lin_ind * valid

    return heatmaps, valid


@torch.no_grad()
def heatmaps_to_keypoints(maps: torch.Tensor, rois: torch.Tensor) -> torch.Tensor:
    """
    Extract predicted keypoint locations from heatmaps.
    Args:
        maps (Tensor): (#ROIs, #keypoints, POOL_H, POOL_W). The predicted heatmap of logits for
        each ROI and each keypoint.
        rois (Tensor): (#ROIs, 4). The box of each ROI.

    Returns:
        Tensor of shape (#ROIs, #keypoints, 4) with the last dimension corresponding to
        (x, y, logit, score) for each keypoint.

    When converting discrete pixel indices in an NxN image to a continuous keypoint coordinate,
    we maintain consistency with :meth:`Keypoints.to_heatmap` by using the conversion from
    Heckbert 1990: c = d + 0.5, where d is a discrete coordinate and c is a continuous coordinate.
    """
    offset_x = rois[:, 0]
    offset_y = rois[:, 1]

    widths = (rois[:, 2] - rois[:, 0]).clamp(min=1)
    heights = (rois[:, 3] - rois[:, 1]).clamp(min=1)
    widths_ceil = widths.ceil()
    heights_ceil = heights.ceil()

    num_rois, num_keypoints = maps.shape[:2]
    xy_preds = maps.new_zeros(rois.shape[0], num_keypoints, 4)

    width_corrections = widths / widths_ceil
    height_corrections = heights / heights_ceil

    keypoints_idx = torch.arange(num_keypoints, device=maps.device)

    for i in range(num_rois):
        outsize = (int(heights_ceil[i]), int(widths_ceil[i]))
        roi_map = interpolate(maps[[i]], size=outsize, mode="bicubic", align_corners=False).squeeze(
            0
        )  # #keypoints x H x W

        # softmax over the spatial region
        max_score, _ = roi_map.view(num_keypoints, -1).max(1)
        max_score = max_score.view(num_keypoints, 1, 1)
        tmp_full_resolution = (roi_map - max_score).exp_()
        tmp_pool_resolution = (maps[i] - max_score).exp_()
        # Produce scores over the region H x W, but normalize with POOL_H x POOL_W,
        # so that the scores of objects of different absolute sizes will be more comparable
        roi_map_scores = tmp_full_resolution / tmp_pool_resolution.sum((1, 2), keepdim=True)

        w = roi_map.shape[2]
        pos = roi_map.view(num_keypoints, -1).argmax(1)

        x_int = pos % w
        y_int = (pos - x_int) // w

        assert (
            roi_map_scores[keypoints_idx, y_int, x_int]
            == roi_map_scores.view(num_keypoints, -1).max(1)[0]
        ).all()

        x = (x_int.float() + 0.5) * width_corrections[i]
        y = (y_int.float() + 0.5) * height_corrections[i]

        xy_preds[i, :, 0] = x + offset_x[i]
        xy_preds[i, :, 1] = y + offset_y[i]
        xy_preds[i, :, 2] = roi_map[keypoints_idx, y_int, x_int]
        xy_preds[i, :, 3] = roi_map_scores[keypoints_idx, y_int, x_int]

    return xy_preds
```

### cvpods/structures/masks.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import copy
import itertools
from typing import Iterator, List, Union

import numpy as np
import pycocotools.mask as mask_utils

import torch

from cvpods.layers import cat
from cvpods.layers.roi_align import ROIAlign

from .boxes import Boxes


def polygon_area(x, y):
    # Using the shoelace formula
    # https://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates
    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))


def polygons_to_bitmask(polygons: List[np.ndarray], height: int, width: int) -> np.ndarray:
    """
    Args:
        polygons (list[ndarray]): each array has shape (Nx2,)
        height, width (int)

    Returns:
        ndarray: a bool mask of shape (height, width)
    """
    assert len(polygons) > 0, "COCOAPI does not support empty polygons"
    rles = mask_utils.frPyObjects(polygons, height, width)
    rle = mask_utils.merge(rles)
    return mask_utils.decode(rle).astype(np.bool)


def rasterize_polygons_within_box(
    polygons: List[np.ndarray], box: np.ndarray, mask_size: int
) -> torch.Tensor:
    """
    Rasterize the polygons into a mask image and
    crop the mask content in the given box.
    The cropped mask is resized to (mask_size, mask_size).

    This function is used when generating training targets for mask head in Mask R-CNN.
    Given original ground-truth masks for an image, new ground-truth mask
    training targets in the size of `mask_size x mask_size`
    must be provided for each predicted box. This function will be called to
    produce such targets.

    Args:
        polygons (list[ndarray[float]]): a list of polygons, which represents an instance.
        box: 4-element numpy array
        mask_size (int):

    Returns:
        Tensor: BoolTensor of shape (mask_size, mask_size)
    """
    # 1. Shift the polygons w.r.t the boxes
    w, h = box[2] - box[0], box[3] - box[1]

    polygons = copy.deepcopy(polygons)
    for p in polygons:
        p[0::2] = p[0::2] - box[0]
        p[1::2] = p[1::2] - box[1]

    # 2. Rescale the polygons to the new box size
    ratio_h = mask_size / max(h, 0.1)
    ratio_w = mask_size / max(w, 0.1)

    if ratio_h == ratio_w:
        for p in polygons:
            p *= ratio_h
    else:
        for p in polygons:
            p[0::2] *= ratio_w
            p[1::2] *= ratio_h

    # 3. Rasterize the polygons with coco api
    mask = polygons_to_bitmask(polygons, mask_size, mask_size)
    mask = torch.from_numpy(mask)
    return mask


class BitMasks:
    """
    This class stores the segmentation masks for all objects in one image, in
    the form of bitmaps.

    Attributes:
        tensor: bool Tensor of N,H,W, representing N instances in the image.
    """

    def __init__(self, tensor: Union[torch.Tensor, np.ndarray]):
        """
        Args:
            tensor: bool Tensor of N,H,W, representing N instances in the image.
        """
        device = tensor.device if isinstance(tensor, torch.Tensor) else torch.device("cpu")
        tensor = torch.as_tensor(tensor, dtype=torch.bool, device=device)
        assert tensor.dim() == 3, tensor.size()
        self.image_size = tensor.shape[1:]
        self.tensor = tensor

    def to(self, device: str) -> "BitMasks":
        return BitMasks(self.tensor.to(device))

    @property
    def device(self) -> torch.device:
        return self.tensor.device

    def __getitem__(self, item: Union[int, slice, torch.BoolTensor]) -> "BitMasks":
        """
        Returns:
            BitMasks: Create a new :class:`BitMasks` by indexing.

        The following usage are allowed:

        1. `new_masks = masks[3]`: return a `BitMasks` which contains only one mask.
        2. `new_masks = masks[2:10]`: return a slice of masks.
        3. `new_masks = masks[vector]`, where vector is a torch.BoolTensor
           with `length = len(masks)`. Nonzero elements in the vector will be selected.

        Note that the returned object might share storage with this object,
        subject to Pytorch's indexing semantics.
        """
        if isinstance(item, int):
            return BitMasks(self.tensor[item].view(1, -1))
        m = self.tensor[item]
        assert m.dim() == 3, "Indexing on BitMasks with {} returns a tensor with shape {}!".format(
            item, m.shape
        )
        return BitMasks(m)

    def __iter__(self) -> torch.Tensor:
        yield from self.tensor

    def __repr__(self) -> str:
        s = self.__class__.__name__ + "("
        s += "num_instances={})".format(len(self.tensor))
        return s

    def __len__(self) -> int:
        return self.tensor.shape[0]

    def nonempty(self) -> torch.Tensor:
        """
        Find masks that are non-empty.

        Returns:
            Tensor: a BoolTensor which represents
                whether each mask is empty (False) or non-empty (True).
        """
        return self.tensor.flatten(1).any(dim=1)

    @staticmethod
    def from_polygon_masks(
        polygon_masks: Union["PolygonMasks", List[List[np.ndarray]]], height: int, width: int
    ) -> "BitMasks":
        """
        Args:
            polygon_masks (list[list[ndarray]] or PolygonMasks)
            height, width (int)
        """
        if isinstance(polygon_masks, PolygonMasks):
            polygon_masks = polygon_masks.polygons
        masks = [polygons_to_bitmask(p, height, width) for p in polygon_masks]
        return BitMasks(torch.stack([torch.from_numpy(x) for x in masks]))

    def crop_and_resize(self, boxes: torch.Tensor, mask_size: int) -> torch.Tensor:
        """
        Crop each bitmask by the given box, and resize results to (mask_size, mask_size).
        This can be used to prepare training targets for Mask R-CNN.
        It has less reconstruction error compared to rasterization with polygons.
        However we observe no difference in accuracy,
        but BitMasks requires more memory to store all the masks.

        Args:
            boxes (Tensor): Nx4 tensor storing the boxes for each mask
            mask_size (int): the size of the rasterized mask.

        Returns:
            Tensor:
                A bool tensor of shape (N, mask_size, mask_size), where
                N is the number of predicted boxes for this image.
        """
        assert len(boxes) == len(self), "{} != {}".format(len(boxes), len(self))
        device = self.tensor.device

        batch_inds = torch.arange(len(boxes), device=device).to(dtype=boxes.dtype)[:, None]
        rois = torch.cat([batch_inds, boxes], dim=1)  # Nx5

        bit_masks = self.tensor.to(dtype=torch.float32)
        rois = rois.to(device=device)
        output = (
            ROIAlign((mask_size, mask_size), 1.0, 0, aligned=True)
            .forward(bit_masks[:, None, :, :], rois)
            .squeeze(1)
        )
        output = output >= 0.5
        return output

    def get_bounding_boxes(self) -> None:
        """
        Returns:
            Boxes: tight bounding boxes around bit masks.
        """
        # TODO Make this method faster
        boxes = torch.zeros(len(self.tensor), 4, dtype=torch.float32)
        shift_y, shift_x = torch.meshgrid(torch.arange(self.tensor.shape[-2]),
                                          torch.arange(self.tensor.shape[-1]))
        for idx in range(len(self.tensor)):
            bitmask = self.tensor[idx]
            if bitmask.sum() == 0:
                continue
            row_mask = shift_y[bitmask]
            col_mask = shift_x[bitmask]
            boxes[idx, 0] = col_mask.min()
            boxes[idx, 1] = row_mask.min()
            boxes[idx, 2] = col_mask.max()
            boxes[idx, 3] = row_mask.max()
        return Boxes(boxes)

    @staticmethod
    def cat(bitmasks_list: List["BitMasks"]) -> "BitMasks":
        """
        Concatenates a list of BitMasks into a single BitMasks
        Arguments:
            bitmasks_list (list[BitMasks])
        Returns:
            BitMasks: the concatenated BitMasks
        """
        assert isinstance(bitmasks_list, (list, tuple))
        assert len(bitmasks_list) > 0
        assert all(isinstance(bitmask, BitMasks) for bitmask in bitmasks_list)

        cat_bitmasks = type(bitmasks_list[0])(cat([bm.tensor for bm in bitmasks_list], dim=0))
        return cat_bitmasks


class PolygonMasks:
    """
    This class stores the segmentation masks for all objects in one image, in the form of polygons.

    Attributes:
        polygons: list[list[ndarray]]. Each ndarray is a float64 vector representing a polygon.
    """

    def __init__(self, polygons: List[List[Union[torch.Tensor, np.ndarray]]]):
        """
        Arguments:
            polygons (list[list[Tensor[float]]]): The first
                level of the list correspond to individual instances,
                the second level to all the polygons that compose the
                instance, and the third level to the polygon coordinates.
                The third level Tensor should have the format of
                torch.Tensor([x0, y0, x1, y1, ..., xn, yn]) (n >= 3).
        """
        assert isinstance(polygons, list), (
            "Cannot create PolygonMasks: Expect a list of list of polygons per image. "
            "Got '{}' instead.".format(type(polygons))
        )

        def _make_array(t: Union[torch.Tensor, np.ndarray]) -> np.ndarray:
            # Use float64 for higher precision, because why not?
            # Always put polygons on CPU (self.to is a no-op) since they
            # are supposed to be small tensors.
            # May need to change this assumption if GPU placement becomes useful
            if isinstance(t, torch.Tensor):
                t = t.cpu().numpy()
            return np.asarray(t).astype("float64")

        def process_polygons(
            polygons_per_instance: List[Union[torch.Tensor, np.ndarray]]
        ) -> List[torch.Tensor]:
            assert isinstance(polygons_per_instance, list), (
                "Cannot create polygons: Expect a list of polygons per instance. "
                "Got '{}' instead.".format(type(polygons_per_instance))
            )
            # transform the polygon to a tensor
            polygons_per_instance = [_make_array(p) for p in polygons_per_instance]
            for polygon in polygons_per_instance:
                assert len(polygon) % 2 == 0 and len(polygon) >= 6
            return polygons_per_instance

        self.polygons: List[List[torch.Tensor]] = [
            process_polygons(polygons_per_instance) for polygons_per_instance in polygons
        ]

    # pylint: disable=W0613
    def to(self, device: str) -> "PolygonMasks":
        return self

    @property
    def device(self) -> torch.device:
        return self.tensor.device

    def get_bounding_boxes(self) -> Boxes:
        """
        Returns:
            Boxes: tight bounding boxes around polygon masks.
        """
        boxes = torch.zeros(len(self.polygons), 4, dtype=torch.float32)
        for idx, polygons_per_instance in enumerate(self.polygons):
            minxy = torch.as_tensor([float("inf"), float("inf")], dtype=torch.float32)
            maxxy = torch.zeros(2, dtype=torch.float32)
            for polygon in polygons_per_instance:
                coords = torch.from_numpy(polygon).view(-1, 2).to(dtype=torch.float32)
                minxy = torch.min(minxy, torch.min(coords, dim=0).values)
                maxxy = torch.max(maxxy, torch.max(coords, dim=0).values)
            boxes[idx, :2] = minxy
            boxes[idx, 2:] = maxxy
        return Boxes(boxes)

    def nonempty(self) -> torch.Tensor:
        """
        Find masks that are non-empty.

        Returns:
            Tensor:
                a BoolTensor which represents whether each mask is empty (False) or not (True).
        """
        keep = [1 if len(polygon) > 0 else 0 for polygon in self.polygons]
        return torch.as_tensor(keep, dtype=torch.bool)

    def __getitem__(self, item: Union[int, slice, List[int], torch.BoolTensor]) -> "PolygonMasks":
        """
        Support indexing over the instances and return a `PolygonMasks` object.
        `item` can be:

        1. An integer. It will return an object with only one instance.
        2. A slice. It will return an object with the selected instances.
        3. A list[int]. It will return an object with the selected instances,
           correpsonding to the indices in the list.
        4. A vector mask of type BoolTensor, whose length is num_instances.
           It will return an object with the instances whose mask is nonzero.
        """
        if isinstance(item, int):
            selected_polygons = [self.polygons[item]]
        elif isinstance(item, slice):
            selected_polygons = self.polygons[item]
        elif isinstance(item, list):
            selected_polygons = [self.polygons[i] for i in item]
        elif isinstance(item, torch.Tensor):
            # Polygons is a list, so we have to move the indices back to CPU.
            if item.dtype == torch.bool:
                assert item.dim() == 1, item.shape
                item = item.nonzero(as_tuple=False).squeeze(1).cpu().numpy().tolist()
            elif item.dtype in [torch.int32, torch.int64]:
                item = item.cpu().numpy().tolist()
            else:
                raise ValueError("Unsupported tensor dtype={} for indexing!".format(item.dtype))
            selected_polygons = [self.polygons[i] for i in item]
        return PolygonMasks(selected_polygons)

    def __iter__(self) -> Iterator[List[torch.Tensor]]:
        """
        Yields:
            list[ndarray]: the polygons for one instance.
            Each Tensor is a float64 vector representing a polygon.
        """
        return iter(self.polygons)

    def __repr__(self) -> str:
        s = self.__class__.__name__ + "("
        s += "num_instances={})".format(len(self.polygons))
        return s

    def __len__(self) -> int:
        return len(self.polygons)

    def crop_and_resize(self, boxes: torch.Tensor, mask_size: int) -> torch.Tensor:
        """
        Crop each mask by the given box, and resize results to (mask_size, mask_size).
        This can be used to prepare training targets for Mask R-CNN.

        Args:
            boxes (Tensor): Nx4 tensor storing the boxes for each mask
            mask_size (int): the size of the rasterized mask.

        Returns:
            Tensor: A bool tensor of shape (N, mask_size, mask_size), where
            N is the number of predicted boxes for this image.
        """
        assert len(boxes) == len(self), "{} != {}".format(len(boxes), len(self))

        device = boxes.device
        # Put boxes on the CPU, as the polygon representation is not efficient GPU-wise
        # (several small tensors for representing a single instance mask)
        boxes = boxes.to(torch.device("cpu"))

        results = [
            rasterize_polygons_within_box(poly, box.numpy(), mask_size)
            for poly, box in zip(self.polygons, boxes)
        ]
        """
        poly: list[list[float]], the polygons for one instance
        box: a tensor of shape (4,)
        """
        if len(results) == 0:
            return torch.empty(0, mask_size, mask_size, dtype=torch.bool, device=device)
        return torch.stack(results, dim=0).to(device=device)

    def area(self):
        """
        Computes area of the mask.
        Only works with Polygons, using the shoelace formula:
        https://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates
        Returns:
            Tensor: a vector, area for each instance
        """

        area = []
        for polygons_per_instance in self.polygons:
            area_per_instance = 0
            for p in polygons_per_instance:
                area_per_instance += polygon_area(p[0::2], p[1::2])
            area.append(area_per_instance)

        return torch.tensor(area)

    @staticmethod
    def cat(polymasks_list: List["PolygonMasks"]) -> "PolygonMasks":
        """
        Concatenates a list of PolygonMasks into a single PolygonMasks
        Arguments:
            polymasks_list (list[PolygonMasks])
        Returns:
            PolygonMasks: the concatenated PolygonMasks
        """
        assert isinstance(polymasks_list, (list, tuple))
        assert len(polymasks_list) > 0
        assert all(isinstance(polymask, PolygonMasks) for polymask in polymasks_list)

        cat_polymasks = type(polymasks_list[0])(
            list(itertools.chain.from_iterable(pm.polygons for pm in polymasks_list))
        )
        return cat_polymasks
```

### cvpods/structures/image_list.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
from __future__ import division
from typing import Any, List, Sequence, Tuple, Union

import torch
from torch.nn import functional as F


class ImageList(object):
    """
    Structure that holds a list of images (of possibly
    varying sizes) as a single tensor.
    This works by padding the images to the same size,
    and storing in a field the original sizes of each image

    Attributes:
        image_sizes (list[tuple[int, int]]): each tuple is (h, w)
    """

    def __init__(self, tensor: torch.Tensor, image_sizes: List[Tuple[int, int]]):
        """
        Arguments:
            tensor (Tensor): of shape (N, H, W) or (N, C_1, ..., C_K, H, W) where K >= 1
            image_sizes (list[tuple[int, int]]): Each tuple is (h, w).
        """
        self.tensor = tensor
        self.image_sizes = image_sizes

    def __len__(self) -> int:
        return len(self.image_sizes)

    def __getitem__(self, idx: Union[int, slice]) -> torch.Tensor:
        """
        Access the individual image in its original size.

        Returns:
            Tensor: an image of shape (H, W) or (C_1, ..., C_K, H, W) where K >= 1
        """
        size = self.image_sizes[idx]
        return self.tensor[idx, ..., : size[0], : size[1]]  # type: ignore

    def to(self, *args: Any, **kwargs: Any) -> "ImageList":
        cast_tensor = self.tensor.to(*args, **kwargs)
        return ImageList(cast_tensor, self.image_sizes)

    @property
    def device(self) -> torch.device:
        return self.tensor.device

    @staticmethod
    def from_tensors(
        tensors: Sequence[torch.Tensor],
        size_divisibility: int = 0,
        pad_ref_long: bool = False,
        pad_value: float = 0.0,
    ) -> "ImageList":
        """
        Args:
            tensors: a tuple or list of `torch.Tensors`, each of shape (Hi, Wi) or
                (C_1, ..., C_K, Hi, Wi) where K >= 1. The Tensors will be padded with `pad_value`
                so that they will have the same shape.
            size_divisibility (int): If `size_divisibility > 0`, also adds padding to ensure
                the common height and width is divisible by `size_divisibility`
            pad_value (float): value to pad

        Returns:
            an `ImageList`.
        """
        assert len(tensors) > 0
        assert isinstance(tensors, (tuple, list))
        for t in tensors:
            assert isinstance(t, torch.Tensor), type(t)
            assert t.shape[1:-2] == tensors[0].shape[1:-2], t.shape
        # per dimension maximum (H, W) or (C_1, ..., C_K, H, W) where K >= 1 among all tensors
        max_size = list(max(s) for s in zip(*[img.shape for img in tensors]))
        if pad_ref_long:
            max_size_max = max(max_size[-2:])
            max_size[-2:] = [max_size_max] * 2
        max_size = tuple(max_size)

        if size_divisibility > 0:
            import math

            stride = size_divisibility
            max_size = list(max_size)  # type: ignore
            max_size[-2] = int(math.ceil(max_size[-2] / stride) * stride)  # type: ignore
            max_size[-1] = int(math.ceil(max_size[-1] / stride) * stride)  # type: ignore
            max_size = tuple(max_size)

        image_sizes = [im.shape[-2:] for im in tensors]

        if len(tensors) == 1:
            # This seems slightly (2%) faster.
            # TODO: check whether it's faster for multiple images as well
            image_size = image_sizes[0]
            padding_size = [0, max_size[-1] - image_size[1], 0, max_size[-2] - image_size[0]]
            if all(x == 0 for x in padding_size):  # https://github.com/pytorch/pytorch/issues/31734
                batched_imgs = tensors[0].unsqueeze(0)
            else:
                padded = F.pad(tensors[0], padding_size, value=pad_value)
                batched_imgs = padded.unsqueeze_(0)
        else:
            batch_shape = (len(tensors),) + max_size
            batched_imgs = tensors[0].new_full(batch_shape, pad_value)
            for img, pad_img in zip(tensors, batched_imgs):
                pad_img[..., : img.shape[-2], : img.shape[-1]].copy_(img)

        return ImageList(batched_imgs.contiguous(), image_sizes)
```

### cvpods/structures/rotated_boxes.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import math
from typing import Iterator, List, Union

import torch

from cvpods.layers import cat
from cvpods.layers.rotated_boxes import pairwise_iou_rotated

from .boxes import Boxes


class RotatedBoxes(Boxes):
    """
    This structure stores a list of rotated boxes as a Nx5 torch.Tensor.
    It supports some common methods about boxes
    (`area`, `clip`, `nonempty`, etc),
    and also behaves like a Tensor
    (support indexing, `to(device)`, `.device`, and iteration over all boxes)
    """

    def __init__(self, tensor: torch.Tensor):
        """
        Args:
            tensor (Tensor[float]): a Nx5 matrix.  Each row is
                (x_center, y_center, width, height, angle),
                in which angle is represented in degrees.
                While there's no strict range restriction for it,
                the recommended principal range is between [-180, 180) degrees.

        Assume we have a horizontal box B = (x_center, y_center, width, height),
        where width is along the x-axis and height is along the y-axis.
        The rotated box B_rot (x_center, y_center, width, height, angle)
        can be seen as:

        1. When angle == 0:
           B_rot == B
        2. When angle > 0:
           B_rot is obtained by rotating B w.r.t its center by :math:`|angle|` degrees CCW;
        3. When angle < 0:
           B_rot is obtained by rotating B w.r.t its center by :math:`|angle|` degrees CW.

        Mathematically, since the right-handed coordinate system for image space
        is (y, x), where y is top->down and x is left->right, the 4 vertices of the
        rotated rectangle :math:`(yr_i, xr_i)` (i = 1, 2, 3, 4) can be obtained from
        the vertices of the horizontal rectangle (y_i, x_i) (i = 1, 2, 3, 4)
        in the following way (:math:`\\theta = angle*\\pi/180` is the angle in radians,
        (y_c, x_c) is the center of the rectangle):

        .. math::

            yr_i = \\cos(\\theta) (y_i - y_c) - \\sin(\\theta) (x_i - x_c) + y_c,

            xr_i = \\sin(\\theta) (y_i - y_c) + \\cos(\\theta) (x_i - x_c) + x_c,

        which is the standard rigid-body rotation transformation.

        Intuitively, the angle is
        (1) the rotation angle from y-axis in image space
        to the height vector (top->down in the box's local coordinate system)
        of the box in CCW, and
        (2) the rotation angle from x-axis in image space
        to the width vector (left->right in the box's local coordinate system)
        of the box in CCW.

        More intuitively, consider the following horizontal box ABCD represented
        in (x1, y1, x2, y2): (3, 2, 7, 4),
        covering the [3, 7] x [2, 4] region of the continuous coordinate system
        which looks like this:

        .. code:: none

            O--------> x
            |
            |  A---B
            |  |   |
            |  D---C
            |
            v y

        Note that each capital letter represents one 0-dimensional geometric point
        instead of a 'square pixel' here.

        In the example above, using (x, y) to represent a point we have:

        .. math::

            O = (0, 0), A = (3, 2), B = (7, 2), C = (7, 4), D = (3, 4)

        We name vector AB = vector DC as the width vector in box's local coordinate system, and
        vector AD = vector BC as the height vector in box's local coordinate system. Initially,
        when angle = 0 degree, they're aligned with the positive directions of x-axis and y-axis
        in the image space, respectively.

        For better illustration, we denote the center of the box as E,

        .. code:: none

            O--------> x
            |
            |  A---B
            |  | E |
            |  D---C
            |
            v y

        where the center E = ((3+7)/2, (2+4)/2) = (5, 3).

        Also,

        .. math::

            width = |AB| = |CD| = 7 - 3 = 4,
            height = |AD| = |BC| = 4 - 2 = 2.

        Therefore, the corresponding representation for the same shape in rotated box in
        (x_center, y_center, width, height, angle) format is:

        (5, 3, 4, 2, 0),

        Now, let's consider (5, 3, 4, 2, 90), which is rotated by 90 degrees
        CCW (counter-clockwise) by definition. It looks like this:

        .. code:: none

            O--------> x
            |   B-C
            |   | |
            |   |E|
            |   | |
            |   A-D
            v y

        The center E is still located at the same point (5, 3), while the vertices
        ABCD are rotated by 90 degrees CCW with regard to E:
        A = (4, 5), B = (4, 1), C = (6, 1), D = (6, 5)

        Here, 90 degrees can be seen as the CCW angle to rotate from y-axis to
        vector AD or vector BC (the top->down height vector in box's local coordinate system),
        or the CCW angle to rotate from x-axis to vector AB or vector DC (the left->right
        width vector in box's local coordinate system).

        .. math::

            width = |AB| = |CD| = 5 - 1 = 4,
            height = |AD| = |BC| = 6 - 4 = 2.

        Next, how about (5, 3, 4, 2, -90), which is rotated by 90 degrees CW (clockwise)
        by definition? It looks like this:

        .. code:: none

            O--------> x
            |   D-A
            |   | |
            |   |E|
            |   | |
            |   C-B
            v y

        The center E is still located at the same point (5, 3), while the vertices
        ABCD are rotated by 90 degrees CW with regard to E:
        A = (6, 1), B = (6, 5), C = (4, 5), D = (4, 1)

        .. math::

            width = |AB| = |CD| = 5 - 1 = 4,
            height = |AD| = |BC| = 6 - 4 = 2.

        This covers exactly the same region as (5, 3, 4, 2, 90) does, and their IoU
        will be 1. However, these two will generate different RoI Pooling results and
        should not be treated as an identical box.

        On the other hand, it's easy to see that (X, Y, W, H, A) is identical to
        (X, Y, W, H, A+360N), for any integer N. For example (5, 3, 4, 2, 270) would be
        identical to (5, 3, 4, 2, -90), because rotating the shape 270 degrees CCW is
        equivalent to rotating the same shape 90 degrees CW.

        We could rotate further to get (5, 3, 4, 2, 180), or (5, 3, 4, 2, -180):

        .. code:: none

            O--------> x
            |
            |  C---D
            |  | E |
            |  B---A
            |
            v y

        .. math::

            A = (7, 4), B = (3, 4), C = (3, 2), D = (7, 2),

            width = |AB| = |CD| = 7 - 3 = 4,
            height = |AD| = |BC| = 4 - 2 = 2.

        Finally, this is a very inaccurate (heavily quantized) illustration of
        how (5, 3, 4, 2, 60) looks like in case anyone wonders:

        .. code:: none

            O--------> x
            |     B\
            |    /  C
            |   /E /
            |  A  /
            |   `D
            v y

        It's still a rectangle with center of (5, 3), width of 4 and height of 2,
        but its angle (and thus orientation) is somewhere between
        (5, 3, 4, 2, 0) and (5, 3, 4, 2, 90).
        """
        device = tensor.device if isinstance(tensor, torch.Tensor) else torch.device("cpu")
        tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)
        if tensor.numel() == 0:
            tensor = torch.zeros(0, 5, dtype=torch.float32, device=device)
        assert tensor.dim() == 2 and tensor.size(-1) == 5, tensor.size()

        self.tensor = tensor

    def clone(self) -> "RotatedBoxes":
        """
        Clone the RotatedBoxes.

        Returns:
            RotatedBoxes
        """
        return RotatedBoxes(self.tensor.clone())

    def to(self, device: str) -> "RotatedBoxes":
        return RotatedBoxes(self.tensor.to(device))

    def area(self) -> torch.Tensor:
        """
        Computes the area of all the boxes.

        Returns:
            torch.Tensor: a vector with areas of each box.
        """
        box = self.tensor
        area = box[:, 2] * box[:, 3]
        return area

    def normalize_angles(self) -> None:
        """
        Restrict angles to the range of [-180, 180) degrees
        """
        self.tensor[:, 4] = (self.tensor[:, 4] + 180.0) % 360.0 - 180.0

    def clip(self, box_size: Boxes.BoxSizeType, clip_angle_threshold: float = 1.0) -> None:
        """
        Clip (in place) the boxes by limiting x coordinates to the range [0, width]
        and y coordinates to the range [0, height].

        For RRPN:
        Only clip boxes that are almost horizontal with a tolerance of
        clip_angle_threshold to maintain backward compatibility.

        Rotated boxes beyond this threshold are not clipped for two reasons:

        1. There are potentially multiple ways to clip a rotated box to make it
           fit within the image.
        2. It's tricky to make the entire rectangular box fit within the image
           and still be able to not leave out pixels of interest.

        Therefore we rely on ops like RoIAlignRotated to safely handle this.

        Args:
            box_size (height, width): The clipping box's size.
            clip_angle_threshold:
                Iff. abs(normalized(angle)) <= clip_angle_threshold (in degrees),
                we do the clipping as horizontal boxes.
        """
        h, w = box_size

        # normalize angles to be within (-180, 180] degrees
        self.normalize_angles()

        idx = torch.where(torch.abs(self.tensor[:, 4]) <= clip_angle_threshold)[0]

        # convert to (x1, y1, x2, y2)
        x1 = self.tensor[idx, 0] - self.tensor[idx, 2] / 2.0
        y1 = self.tensor[idx, 1] - self.tensor[idx, 3] / 2.0
        x2 = self.tensor[idx, 0] + self.tensor[idx, 2] / 2.0
        y2 = self.tensor[idx, 1] + self.tensor[idx, 3] / 2.0

        # clip
        x1.clamp_(min=0, max=w)
        y1.clamp_(min=0, max=h)
        x2.clamp_(min=0, max=w)
        y2.clamp_(min=0, max=h)

        # convert back to (xc, yc, w, h)
        self.tensor[idx, 0] = (x1 + x2) / 2.0
        self.tensor[idx, 1] = (y1 + y2) / 2.0
        # make sure widths and heights do not increase due to numerical errors
        self.tensor[idx, 2] = torch.min(self.tensor[idx, 2], x2 - x1)
        self.tensor[idx, 3] = torch.min(self.tensor[idx, 3], y2 - y1)

    def nonempty(self, threshold: int = 0) -> torch.Tensor:
        """
        Find boxes that are non-empty.
        A box is considered empty, if either of its side is no larger than threshold.

        Returns:
            Tensor: a binary vector which represents
            whether each box is empty (False) or non-empty (True).
        """
        box = self.tensor
        widths = box[:, 2]
        heights = box[:, 3]
        keep = (widths > threshold) & (heights > threshold)
        return keep

    def __getitem__(self, item: Union[int, slice, torch.BoolTensor]) -> "RotatedBoxes":
        """
        Returns:
            RotatedBoxes: Create a new :class:`RotatedBoxes` by indexing.

        The following usage are allowed:

        1. `new_boxes = boxes[3]`: return a `RotatedBoxes` which contains only one box.
        2. `new_boxes = boxes[2:10]`: return a slice of boxes.
        3. `new_boxes = boxes[vector]`, where vector is a torch.ByteTensor
           with `length = len(boxes)`. Nonzero elements in the vector will be selected.

        Note that the returned RotatedBoxes might share storage with this RotatedBoxes,
        subject to Pytorch's indexing semantics.
        """
        if isinstance(item, int):
            return RotatedBoxes(self.tensor[item].view(1, -1))
        b = self.tensor[item]
        assert b.dim() == 2, "Indexing on RotatedBoxes with {} failed to return a matrix!".format(
            item
        )
        return RotatedBoxes(b)

    def __len__(self) -> int:
        return self.tensor.shape[0]

    def __repr__(self) -> str:
        return "RotatedBoxes(" + str(self.tensor) + ")"

    def inside_box(self, box_size: Boxes.BoxSizeType, boundary_threshold: int = 0) -> torch.Tensor:
        """
        Args:
            box_size (height, width): Size of the reference box covering
                [0, width] x [0, height]
            boundary_threshold (int): Boxes that extend beyond the reference box
                boundary by more than boundary_threshold are considered "outside".

        For RRPN, it might not be necessary to call this function since it's common
        for rotated box to extend to outside of the image boundaries
        (the clip function only clips the near-horizontal boxes)

        Returns:
            a binary vector, indicating whether each box is inside the reference box.
        """
        height, width = box_size

        cnt_x = self.tensor[..., 0]
        cnt_y = self.tensor[..., 1]
        half_w = self.tensor[..., 2] / 2.0
        half_h = self.tensor[..., 3] / 2.0
        a = self.tensor[..., 4]
        c = torch.abs(torch.cos(a * math.pi / 180.0))
        s = torch.abs(torch.sin(a * math.pi / 180.0))
        # This basically computes the horizontal bounding rectangle of the rotated box
        max_rect_dx = c * half_w + s * half_h
        max_rect_dy = c * half_h + s * half_w

        inds_inside = (
            (cnt_x - max_rect_dx >= -boundary_threshold)
            & (cnt_y - max_rect_dy >= -boundary_threshold)
            & (cnt_x + max_rect_dx < width + boundary_threshold)
            & (cnt_y + max_rect_dy < height + boundary_threshold)
        )

        return inds_inside

    def get_centers(self) -> torch.Tensor:
        """
        Returns:
            The box centers in a Nx2 array of (x, y).
        """
        return self.tensor[:, :2]

    def scale(self, scale_x: float, scale_y: float) -> None:
        """
        Scale the rotated box with horizontal and vertical scaling factors
        Note: when scale_factor_x != scale_factor_y,
        the rotated box does not preserve the rectangular shape when the angle
        is not a multiple of 90 degrees under resize transformation.
        Instead, the shape is a parallelogram (that has skew)
        Here we make an approximation by fitting a rotated rectangle to the parallelogram.
        """
        self.tensor[:, 0] *= scale_x
        self.tensor[:, 1] *= scale_y
        theta = self.tensor[:, 4] * math.pi / 180.0
        c = torch.cos(theta)
        s = torch.sin(theta)

        # In image space, y is top->down and x is left->right
        # Consider the local coordintate system for the rotated box,
        # where the box center is located at (0, 0), and the four vertices ABCD are
        # A(-w / 2, -h / 2), B(w / 2, -h / 2), C(w / 2, h / 2), D(-w / 2, h / 2)
        # the midpoint of the left edge AD of the rotated box E is:
        # E = (A+D)/2 = (-w / 2, 0)
        # the midpoint of the top edge AB of the rotated box F is:
        # F(0, -h / 2)
        # To get the old coordinates in the global system, apply the rotation transformation
        # (Note: the right-handed coordinate system for image space is yOx):
        # (old_x, old_y) = (s * y + c * x, c * y - s * x)
        # E(old) = (s * 0 + c * (-w/2), c * 0 - s * (-w/2)) = (-c * w / 2, s * w / 2)
        # F(old) = (s * (-h / 2) + c * 0, c * (-h / 2) - s * 0) = (-s * h / 2, -c * h / 2)
        # After applying the scaling factor (sfx, sfy):
        # E(new) = (-sfx * c * w / 2, sfy * s * w / 2)
        # F(new) = (-sfx * s * h / 2, -sfy * c * h / 2)
        # The new width after scaling tranformation becomes:

        # w(new) = |E(new) - O| * 2
        #        = sqrt[(sfx * c * w / 2)^2 + (sfy * s * w / 2)^2] * 2
        #        = sqrt[(sfx * c)^2 + (sfy * s)^2] * w
        # i.e., scale_factor_w = sqrt[(sfx * c)^2 + (sfy * s)^2]
        #
        # For example,
        # when angle = 0 or 180, |c| = 1, s = 0, scale_factor_w == scale_factor_x;
        # when |angle| = 90, c = 0, |s| = 1, scale_factor_w == scale_factor_y
        self.tensor[:, 2] *= torch.sqrt((scale_x * c) ** 2 + (scale_y * s) ** 2)

        # h(new) = |F(new) - O| * 2
        #        = sqrt[(sfx * s * h / 2)^2 + (sfy * c * h / 2)^2] * 2
        #        = sqrt[(sfx * s)^2 + (sfy * c)^2] * h
        # i.e., scale_factor_h = sqrt[(sfx * s)^2 + (sfy * c)^2]
        #
        # For example,
        # when angle = 0 or 180, |c| = 1, s = 0, scale_factor_h == scale_factor_y;
        # when |angle| = 90, c = 0, |s| = 1, scale_factor_h == scale_factor_x
        self.tensor[:, 3] *= torch.sqrt((scale_x * s) ** 2 + (scale_y * c) ** 2)

        # The angle is the rotation angle from y-axis in image space to the height
        # vector (top->down in the box's local coordinate system) of the box in CCW.
        #
        # angle(new) = angle_yOx(O - F(new))
        #            = angle_yOx( (sfx * s * h / 2, sfy * c * h / 2) )
        #            = atan2(sfx * s * h / 2, sfy * c * h / 2)
        #            = atan2(sfx * s, sfy * c)
        #
        # For example,
        # when sfx == sfy, angle(new) == atan2(s, c) == angle(old)
        self.tensor[:, 4] = torch.atan2(scale_x * s, scale_y * c) * 180 / math.pi

    @classmethod
    def cat(cls, boxes_list: List["RotatedBoxes"]) -> "RotatedBoxes":  # type: ignore
        """
        Concatenates a list of RotatedBoxes into a single RotatedBoxes

        Arguments:
            boxes_list (list[RotatedBoxes])

        Returns:
            RotatedBoxes: the concatenated RotatedBoxes
        """
        assert isinstance(boxes_list, (list, tuple))

        if len(boxes_list) == 0:
            return cls(torch.empty(0))

        assert all(isinstance(box, RotatedBoxes) for box in boxes_list)

        cat_boxes = type(boxes_list[0])(cat([b.tensor for b in boxes_list], dim=0))
        return cat_boxes

    @property
    def device(self) -> str:
        return self.tensor.device

    def __iter__(self) -> Iterator[torch.Tensor]:
        """
        Yield a box as a Tensor of shape (5,) at a time.
        """
        yield from self.tensor


def pairwise_iou(boxes1: RotatedBoxes, boxes2: RotatedBoxes) -> None:
    """
    Given two lists of rotated boxes of size N and M,
    compute the IoU (intersection over union)
    between __all__ N x M pairs of boxes.
    The box order must be (x_center, y_center, width, height, angle).

    Args:
        boxes1, boxes2 (RotatedBoxes):
            two `RotatedBoxes`. Contains N & M rotated boxes, respectively.

    Returns:
        Tensor: IoU, sized [N,M].
    """

    return pairwise_iou_rotated(boxes1.tensor, boxes2.tensor)
```

### cvpods/utils/memory.py

```python
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.

import logging
from contextlib import contextmanager
from functools import wraps

import torch

__all__ = ["retry_if_cuda_oom"]


@contextmanager
def _ignore_torch_cuda_oom():
    """
    A context which ignores CUDA OOM exception from pytorch.
    """
    try:
        yield
    except RuntimeError as e:
        # NOTE: the string may change?
        if "CUDA out of memory. " in str(e):
            pass
        else:
            raise


def retry_if_cuda_oom(func):
    r"""
    Makes a function retry itself after encountering
    pytorch's CUDA OOM error.
    It will first retry after calling `torch.cuda.empty_cache()`.

    If that still fails, it will then retry by trying to convert inputs to CPUs.
    In this case, it expects the function to dispatch to CPU implementation.
    The return values may become CPU tensors as well and it's user's
    responsibility to convert it back to CUDA tensor if needed.

    Args:
        func: a stateless callable that takes tensor-like objects as arguments

    Returns:
        a callable which retries `func` if OOM is encountered.

    Examples:

    .. code-block:: python

        output = retry_if_cuda_oom(some_torch_function)(input1, input2)
        # output may be on CPU even if inputs are on GPU

    Note:
        1. When converting inputs to CPU, it will only look at each argument and check
           if it has `.device` and `.to` for conversion. Nested structures of tensors
           are not supported.

        2. Since the function might be called more than once, it has to be
           stateless.
    """

    def maybe_to_cpu(x):
        try:
            like_gpu_tensor = x.device.type == "cuda" and hasattr(x, "to")
        except AttributeError:
            like_gpu_tensor = False
        if like_gpu_tensor:
            return x.to(device="cpu")
        else:
            return x

    @wraps(func)
    def wrapped(*args, **kwargs):
        with _ignore_torch_cuda_oom():
            return func(*args, **kwargs)

        # Clear cache and retry
        torch.cuda.empty_cache()
        with _ignore_torch_cuda_oom():
            return func(*args, **kwargs)

        # Try on CPU. This slows down the code significantly, therefore print a notice.
        logger = logging.getLogger(__name__)
        logger.info("Attempting to copy inputs of {} to CPU due to CUDA OOM".format(str(func)))
        new_args = (maybe_to_cpu(x) for x in args)
        new_kwargs = {k: maybe_to_cpu(v) for k, v in kwargs.items()}
        return func(*new_args, **new_kwargs)

    return wrapped
```

### cvpods/utils/registry.py

```python
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

from typing import Dict, Optional
from tabulate import tabulate


class Registry(object):
    """
    The registry that provides name -> object mapping, to support third-party
    users' custom modules.
    To create a registry (e.g. a backbone registry):
    .. code-block:: python
        BACKBONE_REGISTRY = Registry('BACKBONE')
    To register an object:
    .. code-block:: python
        @BACKBONE_REGISTRY.register()
        class MyBackbone():
            ...
    Or:
    .. code-block:: python
        BACKBONE_REGISTRY.register(MyBackbone)
    """

    def __init__(self, name: str) -> None:
        """
        Args:
            name (str): the name of this registry
        """
        self._name: str = name
        self._obj_map: Dict[str, object] = {}

    def _do_register(self, name: str, obj: object) -> None:
        assert (
            name not in self._obj_map
        ), "An object named '{}' was already registered in '{}' registry!".format(
            name, self._name
        )
        self._obj_map[name] = obj

    def register(self, obj: object = None, name: str = None) -> Optional[object]:
        """
        Register the given object under the the name `obj.__name__`.
        Can be used as either a decorator or not. See docstring of this class for usage.
        """
        if obj is None:
            # used as a decorator
            def deco(func_or_class: object) -> object:
                nonlocal name
                if name is None:
                    name = func_or_class.__name__  # pyre-ignore
                self._do_register(name, func_or_class)
                return func_or_class

            return deco

        # used as a function call
        if name is None:
            name = obj.__name__  # pyre-ignore
        self._do_register(name, obj)

    def get(self, name: str) -> object:
        ret = self._obj_map.get(name)
        if ret is None:
            raise KeyError(
                "No object named '{}' found in '{}' registry!".format(name, self._name)
            )
        return ret

    def __contains__(self, name: str) -> bool:
        return name in self._obj_map

    def __repr__(self) -> str:
        table_headers = ["Names", "Objects"]
        table = tabulate(self._obj_map.items(), headers=table_headers, tablefmt="fancy_grid")
        return "Registry of {}:\n".format(self._name) + table
```

### cvpods/utils/__init__.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-

from .benchmark import Timer, benchmark, timeit
from .distributed import comm
from .dump import (
    CommonMetricPrinter,
    EventStorage,
    EventWriter,
    HistoryBuffer,
    JSONWriter,
    TensorboardXWriter,
    create_small_table,
    create_table_with_header,
    get_event_storage,
    log_every_n,
    log_every_n_seconds,
    log_first_n,
    setup_logger
)
from .env import collect_env_info, seed_all_rng, setup_custom_environment, setup_environment
from .file import PathHandler, PathManager, PicklableWrapper, download, file_lock, get_cache_dir
from .imports import dynamic_import
from .memory import retry_if_cuda_oom
from .metrics import accuracy
from .registry import Registry
from .visualizer import ColorMode, VideoVisualizer, VisImage, Visualizer, colormap, random_color

__all__ = [k for k in globals().keys() if not k.startswith("_")]
```

### cvpods/utils/imports.py

```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-

import imp


def dynamic_import(config_name, config_path):
    """
    Dynamic import a project.

    Args:
        config_name (str): module name
        config_path (str): the dir that contains the .py with this module.

    Examples::
        >>> root = "/path/to/your/retinanet/"
        >>> project = root + "retinanet.res50.fpn.coco.800size.1x.mrcnn_sigmoid"
        >>> cfg = dynamic_import("config", project).config
        >>> net = dynamic_import("net", project)
    """
    fp, pth, desc = imp.find_module(config_name, [config_path])

    return imp.load_module(config_name, fp, pth, desc)
```

### cvpods/utils/apex_wrapper.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-

import functools

try:
    from apex import amp
except ImportError:
    pass


def is_amp_training():
    """
    check weather amp training is enabled.

    Returns:
        bool: True if amp training is enabled
    """
    try:
        return hasattr(amp._amp_state, "loss_scalers")
    except Exception:
        return False


def float_function(func):

    @functools.wraps(func)
    def float_wraps(*args, **kwargs):
        if is_amp_training():
            return amp.float_function(func)(*args, **kwargs)
        else:
            return func(*args, **kwargs)
    return float_wraps
```

#### cvpods/utils/benchmark/benchmark.py

```python
#!/usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates.

import sys
import time
from typing import Dict, List

import numpy as np


def timeit(num_iters: int = -1, warmup_iters: int = 0):
    """
    This is intened to be used as a decorator to time any function.

    Args:
        num_iters (int): number of iterations used to compute the average time
            (sec) required to run the function. If negative, the number of
            iterations is determined dynamically by running the function a few
            times to make sure the estimate is stable.
        warmup_iters (int): number of iterations used to warm up the function.
            This is useful for functions that exhibit poor performance during
            the first few times they run (due to caches, autotuning, etc).

    Returns:
        Dict[str, float]: dictionary of the aggregated timing estimates.
            "iterations": number of iterations used to compute the estimated
                          time.
            "mean": averate time (sec) used to run the function.
            "median": median time (sec) used to run the function.
            "min": minimal time (sec) used to run the function.
            "max": maximal time (sec) used to run the function.
            "stddev": standard deviation of the time (sec) used to run the
                      function.
    """

    def decorator(func):
        def decorated(*args, **kwargs) -> Dict[str, float]:
            # Warmup phase.
            for _ in range(warmup_iters):
                func(*args, **kwargs)

            # Estimate the run time of the function.
            total_time: float = 0
            count = 0
            run_times: List[float] = []
            max_num_iters = num_iters if num_iters > 0 else sys.maxsize
            for _ in range(max_num_iters):
                start_time = time.time()
                func(*args, **kwargs)
                run_time = time.time() - start_time

                run_times.append(run_time)
                total_time += run_time
                count += 1
                if num_iters < 0 and total_time >= 0.5:
                    # If num_iters is negative, run the function enough times so
                    # that we can have a more robust estimate of the average time.
                    break
            assert count == len(run_times)
            ret: Dict[str, float] = {}
            ret["iterations"] = count
            ret["mean"] = total_time / count
            ret["median"] = np.median(run_times)
            ret["min"] = np.min(run_times)
            ret["max"] = np.max(run_times)
            ret["stddev"] = np.std(run_times)
            return ret

        return decorated

    return decorator


def benchmark(
    func,
    bm_name: str,
    kwargs_list: List[Dict],
    *,
    num_iters: int = -1,
    warmup_iters: int = 0
) -> None:
    """
    Benchmark the input function and print out the results.

    Args:
        func (callable): a closure that returns a function for benchmarking,
            where initialization can be done before the function to benchmark.
        bm_name (str): name of the benchmark to print out, e.g. "BM_UPDATE".
        kwargs_list (list): a list of argument dict to pass to the function. The
            intput function will be timed separately for each argument dict.
        num_iters (int): number of iterations to run. Defaults to run until 0.5s.
        warmup_iters (int): number of iterations used to warm up the function.

    Outputs:
        For each argument dict, print out the time (in microseconds) required
        to run the function along with the number of iterations used to get
        the timing estimate. Example output:

        Benchmark               Avg Time(μs)   Peak Time(μs)     Iterations
        -------------------------------------------------------------------
        BM_UPDATE_100                    820             914            610
        BM_UPDATE_1000                  7655            8709             66
        BM_UPDATE_10000                78062           81748              7
        -------------------------------------------------------------------
    """

    print("")
    outputs = []
    for kwargs in kwargs_list:
        func_bm = func(**kwargs)

        time_func = timeit(num_iters=num_iters, warmup_iters=warmup_iters)(
            func_bm
        )

        ret = time_func()
        name = bm_name
        if kwargs:
            name += "_" + "_".join(str(v) for k, v in kwargs.items())
        outputs.append(
            [
                name,
                str(ret["mean"] * 1000000),
                str(ret["max"] * 1000000),
                str(ret["iterations"]),
            ]
        )
    outputs = np.array(outputs)
    # Calculate column widths for metrics table.
    c1 = len(max(outputs[:, 0], key=len))
    c2 = len(max(outputs[:, 1], key=len))
    c3 = len(max(outputs[:, 2], key=len))
    c4 = len(max(outputs[:, 3], key=len))
    dash = "-" * 80
    print(
        "{:{}s} {:>{}s} {:>{}s} {:>{}s}".format(
            "Benchmark",
            c1,
            "Avg Time(μs)",
            c2,
            "Peak Time(μs)",
            c3,
            "Iterations",
            c4,
        )
    )
    print(dash)
    for output in outputs:
        print(
            "{:{}s} {:15.0f} {:15.0f} {:14d}".format(
                output[0],
                c1,
                float(output[1]),
                float(output[2]),
                int(output[3]),
            )
        )
    print(dash)
```

#### cvpods/utils/benchmark/timer.py

```python
#!/usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
# -*- coding: utf-8 -*-

from time import perf_counter
from typing import Optional


class Timer:
    """
    A timer which computes the time elapsed since the start/reset of the timer.
    """

    def __init__(self):
        self.reset()

    def reset(self):
        """
        Reset the timer.
        """
        self._start = perf_counter()
        self._paused: Optional[float] = None
        self._total_paused = 0

    def pause(self):
        """
        Pause the timer.
        """
        if self._paused is not None:
            raise ValueError("Trying to pause a Timer that is already paused!")
        self._paused = perf_counter()

    def is_paused(self) -> bool:
        """
        Returns:
            bool: whether the timer is currently paused
        """
        return self._paused is not None

    def resume(self):
        """
        Resume the timer.
        """
        if self._paused is None:
            raise ValueError("Trying to resume a Timer that is not paused!")
        self._total_paused += perf_counter() - self._paused
        self._paused = None

    def seconds(self) -> float:
        """
        Returns:
            (float): the total number of seconds since the start/reset of the
                timer, excluding the time when the timer is paused.
        """
        if self._paused is not None:
            end_time: float = self._paused  # type: ignore
        else:
            end_time = perf_counter()
        return end_time - self._start - self._total_paused
```

#### cvpods/utils/benchmark/__init__.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-

from .benchmark import *
from .timer import *
```

#### cvpods/utils/metrics/__init__.py

```python
from .accuracy import accuracy
```

#### cvpods/utils/metrics/accuracy.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.
import torch


@torch.no_grad()
def accuracy(output, target, topk=(1,)):
    """Computes the precision@k for the specified values of k"""
    if target.numel() == 0:
        return [torch.zeros([], device=output.device)]
    maxk = max(topk)
    batch_size = target.size(0)

    _, pred = output.topk(maxk, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))

    res = []
    for k in topk:
        correct_k = correct[:k].view(-1).float().sum(0)
        res.append(correct_k.mul_(100.0 / batch_size))
    return res
```

#### cvpods/utils/dump/events.py

```python
# Copyright (c) Facebook, Inc. and its affiliates
# Modified by BaseDetection, Inc. and its affiliates.

# pylint: disable=W0613

import datetime
import json
import logging
import os
import time
from collections import defaultdict
from contextlib import contextmanager

import torch

from cvpods.utils.file import PathManager

from .history_buffer import HistoryBuffer

_CURRENT_STORAGE_STACK = []


def get_event_storage():
    """
    Returns:
        The :class:`EventStorage` object that's currently being used.
        Throws an error if no :class`EventStorage` is currently enabled.
    """
    assert len(
        _CURRENT_STORAGE_STACK
    ), "get_event_storage() has to be called inside a 'with EventStorage(...)' context!"
    return _CURRENT_STORAGE_STACK[-1]


class EventWriter:
    """
    Base class for writers that obtain events from :class:`EventStorage` and process them.
    """

    def write(self):
        raise NotImplementedError

    def close(self):
        pass


class JSONWriter(EventWriter):
    """
    Write scalars to a json file.

    It saves scalars as one json per line (instead of a big json) for easy parsing.

    Examples parsing such a json file:

    .. code-block:: none

        $ cat metrics.json | jq -s '.[0:2]'
        [
          {
            "data_time": 0.008433341979980469,
            "iteration": 20,
            "loss": 1.9228371381759644,
            "loss_box_reg": 0.050025828182697296,
            "loss_classifier": 0.5316952466964722,
            "loss_mask": 0.7236229181289673,
            "loss_rpn_box": 0.0856662318110466,
            "loss_rpn_cls": 0.48198649287223816,
            "lr": 0.007173333333333333,
            "time": 0.25401854515075684
          },
          {
            "data_time": 0.007216215133666992,
            "iteration": 40,
            "loss": 1.282649278640747,
            "loss_box_reg": 0.06222952902317047,
            "loss_classifier": 0.30682939291000366,
            "loss_mask": 0.6970193982124329,
            "loss_rpn_box": 0.038663312792778015,
            "loss_rpn_cls": 0.1471673548221588,
            "lr": 0.007706666666666667,
            "time": 0.2490077018737793
          }
        ]

        $ cat metrics.json | jq '.loss_mask'
        0.7126231789588928
        0.689423680305481
        0.6776131987571716
        ...

    """

    def __init__(self, json_file, window_size=20):
        """
        Args:
            json_file (str): path to the json file. New data will be appended if the file exists.
            window_size (int): the window size of median smoothing for the scalars whose
                `smoothing_hint` are True.
        """
        self._file_handle = PathManager.open(json_file, "a")
        self._window_size = window_size

    def write(self):
        storage = get_event_storage()
        to_save = {"iteration": storage.iter}
        to_save.update(storage.latest_with_smoothing_hint())
        self._file_handle.write(json.dumps(to_save, sort_keys=True) + "\n")
        self._file_handle.flush()
        try:
            os.fsync(self._file_handle.fileno())
        except AttributeError:
            pass

    def close(self):
        self._file_handle.close()


class TensorboardXWriter(EventWriter):
    """
    Write all scalars to a tensorboard file.
    """

    def __init__(self, log_dir: str, window_size: int = 20, **kwargs):
        """
        Args:
            log_dir (str): the directory to save the output events
            window_size (int): the scalars will be median-smoothed by this window size
            kwargs: other arguments passed to `torch.utils.tensorboard.SummaryWriter(...)`
        """
        self._window_size = window_size
        from torch.utils.tensorboard import SummaryWriter

        self._writer = SummaryWriter(log_dir, **kwargs)

    def write(self):
        storage = get_event_storage()
        for k, v in storage.latest_with_smoothing_hint().items():
            self._writer.add_scalar(k, v, storage.iter)

        if len(storage.vis_data) >= 1:
            for img_name, img, step_num in storage.vis_data:
                self._writer.add_image(img_name, img, step_num)
            storage.clear_images()

    def close(self):
        if hasattr(self, "_writer"):  # doesn't exist when the code fails at import
            self._writer.close()


class CommonMetricPrinter(EventWriter):
    """
    Print **common** metrics to the terminal, including
    iteration time, ETA, memory, all losses, and the learning rate.

    To print something different, please implement a similar printer by yourself.
    """

    def __init__(self, max_iter, window_size=20, **kwargs):
        """
        Args:
            max_iter (int): the maximum number of iterations to train.
                Used to compute ETA.
        """
        self.logger = logging.getLogger(__name__)
        self._max_iter = max_iter
        if "epoch" in kwargs:
            self._epoch = kwargs["epoch"]
            self._epoch_iters = self._max_iter // self._epoch if self._epoch is not None else None
        else:
            self._epoch = None
            self._epoch_iters = None

        self._window_size = window_size
        self._last_write = None

    def write(self):

        storage = get_event_storage()
        iteration = storage.iter

        try:
            data_time = storage.history("data_time").avg(self._window_size)
        except KeyError:
            # they may not exist in the first few iterations (due to warmup)
            # or when SimpleTrainer is not used
            data_time = None

        eta_string = "N/A"
        try:
            iter_time = storage.history("time").global_avg()
            eta_seconds = storage.history("time").median(1000) * (self._max_iter - iteration)
            # storage.put_scalar("eta_seconds", eta_seconds, smoothing_hint=False)
            eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))
        # they may not exist in the first few iterations (due to warmup)
        except KeyError:
            iter_time = None
            # estimate eta on our own - more noisy
            if self._last_write is not None:
                estimate_iter_time = (time.perf_counter() - self._last_write[1]) / (
                    iteration - self._last_write[0]
                )
                eta_seconds = estimate_iter_time * (self._max_iter - iteration)
                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))
            self._last_write = (iteration, time.perf_counter())

        try:
            lr = "{:.6f}".format(storage.history("lr").latest())
        except KeyError:
            lr = "N/A"

        if torch.cuda.is_available():
            max_mem_mb = torch.cuda.max_memory_allocated() / 1024.0 / 1024.0
        else:
            max_mem_mb = None

        # NOTE: max_mem is parsed by grep in "dev/parse_results.sh"
        losses = "  ".join(
            [
                "{}: {:.3f}".format(k, v.median(self._window_size))
                for k, v in storage.histories().items()
                if "loss" in k
            ]
        )
        other_metrics = "  ".join(
            [
                "{}: {:.3f}".format(k, v.median(self._window_size))
                for k, v in storage.histories().items()
                if "loss" not in k and k not in ["data_time", "time", "lr"] and "/" not in k
            ]
        )

        if self._epoch_iters is not None:
            progress_string = "epoch: {cur_epoch}/{max_epoch}  iter: {cur_iter}/{max_iter}".format(
                cur_epoch=(iteration + 1) // self._epoch_iters + 1,
                max_epoch=self._max_iter // self._epoch_iters,
                cur_iter=(iteration + 1) % self._epoch_iters,
                max_iter=self._epoch_iters,
            )
        else:
            progress_string = "iter: {cur_iter}/{max_iter}".format(
                cur_iter=iteration + 1,
                max_iter=self._max_iter,
            )
        self.logger.info(
            ("eta: {eta}  {progress}  {losses}  {other_metrics}  {time}  "
             "{data_time}  lr: {lr}  {memory}").format(
                 eta=eta_string,
                 progress=progress_string,
                 losses=losses,
                 other_metrics=other_metrics,
                 time="time: {:.4f}".format(iter_time) if iter_time is not None else "",
                 data_time="data_time: {:.4f}".format(data_time) if data_time is not None else "",
                 lr=lr,
                 memory="max_mem: {:.0f}M".format(max_mem_mb) if max_mem_mb is not None else "",
            )
        )


class EventStorage:
    """
    The user-facing class that provides metric storage functionalities.

    In the future we may add support for storing / logging other types of data if needed.
    """

    def __init__(self, start_iter=0, window_size=20):
        """
        Args:
            start_iter (int): the iteration number to start with
        """
        self._history = defaultdict(HistoryBuffer)
        self._smoothing_hints = {}
        self._latest_scalars = {}
        self._window_size = window_size
        self._iter = start_iter
        self._current_prefix = ""
        self._vis_data = []

    def put_image(self, img_name, img_tensor):
        """
        Add an `img_tensor` to the `_vis_data` associated with `img_name`.

        Args:
            img_name (str): The name of the image to put into tensorboard.
            img_tensor (torch.Tensor or numpy.array): An `uint8` or `float`
                Tensor of shape `[channel, height, width]` where `channel` is
                3. The image format should be RGB. The elements in img_tensor
                can either have values in [0, 1] (float32) or [0, 255] (uint8).
                The `img_tensor` will be visualized in tensorboard.
        """
        self._vis_data.append((img_name, img_tensor, self._iter))

    def clear_images(self):
        """
        Delete all the stored images for visualization. This should be called
        after images are written to tensorboard.
        """
        self._vis_data = []

    def put_scalar(self, name, value, smoothing_hint=True):
        """
        Add a scalar `value` to the `HistoryBuffer` associated with `name`.

        Args:
            smoothing_hint (bool): a 'hint' on whether this scalar is noisy and should be
                smoothed when logged. The hint will be accessible through
                :meth:`EventStorage.smoothing_hints`.  A writer may ignore the hint
                and apply custom smoothing rule.

                It defaults to True because most scalars we save need to be smoothed to
                provide any useful signal.
        """
        name = self._current_prefix + name
        history = self._history[name]
        value = float(value)
        history.update(value, self._iter)
        self._latest_scalars[name] = value

        existing_hint = self._smoothing_hints.get(name)
        if existing_hint is not None:
            assert (
                existing_hint == smoothing_hint
            ), "Scalar {} was put with a different smoothing_hint!".format(name)
        else:
            self._smoothing_hints[name] = smoothing_hint

    def put_scalars(self, *, smoothing_hint=True, **kwargs):
        """
        Put multiple scalars from keyword arguments.

        Examples:

            storage.put_scalars(loss=my_loss, accuracy=my_accuracy, smoothing_hint=True)
        """
        for k, v in kwargs.items():
            self.put_scalar(k, v, smoothing_hint=smoothing_hint)

    def history(self, name):
        """
        Returns:
            HistoryBuffer: the scalar history for name
        """
        ret = self._history.get(name, None)
        if ret is None:
            raise KeyError("No history metric available for {}!".format(name))
        return ret

    def histories(self):
        """
        Returns:
            dict[name -> HistoryBuffer]: the HistoryBuffer for all scalars
        """
        return self._history

    def latest(self):
        """
        Returns:
            dict[name -> number]: the scalars that's added in the current iteration.
        """
        return self._latest_scalars

    def latest_with_smoothing_hint(self):
        """
        Similar to :meth:`latest`, but the returned values
        are either the un-smoothed original latest value,
        or a median of the given window_size,
        depend on whether the smoothing_hint is True.

        This provides a default behavior that other writers can use.
        """
        result = {}
        for k, v in self._latest_scalars.items():
            result[k] = (
                self._history[k].median(self._window_size) if self._smoothing_hints[k] else v
            )
        return result

    def smoothing_hints(self):
        """
        Returns:
            dict[name -> bool]: the user-provided hint on whether the scalar
                is noisy and needs smoothing.
        """
        return self._smoothing_hints

    def step(self):
        """
        User should call this function at the beginning of each iteration, to
        notify the storage of the start of a new iteration.
        The storage will then be able to associate the new data with the
        correct iteration number.
        """
        self._iter += 1
        self._latest_scalars = {}

    @property
    def vis_data(self):
        return self._vis_data

    @property
    def iter(self):
        return self._iter

    @property
    def iteration(self):
        # for backward compatibility
        return self._iter

    def __enter__(self):
        _CURRENT_STORAGE_STACK.append(self)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        assert _CURRENT_STORAGE_STACK[-1] == self
        _CURRENT_STORAGE_STACK.pop()

    @contextmanager
    def name_scope(self, name):
        """
        Yields:
            A context within which all the events added to this storage
            will be prefixed by the name scope.
        """
        old_prefix = self._current_prefix
        self._current_prefix = name.rstrip("/") + "/"
        yield
        self._current_prefix = old_prefix
```

#### cvpods/utils/dump/__init__.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-

from .events import *
from .history_buffer import *
from .logger import *
```

#### cvpods/utils/dump/logger.py

```python
# Copyright (c) Facebook, Inc. and its affiliates.
import atexit
import functools
import itertools
import logging
import os
import sys
import time
from collections import Counter
from tabulate import tabulate
from termcolor import colored

from cvpods.utils.file import PathManager


class _ColorfulFormatter(logging.Formatter):
    def __init__(self, *args, **kwargs):
        self._root_name = kwargs.pop("root_name") + "."
        self._abbrev_name = kwargs.pop("abbrev_name", "")
        if len(self._abbrev_name):
            self._abbrev_name = self._abbrev_name + "."
        super(_ColorfulFormatter, self).__init__(*args, **kwargs)

    def formatMessage(self, record):
        record.name = record.name.replace(self._root_name, self._abbrev_name)
        log = super(_ColorfulFormatter, self).formatMessage(record)
        if record.levelno == logging.WARNING:
            prefix = colored("WARNING", "red", attrs=["blink"])
        elif record.levelno == logging.ERROR or record.levelno == logging.CRITICAL:
            prefix = colored("ERROR", "red", attrs=["blink", "underline"])
        else:
            return log
        return prefix + " " + log


@functools.lru_cache()  # so that calling setup_logger multiple times won't add many handlers
def setup_logger(
    output=None, distributed_rank=0, *, color=True, name="cvpods", abbrev_name=None
):
    """
    Initialize the cvpods logger and set its verbosity level to "INFO".

    Args:
        output (str): a file name or a directory to save log. If None, will not save log file.
            If ends with ".txt" or ".log", assumed to be a file name.
            Otherwise, logs will be saved to `output/log.txt`.
        name (str): the root module name of this logger
        abbrev_name (str): an abbreviation of the module, to avoid long names in logs.
            Set to "" to not log the root module in logs.
            By default, will abbreviate "cvpods" to "c2" and leave other
            modules unchanged.

    Returns:
        logging.Logger: a logger
    """
    logger = logging.getLogger(name)
    logger.setLevel(logging.DEBUG)
    logger.propagate = False

    if abbrev_name is None:
        abbrev_name = "c2" if name == "cvpods" else name

    plain_formatter = logging.Formatter(
        "[%(asctime)s] %(name)s %(levelname)s: %(message)s", datefmt="%m/%d %H:%M:%S"
    )
    # stdout logging: master only
    if distributed_rank == 0:
        ch = logging.StreamHandler(stream=sys.stdout)
        ch.setLevel(logging.DEBUG)
        if color:
            formatter = _ColorfulFormatter(
                colored("[%(asctime)s %(name)s]: ", "green") + "%(message)s",
                datefmt="%m/%d %H:%M:%S",
                root_name=name,
                abbrev_name=str(abbrev_name),
            )
        else:
            formatter = plain_formatter
        ch.setFormatter(formatter)
        logger.addHandler(ch)

    # file logging: all workers
    if output is not None:
        if output.endswith(".txt") or output.endswith(".log"):
            filename = output
        else:
            filename = os.path.join(output, "log.txt")
        if distributed_rank > 0:
            filename = filename + ".rank{}".format(distributed_rank)
        PathManager.mkdirs(os.path.dirname(filename))

        fh = logging.StreamHandler(_cached_log_stream(filename))
        fh.setLevel(logging.DEBUG)
        fh.setFormatter(plain_formatter)
        logger.addHandler(fh)

    return logger


# cache the opened file object, so that different calls to `setup_logger`
# with the same file name can safely write to the same file.
@functools.lru_cache(maxsize=None)
def _cached_log_stream(filename):
    io = PathManager.open(filename, "a")
    atexit.register(io.close)
    return io


"""
Below are some other convenient logging methods.
They are mainly adopted from
https://github.com/abseil/abseil-py/blob/master/absl/logging/__init__.py
"""


def _find_caller():
    """
    Returns:
        str: module name of the caller
        tuple: a hashable key to be used to identify different callers
    """
    frame = sys._getframe(2)
    while frame:
        code = frame.f_code
        if os.path.join("utils", "writer", "logger.") not in code.co_filename:
            mod_name = frame.f_globals["__name__"]
            if mod_name == "__main__":
                mod_name = "cvpods"
            return mod_name, (code.co_filename, frame.f_lineno, code.co_name)
        frame = frame.f_back


_LOG_COUNTER = Counter()
_LOG_TIMER = {}


def log_first_n(lvl, msg, n=1, *, name=None, key="caller"):
    """
    Log only for the first n times.

    Args:
        lvl (int): the logging level
        msg (str):
        n (int):
        name (str): name of the logger to use. Will use the caller's module by default.
        key (str or tuple[str]): the string(s) can be one of "caller" or
            "message", which defines how to identify duplicated logs.
            For example, if called with `n=1, key="caller"`, this function
            will only log the first call from the same caller, regardless of
            the message content.
            If called with `n=1, key="message"`, this function will log the
            same content only once, even if they are called from different places.
            If called with `n=1, key=("caller", "message")`, this function
            will not log only if the same caller has logged the same message before.
    """
    if isinstance(key, str):
        key = (key,)
    assert len(key) > 0

    caller_module, caller_key = _find_caller()
    hash_key = ()
    if "caller" in key:
        hash_key = hash_key + caller_key
    if "message" in key:
        hash_key = hash_key + (msg,)

    _LOG_COUNTER[hash_key] += 1
    if _LOG_COUNTER[hash_key] <= n:
        logging.getLogger(name or caller_module).log(lvl, msg)


def log_every_n(lvl, msg, n=1, *, name=None):
    """
    Log once per n times.

    Args:
        lvl (int): the logging level
        msg (str):
        n (int):
        name (str): name of the logger to use. Will use the caller's module by default.
    """
    caller_module, key = _find_caller()
    _LOG_COUNTER[key] += 1
    if n == 1 or _LOG_COUNTER[key] % n == 1:
        logging.getLogger(name or caller_module).log(lvl, msg)


def log_every_n_seconds(lvl, msg, n=1, *, name=None):
    """
    Log no more than once per n seconds.
    Args:
        lvl (int): the logging level
        msg (str):
        n (int):
        name (str): name of the logger to use. Will use the caller's module by default.
    """
    caller_module, key = _find_caller()
    last_logged = _LOG_TIMER.get(key, None)
    current_time = time.time()
    if last_logged is None or current_time - last_logged >= n:
        logging.getLogger(name or caller_module).log(lvl, msg)
        _LOG_TIMER[key] = current_time


def create_small_table(small_dict):
    """
    Create a small table using the keys of small_dict as headers. This is only
    suitable for small dictionaries.

    Args:
        small_dict (dict): a result dictionary of only a few items.

    Returns:
        str: the table as a string.
    """
    keys, values = tuple(zip(*small_dict.items()))
    table = tabulate(
        [values],
        headers=keys,
        tablefmt="pipe",
        floatfmt=".3f",
        stralign="center",
        numalign="center",
    )
    return table


def create_table_with_header(header_dict, headers=["category", "AP"], min_cols=6):
    """
    create a table with given header.

    Args:
        header_dict (dict):
        headers (list):
        min_cols (int):

    Returns:
        str: the table as a string
    """
    assert min_cols % len(headers) == 0, "bad table format"
    num_cols = min(min_cols, len(header_dict) * len(headers))
    result_pair = [x for pair in header_dict.items() for x in pair]
    row_pair = itertools.zip_longest(*[result_pair[i::num_cols] for i in range(num_cols)])
    table = tabulate(
        row_pair, tablefmt="pipe", floatfmt=".3f",
        headers=headers * (num_cols // len(headers)),
        numalign="left")
    return table
```

#### cvpods/utils/dump/history_buffer.py

```python
#!/usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates.

from typing import List, Tuple

import numpy as np


class HistoryBuffer:
    """
    Track a series of scalar values and provide access to smoothed values over a
    window or the global average of the series.
    """

    def __init__(self, max_length: int = 1000000):
        """
        Args:
            max_length: maximal number of values that can be stored in the
                buffer. When the capacity of the buffer is exhausted, old
                values will be removed.
        """
        self._max_length: int = max_length
        self._data: List[Tuple[float, float]] = []  # (value, iteration) pairs
        self._count: int = 0
        self._global_avg: float = 0

    def update(self, value: float, iteration: float = None):
        """
        Add a new scalar value produced at certain iteration. If the length
        of the buffer exceeds self._max_length, the oldest element will be
        removed from the buffer.
        """
        if iteration is None:
            iteration = self._count
        if len(self._data) == self._max_length:
            self._data.pop(0)
        self._data.append((value, iteration))

        self._count += 1
        self._global_avg += (value - self._global_avg) / self._count

    def latest(self):
        """
        Return the latest scalar value added to the buffer.
        """
        return self._data[-1][0]

    def median(self, window_size: int):
        """
        Return the median of the latest `window_size` values in the buffer.
        """
        return np.median([x[0] for x in self._data[-window_size:]])

    def avg(self, window_size: int):
        """
        Return the mean of the latest `window_size` values in the buffer.
        """
        return np.mean([x[0] for x in self._data[-window_size:]])

    def global_avg(self):
        """
        Return the mean of all the elements in the buffer. Note that this
        includes those getting removed due to limited buffer storage.
        """
        return self._global_avg

    def values(self):
        """
        Returns:
            list[(number, iteration)]: content of the current buffer.
        """
        return self._data
```

#### cvpods/utils/file/serialize.py

```python
# Copyright (c) Facebook, Inc. and its affiliates.
import cloudpickle


class PicklableWrapper(object):
    """
    Wrap an object to make it more picklable, note that it uses
    heavy weight serialization libraries that are slower than pickle.
    It's best to use it only on closures (which are usually not picklable).

    This is a simplified version of
    https://github.com/joblib/joblib/blob/master/joblib/externals/loky/cloudpickle_wrapper.py
    """

    def __init__(self, obj):
        self._obj = obj

    def __reduce__(self):
        s = cloudpickle.dumps(self._obj)
        return cloudpickle.loads, (s,)

    def __call__(self, *args, **kwargs):
        return self._obj(*args, **kwargs)

    def __getattr__(self, attr):
        # Ensure that the wrapped object can be used seamlessly as the previous object.
        if attr not in ["_obj"]:
            return getattr(self._obj, attr)
        return getattr(self, attr)
```

#### cvpods/utils/file/download.py

```python
#!/usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates.

import logging
import os
import shutil
from typing import Callable, Optional
from urllib import request


def download(
    url: str, dir: str, *, filename: Optional[str] = None, progress: bool = True
) -> str:
    """
    Download a file from a given URL to a directory. If file exists, will not
        overwrite the existing file.

    Args:
        url (str):
        dir (str): the directory to download the file
        filename (str or None): the basename to save the file.
            Will use the name in the URL if not given.
        progress (bool): whether to use tqdm to draw a progress bar.

    Returns:
        str: the path to the downloaded file or the existing one.
    """
    os.makedirs(dir, exist_ok=True)
    if filename is None:
        filename = url.split("/")[-1]
        assert len(filename), "Cannot obtain filename from url {}".format(url)
    fpath = os.path.join(dir, filename)
    logger = logging.getLogger(__name__)

    if os.path.isfile(fpath):
        logger.info("File {} exists! Skipping download.".format(filename))
        return fpath

    tmp = fpath + ".tmp"  # download to a tmp file first, to be more atomic.
    try:
        logger.info("Downloading from {} ...".format(url))
        if progress:
            import tqdm

            def hook(t: tqdm.tqdm) -> Callable[[int, int, Optional[int]], None]:
                last_b = [0]

                def inner(
                    b: int, bsize: int, tsize: Optional[int] = None
                ) -> None:
                    if tsize is not None:
                        t.total = tsize
                    t.update((b - last_b[0]) * bsize)  # type: ignore
                    last_b[0] = b

                return inner

            with tqdm.tqdm(  # type: ignore
                unit="B", unit_scale=True, miniters=1, desc=filename, leave=True
            ) as t:
                tmp, _ = request.urlretrieve(
                    url, filename=tmp, reporthook=hook(t)
                )

        else:
            tmp, _ = request.urlretrieve(url, filename=tmp)
        statinfo = os.stat(tmp)
        size = statinfo.st_size
        if size == 0:
            raise IOError("Downloaded an empty file from {}!".format(url))
        # download to tmp first and move to fpath, to make this function more
        # atomic.
        shutil.move(tmp, fpath)
    except IOError:
        logger.error("Failed to download {}".format(url))
        raise
    finally:
        try:
            os.unlink(tmp)
        except IOError:
            pass

    logger.info(
        "Successfully downloaded " + fpath + ". " + str(size) + " bytes."
    )
    return fpath
```

#### cvpods/utils/file/__init__.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-

from .download import *
from .file_io import *
from .serialize import *
```

#### cvpods/utils/file/file_io.py

```python
#!/usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates.

import errno
import logging
import os
import shutil
from collections import OrderedDict
from typing import IO, Any, Dict, List, MutableMapping, Optional
from urllib.parse import urlparse
import portalocker

from .download import download

__all__ = ["PathHandler", "PathManager", "get_cache_dir", "file_lock"]


def get_cache_dir(cache_dir: Optional[str] = None) -> str:
    """
    Returns a default directory to cache static files
    (usually downloaded from Internet), if None is provided.

    Args:
        cache_dir (None or str): if not None, will be returned as is.
            If None, returns the default cache directory as:

        1) $CVPODS_CACHE, if set
        2) otherwise ~/.torch/cvpods_cache
    """
    if cache_dir is None:
        cache_dir = os.path.expanduser(
            os.getenv("CVPODS_CACHE", "~/.torch/cvpods_cache")
        )
    return cache_dir


def file_lock(path: str):  # type: ignore
    """
    A file lock. Once entered, it is guaranteed that no one else holds the
    same lock. Others trying to enter the lock will block for 30 minutes and
    raise an exception.

    This is useful to make sure workers don't cache files to the same location.

    Args:
        path (str): a path to be locked. This function will create a lock named
            `path + ".lock"`

    Examples:

    >>> filename = "/path/to/file"
    >>> with file_lock(filename):
            if not os.path.isfile(filename):
                do_create_file()
    """
    dirname = os.path.dirname(path)
    try:
        os.makedirs(dirname, exist_ok=True)
    except OSError:
        # makedir is not atomic. Exceptions can happen when multiple workers try
        # to create the same dir, despite exist_ok=True.
        # When this happens, we assume the dir is created and proceed to creating
        # the lock. If failed to create the directory, the next line will raise
        # exceptions.
        pass
    return portalocker.Lock(path + ".lock", timeout=1800)  # type: ignore


class PathHandler:
    """
    PathHandler is a base class that defines common I/O functionality for a URI
    protocol. It routes I/O for a generic URI which may look like "protocol://*"
    or a canonical filepath "/foo/bar/baz".
    """

    def _get_supported_prefixes(self) -> List[str]:
        """
        Returns:
            List[str]: the list of URI prefixes this PathHandler can support
        """
        raise NotImplementedError()

    def _get_local_path(self, path: str) -> str:
        """
        Get a filepath which is compatible with native Python I/O such as `open`
        and `os.path`.

        If URI points to a remote resource, this function may download and cache
        the resource to local disk. In this case, this function is meant to be
        used with read-only resources.

        Args:
            path (str): A URI supported by this PathHandler

        Returns:
            local_path (str): a file path which exists on the local file system
        """
        raise NotImplementedError()

    def _open(self, path: str, mode: str = "r") -> IO[Any]:
        """
        Open a stream to a URI, similar to the built-in `open`.

        Args:
            path (str): A URI supported by this PathHandler
            mode (str): Specifies the mode in which the file is opened. It defaults
                to 'r'.

        Returns:
            file: a file-like object.
        """
        raise NotImplementedError()

    def _copy(
        self, src_path: str, dst_path: str, overwrite: bool = False
    ) -> bool:
        """
        Copies a source path to a destination path.

        Args:
            src_path (str): A URI supported by this PathHandler
            dst_path (str): A URI supported by this PathHandler
            overwrite (bool): Bool flag for forcing overwrite of existing file

        Returns:
            status (bool): True on success
        """
        raise NotImplementedError()

    def _exists(self, path: str) -> bool:
        """
        Checks if there is a resource at the given URI.

        Args:
            path (str): A URI supported by this PathHandler

        Returns:
            bool: true if the path exists
        """
        raise NotImplementedError()

    def _isfile(self, path: str) -> bool:
        """
        Checks if the resource at the given URI is a file.

        Args:
            path (str): A URI supported by this PathHandler

        Returns:
            bool: true if the path is a file
        """
        raise NotImplementedError()

    def _isdir(self, path: str) -> bool:
        """
        Checks if the resource at the given URI is a directory.

        Args:
            path (str): A URI supported by this PathHandler

        Returns:
            bool: true if the path is a directory
        """
        raise NotImplementedError()

    def _ls(self, path: str) -> List[str]:
        """
        List the contents of the directory at the provided URI.

        Args:
            path (str): A URI supported by this PathHandler

        Returns:
            List[str]: list of contents in given path
        """
        raise NotImplementedError()

    def _mkdirs(self, path: str) -> None:
        """
        Recursive directory creation function. Like mkdir(), but makes all
        intermediate-level directories needed to contain the leaf directory.
        Similar to the native `os.makedirs`.

        Args:
            path (str): A URI supported by this PathHandler
        """
        raise NotImplementedError()

    def _rm(self, path: str) -> None:
        """
        Remove the file (not directory) at the provided URI.

        Args:
            path (str): A URI supported by this PathHandler
        """
        raise NotImplementedError()

    def _stat(self, path: str):
        """
        Get stat of file (not directory) at the provided URI.

        Args:
            path (str): A URI supported by this PathHandler
        """
        raise NotImplementedError()


class NativePathHandler(PathHandler):
    """
    Handles paths that can be accessed using Python native system calls. This
    handler uses `open()` and `os.*` calls on the given path.
    """

    def _get_local_path(self, path: str) -> str:
        return path

    def _open(self, path: str, mode: str = "r") -> IO[Any]:
        return open(path, mode)

    def _copy(
        self, src_path: str, dst_path: str, overwrite: bool = False
    ) -> bool:
        """
        Copies a source path to a destination path.

        Args:
            src_path (str): A URI supported by this PathHandler
            dst_path (str): A URI supported by this PathHandler
            overwrite (bool): Bool flag for forcing overwrite of existing file

        Returns:
            status (bool): True on success
        """
        if os.path.exists(dst_path) and not overwrite:
            logger = logging.getLogger(__name__)
            logger.error("Destination file {} already exists.".format(dst_path))
            return False

        try:
            shutil.copyfile(src_path, dst_path)
            return True
        except Exception as e:
            logger = logging.getLogger(__name__)
            logger.error("Error in file copy - {}".format(str(e)))
            return False

    def _exists(self, path: str) -> bool:
        return os.path.exists(path)

    def _isfile(self, path: str) -> bool:
        return os.path.isfile(path)

    def _isdir(self, path: str) -> bool:
        return os.path.isdir(path)

    def _ls(self, path: str) -> List[str]:
        return os.listdir(path)

    def _mkdirs(self, path: str) -> None:
        try:
            os.makedirs(path, exist_ok=True)
        except OSError as e:
            # EEXIST it can still happen if multiple processes are creating the dir
            if e.errno != errno.EEXIST:
                raise

    def _rm(self, path: str) -> None:
        os.remove(path)

    def _stat(self, path: str):
        return os.stat(path)


class HTTPURLHandler(PathHandler):
    """
    Download URLs and cache them to disk.
    """

    def __init__(self) -> None:
        self.cache_map: Dict[str, str] = {}

    def _get_supported_prefixes(self) -> List[str]:
        return ["http://", "https://", "ftp://"]

    def _get_local_path(self, path: str) -> str:
        """
        This implementation downloads the remote resource and caches it locally.
        The resource will only be downloaded if not previously requested.
        """
        if path not in self.cache_map or not os.path.exists(
            self.cache_map[path]
        ):
            logger = logging.getLogger(__name__)
            parsed_url = urlparse(path)
            dirname = os.path.join(
                get_cache_dir(), os.path.dirname(parsed_url.path.lstrip("/"))
            )
            filename = path.split("/")[-1]
            cached = os.path.join(dirname, filename)
            with file_lock(cached):
                if not os.path.isfile(cached):
                    logger.info("Downloading {} ...".format(path))
                    cached = download(path, dirname, filename=filename)
            logger.info("URL {} cached in {}".format(path, cached))
            self.cache_map[path] = cached
        return self.cache_map[path]

    def _open(self, path: str, mode: str = "r") -> IO[Any]:
        assert mode in (
            "r",
            "rb",
        ), "{} does not support open with {} mode".format(
            self.__class__.__name__, mode
        )
        local_path = self._get_local_path(path)
        return open(local_path, mode)

    def _stat(self, path: str):
        return os.stat(self._get_local_path(path))


class PathManager:
    """
    A class for users to open generic paths or translate generic paths to file names.
    """

    _PATH_HANDLERS: MutableMapping[str, PathHandler] = OrderedDict()
    _NATIVE_PATH_HANDLER = NativePathHandler()

    @staticmethod
    def __get_path_handler(path: str) -> PathHandler:
        """
        Finds a PathHandler that supports the given path. Falls back to the native
        PathHandler if no other handler is found.

        Args:
            path (str): URI path to resource

        Returns:
            handler (PathHandler)
        """
        for p in PathManager._PATH_HANDLERS.keys():
            if path.startswith(p):
                return PathManager._PATH_HANDLERS[p]
        return PathManager._NATIVE_PATH_HANDLER

    @staticmethod
    def open(path: str, mode: str = "r") -> IO[Any]:
        """
        Open a stream to a URI, similar to the built-in `open`.

        Args:
            path (str): A URI supported by this PathHandler

        Returns:
            file: a file-like object.
        """
        return PathManager.__get_path_handler(path)._open(path, mode)

    @staticmethod
    def copy(src_path: str, dst_path: str, overwrite: bool = False) -> bool:
        """
        Copies a source path to a destination path.

        Args:
            src_path (str): A URI supported by this PathHandler
            dst_path (str): A URI supported by this PathHandler
            overwrite (bool): Bool flag for forcing overwrite of existing file

        Returns:
            status (bool): True on success
        """

        # Copying across handlers is not supported.
        assert PathManager.__get_path_handler(
            src_path
        ) == PathManager.__get_path_handler(dst_path)
        return PathManager.__get_path_handler(src_path)._copy(
            src_path, dst_path, overwrite
        )

    @staticmethod
    def get_local_path(path: str) -> str:
        """
        Get a filepath which is compatible with native Python I/O such as `open`
        and `os.path`.

        If URI points to a remote resource, this function may download and cache
        the resource to local disk.

        Args:
            path (str): A URI supported by this PathHandler

        Returns:
            local_path (str): a file path which exists on the local file system
        """
        return PathManager.__get_path_handler(path)._get_local_path(path)

    @staticmethod
    def exists(path: str) -> bool:
        """
        Checks if there is a resource at the given URI.

        Args:
            path (str): A URI supported by this PathHandler

        Returns:
            bool: true if the path exists
        """
        return PathManager.__get_path_handler(path)._exists(path)

    @staticmethod
    def isfile(path: str) -> bool:
        """
        Checks if there the resource at the given URI is a file.

        Args:
            path (str): A URI supported by this PathHandler

        Returns:
            bool: true if the path is a file
        """
        return PathManager.__get_path_handler(path)._isfile(path)

    @staticmethod
    def isdir(path: str) -> bool:
        """
        Checks if the resource at the given URI is a directory.

        Args:
            path (str): A URI supported by this PathHandler

        Returns:
            bool: true if the path is a directory
        """
        return PathManager.__get_path_handler(path)._isdir(path)

    @staticmethod
    def ls(path: str) -> List[str]:
        """
        List the contents of the directory at the provided URI.

        Args:
            path (str): A URI supported by this PathHandler

        Returns:
            List[str]: list of contents in given path
        """
        return PathManager.__get_path_handler(path)._ls(path)

    @staticmethod
    def mkdirs(path: str) -> None:
        """
        Recursive directory creation function. Like mkdir(), but makes all
        intermediate-level directories needed to contain the leaf directory.
        Similar to the native `os.makedirs`.

        Args:
            path (str): A URI supported by this PathHandler
        """
        return PathManager.__get_path_handler(path)._mkdirs(path)

    @staticmethod
    def rm(path: str) -> None:
        """
        Remove the file (not directory) at the provided URI.

        Args:
            path (str): A URI supported by this PathHandler
        """
        return PathManager.__get_path_handler(path)._rm(path)

    @staticmethod
    def stat(path: str):
        """
        get status of the file at the provided URI.

        Args:
            path (str): A URI supported by this PathHandler
        """
        return PathManager.__get_path_handler(path)._stat(path)

    @staticmethod
    def upload(local: str, remote: str):
        """
        Upload the local file (not directory) to the specified remote URI.

        Args:
            local (str): path of the local file to be uploaded.
            remote (str): the remote s3uri.
        """
        handler = PathManager.__get_path_handler(remote)
        return handler._upload(local, remote)

    @staticmethod
    def register_handler(handler: PathHandler) -> None:
        """
        Register a path handler associated with `handler._get_supported_prefixes`
        URI prefixes.

        Args:
            handler (PathHandler)
        """
        assert isinstance(handler, PathHandler), handler
        for prefix in handler._get_supported_prefixes():
            assert prefix not in PathManager._PATH_HANDLERS
            PathManager._PATH_HANDLERS[prefix] = handler

        # Sort path handlers in reverse order so longer prefixes take priority,
        # eg: http://foo/bar before http://foo
        PathManager._PATH_HANDLERS = OrderedDict(
            sorted(
                PathManager._PATH_HANDLERS.items(),
                key=lambda t: t[0],
                reverse=True,
            )
        )


PathManager.register_handler(HTTPURLHandler())
```

#### cvpods/utils/distributed/comm.py

```python
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.
"""
This file contains primitives for multi-gpu communication.
This is useful when doing distributed training.
"""

import functools
import logging
import pickle
import socket

import numpy as np

import torch
import torch.distributed as dist

_LOCAL_PROCESS_GROUP = None
"""
A torch process group which only includes processes that on the same machine as the current process.
This variable is set when processes are spawned by `launch()` in "engine/launch.py".
"""


def get_host_ip():
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(('8.8.8.8', 80))
        ip = s.getsockname()[0]
    finally:
        s.close()

    return ip


def get_world_size() -> int:
    if not dist.is_available():
        return 1
    if not dist.is_initialized():
        return 1
    return dist.get_world_size()


def get_rank() -> int:
    if not dist.is_available():
        return 0
    if not dist.is_initialized():
        return 0
    return dist.get_rank()


def get_local_rank() -> int:
    """
    Returns:
        The rank of the current process within the local (per-machine) process group.
    """
    if not dist.is_available():
        return 0
    if not dist.is_initialized():
        return 0
    assert _LOCAL_PROCESS_GROUP is not None
    return dist.get_rank(group=_LOCAL_PROCESS_GROUP)


def get_local_size() -> int:
    """
    Returns:
        The size of the per-machine process group,
        i.e. the number of processes per machine.
    """
    if not dist.is_available():
        return 1
    if not dist.is_initialized():
        return 1
    return dist.get_world_size(group=_LOCAL_PROCESS_GROUP)


def is_main_process() -> bool:
    return get_rank() == 0


def synchronize():
    """
    Helper function to synchronize (barrier) among all processes when
    using distributed training
    """
    if not dist.is_available():
        return
    if not dist.is_initialized():
        return
    world_size = dist.get_world_size()
    if world_size == 1:
        return
    dist.barrier()


@functools.lru_cache()
def _get_global_gloo_group():
    """
    Return a process group based on gloo backend, containing all the ranks
    The result is cached.
    """
    if dist.get_backend() == "nccl":
        return dist.new_group(backend="gloo")
    else:
        return dist.group.WORLD


def _serialize_to_tensor(data, group):
    backend = dist.get_backend(group)
    assert backend in ["gloo", "nccl"]
    device = torch.device("cpu" if backend == "gloo" else "cuda")

    buffer = pickle.dumps(data)
    if len(buffer) > 1024 ** 3:
        logger = logging.getLogger(__name__)
        logger.warning(
            "Rank {} trying to all-gather {:.2f} GB of data on device {}".format(
                get_rank(), len(buffer) / (1024 ** 3), device
            )
        )
    storage = torch.ByteStorage.from_buffer(buffer)
    tensor = torch.ByteTensor(storage).to(device=device)
    return tensor


def _pad_to_largest_tensor(tensor, group):
    """
    Returns:
        list[int]: size of the tensor, on each rank
        Tensor: padded tensor that has the max size
    """
    world_size = dist.get_world_size(group=group)
    assert (
        world_size >= 1
    ), "comm.gather/all_gather must be called from ranks within the given group!"
    local_size = torch.tensor([tensor.numel()], dtype=torch.int64, device=tensor.device)
    size_list = [
        torch.zeros([1], dtype=torch.int64, device=tensor.device) for _ in range(world_size)
    ]
    dist.all_gather(size_list, local_size, group=group)
    size_list = [int(size.item()) for size in size_list]

    max_size = max(size_list)

    # we pad the tensor because torch all_gather does not support
    # gathering tensors of different shapes
    if local_size != max_size:
        padding = torch.zeros((max_size - local_size,), dtype=torch.uint8, device=tensor.device)
        tensor = torch.cat((tensor, padding), dim=0)
    return size_list, tensor


def all_gather(data, group=None):
    """
    Run all_gather on arbitrary picklable data (not necessarily tensors).

    Args:
        data: any picklable object
        group: a torch process group. By default, will use a group which
            contains all ranks on gloo backend.

    Returns:
        list[data]: list of data gathered from each rank
    """
    if get_world_size() == 1:
        return [data]
    if group is None:
        group = _get_global_gloo_group()
    if dist.get_world_size(group) == 1:
        return [data]

    tensor = _serialize_to_tensor(data, group)

    size_list, tensor = _pad_to_largest_tensor(tensor, group)
    max_size = max(size_list)

    # receiving Tensor from all ranks
    tensor_list = [
        torch.empty((max_size,), dtype=torch.uint8, device=tensor.device) for _ in size_list
    ]
    dist.all_gather(tensor_list, tensor, group=group)

    data_list = []
    for size, tensor in zip(size_list, tensor_list):
        buffer = tensor.cpu().numpy().tobytes()[:size]
        data_list.append(pickle.loads(buffer))

    return data_list


def gather(data, dst=0, group=None):
    """
    Run gather on arbitrary picklable data (not necessarily tensors).

    Args:
        data: any picklable object
        dst (int): destination rank
        group: a torch process group. By default, will use a group which
            contains all ranks on gloo backend.

    Returns:
        list[data]: on dst, a list of data gathered from each rank. Otherwise,
            an empty list.
    """
    if get_world_size() == 1:
        return [data]
    if group is None:
        group = _get_global_gloo_group()
    if dist.get_world_size(group=group) == 1:
        return [data]
    rank = dist.get_rank(group=group)

    tensor = _serialize_to_tensor(data, group)
    size_list, tensor = _pad_to_largest_tensor(tensor, group)

    # receiving Tensor from all ranks
    if rank == dst:
        max_size = max(size_list)
        tensor_list = [
            torch.empty((max_size,), dtype=torch.uint8, device=tensor.device) for _ in size_list
        ]
        dist.gather(tensor, tensor_list, dst=dst, group=group)

        data_list = []
        for size, tensor in zip(size_list, tensor_list):
            buffer = tensor.cpu().numpy().tobytes()[:size]
            data_list.append(pickle.loads(buffer))
        return data_list
    else:
        dist.gather(tensor, [], dst=dst, group=group)
        return []


def all_reduce(data, op="sum"):

    def op_map(op):
        op_dict = {
            "SUM": dist.ReduceOp.SUM,
            "MAX": dist.ReduceOp.MAX,
            "MIN": dist.ReduceOp.MIN,
            "BAND": dist.ReduceOp.BAND,
            "BOR": dist.ReduceOp.BOR,
            "BXOR": dist.ReduceOp.BXOR,
            "PRODUCT": dist.ReduceOp.PRODUCT,
        }
        return op_dict[op]

    world_size = get_world_size()
    if world_size > 1:
        reduced_data = data.clone()
        dist.all_reduce(reduced_data, op=op_map(op.upper()))
        return reduced_data
    return data


def shared_random_seed():
    """
    Returns:
        int: a random number that is the same across all workers.
            If workers need a shared RNG, they can use this shared seed to
            create one.

    All workers must call this function, otherwise it will deadlock.
    """
    ints = np.random.randint(2 ** 31)
    all_ints = all_gather(ints)
    return all_ints[0]


def reduce_dict(input_dict, average=True):
    """
    Reduce the values in the dictionary from all processes so that process with rank
    0 has the reduced results.

    Args:
        input_dict (dict): inputs to be reduced. All the values must be scalar CUDA Tensor.
        average (bool): whether to do average or sum

    Returns:
        a dict with the same keys as input_dict, after reduction.
    """
    world_size = get_world_size()
    if world_size < 2:
        return input_dict
    with torch.no_grad():
        names = []
        values = []
        # sort the keys so that they are consistent across processes
        for k in sorted(input_dict.keys()):
            names.append(k)
            values.append(input_dict[k])
        values = torch.stack(values, dim=0)
        dist.reduce(values, dst=0)
        if dist.get_rank() == 0 and average:
            # only main process gets accumulated, so only divide by
            # world_size in this case
            values /= world_size
        reduced_dict = {k: v for k, v in zip(names, values)}
    return reduced_dict
```

#### cvpods/utils/distributed/__init__.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-

from .comm import *
```

#### cvpods/utils/env/env.py

```python
# Copyright (c) Facebook, Inc. and its affiliates.
import importlib
import importlib.util
import logging
import os
import random
import sys
from datetime import datetime

import numpy as np

import torch

__all__ = ["seed_all_rng", "setup_environment", "setup_custom_environment", "TORCH_VERSION"]

TORCH_VERSION = tuple(int(x) for x in torch.__version__.split(".")[:2])


def seed_all_rng(seed=None):
    """
    Set the random seed for the RNG in torch, numpy and python.

    Args:
        seed (int): if None, will use a strong random seed.

    Returns:
        seed (int): used seed value.
    """
    if seed is None:
        seed = (
            os.getpid()
            + int(datetime.now().strftime("%S%f"))
            + int.from_bytes(os.urandom(2), "big")
        )
        logger = logging.getLogger(__name__)
        logger.info("Using a generated random seed {}".format(seed))
    np.random.seed(seed)
    torch.set_rng_state(torch.manual_seed(seed).get_state())
    random.seed(seed)
    return seed


# from https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path
def _import_file(module_name, file_path, make_importable=False):
    spec = importlib.util.spec_from_file_location(module_name, file_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    if make_importable:
        sys.modules[module_name] = module
    return module


def _configure_libraries():
    """
    Configurations for some libraries.
    """
    # An environment option to disable `import cv2` globally,
    # in case it leads to negative performance impact
    disable_cv2 = int(os.environ.get("cvpods_DISABLE_CV2", False))
    if disable_cv2:
        sys.modules["cv2"] = None
    else:
        # Disable opencl in opencv since its interaction with cuda often has negative effects
        # This envvar is supported after OpenCV 3.4.0
        os.environ["OPENCV_OPENCL_RUNTIME"] = "disabled"
        try:
            import cv2

            if int(cv2.__version__.split(".")[0]) >= 3:
                cv2.ocl.setUseOpenCL(False)
        except ImportError:
            pass


_ENV_SETUP_DONE = False


def setup_environment():
    """Perform environment setup work. The default setup is a no-op, but this
    function allows the user to specify a Python source file or a module in
    the $cvpods_ENV_MODULE environment variable, that performs
    custom setup work that may be necessary to their computing environment.
    """
    global _ENV_SETUP_DONE
    if _ENV_SETUP_DONE:
        return
    _ENV_SETUP_DONE = True

    _configure_libraries()

    custom_module_path = os.environ.get("cvpods_ENV_MODULE")

    if custom_module_path:
        setup_custom_environment(custom_module_path)
    else:
        # The default setup is a no-op
        pass


def setup_custom_environment(custom_module):
    """
    Load custom environment setup by importing a Python source file or a
    module, and run the setup function.
    """
    if custom_module.endswith(".py"):
        module = _import_file("cvpods.utils.env.env.custom_module", custom_module)
    else:
        module = importlib.import_module(custom_module)
    assert hasattr(module, "setup_environment") and callable(module.setup_environment), (
        "Custom environment module defined in {} does not have the "
        "required callable attribute 'setup_environment'."
    ).format(custom_module)
    module.setup_environment()
```

#### cvpods/utils/env/collect_env.py

```python
# Copyright (c) Facebook, Inc. and its affiliates.
import importlib
import os
import re
import subprocess
import sys
from collections import defaultdict
from tabulate import tabulate

import numpy as np
import PIL

import torch
import torchvision

__all__ = ["collect_env_info"]


def collect_torch_env():
    try:
        import torch.__config__

        return torch.__config__.show()
    except ImportError:
        # compatible with older versions of pytorch
        from torch.utils.collect_env import get_pretty_env_info

        return get_pretty_env_info()


def get_env_module():
    var_name = "cvpods_ENV_MODULE"
    return var_name, os.environ.get(var_name, "<not set>")


def detect_compute_compatibility(CUDA_HOME, so_file):
    try:
        cuobjdump = os.path.join(CUDA_HOME, "bin", "cuobjdump")
        if os.path.isfile(cuobjdump):
            output = subprocess.check_output(
                "'{}' --list-elf '{}'".format(cuobjdump, so_file), shell=True
            )
            output = output.decode("utf-8").strip().split("\n")
            sm = []
            for line in output:
                line = re.findall(r"\.sm_[0-9]*\.", line)[0]
                sm.append(line.strip("."))
            sm = sorted(set(sm))
            return ", ".join(sm)
        else:
            return so_file + "; cannot find cuobjdump"
    except Exception:
        # unhandled failure
        return so_file


def collect_env_info():
    has_cuda = torch.cuda.is_available()
    # NOTE: the use of CUDA_HOME requires the CUDA build deps, though in
    # theory cvpods should be made runnable with only the CUDA runtime
    from torch.utils.cpp_extension import CUDA_HOME

    data = []
    data.append(("sys.platform", sys.platform))
    data.append(("Python", sys.version.replace("\n", "")))
    data.append(("numpy", np.__version__))

    try:
        import cvpods
        from cvpods import _C
    except ImportError:
        data.append(("cvpods._C", "failed to import"))
    else:
        data.append(
            ("cvpods", cvpods.__version__ + " @" + os.path.dirname(cvpods.__file__))
        )
        data.append(("cvpods compiler", _C.get_compiler_version()))
        data.append(("cvpods CUDA compiler", _C.get_cuda_version()))
        if has_cuda:
            data.append(
                ("cvpods arch flags", detect_compute_compatibility(CUDA_HOME, _C.__file__))
            )

    data.append(get_env_module())
    data.append(("PyTorch", torch.__version__ + " @" + os.path.dirname(torch.__file__)))
    data.append(("PyTorch debug build", torch.version.debug))

    data.append(("CUDA available", has_cuda))
    if has_cuda:
        devices = defaultdict(list)
        for k in range(torch.cuda.device_count()):
            devices[torch.cuda.get_device_name(k)].append(str(k))
        for name, devids in devices.items():
            data.append(("GPU " + ",".join(devids), name))

        from torch.utils.cpp_extension import CUDA_HOME

        data.append(("CUDA_HOME", str(CUDA_HOME)))

        if CUDA_HOME is not None and os.path.isdir(CUDA_HOME):
            try:
                nvcc = os.path.join(CUDA_HOME, "bin", "nvcc")
                nvcc = subprocess.check_output("'{}' -V | tail -n1".format(nvcc), shell=True)
                nvcc = nvcc.decode("utf-8").strip()
            except subprocess.SubprocessError:
                nvcc = "Not Available"
            data.append(("NVCC", nvcc))

        cuda_arch_list = os.environ.get("TORCH_CUDA_ARCH_LIST", None)
        if cuda_arch_list:
            data.append(("TORCH_CUDA_ARCH_LIST", cuda_arch_list))
    data.append(("Pillow", PIL.__version__))

    try:
        data.append(
            (
                "torchvision",
                str(torchvision.__version__) + " @" + os.path.dirname(torchvision.__file__),
            )
        )
        if has_cuda:
            try:
                torchvision_C = importlib.util.find_spec("torchvision._C").origin
                msg = detect_compute_compatibility(CUDA_HOME, torchvision_C)
                data.append(("torchvision arch flags", msg))
            except ImportError:
                data.append(("torchvision._C", "failed to find"))
    except AttributeError:
        data.append(("torchvision", "unknown"))

    try:
        import cv2

        data.append(("cv2", cv2.__version__))
    except ImportError:
        pass
    env_str = tabulate(data) + "\n"
    env_str += collect_torch_env()
    return env_str


if __name__ == "__main__":
    print(collect_env_info())
```

#### cvpods/utils/env/__init__.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-

from .collect_env import *
from .env import *
```

#### cvpods/utils/visualizer/colormap.py

```python
# Copyright (c) Facebook, Inc. and its affiliates.

"""
An awesome colormap for really neat visualizations.
Copied from Detectron, and removed gray colors.
"""

import numpy as np

__all__ = ["colormap", "random_color"]

# fmt: off
# RGB:
_COLORS = np.array(
    [
        0.000, 0.447, 0.741,
        0.850, 0.325, 0.098,
        0.929, 0.694, 0.125,
        0.494, 0.184, 0.556,
        0.466, 0.674, 0.188,
        0.301, 0.745, 0.933,
        0.635, 0.078, 0.184,
        0.300, 0.300, 0.300,
        0.600, 0.600, 0.600,
        1.000, 0.000, 0.000,
        1.000, 0.500, 0.000,
        0.749, 0.749, 0.000,
        0.000, 1.000, 0.000,
        0.000, 0.000, 1.000,
        0.667, 0.000, 1.000,
        0.333, 0.333, 0.000,
        0.333, 0.667, 0.000,
        0.333, 1.000, 0.000,
        0.667, 0.333, 0.000,
        0.667, 0.667, 0.000,
        0.667, 1.000, 0.000,
        1.000, 0.333, 0.000,
        1.000, 0.667, 0.000,
        1.000, 1.000, 0.000,
        0.000, 0.333, 0.500,
        0.000, 0.667, 0.500,
        0.000, 1.000, 0.500,
        0.333, 0.000, 0.500,
        0.333, 0.333, 0.500,
        0.333, 0.667, 0.500,
        0.333, 1.000, 0.500,
        0.667, 0.000, 0.500,
        0.667, 0.333, 0.500,
        0.667, 0.667, 0.500,
        0.667, 1.000, 0.500,
        1.000, 0.000, 0.500,
        1.000, 0.333, 0.500,
        1.000, 0.667, 0.500,
        1.000, 1.000, 0.500,
        0.000, 0.333, 1.000,
        0.000, 0.667, 1.000,
        0.000, 1.000, 1.000,
        0.333, 0.000, 1.000,
        0.333, 0.333, 1.000,
        0.333, 0.667, 1.000,
        0.333, 1.000, 1.000,
        0.667, 0.000, 1.000,
        0.667, 0.333, 1.000,
        0.667, 0.667, 1.000,
        0.667, 1.000, 1.000,
        1.000, 0.000, 1.000,
        1.000, 0.333, 1.000,
        1.000, 0.667, 1.000,
        0.333, 0.000, 0.000,
        0.500, 0.000, 0.000,
        0.667, 0.000, 0.000,
        0.833, 0.000, 0.000,
        1.000, 0.000, 0.000,
        0.000, 0.167, 0.000,
        0.000, 0.333, 0.000,
        0.000, 0.500, 0.000,
        0.000, 0.667, 0.000,
        0.000, 0.833, 0.000,
        0.000, 1.000, 0.000,
        0.000, 0.000, 0.167,
        0.000, 0.000, 0.333,
        0.000, 0.000, 0.500,
        0.000, 0.000, 0.667,
        0.000, 0.000, 0.833,
        0.000, 0.000, 1.000,
        0.000, 0.000, 0.000,
        0.143, 0.143, 0.143,
        0.857, 0.857, 0.857,
        1.000, 1.000, 1.000
    ]
).astype(np.float32).reshape(-1, 3)
# fmt: on


def colormap(rgb=False, maximum=255):
    """
    Args:
        rgb (bool): whether to return RGB colors or BGR colors.
        maximum (int): either 255 or 1

    Returns:
        ndarray: a float32 array of Nx3 colors, in range [0, 255] or [0, 1]
    """
    assert maximum in [255, 1], maximum
    c = _COLORS * maximum
    if not rgb:
        c = c[:, ::-1]
    return c


def random_color(rgb=False, maximum=255):
    """
    Args:
        rgb (bool): whether to return RGB colors or BGR colors.
        maximum (int): either 255 or 1

    Returns:
        ndarray: a vector of 3 numbers
    """
    idx = np.random.randint(0, len(_COLORS))
    ret = _COLORS[idx] * maximum
    if not rgb:
        ret = ret[::-1]
    return ret


if __name__ == "__main__":
    import cv2

    size = 100
    H, W = 10, 10
    canvas = np.random.rand(H * size, W * size, 3).astype("float32")
    for h in range(H):
        for w in range(W):
            idx = h * W + w
            if idx >= len(_COLORS):
                break
            canvas[h * size: (h + 1) * size, w * size: (w + 1) * size] = _COLORS[idx]
    cv2.imshow("a", canvas)
    cv2.waitKey(0)
```

#### cvpods/utils/visualizer/show.py

```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.

import copy
import pylab as plt

import numpy as np


def draw_box(ax, vertices, color='black'):
    """
    Draw box with color.

    Args:
        ax (list): axes to draw box along
        vertices (ndarray): indices of shape (N x 2)
        color (str): plotted color
    """
    connections = [
        [0, 1],
        [1, 2],
        [2, 3],
        [3, 0],
    ]
    for connection in connections:
        ax.plot(*vertices[:, connection], c=color, lw=5)


def visualize_feature_maps(
        fm,
        boxes=[],
        keypoints=[],
        stride=1,
        save_filename=None
):
    """
    Visualize feature map with boxes or key points.

    Args:
        fm (torch.Tensor): feature map of shape H x W x c, c is channel
        boxes (ndarray): boxes to be visualized.
        keypoints (ndarray): key points to be visualized
        stride (int): used to normalize boxes or keypoints
        save_filename (bool): whether save to disk
    """
    nc = np.ceil(np.sqrt(fm.shape[2]))  # column
    nr = np.ceil(fm.shape[2] / nc)  # row
    nc = int(nc)
    nr = int(nr)
    plt.figure(figsize=(64, 64))
    for i in range(fm.shape[2]):
        ax = plt.subplot(nr, nc, i + 1)
        ax.imshow(fm[:, :, i], cmap='jet')

        for obj in boxes:
            box = copy.deepcopy(obj) / stride
            draw_box(ax, box, color='g')

        for pts_score in keypoints:
            pts = pts_score[:8]
            pts = pts / stride
            for i in range(4):
                ax.plot(pts[2 * i + 1], pts[2 * i + 0], 'r*')
            ax.plot([pts[1], pts[3]], [pts[0], pts[2]], c='y', lw=5)
            ax.plot([pts[3], pts[5]], [pts[2], pts[4]], c='g', lw=5)
            ax.plot([pts[5], pts[7]], [pts[4], pts[6]], c='b', lw=5)
            ax.plot([pts[7], pts[1]], [pts[6], pts[0]], c='r', lw=5)

        # plt.colorbar()
        ax.axis('off')
    if save_filename:
        plt.savefig(save_filename)
    else:
        plt.show()
    plt.close()
```

#### cvpods/utils/visualizer/video_visualizer.py

```python
# Copyright (c) Facebook, Inc. and its affiliates.
import numpy as np
import pycocotools.mask as mask_util

from .colormap import random_color
from .visualizer import ColorMode, Visualizer, _create_text_labels, _PanopticPrediction


class _DetectedInstance:
    """
    Used to store data about detected objects in video frame,
    in order to transfer color to objects in the future frames.

    Attributes:
        label (int):
        bbox (tuple[float]):
        mask_rle (dict):
        color (tuple[float]): RGB colors in range (0, 1)
        ttl (int): time-to-live for the instance. For example, if ttl=2,
            the instance color can be transferred to objects in the next two frames.
    """

    __slots__ = ["label", "bbox", "mask_rle", "color", "ttl"]

    def __init__(self, label, bbox, mask_rle, color, ttl):
        self.label = label
        self.bbox = bbox
        self.mask_rle = mask_rle
        self.color = color
        self.ttl = ttl


class VideoVisualizer:
    def __init__(self, metadata, instance_mode=ColorMode.IMAGE):
        """
        Args:
            metadata (MetadataCatalog): image metadata.
        """
        self.metadata = metadata
        self._old_instances = []
        assert instance_mode in [
            ColorMode.IMAGE,
            ColorMode.IMAGE_BW,
        ], "Other mode not supported yet."
        self._instance_mode = instance_mode

    def draw_instance_predictions(self, frame, predictions):
        """
        Draw instance-level prediction results on an image.

        Args:
            frame (ndarray): an RGB image of shape (H, W, C), in the range [0, 255].
            predictions (Instances): the output of an instance detection/segmentation
                model. Following fields will be used to draw:
                "pred_boxes", "pred_classes", "scores", "pred_masks" (or "pred_masks_rle").

        Returns:
            output (VisImage): image object with visualizations.
        """
        frame_visualizer = Visualizer(frame, self.metadata)
        num_instances = len(predictions)
        if num_instances == 0:
            return frame_visualizer.output

        boxes = predictions.pred_boxes.tensor.numpy() if predictions.has("pred_boxes") else None
        scores = predictions.scores if predictions.has("scores") else None
        classes = predictions.pred_classes.numpy() if predictions.has("pred_classes") else None
        keypoints = predictions.pred_keypoints if predictions.has("pred_keypoints") else None

        if predictions.has("pred_masks"):
            masks = predictions.pred_masks
            # mask IOU is not yet enabled
            # masks_rles = mask_util.encode(np.asarray(masks.permute(1, 2, 0), order="F"))
            # assert len(masks_rles) == num_instances
        else:
            masks = None

        detected = [
            _DetectedInstance(classes[i], boxes[i], mask_rle=None, color=None, ttl=8)
            for i in range(num_instances)
        ]
        colors = self._assign_colors(detected)

        labels = _create_text_labels(classes, scores, getattr(self.metadata, "thing_classes", None))

        if self._instance_mode == ColorMode.IMAGE_BW:
            # any() returns uint8 tensor
            frame_visualizer.output.img = frame_visualizer._create_grayscale_image(
                (masks.any(dim=0) > 0).numpy() if masks is not None else None
            )
            alpha = 0.3
        else:
            alpha = 0.5

        frame_visualizer.overlay_instances(
            boxes=None if masks is not None else boxes,  # boxes are a bit distracting
            masks=masks,
            labels=labels,
            keypoints=keypoints,
            assigned_colors=colors,
            alpha=alpha,
        )

        return frame_visualizer.output

    def draw_sem_seg(self, frame, sem_seg, area_threshold=None):
        """
        Args:
            sem_seg (ndarray or Tensor): semantic segmentation of shape (H, W),
                each value is the integer label.
            area_threshold (Optional[int]): only draw segmentations larger than the threshold
        """
        # don't need to do anything special
        frame_visualizer = Visualizer(frame, self.metadata)
        frame_visualizer.draw_sem_seg(sem_seg, area_threshold=area_threshold)
        return frame_visualizer.output

    def draw_panoptic_seg_predictions(
        self, frame, panoptic_seg, segments_info, area_threshold=None, alpha=0.5
    ):
        frame_visualizer = Visualizer(frame, self.metadata)
        pred = _PanopticPrediction(panoptic_seg, segments_info)

        if self._instance_mode == ColorMode.IMAGE_BW:
            frame_visualizer.output.img = frame_visualizer._create_grayscale_image(
                pred.non_empty_mask()
            )

        # draw mask for all semantic segments first i.e. "stuff"
        for mask, sinfo in pred.semantic_masks():
            category_idx = sinfo["category_id"]
            try:
                mask_color = [x / 255 for x in self.metadata.stuff_colors[category_idx]]
            except AttributeError:
                mask_color = None

            frame_visualizer.draw_binary_mask(
                mask,
                color=mask_color,
                text=self.metadata.stuff_classes[category_idx],
                alpha=alpha,
                area_threshold=area_threshold,
            )

        all_instances = list(pred.instance_masks())
        if len(all_instances) == 0:
            return frame_visualizer.output
        # draw mask for all instances second
        masks, sinfo = list(zip(*all_instances))
        num_instances = len(masks)
        masks_rles = mask_util.encode(
            np.asarray(np.asarray(masks).transpose(1, 2, 0), dtype=np.uint8, order="F")
        )
        assert len(masks_rles) == num_instances

        category_ids = [x["category_id"] for x in sinfo]
        detected = [
            _DetectedInstance(category_ids[i], bbox=None, mask_rle=masks_rles[i], color=None, ttl=8)
            for i in range(num_instances)
        ]
        colors = self._assign_colors(detected)
        labels = [self.metadata.thing_classes[k] for k in category_ids]

        frame_visualizer.overlay_instances(
            boxes=None,
            masks=masks,
            labels=labels,
            keypoints=None,
            assigned_colors=colors,
            alpha=alpha,
        )
        return frame_visualizer.output

    def _assign_colors(self, instances):
        """
        Naive tracking heuristics to assign same color to the same instance,
        will update the internal state of tracked instances.

        Returns:
            list[tuple[float]]: list of colors.
        """

        # Compute iou with either boxes or masks:
        is_crowd = np.zeros((len(instances),), dtype=np.bool)
        if instances[0].bbox is None:
            assert instances[0].mask_rle is not None
            # use mask iou only when box iou is None
            # because box seems good enough
            rles_old = [x.mask_rle for x in self._old_instances]
            rles_new = [x.mask_rle for x in instances]
            ious = mask_util.iou(rles_old, rles_new, is_crowd)
            threshold = 0.5
        else:
            boxes_old = [x.bbox for x in self._old_instances]
            boxes_new = [x.bbox for x in instances]
            ious = mask_util.iou(boxes_old, boxes_new, is_crowd)
            threshold = 0.6
        if len(ious) == 0:
            ious = np.zeros((len(self._old_instances), len(instances)), dtype="float32")

        # Only allow matching instances of the same label:
        for old_idx, old in enumerate(self._old_instances):
            for new_idx, new in enumerate(instances):
                if old.label != new.label:
                    ious[old_idx, new_idx] = 0

        matched_new_per_old = np.asarray(ious).argmax(axis=1)
        max_iou_per_old = np.asarray(ious).max(axis=1)

        # Try to find match for each old instance:
        extra_instances = []
        for idx, inst in enumerate(self._old_instances):
            if max_iou_per_old[idx] > threshold:
                newidx = matched_new_per_old[idx]
                if instances[newidx].color is None:
                    instances[newidx].color = inst.color
                    continue
            # If an old instance does not match any new instances,
            # keep it for the next frame in case it is just missed by the detector
            inst.ttl -= 1
            if inst.ttl > 0:
                extra_instances.append(inst)

        # Assign random color to newly-detected instances:
        for inst in instances:
            if inst.color is None:
                inst.color = random_color(rgb=True, maximum=1)
        self._old_instances = instances[:] + extra_instances
        return [d.color for d in instances]
```

#### cvpods/utils/visualizer/__init__.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-

from .colormap import *
from .video_visualizer import *
from .visualizer import *
```

#### cvpods/utils/visualizer/visualizer.py

```python
# Copyright (c) Facebook, Inc. and its affiliates.
import colorsys
import logging
import math
from enum import Enum, unique

import cv2
import matplotlib as mpl
import matplotlib.colors as mplc
import matplotlib.figure as mplfigure
import numpy as np
from matplotlib.backends.backend_agg import FigureCanvasAgg
import pycocotools.mask as mask_util

import torch

from cvpods.structures import BitMasks, Boxes, BoxMode, Keypoints, PolygonMasks, RotatedBoxes

from .colormap import random_color

logger = logging.getLogger(__name__)

__all__ = ["ColorMode", "VisImage", "Visualizer"]


_SMALL_OBJECT_AREA_THRESH = 1000
_LARGE_MASK_AREA_THRESH = 120000
_OFF_WHITE = (1.0, 1.0, 240.0 / 255)
_BLACK = (0, 0, 0)
_RED = (1.0, 0, 0)

_KEYPOINT_THRESHOLD = 0.05


@unique
class ColorMode(Enum):
    """
    Enum of different color modes to use for instance visualizations.

    Attributes:
        IMAGE: Picks a random color for every instance and overlay segmentations with low opacity.
        SEGMENTATION: Let instances of the same category have similar colors, and overlay them with
            high opacity. This provides more attention on the quality of segmentation.
        IMAGE_BW: same as IMAGE, but convert all areas without masks to gray-scale.
            Only available for drawing per-instance mask predictions.
    """

    IMAGE = 0
    SEGMENTATION = 1
    IMAGE_BW = 2


class GenericMask:
    """
    Attribute:
        polygons (list[ndarray]): list[ndarray]: polygons for this mask.
            Each ndarray has format [x, y, x, y, ...]
        mask (ndarray): a binary mask
    """

    def __init__(self, mask_or_polygons, height, width):
        self._mask = self._polygons = self._has_holes = None
        self.height = height
        self.width = width

        m = mask_or_polygons
        if isinstance(m, dict):
            # RLEs
            assert "counts" in m and "size" in m
            if isinstance(m["counts"], list):  # uncompressed RLEs
                h, w = m["size"]
                assert h == height and w == width
                m = mask_util.frPyObjects(m, h, w)
            self._mask = mask_util.decode(m)[:, :]
            return

        if isinstance(m, list):  # list[ndarray]
            self._polygons = [np.asarray(x).reshape(-1) for x in m]
            return

        if isinstance(m, np.ndarray):  # assumed to be a binary mask
            assert m.shape[1] != 2, m.shape
            assert m.shape == (height, width), m.shape
            self._mask = m.astype("uint8")
            return

        raise ValueError("GenericMask cannot handle object {} of type '{}'".format(m, type(m)))

    @property
    def mask(self):
        if self._mask is None:
            self._mask = self.polygons_to_mask(self._polygons)
        return self._mask

    @property
    def polygons(self):
        if self._polygons is None:
            self._polygons, self._has_holes = self.mask_to_polygons(self._mask)
        return self._polygons

    @property
    def has_holes(self):
        if self._has_holes is None:
            if self._mask is not None:
                self._polygons, self._has_holes = self.mask_to_polygons(self._mask)
            else:
                self._has_holes = False  # if original format is polygon, does not have holes
        return self._has_holes

    def mask_to_polygons(self, mask):
        # cv2.RETR_CCOMP flag retrieves all the contours and arranges them to a 2-level
        # hierarchy. External contours (boundary) of the object are placed in hierarchy-1.
        # Internal contours (holes) are placed in hierarchy-2.
        # cv2.CHAIN_APPROX_NONE flag gets vertices of polygons from contours.
        mask = np.ascontiguousarray(mask)  # some versions of cv2 does not support incontiguous arr
        res = cv2.findContours(mask.astype("uint8"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)
        hierarchy = res[-1]
        if hierarchy is None:  # empty mask
            return [], False
        has_holes = (hierarchy.reshape(-1, 4)[:, 3] >= 0).sum() > 0
        res = res[-2]
        res = [x.flatten() for x in res]
        res = [x for x in res if len(x) >= 6]
        return res, has_holes

    def polygons_to_mask(self, polygons):
        rle = mask_util.frPyObjects(polygons, self.height, self.width)
        rle = mask_util.merge(rle)
        return mask_util.decode(rle)[:, :]

    def area(self):
        return self.mask.sum()

    def bbox(self):
        p = mask_util.frPyObjects(self.polygons, self.height, self.width)
        p = mask_util.merge(p)
        bbox = mask_util.toBbox(p)
        bbox[2] += bbox[0]
        bbox[3] += bbox[1]
        return bbox


class _PanopticPrediction:
    def __init__(self, panoptic_seg, segments_info):
        self._seg = panoptic_seg

        self._sinfo = {s["id"]: s for s in segments_info}  # seg id -> seg info
        segment_ids, areas = torch.unique(panoptic_seg, sorted=True, return_counts=True)
        areas = areas.numpy()
        sorted_idxs = np.argsort(-areas)
        self._seg_ids, self._seg_areas = segment_ids[sorted_idxs], areas[sorted_idxs]
        self._seg_ids = self._seg_ids.tolist()
        for sid, area in zip(self._seg_ids, self._seg_areas):
            if sid in self._sinfo:
                self._sinfo[sid]["area"] = float(area)

    def non_empty_mask(self):
        """
        Returns:
            (H, W) array, a mask for all pixels that have a prediction
        """
        empty_ids = []
        for id in self._seg_ids:
            if id not in self._sinfo:
                empty_ids.append(id)
        if len(empty_ids) == 0:
            return np.zeros(self._seg.shape, dtype=np.uint8)
        assert (
            len(empty_ids) == 1
        ), ">1 ids corresponds to no labels. This is currently not supported"
        return (self._seg != empty_ids[0]).numpy().astype(np.bool)

    def semantic_masks(self):
        for sid in self._seg_ids:
            sinfo = self._sinfo.get(sid)
            if sinfo is None or sinfo["isthing"]:
                # Some pixels (e.g. id 0 in PanopticFPN) have no instance or semantic predictions.
                continue
            yield (self._seg == sid).numpy().astype(np.bool), sinfo

    def instance_masks(self):
        for sid in self._seg_ids:
            sinfo = self._sinfo.get(sid)
            if sinfo is None or not sinfo["isthing"]:
                continue
            mask = (self._seg == sid).numpy().astype(np.bool)
            if mask.sum() > 0:
                yield mask, sinfo


def _create_text_labels(classes, scores, class_names):
    """
    Args:
        classes (list[int] or None):
        scores (list[float] or None):
        class_names (list[str] or None):

    Returns:
        list[str] or None
    """
    labels = None
    if classes is not None and class_names is not None and len(class_names) > 1:
        labels = [class_names[int(i)] for i in classes]
    if scores is not None:
        if labels is None:
            labels = ["{:.0f}%".format(s * 100) for s in scores]
        else:
            labels = ["{} {:.0f}%".format(label, s * 100) for label, s in zip(labels, scores)]
    return labels


class VisImage:
    def __init__(self, img, scale=1.0):
        """
        Args:
            img (ndarray): an RGB image of shape (H, W, 3).
            scale (float): scale the input image
        """
        self.img = img
        self.scale = scale
        self.width, self.height = img.shape[1], img.shape[0]
        self._setup_figure()

    def _setup_figure(self):
        """
        Args:
            Same as in :meth:`__init__()`.

        Returns:
            fig (matplotlib.pyplot.figure): top level container for all the image plot elements.
            ax (matplotlib.pyplot.Axes): contains figure elements and sets the coordinate system.
        """
        fig = mplfigure.Figure(frameon=False)
        self.dpi = fig.get_dpi()
        # add a small 1e-2 to avoid precision lost due to matplotlib's truncation
        # (https://github.com/matplotlib/matplotlib/issues/15363)
        fig.set_size_inches(
            (self.width * self.scale + 1e-2) / self.dpi,
            (self.height * self.scale + 1e-2) / self.dpi,
        )
        self.canvas = FigureCanvasAgg(fig)
        # self.canvas = mpl.backends.backend_cairo.FigureCanvasCairo(fig)
        ax = fig.add_axes([0.0, 0.0, 1.0, 1.0])
        ax.axis("off")
        ax.set_xlim(0.0, self.width)
        ax.set_ylim(self.height)

        self.fig = fig
        self.ax = ax

    def save(self, filepath):
        """
        Args:
            filepath (str): a string that contains the absolute path, including the file name, where
                the visualized image will be saved.
        """
        if filepath.lower().endswith(".jpg") or filepath.lower().endswith(".png"):
            # faster than matplotlib's imshow
            cv2.imwrite(filepath, self.get_image()[:, :, ::-1])
        else:
            # support general formats (e.g. pdf)
            self.ax.imshow(self.img, interpolation="nearest")
            self.fig.savefig(filepath)

    def get_image(self):
        """
        Returns:
            ndarray: the visualized image of shape (H, W, 3) (RGB) in uint8 type.
              The shape is scaled w.r.t the input image using the given `scale` argument.
        """
        canvas = self.canvas
        s, (width, height) = canvas.print_to_buffer()
        if (self.width, self.height) != (width, height):
            img = cv2.resize(self.img, (width, height))
        else:
            img = self.img

        # buf = io.BytesIO()  # works for cairo backend
        # canvas.print_rgba(buf)
        # width, height = self.width, self.height
        # s = buf.getvalue()

        buffer = np.frombuffer(s, dtype="uint8")

        # imshow is slow. blend manually (still quite slow)
        img_rgba = buffer.reshape(height, width, 4)
        rgb, alpha = np.split(img_rgba, [3], axis=2)

        try:
            import numexpr as ne  # fuse them with numexpr

            visualized_image = ne.evaluate("img * (1 - alpha / 255.0) + rgb * (alpha / 255.0)")
        except ImportError:
            alpha = alpha.astype("float32") / 255.0
            visualized_image = img * (1 - alpha) + rgb * alpha

        visualized_image = visualized_image.astype("uint8")

        return visualized_image


class Visualizer:
    def __init__(self, img_rgb, metadata, scale=1.0, instance_mode=ColorMode.IMAGE):
        """
        Args:
            img_rgb: a numpy array of shape (H, W, C), where H and W correspond to
                the height and width of the image respectively. C is the number of
                color channels. The image is required to be in RGB format since that
                is a requirement of the Matplotlib library. The image is also expected
                to be in the range [0, 255].
            metadata (MetadataCatalog): image metadata.
        """
        self.img = np.asarray(img_rgb).clip(0, 255).astype(np.uint8)
        self.metadata = metadata
        self.output = VisImage(self.img, scale=scale)
        self.cpu_device = torch.device("cpu")

        # too small texts are useless, therefore clamp to 9
        self._default_font_size = max(
            np.sqrt(self.output.height * self.output.width) // 90, 10 // scale
        )
        self._instance_mode = instance_mode

    def draw_instance_predictions(self, predictions):
        """
        Draw instance-level prediction results on an image.

        Args:
            predictions (Instances): the output of an instance detection/segmentation
                model. Following fields will be used to draw:
                "pred_boxes", "pred_classes", "scores", "pred_masks" (or "pred_masks_rle").

        Returns:
            output (VisImage): image object with visualizations.
        """
        boxes = predictions.pred_boxes if predictions.has("pred_boxes") else None
        scores = predictions.scores if predictions.has("scores") else None
        classes = predictions.pred_classes if predictions.has("pred_classes") else None
        labels = _create_text_labels(classes, scores, self.metadata.thing_classes)
        keypoints = predictions.pred_keypoints if predictions.has("pred_keypoints") else None

        if predictions.has("pred_masks"):
            masks = np.asarray(predictions.pred_masks)
            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]
        else:
            masks = None

        if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.thing_colors:
            colors = [
                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes
            ]
            alpha = 0.8
        else:
            colors = None
            alpha = 0.5

        if self._instance_mode == ColorMode.IMAGE_BW:
            assert predictions.has("pred_masks"), "ColorMode.IMAGE_BW requires segmentations"
            self.output.img = self._create_grayscale_image(
                (predictions.pred_masks.any(dim=0) > 0).numpy()
            )
            alpha = 0.3

        self.overlay_instances(
            masks=masks,
            boxes=boxes,
            labels=labels,
            keypoints=keypoints,
            assigned_colors=colors,
            alpha=alpha,
        )
        return self.output

    def draw_sem_seg(self, sem_seg, area_threshold=None, alpha=0.8):
        """
        Draw semantic segmentation predictions/labels.

        Args:
            sem_seg (Tensor or ndarray): the segmentation of shape (H, W).
            area_threshold (int): segments with less than `area_threshold` are not drawn.
            alpha (float): the larger it is, the more opaque the segmentations are.

        Returns:
            output (VisImage): image object with visualizations.
        """
        if isinstance(sem_seg, torch.Tensor):
            sem_seg = sem_seg.numpy()
        labels, areas = np.unique(sem_seg, return_counts=True)
        sorted_idxs = np.argsort(-areas).tolist()
        labels = labels[sorted_idxs]
        for label in filter(lambda l: l < len(self.metadata.stuff_classes), labels):
            try:
                mask_color = [x / 255 for x in self.metadata.stuff_colors[label]]
            except (AttributeError, IndexError):
                mask_color = None

            binary_mask = (sem_seg == label).astype(np.uint8)
            text = self.metadata.stuff_classes[label]
            self.draw_binary_mask(
                binary_mask,
                color=mask_color,
                edge_color=_OFF_WHITE,
                text=text,
                alpha=alpha,
                area_threshold=area_threshold,
            )
        return self.output

    def draw_panoptic_seg_predictions(
        self, panoptic_seg, segments_info, area_threshold=None, alpha=0.7
    ):
        """
        Draw panoptic prediction results on an image.

        Args:
            panoptic_seg (Tensor): of shape (height, width) where the values are ids for each
                segment.
            segments_info (list[dict]): Describe each segment in `panoptic_seg`.
                Each dict contains keys "id", "category_id", "isthing".
            area_threshold (int): stuff segments with less than `area_threshold` are not drawn.

        Returns:
            output (VisImage): image object with visualizations.
        """
        pred = _PanopticPrediction(panoptic_seg, segments_info)

        if self._instance_mode == ColorMode.IMAGE_BW:
            self.output.img = self._create_grayscale_image(pred.non_empty_mask())

        # draw mask for all semantic segments first i.e. "stuff"
        for mask, sinfo in pred.semantic_masks():
            category_idx = sinfo["category_id"]
            try:
                mask_color = [x / 255 for x in self.metadata.stuff_colors[category_idx]]
            except AttributeError:
                mask_color = None

            text = self.metadata.stuff_classes[category_idx]
            self.draw_binary_mask(
                mask,
                color=mask_color,
                edge_color=_OFF_WHITE,
                text=text,
                alpha=alpha,
                area_threshold=area_threshold,
            )

        # draw mask for all instances second
        all_instances = list(pred.instance_masks())
        if len(all_instances) == 0:
            return self.output
        masks, sinfo = list(zip(*all_instances))
        category_ids = [x["category_id"] for x in sinfo]

        try:
            scores = [x["score"] for x in sinfo]
        except KeyError:
            scores = None
        labels = _create_text_labels(category_ids, scores, self.metadata.thing_classes)

        try:
            colors = [random_color(rgb=True, maximum=1) for k in category_ids]
        except AttributeError:
            colors = None
        self.overlay_instances(masks=masks, labels=labels, assigned_colors=colors, alpha=alpha)

        return self.output

    def draw_dataset_dict(self, dic):
        """
        Draw annotations/segmentaions in cvpods Dataset format.

        Args:
            dic (dict): annotation/segmentation data of one image, in cvpods Dataset format.

        Returns:
            output (VisImage): image object with visualizations.
        """
        annos = dic.get("annotations", None)
        if annos:
            if "segmentation" in annos[0]:
                masks = [x["segmentation"] for x in annos]
            else:
                masks = None
            if "keypoints" in annos[0]:
                keypts = [x["keypoints"] for x in annos]
                keypts = np.array(keypts).reshape(len(annos), -1, 3)
            else:
                keypts = None

            boxes = [BoxMode.convert(x["bbox"], x["bbox_mode"], BoxMode.XYXY_ABS) for x in annos]

            labels = [x["category_id"] for x in annos]
            names = self.metadata.thing_classes
            if names:
                labels = [names[i] for i in labels]
            labels = [
                "{}".format(i) + ("|crowd" if a.get("iscrowd", 0) else "")
                for i, a in zip(labels, annos)
            ]
            self.overlay_instances(labels=labels, boxes=boxes, masks=masks, keypoints=keypts)

        sem_seg = dic.get("sem_seg", None)
        if sem_seg is None and "sem_seg_file_name" in dic:
            sem_seg = cv2.imread(dic["sem_seg_file_name"], cv2.IMREAD_GRAYSCALE)
        if sem_seg is not None:
            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)
        return self.output

    def overlay_instances(  # noqa:C901
        self,
        *,
        boxes=None,
        labels=None,
        masks=None,
        keypoints=None,
        assigned_colors=None,
        alpha=0.5
    ):
        """
        Args:
            boxes (Boxes, RotatedBoxes or ndarray): either a :class:`Boxes`,
                or an Nx4 numpy array of XYXY_ABS format for the N objects in a single image,
                or a :class:`RotatedBoxes`,
                or an Nx5 numpy array of (x_center, y_center, width, height, angle_degrees) format
                for the N objects in a single image,
            labels (list[str]): the text to be displayed for each instance.
            masks (masks-like object): Supported types are:

                * `structures.masks.PolygonMasks`, `structures.masks.BitMasks`.
                * list[list[ndarray]]: contains the segmentation masks for all objects in one image.
                    The first level of the list corresponds to individual instances. The second
                    level to all the polygon that compose the instance, and the third level
                    to the polygon coordinates. The third level should have the format of
                    [x0, y0, x1, y1, ..., xn, yn] (n >= 3).
                * list[ndarray]: each ndarray is a binary mask of shape (H, W).
                * list[dict]: each dict is a COCO-style RLE.
            keypoints (Keypoint or array like): an array-like object of shape (N, K, 3),
                where the N is the number of instances and K is the number of keypoints.
                The last dimension corresponds to (x, y, visibility or score).
            assigned_colors (list[matplotlib.colors]): a list of colors, where each color
                corresponds to each mask or box in the image. Refer to 'matplotlib.colors'
                for full list of formats that the colors are accepted in.

        Returns:
            output (VisImage): image object with visualizations.
        """
        num_instances = None
        if boxes is not None:
            boxes = self._convert_boxes(boxes)
            num_instances = len(boxes)
        if masks is not None:
            masks = self._convert_masks(masks)
            if num_instances:
                assert len(masks) == num_instances
            else:
                num_instances = len(masks)
        if keypoints is not None:
            if num_instances:
                assert len(keypoints) == num_instances
            else:
                num_instances = len(keypoints)
            keypoints = self._convert_keypoints(keypoints)
        if labels is not None:
            assert len(labels) == num_instances
        if assigned_colors is None:
            assigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]
        if num_instances == 0:
            return self.output
        if boxes is not None and boxes.shape[1] == 5:
            return self.overlay_rotated_instances(
                boxes=boxes, labels=labels, assigned_colors=assigned_colors
            )

        # Display in largest to smallest order to reduce occlusion.
        areas = None
        if boxes is not None:
            areas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)
        elif masks is not None:
            areas = np.asarray([x.area() for x in masks])

        if areas is not None:
            sorted_idxs = np.argsort(-areas).tolist()
            # Re-order overlapped instances in descending order.
            boxes = boxes[sorted_idxs] if boxes is not None else None
            labels = [labels[k] for k in sorted_idxs] if labels is not None else None
            masks = [masks[idx] for idx in sorted_idxs] if masks is not None else None
            assigned_colors = [assigned_colors[idx] for idx in sorted_idxs]
            keypoints = keypoints[sorted_idxs] if keypoints is not None else None

        for i in range(num_instances):
            color = assigned_colors[i]
            if boxes is not None:
                self.draw_box(boxes[i], edge_color=color)

            if masks is not None:
                for segment in masks[i].polygons:
                    self.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)

            if labels is not None:
                # first get a box
                if boxes is not None:
                    x0, y0, x1, y1 = boxes[i]
                    text_pos = (x0, y0)  # if drawing boxes, put text on the box corner.
                    horiz_align = "left"
                elif masks is not None:
                    x0, y0, x1, y1 = masks[i].bbox()

                    # draw text in the center (defined by median) when box is not drawn
                    # median is less sensitive to outliers.
                    text_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]
                    horiz_align = "center"
                else:
                    continue  # drawing the box confidence for keypoints isn't very useful.
                # for small objects, draw text at the side to avoid occlusion
                instance_area = (y1 - y0) * (x1 - x0)
                if (
                    instance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale
                    or y1 - y0 < 40 * self.output.scale
                ):
                    if y1 >= self.output.height - 5:
                        text_pos = (x1, y0)
                    else:
                        text_pos = (x0, y1)

                height_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)
                lighter_color = self._change_color_brightness(color, brightness_factor=0.7)
                font_size = (
                    np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)
                    * 0.5
                    * self._default_font_size
                )
                self.draw_text(
                    labels[i],
                    text_pos,
                    color=lighter_color,
                    horizontal_alignment=horiz_align,
                    font_size=font_size,
                )

        # draw keypoints
        if keypoints is not None:
            for keypoints_per_instance in keypoints:
                self.draw_and_connect_keypoints(keypoints_per_instance)

        return self.output

    def overlay_rotated_instances(self, boxes=None, labels=None, assigned_colors=None):
        """
        Args:
            boxes (ndarray): an Nx5 numpy array of
                (x_center, y_center, width, height, angle_degrees) format
                for the N objects in a single image.
            labels (list[str]): the text to be displayed for each instance.
            assigned_colors (list[matplotlib.colors]): a list of colors, where each color
                corresponds to each mask or box in the image. Refer to 'matplotlib.colors'
                for full list of formats that the colors are accepted in.

        Returns:
            output (VisImage): image object with visualizations.
        """

        num_instances = len(boxes)

        if assigned_colors is None:
            assigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]
        if num_instances == 0:
            return self.output

        # Display in largest to smallest order to reduce occlusion.
        if boxes is not None:
            areas = boxes[:, 2] * boxes[:, 3]

        sorted_idxs = np.argsort(-areas).tolist()
        # Re-order overlapped instances in descending order.
        boxes = boxes[sorted_idxs]
        labels = [labels[k] for k in sorted_idxs] if labels is not None else None
        colors = [assigned_colors[idx] for idx in sorted_idxs]

        for i in range(num_instances):
            self.draw_rotated_box_with_label(
                boxes[i], edge_color=colors[i], label=labels[i] if labels is not None else None
            )

        return self.output

    def draw_and_connect_keypoints(self, keypoints):
        """
        Draws keypoints of an instance and follows the rules for keypoint connections
        to draw lines between appropriate keypoints. This follows color heuristics for
        line color.

        Args:
            keypoints (Tensor): a tensor of shape (K, 3), where K is the number of keypoints
                and the last dimension corresponds to (x, y, probability).

        Returns:
            output (VisImage): image object with visualizations.
        """
        visible = {}
        keypoint_names = self.metadata.keypoint_names
        for idx, keypoint in enumerate(keypoints):
            # draw keypoint
            x, y, prob = keypoint
            if prob > _KEYPOINT_THRESHOLD:
                self.draw_circle((x, y), color=_RED)
                if keypoint_names:
                    keypoint_name = keypoint_names[idx]
                    visible[keypoint_name] = (x, y)

        if self.metadata.keypoint_connection_rules:
            for kp0, kp1, color in self.metadata.keypoint_connection_rules:
                if kp0 in visible and kp1 in visible:
                    x0, y0 = visible[kp0]
                    x1, y1 = visible[kp1]
                    color = tuple(x / 255.0 for x in color)
                    self.draw_line([x0, x1], [y0, y1], color=color)

        # draw lines from nose to mid-shoulder and mid-shoulder to mid-hip
        # Note that this strategy is specific to person keypoints.
        # For other keypoints, it should just do nothing
        try:
            ls_x, ls_y = visible["left_shoulder"]
            rs_x, rs_y = visible["right_shoulder"]
            mid_shoulder_x, mid_shoulder_y = (ls_x + rs_x) / 2, (ls_y + rs_y) / 2
        except KeyError:
            pass
        else:
            # draw line from nose to mid-shoulder
            nose_x, nose_y = visible.get("nose", (None, None))
            if nose_x is not None:
                self.draw_line([nose_x, mid_shoulder_x], [nose_y, mid_shoulder_y], color=_RED)

            try:
                # draw line from mid-shoulder to mid-hip
                lh_x, lh_y = visible["left_hip"]
                rh_x, rh_y = visible["right_hip"]
            except KeyError:
                pass
            else:
                mid_hip_x, mid_hip_y = (lh_x + rh_x) / 2, (lh_y + rh_y) / 2
                self.draw_line([mid_hip_x, mid_shoulder_x], [mid_hip_y, mid_shoulder_y], color=_RED)
        return self.output

    """
    Primitive drawing functions:
    """

    def draw_text(
        self,
        text,
        position,
        *,
        font_size=None,
        color="g",
        horizontal_alignment="center",
        rotation=0
    ):
        """
        Args:
            text (str): class label
            position (tuple): a tuple of the x and y coordinates to place text on image.
            font_size (int, optional): font of the text. If not provided, a font size
                proportional to the image width is calculated and used.
            color: color of the text. Refer to `matplotlib.colors` for full list
                of formats that are accepted.
            horizontal_alignment (str): see `matplotlib.text.Text`
            rotation: rotation angle in degrees CCW

        Returns:
            output (VisImage): image object with text drawn.
        """
        if not font_size:
            font_size = self._default_font_size

        # since the text background is dark, we don't want the text to be dark
        color = np.maximum(list(mplc.to_rgb(color)), 0.2)
        color[np.argmax(color)] = max(0.8, np.max(color))

        x, y = position
        self.output.ax.text(
            x,
            y,
            text,
            size=font_size * self.output.scale,
            family="sans-serif",
            bbox={"facecolor": "black", "alpha": 0.8, "pad": 0.7, "edgecolor": "none"},
            verticalalignment="top",
            horizontalalignment=horizontal_alignment,
            color=color,
            zorder=10,
            rotation=rotation,
        )
        return self.output

    def draw_box(self, box_coord, alpha=0.5, edge_color="g", line_style="-"):
        """
        Args:
            box_coord (tuple): a tuple containing x0, y0, x1, y1 coordinates, where x0 and y0
                are the coordinates of the image's top left corner. x1 and y1 are the
                coordinates of the image's bottom right corner.
            alpha (float): blending efficient. Smaller values lead to more transparent masks.
            edge_color: color of the outline of the box. Refer to `matplotlib.colors`
                for full list of formats that are accepted.
            line_style (string): the string to use to create the outline of the boxes.

        Returns:
            output (VisImage): image object with box drawn.
        """
        x0, y0, x1, y1 = box_coord
        width = x1 - x0
        height = y1 - y0

        linewidth = max(self._default_font_size / 4, 1)

        self.output.ax.add_patch(
            mpl.patches.Rectangle(
                (x0, y0),
                width,
                height,
                fill=False,
                edgecolor=edge_color,
                linewidth=linewidth * self.output.scale,
                alpha=alpha,
                linestyle=line_style,
            )
        )
        return self.output

    def draw_rotated_box_with_label(
        self, rotated_box, alpha=0.5, edge_color="g", line_style="-", label=None
    ):
        """
        Args:
            rotated_box (tuple): a tuple containing (cnt_x, cnt_y, w, h, angle),
                where cnt_x and cnt_y are the center coordinates of the box.
                w and h are the width and height of the box. angle represents how
                many degrees the box is rotated CCW with regard to the 0-degree box.
            alpha (float): blending efficient. Smaller values lead to more transparent boxes.
            edge_color: color of the outline of the box. Refer to `matplotlib.colors`
                for full list of formats that are accepted.
            line_style (string): the string to use to create the outline of the boxes.
            label (string): label for rotated box. It will not be rendered when set to None.

        Returns:
            output (VisImage): image object with box drawn.
        """
        cnt_x, cnt_y, w, h, angle = rotated_box
        area = w * h
        # use thinner lines when the box is small
        linewidth = self._default_font_size / (
            6 if area < _SMALL_OBJECT_AREA_THRESH * self.output.scale else 3
        )

        theta = angle * math.pi / 180.0
        c = math.cos(theta)
        s = math.sin(theta)
        rect = [(-w / 2, h / 2), (-w / 2, -h / 2), (w / 2, -h / 2), (w / 2, h / 2)]
        # x: left->right ; y: top->down
        rotated_rect = [(s * yy + c * xx + cnt_x, c * yy - s * xx + cnt_y) for (xx, yy) in rect]
        for k in range(4):
            j = (k + 1) % 4
            self.draw_line(
                [rotated_rect[k][0], rotated_rect[j][0]],
                [rotated_rect[k][1], rotated_rect[j][1]],
                color=edge_color,
                linestyle="--" if k == 1 else line_style,
                linewidth=linewidth,
                alpha=alpha
            )

        if label is not None:
            text_pos = rotated_rect[1]  # topleft corner

            height_ratio = h / np.sqrt(self.output.height * self.output.width)
            label_color = self._change_color_brightness(edge_color, brightness_factor=0.7)
            font_size = (
                np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2) * 0.5 * self._default_font_size
            )
            self.draw_text(label, text_pos, color=label_color, font_size=font_size, rotation=angle)

        return self.output

    def draw_circle(self, circle_coord, color, radius=3):
        """
        Args:
            circle_coord (list(int) or tuple(int)): contains the x and y coordinates
                of the center of the circle.
            color: color of the polygon. Refer to `matplotlib.colors` for a full list of
                formats that are accepted.
            radius (int): radius of the circle.

        Returns:
            output (VisImage): image object with box drawn.
        """
        x, y = circle_coord
        self.output.ax.add_patch(
            mpl.patches.Circle(circle_coord, radius=radius, fill=True, color=color)
        )
        return self.output

    def draw_line(self, x_data, y_data, color, linestyle="-", linewidth=None, alpha=1.0):
        """
        Args:
            x_data (list[int]): a list containing x values of all the points being drawn.
                Length of list should match the length of y_data.
            y_data (list[int]): a list containing y values of all the points being drawn.
                Length of list should match the length of x_data.
            color: color of the line. Refer to `matplotlib.colors` for a full list of
                formats that are accepted.
            linestyle: style of the line. Refer to `matplotlib.lines.Line2D`
                for a full list of formats that are accepted.
            linewidth (float or None): width of the line. When it's None,
                a default value will be computed and used.
            alpha (float): blending efficient. Smaller values lead to more transparent lines.

        Returns:
            output (VisImage): image object with line drawn.
        """
        if linewidth is None:
            linewidth = self._default_font_size / 3
        linewidth = max(linewidth, 1)
        self.output.ax.add_line(
            mpl.lines.Line2D(
                x_data,
                y_data,
                linewidth=linewidth * self.output.scale,
                color=color,
                linestyle=linestyle,
                alpha=alpha
            )
        )
        return self.output

    def draw_binary_mask(
        self, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=4096
    ):
        """
        Args:
            binary_mask (ndarray): numpy array of shape (H, W), where H is the image height and
                W is the image width. Each value in the array is either a 0 or 1 value of uint8
                type.
            color: color of the mask. Refer to `matplotlib.colors` for a full list of
                formats that are accepted. If None, will pick a random color.
            edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a
                full list of formats that are accepted.
            text (str): if None, will be drawn in the object's center of mass.
            alpha (float): blending efficient. Smaller values lead to more transparent masks.
            area_threshold (float): a connected component small than this will not be shown.

        Returns:
            output (VisImage): image object with mask drawn.
        """
        if color is None:
            color = random_color(rgb=True, maximum=1)
        if area_threshold is None:
            area_threshold = 4096

        has_valid_segment = False
        binary_mask = binary_mask.astype("uint8")  # opencv needs uint8
        mask = GenericMask(binary_mask, self.output.height, self.output.width)
        shape2d = (binary_mask.shape[0], binary_mask.shape[1])

        if not mask.has_holes:
            # draw polygons for regular masks
            for segment in mask.polygons:
                area = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))
                if area < area_threshold:
                    continue
                has_valid_segment = True
                segment = segment.reshape(-1, 2)
                self.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)
        else:
            rgba = np.zeros(shape2d + (4,), dtype="float32")
            rgba[:, :, :3] = color
            rgba[:, :, 3] = (mask.mask == 1).astype("float32") * alpha
            has_valid_segment = True
            self.output.ax.imshow(rgba)

        if text is not None and has_valid_segment:
            # TODO sometimes drawn on wrong objects. the heuristics here can improve.
            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)
            _num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, 8)
            largest_component_id = np.argmax(stats[1:, -1]) + 1

            # draw text on the largest component, as well as other very large components.
            for cid in range(1, _num_cc):
                if cid == largest_component_id or stats[cid, -1] > _LARGE_MASK_AREA_THRESH:
                    # median is more stable than centroid
                    # center = centroids[largest_component_id]
                    center = np.median((cc_labels == cid).nonzero(as_tuple=False), axis=1)[::-1]
                    self.draw_text(text, center, color=lighter_color)
        return self.output

    def draw_polygon(self, segment, color, edge_color=None, alpha=0.5):
        """
        Args:
            segment: numpy array of shape Nx2, containing all the points in the polygon.
            color: color of the polygon. Refer to `matplotlib.colors` for a full list of
                formats that are accepted.
            edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a
                full list of formats that are accepted. If not provided, a darker shade
                of the polygon color will be used instead.
            alpha (float): blending efficient. Smaller values lead to more transparent masks.

        Returns:
            output (VisImage): image object with polygon drawn.
        """
        if edge_color is None:
            # make edge color darker than the polygon color
            if alpha > 0.8:
                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)
            else:
                edge_color = color
        edge_color = mplc.to_rgb(edge_color) + (1,)

        polygon = mpl.patches.Polygon(
            segment,
            fill=True,
            facecolor=mplc.to_rgb(color) + (alpha,),
            edgecolor=edge_color,
            linewidth=max(self._default_font_size // 15 * self.output.scale, 1),
        )
        self.output.ax.add_patch(polygon)
        return self.output

    """
    Internal methods:
    """

    def _jitter(self, color):
        """
        Randomly modifies given color to produce a slightly different color than the color given.

        Args:
            color (tuple[double]): a tuple of 3 elements, containing the RGB values of the color
                picked. The values in the list are in the [0.0, 1.0] range.

        Returns:
            jittered_color (tuple[double]): a tuple of 3 elements, containing the RGB values of the
                color after being jittered. The values in the list are in the [0.0, 1.0] range.
        """
        color = mplc.to_rgb(color)
        vec = np.random.rand(3)
        # better to do it in another color space
        vec = vec / np.linalg.norm(vec) * 0.5
        res = np.clip(vec + color, 0, 1)
        return tuple(res)

    def _create_grayscale_image(self, mask=None):
        """
        Create a grayscale version of the original image.
        The colors in masked area, if given, will be kept.
        """
        img_bw = self.img.astype("f4").mean(axis=2)
        img_bw = np.stack([img_bw] * 3, axis=2)
        if mask is not None:
            img_bw[mask] = self.img[mask]
        return img_bw

    def _change_color_brightness(self, color, brightness_factor):
        """
        Depending on the brightness_factor, gives a lighter or darker color i.e. a color with
        less or more saturation than the original color.

        Args:
            color: color of the polygon. Refer to `matplotlib.colors` for a full list of
                formats that are accepted.
            brightness_factor (float): a value in [-1.0, 1.0] range. A lightness factor of
                0 will correspond to no change, a factor in [-1.0, 0) range will result in
                a darker color and a factor in (0, 1.0] range will result in a lighter color.

        Returns:
            modified_color (tuple[double]): a tuple containing the RGB values of the
                modified color. Each value in the tuple is in the [0.0, 1.0] range.
        """
        assert brightness_factor >= -1.0 and brightness_factor <= 1.0
        color = mplc.to_rgb(color)
        polygon_color = colorsys.rgb_to_hls(*mplc.to_rgb(color))
        modified_lightness = polygon_color[1] + (brightness_factor * polygon_color[1])
        modified_lightness = 0.0 if modified_lightness < 0.0 else modified_lightness
        modified_lightness = 1.0 if modified_lightness > 1.0 else modified_lightness
        modified_color = colorsys.hls_to_rgb(polygon_color[0], modified_lightness, polygon_color[2])
        return modified_color

    def _convert_boxes(self, boxes):
        """
        Convert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.
        """
        if isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):
            return boxes.tensor.numpy()
        else:
            return np.asarray(boxes)

    def _convert_masks(self, masks_or_polygons):
        """
        Convert different format of masks or polygons to a tuple of masks and polygons.

        Returns:
            list[GenericMask]:
        """

        m = masks_or_polygons
        if isinstance(m, PolygonMasks):
            m = m.polygons
        if isinstance(m, BitMasks):
            m = m.tensor.numpy()
        if isinstance(m, torch.Tensor):
            m = m.numpy()
        ret = []
        for x in m:
            if isinstance(x, GenericMask):
                ret.append(x)
            else:
                ret.append(GenericMask(x, self.output.height, self.output.width))
        return ret

    def _convert_keypoints(self, keypoints):
        if isinstance(keypoints, Keypoints):
            keypoints = keypoints.tensor
        keypoints = np.asarray(keypoints)
        return keypoints

    def get_output(self):
        """
        Returns:
            output (VisImage): the image output containing the visualizations added
            to the image.
        """
        return self.output
```

### cvpods/solver/optimizer_builder.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from typing import Any, Dict, List, Set

import torch
from torch import optim

from cvpods.utils.registry import Registry

OPTIMIZER_BUILDER = Registry("Optimizer builder")

NORM_MODULE_TYPES = (
    torch.nn.BatchNorm1d,
    torch.nn.BatchNorm2d,
    torch.nn.BatchNorm3d,
    torch.nn.SyncBatchNorm,
    # NaiveSyncBatchNorm inherits from BatchNorm2d
    torch.nn.GroupNorm,
    torch.nn.InstanceNorm1d,
    torch.nn.InstanceNorm2d,
    torch.nn.InstanceNorm3d,
    torch.nn.LayerNorm,
    torch.nn.LocalResponseNorm,
)


@OPTIMIZER_BUILDER.register()
class OptimizerBuilder:

    @staticmethod
    def build(model, cfg):
        raise NotImplementedError


@OPTIMIZER_BUILDER.register()
class SGDBuilder(OptimizerBuilder):

    @staticmethod
    def build(model, cfg):
        optimizer = optim.SGD(
            model.parameters(),
            lr=cfg.SOLVER.OPTIMIZER.BASE_LR,
            weight_decay=cfg.SOLVER.OPTIMIZER.WEIGHT_DECAY,
            momentum=cfg.SOLVER.OPTIMIZER.MOMENTUM,
        )
        return optimizer


@OPTIMIZER_BUILDER.register()
class D2SGDBuilder(OptimizerBuilder):

    @staticmethod
    def build(model, cfg):
        params: List[Dict[str, Any]] = []
        memo: Set[torch.nn.parameter.Parameter] = set()
        for name, module in model.named_modules():
            for key, value in module.named_parameters(recurse=False):
                if not value.requires_grad:
                    continue
                # Avoid duplicating parameters
                if value in memo:
                    continue
                memo.add(value)
                lr = cfg.SOLVER.OPTIMIZER.BASE_LR
                weight_decay = cfg.SOLVER.OPTIMIZER.WEIGHT_DECAY
                if name.startswith("backbone"):
                    backbone_lr_factor = cfg.SOLVER.OPTIMIZER.get(
                        "BACKBONE_LR_FACTOR", 1.)
                    lr = lr * backbone_lr_factor
                if isinstance(module, NORM_MODULE_TYPES):
                    weight_decay = cfg.SOLVER.OPTIMIZER.WEIGHT_DECAY_NORM
                elif key == "bias":
                    # NOTE: unlike Detectron v1, we now default BIAS_LR_FACTOR to 1.0
                    # and WEIGHT_DECAY_BIAS to WEIGHT_DECAY so that bias optimizer
                    # hyperparameters are by default exactly the same as for regular
                    # weights.
                    lr = cfg.SOLVER.OPTIMIZER.BASE_LR * cfg.SOLVER.OPTIMIZER.BIAS_LR_FACTOR
                    weight_decay = cfg.SOLVER.OPTIMIZER.WEIGHT_DECAY
                params += [{"params": [value], "lr": lr, "weight_decay": weight_decay}]
        optimizer = optim.SGD(
            params,
            cfg.SOLVER.OPTIMIZER.BASE_LR,
            momentum=cfg.SOLVER.OPTIMIZER.MOMENTUM
        )
        return optimizer


@OPTIMIZER_BUILDER.register()
class AdamBuilder(OptimizerBuilder):

    @staticmethod
    def build(model, cfg):
        lr = cfg.SOLVER.OPTIMIZER.BASE_LR
        optimizer = optim.Adam(
            model.parameters(),
            lr=lr,
            weight_decay=cfg.SOLVER.OPTIMIZER.WEIGHT_DECAY,
            amsgrad=cfg.SOLVER.OPTIMIZER.AMSGRAD
        )
        return optimizer


@OPTIMIZER_BUILDER.register()
class AdamWBuilder(OptimizerBuilder):

    @staticmethod
    def build(model, cfg):
        lr = cfg.SOLVER.OPTIMIZER.BASE_LR
        optimizer = optim.AdamW(
            model.parameters(),
            lr=lr,
            betas=cfg.SOLVER.OPTIMIZER.BETAS,
            weight_decay=cfg.SOLVER.OPTIMIZER.WEIGHT_DECAY,
            amsgrad=cfg.SOLVER.OPTIMIZER.AMSGRAD
        )
        return optimizer


@OPTIMIZER_BUILDER.register()
class SGDGateLRBuilder(OptimizerBuilder):
    """
    SGD Gate LR optimizer builder, used for DynamicRouting in cvpods.
    This optimizer will ultiply lr for gating function.
    """

    @staticmethod
    def build(model, cfg):
        gate_lr_multi = cfg.SOLVER.OPTIMIZER.GATE_LR_MULTI
        params: List[Dict[str, Any]] = []
        memo: Set[torch.nn.parameter.Parameter] = set()
        for name, module in model.named_modules():
            for key, value in module.named_parameters(recurse=False):
                if not value.requires_grad:
                    continue
                # Avoid duplicating parameters
                if value in memo:
                    continue
                memo.add(value)
                lr = cfg.SOLVER.OPTIMIZER.BASE_LR
                weight_decay = cfg.SOLVER.OPTIMIZER.WEIGHT_DECAY
                if isinstance(module, NORM_MODULE_TYPES):
                    weight_decay = cfg.SOLVER.OPTIMIZER.WEIGHT_DECAY_NORM
                elif key == "bias":
                    # NOTE: unlike Detectron v1, we now default BIAS_LR_FACTOR to 1.0
                    # and WEIGHT_DECAY_BIAS to WEIGHT_DECAY so that bias optimizer
                    # hyperparameters are by default exactly the same as for regular
                    # weights.
                    lr = cfg.SOLVER.OPTIMIZER.BASE_LR * cfg.SOLVER.OPTIMIZER.BIAS_LR_FACTOR
                    weight_decay = cfg.SOLVER.OPTIMIZER.WEIGHT_DECAY

                if gate_lr_multi > 0.0 and "gate_conv" in name:
                    lr *= gate_lr_multi

                params += [{"params": [value], "lr": lr, "weight_decay": weight_decay}]
        optimizer = torch.optim.SGD(
            params,
            cfg.SOLVER.OPTIMIZER.BASE_LR,
            momentum=cfg.SOLVER.OPTIMIZER.MOMENTUM
        )
        return optimizer
```

### cvpods/solver/build.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import copy
from enum import Enum
from typing import Callable, Iterable, Type, Union

import torch

from cvpods.layers import LARC

from .optimizer_builder import OPTIMIZER_BUILDER
from .scheduler_builder import SCHEDULER_BUILDER

_GradientClipperInput = Union[torch.Tensor, Iterable[torch.Tensor]]
_GradientClipper = Callable[[_GradientClipperInput], None]


class GradientClipType(Enum):
    VALUE = "value"
    NORM = "norm"


def _create_gradient_clipper(cfg) -> _GradientClipper:
    """
    Creates gradient clipping closure to clip by value or by norm,
    according to the provided config.
    """
    cfg = copy.deepcopy(cfg)

    def clip_grad_norm(p: _GradientClipperInput):
        torch.nn.utils.clip_grad_norm_(p, cfg.CLIP_VALUE, cfg.NORM_TYPE)

    def clip_grad_value(p: _GradientClipperInput):
        torch.nn.utils.clip_grad_value_(p, cfg.CLIP_VALUE)

    _GRADIENT_CLIP_TYPE_TO_CLIPPER = {
        GradientClipType.VALUE: clip_grad_value,
        GradientClipType.NORM: clip_grad_norm,
    }
    return _GRADIENT_CLIP_TYPE_TO_CLIPPER[GradientClipType(cfg.CLIP_TYPE)]


def _generate_optimizer_class_with_gradient_clipping(
    optimizer_type: Type[torch.optim.Optimizer], gradient_clipper: _GradientClipper
) -> Type[torch.optim.Optimizer]:
    """
    Dynamically creates a new type that inherits the type of a given instance
    and overrides the `step` method to add gradient clipping
    """

    def optimizer_wgc_step(self, closure=None):
        for group in self.param_groups:
            for p in group["params"]:
                gradient_clipper(p)
        super(type(self), self).step(closure)

    OptimizerWithGradientClip = type(
        optimizer_type.__name__ + "WithGradientClip",
        (optimizer_type,),
        {"step": optimizer_wgc_step},
    )
    return OptimizerWithGradientClip


def maybe_add_gradient_clipping(cfg, optimizer: torch.optim.Optimizer) -> torch.optim.Optimizer:
    """
    If gradient clipping is enabled through config options, wraps the existing
    optimizer instance of some type OptimizerType to become an instance
    of the new dynamically created class OptimizerTypeWithGradientClip
    that inherits OptimizerType and overrides the `step` method to
    include gradient clipping.
    Args:
        cfg: config dict
            configuration options
        optimizer: torch.optim.Optimizer
            existing optimizer instance
    Return:
        optimizer: torch.optim.Optimizer
            either the unmodified optimizer instance (if gradient clipping is
            disabled), or the same instance with adjusted __class__ to override
            the `step` method and include gradient clipping
    """
    if not cfg.SOLVER.CLIP_GRADIENTS.ENABLED:
        return optimizer
    grad_clipper = _create_gradient_clipper(cfg.SOLVER.CLIP_GRADIENTS)
    OptimizerWithGradientClip = _generate_optimizer_class_with_gradient_clipping(
        type(optimizer), grad_clipper
    )
    optimizer.__class__ = OptimizerWithGradientClip
    return optimizer


def maybe_use_lars_optimizer(cfg, optimizer: torch.optim.Optimizer) -> torch.optim.Optimizer:
    """
    warp optimizer with LARS, see :clsss:`cvpods.solver.lars.LARS` for more information.

    Args:
        cfg(BaseConfig):
        optimizer(Optimizer): optimizer for warp

    Return:
        optimizer(Optimizer): optimizer with LARS warped
    """

    if hasattr(cfg.SOLVER.OPTIMIZER, "LARC") and cfg.SOLVER.OPTIMIZER.LARC.ENABLED:
        eps = cfg.SOLVER.OPTIMIZER.LARC.EPS
        trust_coef = cfg.SOLVER.OPTIMIZER.LARC.TRUST_COEF
        clip = cfg.SOLVER.OPTIMIZER.LARC.CLIP
        optimizer = LARC(optimizer, eps, trust_coef, clip)

    return optimizer


def build_optimizer(cfg, model: torch.nn.Module) -> torch.optim.Optimizer:
    """
    Build an optimizer with clip and LARS wraper from config.
    """
    def map_name(name):
        map_dict = {
            "SGD": "SGDBuilder",
            "D2SGD": "D2SGDBuilder",  # Detectron2's SGD
            "Adam": "AdamBuilder",
            "AdamW": "AdamWBuilder",
            "SGD_GATE_LR_MULTI": "SGDGateLRBuilder",
        }
        if name in map_dict:
            name = map_dict[name]
        return name

    NAME = map_name(cfg.SOLVER.OPTIMIZER.NAME)
    assert NAME in OPTIMIZER_BUILDER, "Please registry your Optimizer Builder first."

    optimizer = OPTIMIZER_BUILDER.get(NAME).build(model, cfg)

    # warp optimizer
    optimizer = maybe_use_lars_optimizer(cfg, optimizer)
    optimizer = maybe_add_gradient_clipping(cfg, optimizer)
    return optimizer


def build_lr_scheduler(
    cfg, optimizer: torch.optim.Optimizer, **kwargs
) -> torch.optim.lr_scheduler._LRScheduler:
    """
    Build a LR scheduler from config.
    """
    def map_name(name):
        map_dict = {
            "WarmupMultiStepLR": "WarmupMultiStepLRBuilder",
            "WarmupCosineLR": "WarmupCosineLRBuilder",
            "PolyLR": "PolyLRBuilder",
            "LambdaLR": "LambdaLRBuilder",
            "OneCycleLR": "OneCycleLRBuilder",
        }
        if name in map_dict:
            name = map_dict[name]
        return name

    name = map_name(cfg.SOLVER.LR_SCHEDULER.NAME)
    assert name in SCHEDULER_BUILDER, "Please registry {} in SCHEDULER_BUILDER".format(name)

    scheduler = SCHEDULER_BUILDER.get(name).build(optimizer, cfg, **kwargs)
    return scheduler
```

### cvpods/solver/lr_scheduler.py

```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

import math
from bisect import bisect_right
from typing import List

import torch
from torch.optim.lr_scheduler import _LRScheduler

# NOTE: PyTorch's LR scheduler interface uses names that assume the LR changes
# only on epoch boundaries. We typically use iteration based schedules instead.
# As a result, "epoch" (e.g., as in self.last_epoch) should be understood to mean
# "iteration" instead.

# FIXME: ideally this would be achieved with a CombinedLRScheduler, separating
# MultiStepLR with WarmupLR but the current LRScheduler design doesn't allow it.


class WarmupMultiStepLR(_LRScheduler):
    def __init__(
        self,
        optimizer: torch.optim.Optimizer,
        milestones: List[int],
        gamma: float = 0.1,
        warmup_factor: float = 0.001,
        warmup_iters: int = 1000,
        warmup_method: str = "linear",
        last_epoch: int = -1,
    ):
        """
        Multi Step LR with warmup

        Args:
            optimizer (torch.optim.Optimizer): optimizer used.
            milestones (list[Int]): a list of increasing integers.
            gamma (float): gamma
            warmup_factor (float): lr = warmup_factor * base_lr
            warmup_iters (int): iters to warmup
            warmup_method (str): warmup method in ["constant", "linear", "burnin"]
            last_epoch(int):  The index of last epoch. Default: -1.
        """
        if not list(milestones) == sorted(milestones):
            raise ValueError(
                "Milestones should be a list of"
                " increasing integers. Got {}",
                milestones,
            )
        self.milestones = milestones
        self.gamma = gamma
        self.warmup_factor = warmup_factor
        self.warmup_iters = warmup_iters
        self.warmup_method = warmup_method
        if isinstance(optimizer, torch.optim.Optimizer):
            super().__init__(optimizer, last_epoch)
        else:
            super().__init__(optimizer.optim, last_epoch)

    def get_lr(self) -> List[float]:
        warmup_factor = _get_warmup_factor_at_iter(
            self.warmup_method, self.last_epoch, self.warmup_iters, self.warmup_factor)
        return [
            base_lr * warmup_factor * self.gamma**bisect_right(self.milestones, self.last_epoch)
            for base_lr in self.base_lrs
        ]

    def _compute_values(self) -> List[float]:
        # The new interface
        return self.get_lr()


class WarmupCosineLR(_LRScheduler):
    def __init__(
        self,
        optimizer: torch.optim.Optimizer,
        max_iters: int,
        warmup_factor: float = 0.001,
        warmup_iters: int = 1000,
        warmup_method: str = "linear",
        last_epoch: int = -1,
        epoch_iters: int = -1,
    ):
        """
        Cosine LR with warmup

        Args:
            optimizer (Optimizer):  Wrapped optimizer.
            max_iters (int): max num of iters
            warmup_factor (float): warmup factor to compute lr
            warmup_iters (int): warmup iters
            warmup_method (str): warmup method in ["constant", "linear", "burnin"]
            last_epoch: The index of last epoch. Default: -1.
        """
        self.max_iters = max_iters
        self.warmup_factor = warmup_factor
        self.warmup_iters = warmup_iters
        self.warmup_method = warmup_method
        self.epoch_iters = epoch_iters
        if isinstance(optimizer, torch.optim.Optimizer):
            super().__init__(optimizer, last_epoch)
        else:
            super().__init__(optimizer.optim, last_epoch)

    def get_lr(self) -> List[float]:
        warmup_factor = _get_warmup_factor_at_iter(
            self.warmup_method, self.last_epoch, self.warmup_iters, self.warmup_factor)
        # Different definitions of half-cosine with warmup are possible. For
        # simplicity we multiply the standard half-cosine schedule by the warmup
        # factor. An alternative is to start the period of the cosine at warmup_iters
        # instead of at 0. In the case that warmup_iters << max_iters the two are
        # very close to each other.
        if self.epoch_iters > 0:
            # epoch wise
            coeff = int(self.last_epoch / self.epoch_iters) / int(
                self.max_iters / self.epoch_iters)
        else:
            # iter wise
            coeff = self.last_epoch / self.max_iters

        return [
            base_lr * warmup_factor * 0.5 * (1.0 + math.cos(math.pi * coeff))
            for base_lr in self.base_lrs
        ]

    def _compute_values(self) -> List[float]:
        # The new interface
        return self.get_lr()


class PolyLR(_LRScheduler):
    def __init__(
        self,
        optimizer: torch.optim.Optimizer,
        max_iters: int,
        power: float = 0.9,
        warmup_factor: float = 0.001,
        warmup_iters: int = 1000,
        warmup_method: str = "linear",
        last_epoch: int = -1,
    ):
        """
        Poly LR with warmup
        Args:
            optimizer (torch.optim.Optimizer): optimizer used.
            max_iters (int): max num of iters.
            power (float): power
            warmup_factor (float): lr = warmup_factor * base_lr
            warmup_iters (int): iters to warmup
            warmup_method (str): warmup method in ["constant", "linear", "burnin"]
            last_epoch(int):  The index of last epoch. Default: -1.
        """
        self.max_iters = max_iters
        self.power = power
        self.warmup_factor = warmup_factor
        self.warmup_iters = warmup_iters
        self.warmup_method = warmup_method
        if isinstance(optimizer, torch.optim.Optimizer):
            super().__init__(optimizer, last_epoch)
        else:
            super().__init__(optimizer.optim, last_epoch)

    def get_lr(self) -> List[float]:
        warmup_factor = _get_warmup_factor_at_iter(
            self.warmup_method, self.last_epoch, self.warmup_iters, self.warmup_factor
        )
        # self.last_epoch is used for current iter here.
        return [
            base_lr * warmup_factor * (
                (1 - float(self.last_epoch) / self.max_iters) ** self.power
            )
            for base_lr in self.base_lrs
        ]

    def _compute_values(self) -> List[float]:
        # The new interface
        return self.get_lr()


def _get_warmup_factor_at_iter(method: str, iter: int, warmup_iters: int,
                               warmup_factor: float) -> float:
    """
    Return the learning rate warmup factor at a specific iteration.
    See https://arxiv.org/abs/1706.02677 for more details.

    Args:
        method (str): warmup method; either "constant" or "linear".
        iter (int): iteration at which to calculate the warmup factor.
        warmup_iters (int): the number of warmup iterations.
        warmup_factor (float): the base warmup factor (the meaning changes according
            to the method used).

    Returns:
        float: the effective warmup factor at the given iteration.
    """
    if iter >= warmup_iters:
        return 1.0

    if method == "constant":
        return warmup_factor
    elif method == "linear":
        alpha = iter / warmup_iters
        return warmup_factor * (1 - alpha) + alpha
    elif method == "burnin":
        return (iter / warmup_iters)**4
    else:
        raise ValueError("Unknown warmup method: {}".format(method))
```

### cvpods/solver/scheduler_builder.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from torch.optim import lr_scheduler

from cvpods.utils.registry import Registry

from .lr_scheduler import PolyLR, WarmupCosineLR, WarmupMultiStepLR

SCHEDULER_BUILDER = Registry("LRScheduler builder")


@SCHEDULER_BUILDER.register()
class BaseSchedulerBuilder:

    @staticmethod
    def build(optimizer, cfg, **kwargs):
        raise NotImplementedError


@SCHEDULER_BUILDER.register()
class WarmupMultiStepLRBuilder(BaseSchedulerBuilder):

    @staticmethod
    def build(optimizer, cfg, **kwargs):
        scheduler = WarmupMultiStepLR(
            optimizer,
            cfg.SOLVER.LR_SCHEDULER.STEPS,
            cfg.SOLVER.LR_SCHEDULER.GAMMA,
            warmup_factor=cfg.SOLVER.LR_SCHEDULER.WARMUP_FACTOR,
            warmup_iters=cfg.SOLVER.LR_SCHEDULER.WARMUP_ITERS,
            warmup_method=cfg.SOLVER.LR_SCHEDULER.WARMUP_METHOD,
        )
        return scheduler


@SCHEDULER_BUILDER.register()
class WarmupCosineLRBuilder(BaseSchedulerBuilder):

    @staticmethod
    def build(optimizer, cfg, **kwargs):
        scheduler = WarmupCosineLR(
            optimizer,
            cfg.SOLVER.LR_SCHEDULER.MAX_ITER,
            warmup_factor=cfg.SOLVER.LR_SCHEDULER.WARMUP_FACTOR,
            warmup_iters=cfg.SOLVER.LR_SCHEDULER.WARMUP_ITERS,
            warmup_method=cfg.SOLVER.LR_SCHEDULER.WARMUP_METHOD,
            epoch_iters=kwargs["epoch_iters"],
        )
        return scheduler


@SCHEDULER_BUILDER.register()
class PolyLRBuilder(BaseSchedulerBuilder):

    @staticmethod
    def build(optimizer, cfg, **kwargs):
        return PolyLR(
            optimizer,
            cfg.SOLVER.LR_SCHEDULER.MAX_ITER,
            cfg.SOLVER.LR_SCHEDULER.POLY_POWER,
            warmup_factor=cfg.SOLVER.LR_SCHEDULER.WARMUP_FACTOR,
            warmup_iters=cfg.SOLVER.LR_SCHEDULER.WARMUP_ITERS,
            warmup_method=cfg.SOLVER.LR_SCHEDULER.WARMUP_METHOD,
        )


@SCHEDULER_BUILDER.register()
class LambdaLRBuilder(BaseSchedulerBuilder):

    @staticmethod
    def build(optimizer, cfg, **kwargs):
        return lr_scheduler.LambdaLR(
            optimizer,
            cfg.SOLVER.LR_SCHEDULER.LAMBDA_SCHEDULE
        )


@SCHEDULER_BUILDER.register()
class OneCycleLRBuilder(BaseSchedulerBuilder):

    @staticmethod
    def build(optimizer, cfg, **kwargs):
        return lr_scheduler.OneCycleLR(
            optimizer,
            cfg.SOLVER.LR_SCHEDULER.MAX_LR,
            total_steps=cfg.SOLVER.LR_SCHEDULER.MAX_ITER,
            pct_start=cfg.SOLVER.LR_SCHEDULER.PCT_START,
            base_momentum=cfg.SOLVER.LR_SCHEDULER.BASE_MOM,
            max_momentum=cfg.SOLVER.LR_SCHEDULER.MAX_MOM,
            div_factor=cfg.SOLVER.LR_SCHEDULER.DIV_FACTOR
        )
```

### cvpods/solver/__init__.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from .build import build_lr_scheduler, build_optimizer
from .optimizer_builder import (
    OPTIMIZER_BUILDER,
    AdamBuilder,
    AdamWBuilder,
    OptimizerBuilder,
    SGDBuilder,
    SGDGateLRBuilder
)
from .scheduler_builder import (
    SCHEDULER_BUILDER,
    BaseSchedulerBuilder,
    LambdaLRBuilder,
    OneCycleLRBuilder,
    PolyLRBuilder,
    WarmupCosineLR,
    WarmupCosineLRBuilder,
    WarmupMultiStepLR,
    WarmupMultiStepLRBuilder
)

__all__ = [k for k in globals().keys() if not k.startswith("_")]
```

### cvpods/modeling/test_time_augmentation.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import copy
from contextlib import contextmanager
from itertools import count

import numpy as np

import torch
from torch import nn
from torch.nn.parallel import DistributedDataParallel

from cvpods.data.detection_utils import read_image
from cvpods.data.transforms import ResizeShortestEdge
from cvpods.layers import generalized_batched_nms
from cvpods.structures import Boxes, Instances

from .meta_arch import GeneralizedRCNN
from .postprocessing import detector_postprocess
from .roi_heads.fast_rcnn import fast_rcnn_inference_single_image

__all__ = ["DatasetMapperTTA", "GeneralizedRCNNWithTTA"]


class DatasetMapperTTA:
    """
    Implement test-time augmentation for detection data.
    It is a callable which takes a dataset dict from a detection dataset,
    and returns a list of dataset dicts where the images
    are augmented from the input image by the transformations defined in the config.
    This is used for test-time augmentation.
    """

    def __init__(self, cfg):
        self.min_sizes = cfg.TEST.AUG.MIN_SIZES
        self.max_size = cfg.TEST.AUG.MAX_SIZE
        self.flip = cfg.TEST.AUG.FLIP
        self.image_format = cfg.INPUT.FORMAT
        self.extra_sizes = cfg.TEST.AUG.EXTRA_SIZES

    def __call__(self, dataset_dict):
        """
        Args:
            dict: a detection dataset dict

        Returns:
            list[dict]:
                a list of dataset dicts, which contain augmented version of the input image.
                The total number of dicts is ``len(min_sizes) * (2 if flip else 1)``.
        """
        ret = []
        if "image" not in dataset_dict:
            numpy_image = read_image(dataset_dict["file_name"], self.image_format)
        else:
            numpy_image = dataset_dict["image"].permute(1, 2, 0).numpy().astype("uint8")

        image_sizes = [(min_size, self.max_size) for min_size in self.min_sizes]
        image_sizes.extend(self.extra_sizes)

        for min_size, max_size in image_sizes:
            image = np.copy(numpy_image)
            tfm = ResizeShortestEdge(min_size, max_size).get_transform(image)
            resized = tfm.apply_image(image)
            resized = torch.as_tensor(resized.transpose(2, 0, 1).astype("float32"))

            dic = copy.deepcopy(dataset_dict)
            dic["horiz_flip"] = False
            dic["image"] = resized
            ret.append(dic)

            if self.flip:
                dic = copy.deepcopy(dataset_dict)
                dic["horiz_flip"] = True
                dic["image"] = torch.flip(resized, dims=[2])
                ret.append(dic)

        return ret


class GeneralizedRCNNWithTTA(nn.Module):
    """
    A GeneralizedRCNN with test-time augmentation enabled.
    Its :meth:`__call__` method has the same interface as :meth:`GeneralizedRCNN.forward`.
    """

    def __init__(self, cfg, model, tta_mapper=None, batch_size=3):
        """
        Args:
            cfg (config dict):
            model (GeneralizedRCNN): a GeneralizedRCNN to apply TTA on.
            tta_mapper (callable): takes a dataset dict and returns a list of
                augmented versions of the dataset dict. Defaults to
                `DatasetMapperTTA(cfg)`.
            batch_size (int): batch the augmented images into this batch size for inference.
        """
        super().__init__()
        if isinstance(model, DistributedDataParallel):
            model = model.module
        assert isinstance(
            model, GeneralizedRCNN
        ), "TTA is only supported on GeneralizedRCNN. Got a model of type {}".format(type(model))
        self.cfg = copy.deepcopy(cfg)
        assert not self.cfg.MODEL.KEYPOINT_ON, "TTA for keypoint is not supported yet"
        assert (
            not self.cfg.MODEL.LOAD_PROPOSALS
        ), "TTA for pre-computed proposals is not supported yet"

        self.model = model

        if tta_mapper is None:
            tta_mapper = DatasetMapperTTA(cfg)
        self.tta_mapper = tta_mapper
        self.batch_size = batch_size

    @contextmanager
    def _turn_off_roi_head(self, attr):
        """
        Open a context where one head in `model.roi_heads` is temporarily turned off.
        Args:
            attr (str): the attribute in `model.roi_heads` which can be used
                to turn off a specific head, e.g., "mask_on", "keypoint_on".
        """
        roi_heads = self.model.roi_heads
        try:
            old = getattr(roi_heads, attr)
        except AttributeError:
            # The head may not be implemented in certain ROIHeads
            old = None

        if old is None:
            yield
        else:
            setattr(roi_heads, attr, False)
            yield
            setattr(roi_heads, attr, old)

    def _batch_inference(self, batched_inputs, detected_instances=None, do_postprocess=True):
        """
        Execute inference on a list of inputs,
        using batch size = self.batch_size, instead of the length of the list.

        Inputs & outputs have the same format as :meth:`GeneralizedRCNN.inference`
        """
        if detected_instances is None:
            detected_instances = [None] * len(batched_inputs)

        outputs = []
        inputs, instances = [], []
        for idx, input, instance in zip(count(), batched_inputs, detected_instances):
            inputs.append(input)
            instances.append(instance)
            if len(inputs) == self.batch_size or idx == len(batched_inputs) - 1:
                outputs.extend(
                    self.model.inference(
                        inputs,
                        instances if instances[0] is not None else None,
                        do_postprocess=do_postprocess,
                    )
                )
                inputs, instances = [], []
        return outputs

    def __call__(self, batched_inputs):
        """
        Same input/output format as :meth:`GeneralizedRCNN.forward`
        """
        return [self._inference_one_image(x) for x in batched_inputs]

    def _inference_one_image(self, input):
        """
        Args:
            input (dict): one dataset dict

        Returns:
            dict: one output dict
        """
        augmented_inputs = self.tta_mapper(input)

        do_hflip = [k.pop("horiz_flip", False) for k in augmented_inputs]
        heights = [k["height"] for k in augmented_inputs]
        widths = [k["width"] for k in augmented_inputs]
        assert (
            len(set(heights)) == 1 and len(set(widths)) == 1
        ), "Augmented version of the inputs should have the same original resolution!"
        height = heights[0]
        width = widths[0]

        # 1. Detect boxes from all augmented versions
        # 1.1: forward with all augmented images
        with self._turn_off_roi_head("mask_on"), self._turn_off_roi_head("keypoint_on"):
            # temporarily disable mask/keypoint head
            outputs = self._batch_inference(augmented_inputs, do_postprocess=False)
        # 1.2: union the results
        all_boxes = []
        all_scores = []
        all_classes = []
        for idx, output in enumerate(outputs):
            rescaled_output = detector_postprocess(output, height, width)
            pred_boxes = rescaled_output.pred_boxes.tensor
            if do_hflip[idx]:
                pred_boxes[:, [0, 2]] = width - pred_boxes[:, [2, 0]]
            all_boxes.append(pred_boxes)
            all_scores.extend(rescaled_output.scores)
            all_classes.extend(rescaled_output.pred_classes)
        all_boxes = torch.cat(all_boxes, dim=0).cpu()
        num_boxes = len(all_boxes)

        # 1.3: select from the union of all results
        num_classes = self.cfg.MODEL.ROI_HEADS.NUM_CLASSES
        # +1 because fast_rcnn_inference expects background scores as well
        all_scores_2d = torch.zeros(num_boxes, num_classes + 1, device=all_boxes.device)
        for idx, cls, score in zip(count(), all_classes, all_scores):
            all_scores_2d[idx, cls] = score

        merged_instances, _ = fast_rcnn_inference_single_image(
            all_boxes,
            all_scores_2d,
            (height, width),
            1e-8,
            self.cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST,
            self.cfg.MODEL.NMS_TYPE,
            self.cfg.TEST.DETECTIONS_PER_IMAGE,
        )

        if not self.cfg.MODEL.MASK_ON:
            return {"instances": merged_instances}

        # 2. Use the detected boxes to obtain masks
        # 2.1: rescale the detected boxes
        augmented_instances = []
        for idx, input in enumerate(augmented_inputs):
            actual_height, actual_width = input["image"].shape[1:3]
            scale_x = actual_width * 1.0 / width
            scale_y = actual_height * 1.0 / height
            pred_boxes = merged_instances.pred_boxes.clone()
            pred_boxes.tensor[:, 0::2] *= scale_x
            pred_boxes.tensor[:, 1::2] *= scale_y
            if do_hflip[idx]:
                pred_boxes.tensor[:, [0, 2]] = actual_width - pred_boxes.tensor[:, [2, 0]]

            aug_instances = Instances(
                image_size=(actual_height, actual_width),
                pred_boxes=pred_boxes,
                pred_classes=merged_instances.pred_classes,
                scores=merged_instances.scores,
            )
            augmented_instances.append(aug_instances)
        # 2.2: run forward on the detected boxes
        outputs = self._batch_inference(augmented_inputs, augmented_instances, do_postprocess=False)
        for idx, output in enumerate(outputs):
            if do_hflip[idx]:
                output.pred_masks = output.pred_masks.flip(dims=[3])
        # 2.3: average the predictions
        all_pred_masks = torch.stack([o.pred_masks for o in outputs], dim=0)
        avg_pred_masks = torch.mean(all_pred_masks, dim=0)
        output = outputs[0]
        output.pred_masks = avg_pred_masks
        output = detector_postprocess(output, height, width)
        return {"instances": output}


class SimpleTTAWarper(nn.Module):

    def __init__(self, cfg, model, tta_mapper=None, batch_size=3):
        """
        Args:
            cfg (config dict):
            model (GeneralizedRCNN): a GeneralizedRCNN to apply TTA on.
            tta_mapper (callable): takes a dataset dict and returns a list of
                augmented versions of the dataset dict. Defaults to
                `DatasetMapperTTA(cfg)`.
            batch_size (int): batch the augmented images into this batch size for inference.
        """
        super().__init__()
        if isinstance(model, DistributedDataParallel):
            model = model.module
        self.cfg = cfg
        assert not self.cfg.MODEL.KEYPOINT_ON, "TTA for keypoint is not supported yet"
        assert (
            not self.cfg.MODEL.LOAD_PROPOSALS
        ), "TTA for pre-computed proposals is not supported yet"

        self.model = model

        if tta_mapper is None:
            tta_mapper = DatasetMapperTTA(cfg)
        self.tta_mapper = tta_mapper
        self.batch_size = batch_size

    def __call__(self, batched_inputs):
        """
        Same input/output format as :meth:`GeneralizedRCNN.forward`
        """
        return [self._inference_one_image(x) for x in batched_inputs]

    def _inference_one_image(self, inputs):
        augmented_inputs = self.tta_mapper(inputs)
        assert len({x["file_name"] for x in augmented_inputs}) == 1, "inference different images"
        heights = [k["height"] for k in augmented_inputs]
        widths = [k["width"] for k in augmented_inputs]
        assert (
            len(set(heights)) == 1
            and len(set(widths)) == 1
        ), "Augmented version of the inputs should have the same original resolution!"

        height = heights[0]
        width = widths[0]
        # 1. Detect boxes from all augmented versions
        all_boxes = []
        all_scores = []
        all_classes = []

        for single_input in augmented_inputs:
            do_hflip = single_input.pop("horiz_flip", False)
            # 1.1: forward with single augmented image
            output = self.model._inference_for_ms_test([single_input])
            # 1.2: union the results
            pred_boxes = output.get("pred_boxes").tensor
            if do_hflip:
                pred_boxes[:, [0, 2]] = width - pred_boxes[:, [2, 0]]
            all_boxes.append(pred_boxes)
            all_scores.append(output.get("scores"))
            all_classes.append(output.get("pred_classes"))

        boxes_all = torch.cat(all_boxes, dim=0)
        scores_all = torch.cat(all_scores, dim=0)
        class_idxs_all = torch.cat(all_classes, dim=0)
        keep = generalized_batched_nms(
            boxes_all, scores_all, class_idxs_all,
            self.model.nms_threshold, nms_type=self.model.nms_type
        )

        keep = keep[:self.model.max_detections_per_image]

        result = Instances((height, width))
        result.pred_boxes = Boxes(boxes_all[keep])
        result.scores = scores_all[keep]
        result.pred_classes = class_idxs_all[keep]
        return {"instances": result}


class TTAWarper(nn.Module):

    def __init__(self, cfg, model, tta_mapper=None, batch_size=3):
        """
        Args:
            cfg (config dict):
            model (GeneralizedRCNN): a GeneralizedRCNN to apply TTA on.
            tta_mapper (callable): takes a dataset dict and returns a list of
                augmented versions of the dataset dict. Defaults to
                `DatasetMapperTTA(cfg)`.
            batch_size (int): batch the augmented images into this batch size for inference.
        """
        super().__init__()
        if isinstance(model, DistributedDataParallel):
            model = model.module
        self.cfg = cfg
        assert not self.cfg.MODEL.KEYPOINT_ON, "TTA for keypoint is not supported yet"
        assert (
            not self.cfg.MODEL.LOAD_PROPOSALS
        ), "TTA for pre-computed proposals is not supported yet"

        self.model = model

        if tta_mapper is None:
            tta_mapper = DatasetMapperTTA(cfg)
        self.tta_mapper = tta_mapper
        self.batch_size = batch_size
        self.max_detection = cfg.TEST.DETECTIONS_PER_IMAGE

        self.enable_scale_filter = cfg.TEST.AUG.SCALE_FILTER
        self.scale_ranges = cfg.TEST.AUG.SCALE_RANGES

    def __call__(self, batched_inputs):
        """
        Same input/output format as :meth:`GeneralizedRCNN.forward`
        """
        return [self._inference_one_image(x) for x in batched_inputs]

    def _inference_one_image(self, inputs):
        augmented_inputs = self.tta_mapper(inputs)
        assert len({x["file_name"] for x in augmented_inputs}) == 1, "inference different images"
        heights = [k["height"] for k in augmented_inputs]
        widths = [k["width"] for k in augmented_inputs]
        assert (
            len(set(heights)) == 1
            and len(set(widths)) == 1
        ), "Augmented version of the inputs should have the same original resolution!"

        height = heights[0]
        width = widths[0]
        # 1. Detect boxes from all augmented versions
        # TODO wangfeng02: use box structures instead of boxes, scores and classes
        all_boxes = []
        all_scores = []
        all_classes = []

        factors = 2 if self.tta_mapper.flip else 1
        if self.enable_scale_filter:
            assert len(augmented_inputs) == len(self.scale_ranges) * factors

        for i, single_input in enumerate(augmented_inputs):
            do_hflip = single_input.pop("horiz_flip", False)
            # 1.1: forward with single augmented image
            output = self.model._inference_for_ms_test([single_input])
            # 1.2: union the results
            pred_boxes = output.get("pred_boxes").tensor
            if do_hflip:
                pred_boxes[:, [0, 2]] = width - pred_boxes[:, [2, 0]]

            pred_scores = output.get("scores")
            pred_classes = output.get("pred_classes")
            if self.enable_scale_filter:
                keep = filter_boxes(pred_boxes, *self.scale_ranges[i // factors])
                pred_boxes = pred_boxes[keep]
                pred_scores = pred_scores[keep]
                pred_classes = pred_classes[keep]

            all_boxes.append(pred_boxes)
            all_scores.append(pred_scores)
            all_classes.append(pred_classes)

        boxes_all = torch.cat(all_boxes, dim=0)
        scores_all = torch.cat(all_scores, dim=0)
        class_idxs_all = torch.cat(all_classes, dim=0)
        boxes_all, scores_all, class_idxs_all = merge_result_from_multi_scales(
            boxes_all, scores_all, class_idxs_all,
            nms_type="soft_vote", vote_thresh=0.65,
            max_detection=self.max_detection
        )

        result = Instances((height, width))
        result.pred_boxes = Boxes(boxes_all)
        result.scores = scores_all
        result.pred_classes = class_idxs_all
        return {"instances": result}


def filter_boxes(boxes, min_scale, max_scale):
    """
    boxes: (N, 4) shape
    """
    # assert boxes.mode == "xyxy"
    w = boxes[:, 2] - boxes[:, 0]
    h = boxes[:, 3] - boxes[:, 1]
    keep = (w * h > min_scale * min_scale) & (w * h < max_scale * max_scale)
    return keep


def merge_result_from_multi_scales(
    boxes, scores, labels, nms_type="soft-vote", vote_thresh=0.65, max_detection=100
):
    boxes, scores, labels = batched_vote_nms(
        boxes, scores, labels, nms_type, vote_thresh
    )

    number_of_detections = boxes.shape[0]
    # Limit to max_per_image detections **over all classes**
    if number_of_detections > max_detection > 0:
        boxes = boxes[:max_detection]
        scores = scores[:max_detection]
        labels = labels[:max_detection]

    return boxes, scores, labels


def batched_vote_nms(boxes, scores, labels, vote_type, vote_thresh=0.65):
    # apply per class level nms, add max_coordinates on boxes first, then remove it.
    labels = labels.float()
    max_coordinates = boxes.max() + 1
    offsets = labels.reshape(-1, 1) * max_coordinates
    boxes = boxes + offsets

    boxes, scores, labels = bbox_vote(boxes, scores, labels, vote_thresh, vote_type)
    boxes -= labels.reshape(-1, 1) * max_coordinates

    return boxes, scores, labels


def bbox_vote(boxes, scores, labels, vote_thresh, vote_type="softvote"):
    assert boxes.shape[0] == scores.shape[0] == labels.shape[0]
    det = torch.cat((boxes, scores.reshape(-1, 1), labels.reshape(-1, 1)), dim=1)

    vote_results = torch.zeros(0, 6, device=det.device)
    if det.numel() == 0:
        return vote_results[:, :4], vote_results[:, 4], vote_results[:, 5]

    order = scores.argsort(descending=True)
    det = det[order]

    while det.shape[0] > 0:
        # IOU
        area = (det[:, 2] - det[:, 0]) * (det[:, 3] - det[:, 1])
        xx1 = torch.max(det[0, 0], det[:, 0])
        yy1 = torch.max(det[0, 1], det[:, 1])
        xx2 = torch.min(det[0, 2], det[:, 2])
        yy2 = torch.min(det[0, 3], det[:, 3])
        w = torch.clamp(xx2 - xx1, min=0.)
        h = torch.clamp(yy2 - yy1, min=0.)
        inter = w * h
        iou = inter / (area[0] + area[:] - inter)

        # get needed merge det and delete these  det
        merge_index = torch.where(iou >= vote_thresh)[0]
        vote_det = det[merge_index, :]
        det = det[iou < vote_thresh]

        if merge_index.shape[0] <= 1:
            vote_results = torch.cat((vote_results, vote_det), dim=0)
        else:
            if vote_type == "soft_vote":
                vote_det_iou = iou[merge_index]
                det_accu_sum = get_soft_dets_sum(vote_det, vote_det_iou)
            elif vote_type == "vote":
                det_accu_sum = get_dets_sum(vote_det)
            vote_results = torch.cat((vote_results, det_accu_sum), dim=0)

    order = vote_results[:, 4].argsort(descending=True)
    vote_results = vote_results[order, :]

    return vote_results[:, :4], vote_results[:, 4], vote_results[:, 5]


def get_dets_sum(vote_det):
    vote_det[:, :4] *= vote_det[:, 4:5].repeat(1, 4)
    max_score = vote_det[:, 4].max()
    det_accu_sum = torch.zeros((1, 6), device=vote_det.device)
    det_accu_sum[:, :4] = torch.sum(vote_det[:, :4], dim=0) / torch.sum(vote_det[:, 4])
    det_accu_sum[:, 4] = max_score
    det_accu_sum[:, 5] = vote_det[0, 5]
    return det_accu_sum


def get_soft_dets_sum(vote_det, vote_det_iou):
    soft_vote_det = vote_det.detach().clone()
    soft_vote_det[:, 4] *= (1 - vote_det_iou)

    INFERENCE_TH = 0.05
    soft_index = torch.where(soft_vote_det[:, 4] >= INFERENCE_TH)[0]
    soft_vote_det = soft_vote_det[soft_index, :]

    vote_det[:, :4] *= vote_det[:, 4:5].repeat(1, 4)
    max_score = vote_det[:, 4].max()
    det_accu_sum = torch.zeros((1, 6), device=vote_det.device)
    det_accu_sum[:, :4] = torch.sum(vote_det[:, :4], dim=0) / torch.sum(vote_det[:, 4])
    det_accu_sum[:, 4] = max_score
    det_accu_sum[:, 5] = vote_det[0, 5]

    if soft_vote_det.shape[0] > 0:
        det_accu_sum = torch.cat((det_accu_sum, soft_vote_det), dim=0)
    return det_accu_sum
```

### cvpods/modeling/poolers.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
# Modified by BaseDetection, Inc. and its affiliates. All Rights Reserved.
import math
import sys

import torch
from torch import nn
from torchvision.ops import PSRoIAlign, PSRoIPool, RoIPool

from cvpods.layers import ROIAlign, ROIAlignRotated, cat
from cvpods.utils.apex_wrapper import float_function

__all__ = ["ROIPooler"]


def assign_boxes_to_levels(box_lists, min_level, max_level, canonical_box_size, canonical_level):
    """
    Map each box in `box_lists` to a feature map level index and return the assignment
    vector.

    Args:
        box_lists (list[Boxes] | list[RotatedBoxes]): A list of N Boxes or N RotatedBoxes,
            where N is the number of images in the batch.
        min_level (int): Smallest feature map level index. The input is considered index 0,
            the output of stage 1 is index 1, and so.
        max_level (int): Largest feature map level index.
        canonical_box_size (int): A canonical box size in pixels (sqrt(box area)).
        canonical_level (int): The feature map level index on which a canonically-sized box
            should be placed.

    Returns:
        A tensor of length M, where M is the total number of boxes aggregated over all
            N batch images. The memory layout corresponds to the concatenation of boxes
            from all images. Each element is the feature map index, as an offset from
            `self.min_level`, for the corresponding box (so value i means the box is at
            `self.min_level + i`).
    """
    eps = sys.float_info.epsilon
    box_sizes = torch.sqrt(cat([boxes.area() for boxes in box_lists]))
    # Eqn.(1) in FPN paper
    level_assignments = torch.floor(
        canonical_level + torch.log2(box_sizes / canonical_box_size + eps)
    )
    # clamp level to (min, max), in case the box size is too large or too small
    # for the available feature maps
    level_assignments = torch.clamp(level_assignments, min=min_level, max=max_level)
    return level_assignments.to(torch.int64) - min_level


def convert_boxes_to_pooler_format(box_lists):
    """
    Convert all boxes in `box_lists` to the low-level format used by ROI pooling ops
    (see description under Returns).

    Args:
        box_lists (list[Boxes] | list[RotatedBoxes]):
            A list of N Boxes or N RotatedBoxes, where N is the number of images in the batch.

    Returns:
        When input is list[Boxes]:
            A tensor of shape (M, 5), where M is the total number of boxes aggregated over all
            N batch images.
            The 5 columns are (batch index, x0, y0, x1, y1), where batch index
            is the index in [0, N) identifying which batch image the box with corners at
            (x0, y0, x1, y1) comes from.
        When input is list[RotatedBoxes]:
            A tensor of shape (M, 6), where M is the total number of boxes aggregated over all
            N batch images.
            The 6 columns are (batch index, x_ctr, y_ctr, width, height, angle_degrees),
            where batch index is the index in [0, N) identifying which batch image the
            rotated box (x_ctr, y_ctr, width, height, angle_degrees) comes from.
    """

    def fmt_box_list(box_tensor, batch_index):
        repeated_index = torch.full(
            (len(box_tensor), 1), batch_index, dtype=box_tensor.dtype, device=box_tensor.device
        )
        return cat((repeated_index, box_tensor), dim=1)

    pooler_fmt_boxes = cat(
        [fmt_box_list(box_list.tensor, i) for i, box_list in enumerate(box_lists)], dim=0
    )

    return pooler_fmt_boxes


class ROIPooler(nn.Module):
    """
    Region of interest feature map pooler that supports pooling from one or more
    feature maps.
    """

    def __init__(
        self,
        output_size,
        scales,
        sampling_ratio,
        pooler_type,
        canonical_box_size=224,
        canonical_level=4,
    ):
        """
        Args:
            output_size (int, tuple[int] or list[int]): output size of the pooled region,
                e.g., 14 x 14. If tuple or list is given, the length must be 2.
            scales (list[float]): The scale for each low-level pooling op relative to
                the input image. For a feature map with stride s relative to the input
                image, scale is defined as a 1 / s. The stride must be power of 2.
                When there are multiple scales, they must form a pyramid, i.e. they must be
                a monotically decreasing geometric sequence with a factor of 1/2.
            sampling_ratio (int): The `sampling_ratio` parameter for the ROIAlign op.
            pooler_type (string): Name of the type of pooling operation that should be applied.
                For instance, "ROIPool" or "ROIAlignV2".
            canonical_box_size (int): A canonical box size in pixels (sqrt(box area)). The default
                is heuristically defined as 224 pixels in the FPN paper (based on ImageNet
                pre-training).
            canonical_level (int): The feature map level index from which a canonically-sized box
                should be placed. The default is defined as level 4 (stride=16) in the FPN paper,
                i.e., a box of size 224x224 will be placed on the feature with stride=16.
                The box placement for all boxes will be determined from their sizes w.r.t
                canonical_box_size. For example, a box whose area is 4x that of a canonical box
                should be used to pool features from feature level ``canonical_level+1``.
                Note that the actual input feature maps given to this module may not have
                sufficiently many levels for the input boxes. If the boxes are too large or too
                small for the input feature maps, the closest level will be used.
        """
        super().__init__()

        if isinstance(output_size, int):
            output_size = (output_size, output_size)
        assert len(output_size) == 2
        assert isinstance(output_size[0], int) and isinstance(output_size[1], int)
        self.output_size = output_size

        if pooler_type == "ROIAlign":
            self.level_poolers = nn.ModuleList(
                ROIAlign(
                    output_size, spatial_scale=scale, sampling_ratio=sampling_ratio, aligned=False
                )
                for scale in scales
            )
        elif pooler_type == "ROIAlignV2":
            self.level_poolers = nn.ModuleList(
                ROIAlign(
                    output_size, spatial_scale=scale, sampling_ratio=sampling_ratio, aligned=True
                )
                for scale in scales
            )
        elif pooler_type == "ROIPool":
            self.level_poolers = nn.ModuleList(
                RoIPool(output_size, spatial_scale=scale) for scale in scales
            )
        elif pooler_type == "ROIAlignRotated":
            self.level_poolers = nn.ModuleList(
                ROIAlignRotated(output_size, spatial_scale=scale, sampling_ratio=sampling_ratio)
                for scale in scales
            )
        elif pooler_type == "PSROIPool":
            self.level_poolers = nn.ModuleList(
                PSRoIPool(output_size, spatial_scale=scale)
                for scale in scales
            )
        elif pooler_type == "PSROIAlign":
            self.level_poolers = nn.ModuleList(
                PSRoIAlign(output_size, spatial_scale=scale, sampling_ratio=sampling_ratio)
                for scale in scales
            )
        else:
            raise ValueError("Unknown pooler type: {}".format(pooler_type))

        # Map scale (defined as 1 / stride) to its feature map level under the
        # assumption that stride is a power of 2.
        min_level = -math.log2(scales[0])
        max_level = -math.log2(scales[-1])
        assert math.isclose(min_level, int(min_level)) and math.isclose(
            max_level, int(max_level)
        ), "Featuremap stride is not power of 2!"
        self.min_level = int(min_level)
        self.max_level = int(max_level)
        assert (
            len(scales) == self.max_level - self.min_level + 1
        ), "[ROIPooler] Sizes of input featuremaps do not form a pyramid!"
        assert 0 <= self.min_level and self.min_level <= self.max_level
        if len(scales) > 1:
            # When there is only one feature map, canonical_level is redundant and we should not
            # require it to be a sensible value. Therefore we skip this assertion
            assert self.min_level <= canonical_level and canonical_level <= self.max_level
        self.canonical_level = canonical_level
        assert canonical_box_size > 0
        self.canonical_box_size = canonical_box_size

    @float_function
    def forward(self, x, box_lists):
        """
        Args:
            x (list[Tensor]): A list of feature maps of NCHW shape, with scales matching those
                used to construct this module.
            box_lists (list[Boxes] | list[RotatedBoxes]):
                The box coordinates are defined on the original image and
                will be scaled by the `scales` argument of :class:`ROIPooler`.

        Returns:
            Tensor:
                A tensor of shape (M, C, output_size, output_size) where M is the total number of
                boxes aggregated over all N batch images and C is the number of channels in `x`.
        """
        num_level_assignments = len(self.level_poolers)

        assert isinstance(x, list) and isinstance(
            box_lists, list
        ), "Arguments to pooler must be lists"
        assert (
            len(x) == num_level_assignments
        ), "unequal value, num_level_assignments={}, but x is list of {} Tensors".format(
            num_level_assignments, len(x)
        )
        assert len(box_lists) == x[0].size(
            0
        ), "unequal value, x[0] batch dim 0 is {}, but box_list has length {}".format(
            x[0].size(0), len(box_lists)
        )
        if len(box_lists) == 0:
            return torch.zeros(
                (0, x[0].shape[1]) + self.output_size, device=x[0].device, dtype=x[0].dtype
            )

        pooler_fmt_boxes = convert_boxes_to_pooler_format(box_lists)

        if num_level_assignments == 1:
            return self.level_poolers[0](x[0], pooler_fmt_boxes)

        level_assignments = assign_boxes_to_levels(
            box_lists, self.min_level, self.max_level, self.canonical_box_size, self.canonical_level
        )

        num_boxes = len(pooler_fmt_boxes)
        num_channels = x[0].shape[1]
        output_size = self.output_size[0]

        dtype, device = x[0].dtype, x[0].device
        output = torch.zeros(
            (num_boxes, num_channels, output_size, output_size), dtype=dtype, device=device
        )

        for level, (x_level, pooler) in enumerate(zip(x, self.level_poolers)):
            inds = torch.nonzero(level_assignments == level, as_tuple=False).squeeze(1)
            pooler_fmt_boxes_level = pooler_fmt_boxes[inds]
            output[inds] = pooler(x_level, pooler_fmt_boxes_level)

        return output
```

### cvpods/modeling/matcher.py

```python
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.
from scipy.optimize import linear_sum_assignment

import torch
from torch import nn

from cvpods.structures.boxes import box_cxcywh_to_xyxy, generalized_box_iou


class Matcher(object):
    """
    This class assigns to each predicted "element" (e.g., a box) a ground-truth
    element. Each predicted element will have exactly zero or one matches; each
    ground-truth element may be matched to zero or more predicted elements.

    The matching is determined by the MxN match_quality_matrix, that characterizes
    how well each (ground-truth, prediction)-pair match each other. For example,
    if the elements are boxes, this matrix may contain box intersection-over-union
    overlap values.

    The matcher returns (a) a vector of length N containing the index of the
    ground-truth element m in [0, M) that matches to prediction n in [0, N).
    (b) a vector of length N containing the labels for each prediction.
    """

    def __init__(self, thresholds, labels, allow_low_quality_matches=False):
        """
        Args:
            thresholds (list): a list of thresholds used to stratify predictions
                into levels.
            labels (list): a list of values to label predictions belonging at
                each level. A label can be one of {-1, 0, 1} signifying
                {ignore, negative class, positive class}, respectively.
            allow_low_quality_matches (bool): if True, produce additional matches
                for predictions with maximum match quality lower than high_threshold.
                See set_low_quality_matches_ for more details.

            For example,
                thresholds = [0.3, 0.5]
                labels = [0, -1, 1]
                All predictions with iou < 0.3 will be marked with 0 and
                thus will be considered as false positives while training.
                All predictions with 0.3 <= iou < 0.5 will be marked with -1 and
                thus will be ignored.
                All predictions with 0.5 <= iou will be marked with 1 and
                thus will be considered as true positives.
        """
        # Add -inf and +inf to first and last position in thresholds
        thresholds = thresholds[:]
        assert thresholds[0] > 0
        thresholds.insert(0, -float("inf"))
        thresholds.append(float("inf"))
        assert all(low <= high for (low, high) in zip(thresholds[:-1], thresholds[1:]))
        assert all(label in [-1, 0, 1] for label in labels)
        assert len(labels) == len(thresholds) - 1
        self.thresholds = thresholds
        self.labels = labels
        self.allow_low_quality_matches = allow_low_quality_matches

    def __call__(self, match_quality_matrix):
        """
        Args:
            match_quality_matrix (Tensor[float]): an MxN tensor, containing the
                pairwise quality between M ground-truth elements and N predicted
                elements. All elements must be >= 0 (due to the us of `torch.nonzero`
                for selecting indices in :meth:`set_low_quality_matches_`).

        Returns:
            matches (Tensor[int64]): a vector of length N, where matches[i] is a matched
                ground-truth index in [0, M)
            match_labels (Tensor[int8]): a vector of length N, where pred_labels[i] indicates
                whether a prediction is a true or false positive or ignored
        """
        assert match_quality_matrix.dim() == 2
        if match_quality_matrix.numel() == 0:
            default_matches = match_quality_matrix.new_full(
                (match_quality_matrix.size(1),), 0, dtype=torch.int64
            )
            # When no gt boxes exist, we define IOU = 0 and therefore set labels
            # to `self.labels[0]`, which usually defaults to background class 0
            # To choose to ignore instead, can make labels=[-1,0,-1,1] + set appropriate thresholds
            default_match_labels = match_quality_matrix.new_full(
                (match_quality_matrix.size(1),), self.labels[0], dtype=torch.int8
            )
            return default_matches, default_match_labels

        assert torch.all(match_quality_matrix >= 0)

        # match_quality_matrix is M (gt) x N (predicted)
        # Max over gt elements (dim 0) to find best gt candidate for each prediction
        matched_vals, matches = match_quality_matrix.max(dim=0)

        match_labels = matches.new_full(matches.size(), 1, dtype=torch.int8)

        for (l, low, high) in zip(self.labels, self.thresholds[:-1], self.thresholds[1:]):
            low_high = (matched_vals >= low) & (matched_vals < high)
            match_labels[low_high] = l

        if self.allow_low_quality_matches:
            self.set_low_quality_matches_(match_labels, match_quality_matrix)

        return matches, match_labels

    def set_low_quality_matches_(self, match_labels, match_quality_matrix):
        """
        Produce additional matches for predictions that have only low-quality matches.
        Specifically, for each ground-truth G find the set of predictions that have
        maximum overlap with it (including ties); for each prediction in that set, if
        it is unmatched, then match it to the ground-truth G.

        This function implements the RPN assignment case (i) in Sec. 3.1.2 of the
        Faster R-CNN paper: https://arxiv.org/pdf/1506.01497v3.pdf.
        """
        # For each gt, find the prediction with which it has highest quality
        highest_quality_foreach_gt, _ = match_quality_matrix.max(dim=1)
        # Find the highest quality match available, even if it is low, including ties.
        # Note that the matches qualities must be positive due to the use of
        # `torch.nonzero`.
        gt_pred_pairs_of_highest_quality = torch.nonzero(
            match_quality_matrix == highest_quality_foreach_gt[:, None],
            as_tuple=False
        )
        # Example gt_pred_pairs_of_highest_quality:
        #   tensor([[    0, 39796],
        #           [    1, 32055],
        #           [    1, 32070],
        #           [    2, 39190],
        #           [    2, 40255],
        #           [    3, 40390],
        #           [    3, 41455],
        #           [    4, 45470],
        #           [    5, 45325],
        #           [    5, 46390]])
        # Each row is a (gt index, prediction index)
        # Note how gt items 1, 2, 3, and 5 each have two ties

        pred_inds_to_update = gt_pred_pairs_of_highest_quality[:, 1]
        match_labels[pred_inds_to_update] = 1


class MatcherIgnore(Matcher):
    """
    Matcher with Ignore labels. (e.g. Crowd Human)
    This class assigns to each predicted "element" (e.g., a box) a ground-truth
    element. Each predicted element will have exactly zero or one matches; each
    ground-truth element may be matched to zero or more predicted elements.

    The matching is determined by the MxN match_quality_matrix, that characterizes
    how well each (ground-truth, prediction)-pair match each other. For example,
    if the elements are boxes, this matrix may contain box intersection-over-union
    overlap values.

    The matcher returns (a) a vector of length N containing the index of the
    ground-truth element m in [0, M) that matches to prediction n in [0, N).
    (b) a vector of length N containing the labels for each prediction.
    """

    def __call__(self, match_quality_matrix, match_quality_ignore=None):
        """
        Args:
            match_quality_matrix (Tensor[float]): an MxN tensor, containing the
                pairwise quality between M ground-truth elements and N predicted
                elements. All elements must be >= 0 (due to the us of `torch.nonzero`
                for selecting indices in :meth:`set_low_quality_matches_`).

        Returns:
            matches (Tensor[int64]): a vector of length N, where matches[i] is a matched
                ground-truth index in [0, M)
            match_labels (Tensor[int8]): a vector of length N, where pred_labels[i] indicates
                whether a prediction is a true or false positive or ignored
        """
        assert match_quality_matrix.dim() == 2
        if match_quality_matrix.numel() == 0:
            default_matches = match_quality_matrix.new_full(
                (match_quality_matrix.size(1),), 0, dtype=torch.int64
            )
            # When no gt boxes exist, we define IOU = 0 and therefore set labels
            # to `self.labels[0]`, which usually defaults to background class 0
            # To choose to ignore instead, can make labels=[-1,0,-1,1] + set appropriate thresholds
            default_match_labels = match_quality_matrix.new_full(
                (match_quality_matrix.size(1),), self.labels[0], dtype=torch.int8
            )
            return default_matches, default_match_labels

        assert torch.all(match_quality_matrix >= 0)

        # match_quality_matrix is M (gt) x N (predicted)
        # Max over gt elements (dim 0) to find best gt candidate for each prediction
        matched_vals, matches = match_quality_matrix.max(dim=0)
        matched_vals_ign, matches_ign = match_quality_ignore.max(dim=0)

        assert len(self.thresholds) == 3
        high = self.thresholds[1]
        ign_mask = ((matched_vals_ign > matched_vals) & (matched_vals < high)).float()
        matched_vals = matched_vals * (1 - ign_mask) + matched_vals_ign * ign_mask
        matches = (matches * (1 - ign_mask) + matches_ign * ign_mask).long()

        match_labels = matches.new_full(matches.size(), 1, dtype=torch.int8)

        for (l, low, high) in zip(self.labels, self.thresholds[:-1], self.thresholds[1:]):
            low_high = (matched_vals >= low) & (matched_vals < high)
            match_labels[low_high] = l

        if self.allow_low_quality_matches:
            self.set_low_quality_matches_(match_labels, match_quality_matrix)

        return matches, match_labels


class HungarianMatcher(nn.Module):
    """This class computes an assignment between the targets and the predictions of the network
    For efficiency reasons, the targets don't include the no_object. Because of this, in general,
    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best
    predictions, while the others are un-matched (and thus treated as non-objects).
    """

    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):
        """Creates the matcher
        Params:
            cost_class: This is the relative weight of the classification error in the matching cost
            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates
            in the matching cost cost_giou: This is the relative weight of the giou loss of the
            bounding box in the matching cost
        """
        super().__init__()
        self.cost_class = cost_class
        self.cost_bbox = cost_bbox
        self.cost_giou = cost_giou
        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, "all costs cant be 0"

    @torch.no_grad()
    def forward(self, outputs, targets):
        """ Performs the matching
        Params:
            outputs: This is a dict that contains at least these entries:
                 "pred_logits": Tensor of dim [batch_size, num_queries, num_classes] with the
                            classification logits
                 "pred_boxes": Tensor of dim [batch_size, num_queries, 4] with the predicted
                            box coordinates
            targets: This is a list of targets (len(targets) = batch_size), where each target
                            is a dict containing:
                 "labels": Tensor of dim [num_target_boxes] (where num_target_boxes is the number
                            of ground-truth objects in the target) containing the class labels
                 "boxes": Tensor of dim [num_target_boxes, 4] containing the target box coordinates
        Returns:
            A list of size batch_size, containing tuples of (index_i, index_j) where:
                - index_i is the indices of the selected predictions (in order)
                - index_j is the indices of the corresponding selected targets (in order)
            For each batch element, it holds:
                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)
        """
        bs, num_queries = outputs["pred_logits"].shape[:2]

        # We flatten to compute the cost matrices in a batch
        out_prob = (
            outputs["pred_logits"].flatten(0, 1).softmax(-1)
        )  # [batch_size * num_queries, num_classes]
        out_bbox = outputs["pred_boxes"].flatten(0, 1)  # [batch_size * num_queries, 4]

        # Also concat the target labels and boxes
        tgt_ids = torch.cat([v["labels"] for v in targets])
        tgt_bbox = torch.cat([v["boxes"] for v in targets])

        # Compute the classification cost. Contrary to the loss, we don't use the NLL,
        # but approximate it in 1 - proba[target class].
        # The 1 is a constant that doesn't change the matching, it can be ommitted.
        cost_class = -out_prob[:, tgt_ids]

        # Compute the L1 cost between boxes
        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)

        # Compute the giou cost betwen boxes
        cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))

        # Final cost matrix
        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou
        C = C.view(bs, num_queries, -1).cpu()

        sizes = [len(v["boxes"]) for v in targets]
        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]
        return [
            (torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64))
            for i, j in indices
        ]
```

### cvpods/modeling/__init__.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import torch

from cvpods.layers import ShapeSpec

# from .anchor_generator import build_anchor_generator
from .backbone import FPN, Backbone, ResNet, ResNetBlockBase, build_resnet_backbone, make_stage
from .matcher import Matcher
from .meta_arch import GeneralizedRCNN, PanopticFPN, ProposalNetwork, RetinaNet, SemanticSegmentor
from .postprocessing import detector_postprocess
from .roi_heads import ROIHeads, StandardROIHeads
from .test_time_augmentation import DatasetMapperTTA, GeneralizedRCNNWithTTA, TTAWarper

_EXCLUDE = {"torch", "ShapeSpec"}
__all__ = [k for k in globals().keys() if k not in _EXCLUDE and not k.startswith("_")]

assert (
    torch.Tensor([1]) == torch.Tensor([2])
).dtype == torch.bool, ("Your Pytorch is too old. "
                        "Please update to contain https://github.com/pytorch/pytorch/pull/21113")
```

### cvpods/modeling/box_regression.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import math

import torch

# Value for clamping large dw and dh predictions. The heuristic is that we clamp
# such that dw and dh are no larger than what would transform a 16px box into a
# 1000px box (based on a small anchor, 16px, and a typical image size, 1000px).
_DEFAULT_SCALE_CLAMP = math.log(1000.0 / 16)

__all__ = ["Box2BoxTransform", "Box2BoxTransformRotated"]


class Box2BoxTransform(object):
    """
    The box-to-box transform defined in R-CNN. The transformation is parameterized
    by 4 deltas: (dx, dy, dw, dh). The transformation scales the box's width and height
    by exp(dw), exp(dh) and shifts a box's center by the offset (dx * width, dy * height).
    """

    def __init__(
            self,
            weights,
            scale_clamp=_DEFAULT_SCALE_CLAMP,
            add_ctr_clamp=False,
            ctr_clamp=32
    ):
        """
        Args:
            weights (4-element tuple): Scaling factors that are applied to the
                (dx, dy, dw, dh) deltas. In Fast R-CNN, these were originally set
                such that the deltas have unit variance; now they are treated as
                hyperparameters of the system.
            scale_clamp (float): When predicting deltas, the predicted box scaling
                factors (dw and dh) are clamped such that they are <= scale_clamp.
            add_ctr_clamp (bool): Whether to add center clamp, when added, the
                predicted box is clamped is its center is too far away from
                the original anchor's center.
            ctr_clamp (int): the maximum pixel shift to clamp.
        """
        self.weights = weights
        self.scale_clamp = scale_clamp
        self.add_ctr_clamp = add_ctr_clamp
        self.ctr_clamp = ctr_clamp

    def get_deltas(self, src_boxes, target_boxes):
        """
        Get box regression transformation deltas (dx, dy, dw, dh) that can be used
        to transform the `src_boxes` into the `target_boxes`. That is, the relation
        ``target_boxes == self.apply_deltas(deltas, src_boxes)`` is true (unless
        any delta is too large and is clamped).

        Args:
            src_boxes (Tensor): source boxes, e.g., object proposals
            target_boxes (Tensor): target of the transformation, e.g., ground-truth
                boxes.
        """
        assert isinstance(src_boxes, torch.Tensor), type(src_boxes)
        assert isinstance(target_boxes, torch.Tensor), type(target_boxes)

        src_widths = src_boxes[..., 2] - src_boxes[..., 0]
        src_heights = src_boxes[..., 3] - src_boxes[..., 1]
        src_ctr_x = src_boxes[..., 0] + 0.5 * src_widths
        src_ctr_y = src_boxes[..., 1] + 0.5 * src_heights

        target_widths = target_boxes[..., 2] - target_boxes[..., 0]
        target_heights = target_boxes[..., 3] - target_boxes[..., 1]
        target_ctr_x = target_boxes[..., 0] + 0.5 * target_widths
        target_ctr_y = target_boxes[..., 1] + 0.5 * target_heights

        wx, wy, ww, wh = self.weights
        dx = wx * (target_ctr_x - src_ctr_x) / src_widths
        dy = wy * (target_ctr_y - src_ctr_y) / src_heights
        dw = ww * torch.log(target_widths / src_widths)
        dh = wh * torch.log(target_heights / src_heights)

        deltas = torch.stack((dx, dy, dw, dh), dim=-1)
        assert (
                    src_widths > 0).all().item(), "Input boxes to Box2BoxTransform are not valid!"
        return deltas

    def apply_deltas(self, deltas, boxes):
        """
        Apply transformation `deltas` (dx, dy, dw, dh) to `boxes`.

        Args:
            deltas (Tensor): transformation deltas of shape (N, k*4), where k >= 1.
                deltas[i] represents k potentially different class-specific
                box transformations for the single box boxes[i].
            boxes (Tensor): boxes to transform, of shape (N, 4)
        """
        assert torch.isfinite(
            deltas).all().item(), "Box regression deltas become infinite or NaN!"
        boxes = boxes.to(deltas.dtype)

        widths = boxes[..., 2] - boxes[..., 0]
        heights = boxes[..., 3] - boxes[..., 1]
        ctr_x = boxes[..., 0] + 0.5 * widths
        ctr_y = boxes[..., 1] + 0.5 * heights

        wx, wy, ww, wh = self.weights
        dx = deltas[..., 0::4] / wx
        dy = deltas[..., 1::4] / wy
        dw = deltas[..., 2::4] / ww
        dh = deltas[..., 3::4] / wh

        # Prevent sending too large values into torch.exp()
        dx_width = dx * widths[..., None]
        dy_height = dy * heights[..., None]
        if self.add_ctr_clamp:
            dx_width = torch.clamp(dx_width,
                                   max=self.ctr_clamp,
                                   min=-self.ctr_clamp)
            dy_height = torch.clamp(dy_height,
                                    max=self.ctr_clamp,
                                    min=-self.ctr_clamp)
        dw = torch.clamp(dw, max=self.scale_clamp)
        dh = torch.clamp(dh, max=self.scale_clamp)

        pred_ctr_x = dx_width + ctr_x[..., None]
        pred_ctr_y = dy_height + ctr_y[..., None]
        pred_w = torch.exp(dw) * widths[..., None]
        pred_h = torch.exp(dh) * heights[..., None]

        pred_boxes = torch.zeros_like(deltas)
        pred_boxes[..., 0::4] = pred_ctr_x - 0.5 * pred_w  # x1
        pred_boxes[..., 1::4] = pred_ctr_y - 0.5 * pred_h  # y1
        pred_boxes[..., 2::4] = pred_ctr_x + 0.5 * pred_w  # x2
        pred_boxes[..., 3::4] = pred_ctr_y + 0.5 * pred_h  # y2
        return pred_boxes


class Box2BoxTransformRotated(object):
    """
    The box-to-box transform defined in Rotated R-CNN. The transformation is parameterized
    by 5 deltas: (dx, dy, dw, dh, da). The transformation scales the box's width and height
    by exp(dw), exp(dh), shifts a box's center by the offset (dx * width, dy * height),
    and rotate a box's angle by da (radians).
    Note: angles of deltas are in radians while angles of boxes are in degrees.
    """

    def __init__(self, weights, scale_clamp=_DEFAULT_SCALE_CLAMP):
        """
        Args:
            weights (5-element tuple): Scaling factors that are applied to the
                (dx, dy, dw, dh, da) deltas. These are treated as
                hyperparameters of the system.
            scale_clamp (float): When predicting deltas, the predicted box scaling
                factors (dw and dh) are clamped such that they are <= scale_clamp.
        """
        self.weights = weights
        self.scale_clamp = scale_clamp

    def get_deltas(self, src_boxes, target_boxes):
        """
        Get box regression transformation deltas (dx, dy, dw, dh, da) that can be used
        to transform the `src_boxes` into the `target_boxes`. That is, the relation
        ``target_boxes == self.apply_deltas(deltas, src_boxes)`` is true (unless
        any delta is too large and is clamped).

        Args:
            src_boxes (Tensor): Nx5 source boxes, e.g., object proposals
            target_boxes (Tensor): Nx5 target of the transformation, e.g., ground-truth
                boxes.
        """
        assert isinstance(src_boxes, torch.Tensor), type(src_boxes)
        assert isinstance(target_boxes, torch.Tensor), type(target_boxes)

        src_ctr_x, src_ctr_y, src_widths, src_heights, src_angles = torch.unbind(
            src_boxes, dim=1)

        target_ctr_x, target_ctr_y, target_widths, target_heights, target_angles = torch.unbind(
            target_boxes, dim=1
        )

        wx, wy, ww, wh, wa = self.weights
        dx = wx * (target_ctr_x - src_ctr_x) / src_widths
        dy = wy * (target_ctr_y - src_ctr_y) / src_heights
        dw = ww * torch.log(target_widths / src_widths)
        dh = wh * torch.log(target_heights / src_heights)
        # Angles of deltas are in radians while angles of boxes are in degrees.
        # the conversion to radians serve as a way to normalize the values
        da = target_angles - src_angles
        da = (da + 180.0) % 360.0 - 180.0  # make it in [-180, 180)
        da *= wa * math.pi / 180.0

        deltas = torch.stack((dx, dy, dw, dh, da), dim=1)
        assert (
            (src_widths > 0).all().item()
        ), "Input boxes to Box2BoxTransformRotated are not valid!"
        return deltas

    def apply_deltas(self, deltas, boxes):
        """
        Apply transformation `deltas` (dx, dy, dw, dh, da) to `boxes`.

        Args:
            deltas (Tensor): transformation deltas of shape (N, 5).
                deltas[i] represents box transformation for the single box boxes[i].
            boxes (Tensor): boxes to transform, of shape (N, 5)
        """
        assert deltas.shape[1] == 5 and boxes.shape[1] == 5
        assert torch.isfinite(
            deltas).all().item(), "Box regression deltas become infinite or NaN!"

        boxes = boxes.to(deltas.dtype)

        ctr_x = boxes[:, 0]
        ctr_y = boxes[:, 1]
        widths = boxes[:, 2]
        heights = boxes[:, 3]
        angles = boxes[:, 4]

        wx, wy, ww, wh, wa = self.weights

        dx = deltas[:, 0] / wx
        dy = deltas[:, 1] / wy
        dw = deltas[:, 2] / ww
        dh = deltas[:, 3] / wh
        da = deltas[:, 4] / wa

        # Prevent sending too large values into torch.exp()
        dw = torch.clamp(dw, max=self.scale_clamp)
        dh = torch.clamp(dh, max=self.scale_clamp)

        pred_boxes = torch.zeros_like(deltas)
        pred_boxes[..., 0] = dx * widths + ctr_x  # x_ctr
        pred_boxes[..., 1] = dy * heights + ctr_y  # y_ctr
        pred_boxes[..., 2] = torch.exp(dw) * widths  # width
        pred_boxes[..., 3] = torch.exp(dh) * heights  # height

        # Following original RRPN implementation,
        # angles of deltas are in radians while angles of boxes are in degrees.
        pred_angle = da * 180.0 / math.pi + angles
        pred_angle = (
                                 pred_angle + 180.0) % 360.0 - 180.0  # make it in [-180, 180)

        pred_boxes[..., 4] = pred_angle

        return pred_boxes


class Shift2BoxTransform(object):
    def __init__(self, weights):
        """
        Args:
            weights (4-element tuple): Scaling factors that are applied to the
                (dl, dt, dr, db) deltas.
        """
        self.weights = weights

    def get_deltas(self, shifts, boxes):
        """
        Get box regression transformation deltas (dl, dt, dr, db) that can be used
        to transform the `shifts` into the `boxes`. That is, the relation
        ``boxes == self.apply_deltas(deltas, shifts)`` is true.

        Args:
            shifts (Tensor): shifts, e.g., feature map coordinates
            boxes (Tensor): target of the transformation, e.g., ground-truth
                boxes.
        """
        assert isinstance(shifts, torch.Tensor), type(shifts)
        assert isinstance(boxes, torch.Tensor), type(boxes)

        deltas = torch.cat((shifts - boxes[..., :2], boxes[..., 2:] - shifts),
                           dim=-1) * shifts.new_tensor(self.weights)
        return deltas

    def apply_deltas(self, deltas, shifts):
        """
        Apply transformation `deltas` (dl, dt, dr, db) to `shifts`.

        Args:
            deltas (Tensor): transformation deltas of shape (N, k*4), where k >= 1.
                deltas[i] represents k potentially different class-specific
                box transformations for the single shift shifts[i].
            shifts (Tensor): shifts to transform, of shape (N, 2)
        """
        assert torch.isfinite(deltas).all().item()
        shifts = shifts.to(deltas.dtype)

        if deltas.numel() == 0:
            return torch.empty_like(deltas)

        deltas = deltas.view(deltas.size()[:-1] + (-1, 4)) / shifts.new_tensor(
            self.weights)
        boxes = torch.cat((shifts.unsqueeze(-2) - deltas[..., :2],
                           shifts.unsqueeze(-2) + deltas[..., 2:]),
                          dim=-1).view(deltas.size()[:-2] + (-1,))
        return boxes
```

### cvpods/modeling/anchor_generator.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import copy
import math
from typing import List

import torch
from torch import nn

from cvpods.layers import ShapeSpec
from cvpods.structures import Boxes, RotatedBoxes


"""
Registry for modules that creates object detection anchors for feature maps.

The registered object will be called with `obj(cfg, input_shape)`.
"""


class BufferList(nn.Module):
    """
    Similar to nn.ParameterList, but for buffers
    """

    def __init__(self, buffers=None):
        super(BufferList, self).__init__()
        if buffers is not None:
            self.extend(buffers)

    def extend(self, buffers):
        offset = len(self)
        for i, buffer in enumerate(buffers):
            self.register_buffer(str(offset + i), buffer)
        return self

    def __len__(self):
        return len(self._buffers)

    def __iter__(self):
        return iter(self._buffers.values())


def _create_grid_offsets(size, stride, offset, device):
    grid_height, grid_width = size
    shifts_start = offset * stride
    shifts_x = torch.arange(
        shifts_start, grid_width * stride + shifts_start, step=stride,
        dtype=torch.float32, device=device
    )
    shifts_y = torch.arange(
        shifts_start, grid_height * stride + shifts_start, step=stride,
        dtype=torch.float32, device=device
    )
    shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)
    shift_x = shift_x.reshape(-1)
    shift_y = shift_y.reshape(-1)
    return shift_x, shift_y


class DefaultAnchorGenerator(nn.Module):
    """
    For a set of image sizes and feature maps, computes a set of anchors.
    """

    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super().__init__()
        # fmt: off
        sizes         = cfg.MODEL.ANCHOR_GENERATOR.SIZES
        aspect_ratios = cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS
        self.strides  = [x.stride for x in input_shape]
        self.offset   = cfg.MODEL.ANCHOR_GENERATOR.OFFSET

        assert 0.0 <= self.offset < 1.0, self.offset

        # fmt: on
        """
        sizes (list[list[int]]): sizes[i] is the list of anchor sizes to use
            for the i-th feature map. If len(sizes) == 1, then the same list of
            anchor sizes, given by sizes[0], is used for all feature maps. Anchor
            sizes are given in absolute lengths in units of the input image;
            they do not dynamically scale if the input image size changes.
        aspect_ratios (list[list[float]]): aspect_ratios[i] is the list of
            anchor aspect ratios to use for the i-th feature map. If
            len(aspect_ratios) == 1, then the same list of anchor aspect ratios,
            given by aspect_ratios[0], is used for all feature maps.
        strides (list[int]): stride of each input feature.
        """

        self.num_features = len(self.strides)
        self.cell_anchors = self._calculate_anchors(sizes, aspect_ratios)

    def _calculate_anchors(self, sizes, aspect_ratios):
        # If one size (or aspect ratio) is specified and there are multiple feature
        # maps, then we "broadcast" anchors of that single size (or aspect ratio)
        # over all feature maps.
        if len(sizes) == 1:
            sizes *= self.num_features
        if len(aspect_ratios) == 1:
            aspect_ratios *= self.num_features
        assert self.num_features == len(sizes)
        assert self.num_features == len(aspect_ratios)

        cell_anchors = [
            self.generate_cell_anchors(s, a).float() for s, a in zip(sizes, aspect_ratios)
        ]

        return BufferList(cell_anchors)

    @property
    def box_dim(self):
        """
        Returns:
            int: the dimension of each anchor box.
        """
        return 4

    @property
    def num_cell_anchors(self):
        """
        Returns:
            list[int]: Each int is the number of anchors at every pixel
                location, on that feature map.
                For example, if at every pixel we use anchors of 3 aspect
                ratios and 5 sizes, the number of anchors is 15.
                (See also ANCHOR_GENERATOR.SIZES and ANCHOR_GENERATOR.ASPECT_RATIOS in config)

                In standard RPN models, `num_cell_anchors` on every feature map is the same.
        """
        return [len(cell_anchors) for cell_anchors in self.cell_anchors]

    def grid_anchors(self, grid_sizes):
        anchors = []
        for size, stride, base_anchors in zip(grid_sizes, self.strides, self.cell_anchors):
            shift_x, shift_y = _create_grid_offsets(size, stride, self.offset, base_anchors.device)
            shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=1)

            anchors.append((shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4))

        return anchors

    def generate_cell_anchors(self, sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.5, 1, 2)):
        """
        Generate a tensor storing anchor boxes, which are continuous geometric rectangles
        centered on one feature map point sample. We can later build the set of anchors
        for the entire feature map by tiling these tensors; see `meth:grid_anchors`.

        Args:
            sizes (tuple[float]): Absolute size of the anchors in the units of the input
                image (the input received by the network, after undergoing necessary scaling).
                The absolute size is given as the side length of a box.
            aspect_ratios (tuple[float]]): Aspect ratios of the boxes computed as box
                height / width.

        Returns:
            Tensor of shape (len(sizes) * len(aspect_ratios), 4) storing anchor boxes
                in XYXY format.
        """

        # This is different from the anchor generator defined in the original Faster R-CNN
        # code or Detectron. They yield the same AP, however the old version defines cell
        # anchors in a less natural way with a shift relative to the feature grid and
        # quantization that results in slightly different sizes for different aspect ratios.
        # See also https://github.com/facebookresearch/Detectron/issues/227

        anchors = []
        for size in sizes:
            area = size ** 2.0
            for aspect_ratio in aspect_ratios:
                # s * s = w * h
                # a = h / w
                # ... some algebra ...
                # w = sqrt(s * s / a)
                # h = a * w
                w = math.sqrt(area / aspect_ratio)
                h = aspect_ratio * w
                x0, y0, x1, y1 = -w / 2.0, -h / 2.0, w / 2.0, h / 2.0
                anchors.append([x0, y0, x1, y1])
        return torch.tensor(anchors)

    def forward(self, features):
        """
        Args:
            features (list[Tensor]): list of backbone feature maps on which to generate anchors.

        Returns:
            list[list[Boxes]]: a list of #image elements. Each is a list of #feature level Boxes.
                The Boxes contains anchors of this image on the specific feature level.
        """
        num_images = len(features[0])
        grid_sizes = [feature_map.shape[-2:] for feature_map in features]
        anchors_over_all_feature_maps = self.grid_anchors(grid_sizes)

        anchors_in_image = []
        for anchors_per_feature_map in anchors_over_all_feature_maps:
            boxes = Boxes(anchors_per_feature_map)
            anchors_in_image.append(boxes)

        anchors = [copy.deepcopy(anchors_in_image) for _ in range(num_images)]
        return anchors


class RotatedAnchorGenerator(nn.Module):
    """
    The anchor generator used by Rotated RPN (RRPN).
    """

    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super().__init__()
        # fmt: off
        sizes         = cfg.MODEL.ANCHOR_GENERATOR.SIZES
        aspect_ratios = cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS
        angles        = cfg.MODEL.ANCHOR_GENERATOR.ANGLES
        self.strides  = [x.stride for x in input_shape]
        self.offset   = cfg.MODEL.ANCHOR_GENERATOR.OFFSET

        assert 0.0 <= self.offset < 1.0, self.offset

        # fmt: on

        self.num_features = len(self.strides)
        self.cell_anchors = self._calculate_anchors(sizes, aspect_ratios, angles)

    def _calculate_anchors(self, sizes, aspect_ratios, angles):
        """
        Args:
            sizes (list[list[int]]): sizes[i] is the list of anchor sizes to use
                for the i-th feature map. If len(sizes) == 1, then the same list of
                anchor sizes, given by sizes[0], is used for all feature maps. Anchor
                sizes are given in absolute lengths in units of the input image;
                they do not dynamically scale if the input image size changes.
            aspect_ratios (list[list[float]]): aspect_ratios[i] is the list of
                anchor aspect ratios to use for the i-th feature map. If
                len(aspect_ratios) == 1, then the same list of anchor aspect ratios,
                given by aspect_ratios[0], is used for all feature maps.
            angles (list[list[float]]): angles[i] is the list of
                anchor angles to use for the i-th feature map. If
                len(angles) == 1, then the same list of anchor angles,
                given by angles[0], is used for all feature maps.
        """

        # If one size (or aspect ratio) is specified and there are multiple feature
        # maps, then we "broadcast" anchors of that single size
        # (or aspect ratio/angle) over all feature maps.

        if len(sizes) == 1:
            sizes *= self.num_features
        if len(aspect_ratios) == 1:
            aspect_ratios *= self.num_features
        if len(angles) == 1:
            angles *= self.num_features
        assert self.num_features == len(sizes)
        assert self.num_features == len(aspect_ratios)
        assert self.num_features == len(angles)

        cell_anchors = [
            self.generate_cell_anchors(size, aspect_ratio, angle).float()
            for size, aspect_ratio, angle in zip(sizes, aspect_ratios, angles)
        ]

        return BufferList(cell_anchors)

    @property
    def box_dim(self):
        """
        Returns:
            int: the dimension of each anchor box.
        """
        return 5

    @property
    def num_cell_anchors(self):
        """
        Returns:
            list[int]: Each int is the number of anchors at every pixel
                location, on that feature map.
                For example, if at every pixel we use anchors of 3 aspect
                ratios, 2 sizes and 5 angles, the number of anchors is 30.
                (See also ANCHOR_GENERATOR.SIZES, ANCHOR_GENERATOR.ASPECT_RATIOS
                and ANCHOR_GENERATOR.ANGLES in config)

                In standard RRPN models, `num_cell_anchors` on every feature map is the same.
        """
        return [len(cell_anchors) for cell_anchors in self.cell_anchors]

    def grid_anchors(self, grid_sizes):
        anchors = []
        for size, stride, base_anchors in zip(grid_sizes, self.strides, self.cell_anchors):
            shift_x, shift_y = _create_grid_offsets(size, stride, self.offset, base_anchors.device)
            zeros = torch.zeros_like(shift_x)
            shifts = torch.stack((shift_x, shift_y, zeros, zeros, zeros), dim=1)

            anchors.append((shifts.view(-1, 1, 5) + base_anchors.view(1, -1, 5)).reshape(-1, 5))

        return anchors

    def generate_cell_anchors(
        self,
        sizes=(32, 64, 128, 256, 512),
        aspect_ratios=(0.5, 1, 2),
        angles=(-90, -60, -30, 0, 30, 60, 90),
    ):
        """
        Generate a tensor storing anchor boxes, which are continuous geometric rectangles
        centered on one feature map point sample. We can later build the set of anchors
        for the entire feature map by tiling these tensors; see `meth:grid_anchors`.

        Args:
            sizes (tuple[float]): Absolute size of the anchors in the units of the input
                image (the input received by the network, after undergoing necessary scaling).
                The absolute size is given as the side length of a box.
            aspect_ratios (tuple[float]]): Aspect ratios of the boxes computed as box
                height / width.
            angles (tuple[float]]): Angles of boxes indicating how many degrees
                the boxes are rotated counter-clockwise.

        Returns:
            Tensor of shape (len(sizes) * len(aspect_ratios) * len(angles), 5)
                storing anchor boxes in (x_ctr, y_ctr, w, h, angle) format.
        """
        anchors = []
        for size in sizes:
            area = size ** 2.0
            for aspect_ratio in aspect_ratios:
                # s * s = w * h
                # a = h / w
                # ... some algebra ...
                # w = sqrt(s * s / a)
                # h = a * w
                w = math.sqrt(area / aspect_ratio)
                h = aspect_ratio * w
                anchors.extend([0, 0, w, h, a] for a in angles)

        return torch.tensor(anchors)

    def forward(self, features):
        """
        Args:
            features (list[Tensor]): list of backbone feature maps on which to generate anchors.

        Returns:
            list[list[RotatedBoxes]]:
                a list of #image elements. Each is a list of #feature level RotatedBoxes.
                The RotatedBoxes contains anchors of this image on the specific feature level.
        """
        num_images = len(features[0])
        grid_sizes = [feature_map.shape[-2:] for feature_map in features]
        anchors_over_all_feature_maps = self.grid_anchors(grid_sizes)

        anchors_in_image = []
        for anchors_per_feature_map in anchors_over_all_feature_maps:
            boxes = RotatedBoxes(anchors_per_feature_map)
            anchors_in_image.append(boxes)

        anchors = [copy.deepcopy(anchors_in_image) for _ in range(num_images)]
        return anchors


class ShiftGenerator(nn.Module):
    """
    For a set of image sizes and feature maps, computes a set of shifts.
    """
    # TODO: unused_arguments: cfg
    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super().__init__()
        # fmt: off
        self.num_shifts = cfg.MODEL.SHIFT_GENERATOR.NUM_SHIFTS
        self.strides    = [x.stride for x in input_shape]
        self.offset     = cfg.MODEL.SHIFT_GENERATOR.OFFSET
        # fmt: on

        self.num_features = len(self.strides)

    @property
    def num_cell_shifts(self):
        return [self.num_shifts for _ in self.strides]

    def grid_shifts(self, grid_sizes, device):
        shifts_over_all = []
        for size, stride in zip(grid_sizes, self.strides):
            shift_x, shift_y = _create_grid_offsets(size, stride, self.offset, device)
            shifts = torch.stack((shift_x, shift_y), dim=1)

            shifts_over_all.append(shifts.repeat_interleave(self.num_shifts, dim=0))

        return shifts_over_all

    def forward(self, features):
        """
        Args:
            features (list[Tensor]): list of backbone feature maps on which to generate shifts.

        Returns:
            list[list[Tensor]]: a list of #image elements. Each is a list of #feature level tensors.
                The tensors contains shifts of this image on the specific feature level.
        """
        num_images = len(features[0])
        grid_sizes = [feature_map.shape[-2:] for feature_map in features]
        shifts_over_all = self.grid_shifts(grid_sizes, features[0].device)

        shifts = [copy.deepcopy(shifts_over_all) for _ in range(num_images)]
        return shifts
```

### cvpods/modeling/sampling.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import torch

__all__ = ["subsample_labels"]


def subsample_labels(labels, num_samples, positive_fraction, bg_label):
    """
    Return `num_samples` (or fewer, if not enough found)
    random samples from `labels` which is a mixture of positives & negatives.
    It will try to return as many positives as possible without
    exceeding `positive_fraction * num_samples`, and then try to
    fill the remaining slots with negatives.

    Args:
        labels (Tensor): (N, ) label vector with values:
            * -1: ignore
            * bg_label: background ("negative") class
            * otherwise: one or more foreground ("positive") classes
        num_samples (int): The total number of labels with value >= 0 to return.
            Values that are not sampled will be filled with -1 (ignore).
        positive_fraction (float): The number of subsampled labels with values > 0
            is `min(num_positives, int(positive_fraction * num_samples))`. The number
            of negatives sampled is `min(num_negatives, num_samples - num_positives_sampled)`.
            In order words, if there are not enough positives, the sample is filled with
            negatives. If there are also not enough negatives, then as many elements are
            sampled as is possible.
        bg_label (int): label index of background ("negative") class.

    Returns:
        pos_idx, neg_idx (Tensor):
            1D vector of indices. The total length of both is `num_samples` or fewer.
    """
    positive = torch.nonzero((labels != -1) & (labels != bg_label), as_tuple=False).squeeze(1)
    negative = torch.nonzero(labels == bg_label, as_tuple=False).squeeze(1)

    num_pos = int(num_samples * positive_fraction)
    # protect against not enough positive examples
    num_pos = min(positive.numel(), num_pos)
    num_neg = num_samples - num_pos
    # protect against not enough negative examples
    num_neg = min(negative.numel(), num_neg)

    # randomly select positive and negative examples
    perm1 = torch.randperm(positive.numel(), device=positive.device)[:num_pos]
    perm2 = torch.randperm(negative.numel(), device=negative.device)[:num_neg]

    pos_idx = positive[perm1]
    neg_idx = negative[perm2]
    return pos_idx, neg_idx
```

### cvpods/modeling/postprocessing.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from torch.nn import functional as F

from cvpods.layers import paste_masks_in_image
from cvpods.structures import Instances


def detector_postprocess(results, output_height, output_width, mask_threshold=0.5):
    """
    Resize the output instances.
    The input images are often resized when entering an object detector.
    As a result, we often need the outputs of the detector in a different
    resolution from its inputs.

    This function will resize the raw outputs of an R-CNN detector
    to produce outputs according to the desired output resolution.

    Args:
        results (Instances): the raw outputs from the detector.
            `results.image_size` contains the input image resolution the detector sees.
            This object might be modified in-place.
        output_height, output_width: the desired output resolution.

    Returns:
        Instances: the resized output from the model, based on the output resolution
    """
    scale_x, scale_y = (output_width / results.image_size[1], output_height / results.image_size[0])
    results = Instances((output_height, output_width), **results.get_fields())

    if results.has("pred_boxes"):
        output_boxes = results.pred_boxes
    elif results.has("proposal_boxes"):
        output_boxes = results.proposal_boxes

    output_boxes.scale(scale_x, scale_y)
    output_boxes.clip(results.image_size)

    results = results[output_boxes.nonempty()]

    if results.has("pred_masks"):
        results.pred_masks = paste_masks_in_image(
            results.pred_masks[:, 0, :, :],  # N, 1, M, M
            results.pred_boxes,
            results.image_size,
            threshold=mask_threshold,
        )

    if results.has("pred_keypoints"):
        results.pred_keypoints[:, :, 0] *= scale_x
        results.pred_keypoints[:, :, 1] *= scale_y

    return results


def sem_seg_postprocess(result, img_size, output_height, output_width):
    """
    Return semantic segmentation predictions in the original resolution.

    The input images are often resized when entering semantic segmentor. Moreover, in same
    cases, they also padded inside segmentor to be divisible by maximum network stride.
    As a result, we often need the predictions of the segmentor in a different
    resolution from its inputs.

    Args:
        result (Tensor): semantic segmentation prediction logits. A tensor of shape (C, H, W),
            where C is the number of classes, and H, W are the height and width of the prediction.
        img_size (tuple): image size that segmentor is taking as input.
        output_height, output_width: the desired output resolution.

    Returns:
        semantic segmentation prediction (Tensor): A tensor of the shape
            (C, output_height, output_width) that contains per-pixel soft predictions.
    """
    result = result[:, : img_size[0], : img_size[1]].expand(1, -1, -1, -1)
    result = F.interpolate(
        result, size=(output_height, output_width), mode="bilinear", align_corners=False
    )[0]
    return result
```

#### cvpods/modeling/losses/dice_loss.py

```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved
import torch


def dice_loss(input, target):
    r"""
    Dice loss defined in the V-Net paper as:

    Loss_dice = 1 - D

            2 * sum(p_i * g_i)
    D = ------------------------------
         sum(p_i ^ 2) + sum(g_i ^ 2)

    where the sums run over the N mask pixels (i = 1 ... N), of the predicted binary segmentation
    pixel p_i ∈ P and the ground truth binary pixel g_i ∈ G.

    Args:
        input (Tensor): predicted binary mask, each pixel value should be in range [0, 1].
        target (Tensor): ground truth binary mask.

    Returns:
        Tensor: dice loss.
    """
    assert input.shape[-2:] == target.shape[-2:]
    input = input.view(input.size(0), -1).float()
    target = target.view(target.size(0), -1).float()

    d = (
        2 * torch.sum(input * target, dim=1)
    ) / (
        torch.sum(input * input, dim=1) + torch.sum(target * target, dim=1) + 1e-4
    )

    return 1 - d
```

#### cvpods/modeling/losses/circle_loss.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from typing import Tuple

import torch
from torch import nn
from torch.nn.functional import cross_entropy


class ClassificationCircleLoss(nn.Module):
    """Circle loss for class-level labels as described in the paper
    `"Circle Loss: A Unified Perspective of Pair Similarity Optimization" <#>`_

    Args:
        scale (float): the scale factor. Default: 256.0
        margin (float): the relax margin value. Default: 0.25
        circle_center (tuple[float]): the center of the circle (logit_ap, logit_an). Default: (1, 0)
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Default: ``'mean'``
    """

    def __init__(
        self,
        scale: float = 256.0,
        margin: float = 0.25,
        circle_center: Tuple[float, float] = (1, 0),
        reduction: str = "mean",
    ) -> None:
        super(ClassificationCircleLoss, self).__init__()
        self.scale = scale
        self.margin = margin
        self.circle_center = circle_center
        self.reduction = reduction

    def forward(self, logits: torch.Tensor, targets: torch.LongTensor) -> torch.Tensor:
        r"""

        Args:
            logits (torch.Tensor): The predicted logits before softmax,
                namely :math:`\cos \theta` in the above equation, with shape of :math:`(N, C)`
            targets (torch.LongTensor): The ground-truth label long vector,
                namely :math:`y` in the above equation, with shape of :math:`(N,)`

        Returns:
            torch.Tensor: loss
                the computed loss
        """

        mask = torch.zeros(logits.shape, dtype=torch.bool, device=logits.device).scatter_(
            dim=1, index=targets.unsqueeze(1), value=1
        )
        positive_weighting = torch.clamp(
            self.circle_center[0] + self.margin - logits.detach(), min=0)
        negative_weighting = torch.clamp(
            logits.detach() - self.circle_center[1] + self.margin, min=0)
        logits = torch.where(
            mask,
            self.scale * positive_weighting * (logits - (self.circle_center[0] - self.margin)),
            self.scale * negative_weighting * (logits - self.circle_center[1] - self.margin),
        )
        loss = cross_entropy(input=logits, target=targets, reduction=self.reduction)

        return loss
```

#### cvpods/modeling/losses/__init__.py

```python
from .dice_loss import dice_loss
from .focal_loss import (
    sigmoid_focal_loss,
    sigmoid_focal_loss_jit,
    sigmoid_focal_loss_star,
    sigmoid_focal_loss_star_jit
)
from .iou_loss import IOULoss, iou_loss
from .label_smooth_ce_loss import LabelSmoothCELoss, label_smooth_ce_loss
from .reg_l1_loss import reg_l1_loss
from .sigmoid_focal_loss import SigmoidFocalLoss, sigmoid_focal_loss_cuda
from .smooth_l1_loss import smooth_l1_loss
```

#### cvpods/modeling/losses/sigmoid_focal_loss.py

```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import torch
from torch import nn
from torch.autograd import Function
from torch.autograd.function import once_differentiable

from cvpods import _C


# TODO: Use JIT to replace CUDA implementation in the future.
class _SigmoidFocalLoss(Function):

    @staticmethod
    def forward(ctx, logits, targets, gamma, alpha):
        """
        Sigmoid Focal Loss forward func

        Args:
            ctx:
            logits (torch.Tensor): predicted logits
            targets (torch.Tensor): target logits
            gamma (float): focal loss gamma
            alpha (float): focal loss alpha
        """
        ctx.save_for_backward(logits, targets)
        num_classes = logits.shape[1]
        ctx.num_classes = num_classes
        ctx.gamma = gamma
        ctx.alpha = alpha

        losses = _C.sigmoid_focalloss_forward(
            logits, targets, num_classes, gamma, alpha
        )
        return losses

    @staticmethod
    @once_differentiable
    def backward(ctx, d_loss):
        logits, targets = ctx.saved_tensors
        num_classes = ctx.num_classes
        gamma = ctx.gamma
        alpha = ctx.alpha
        d_loss = d_loss.contiguous()
        d_logits = _C.sigmoid_focalloss_backward(
            logits, targets, d_loss, num_classes, gamma, alpha
        )
        return d_logits, None, None, None, None


sigmoid_focal_loss_cuda = _SigmoidFocalLoss.apply


def sigmoid_focal_loss_cpu(logits, targets, gamma, alpha):
    """
    Cpu version of Sigmoid Focal Loss, the same to :class:`_SigmoidFocalLoss`.

    """
    num_classes = logits.shape[1]
    gamma = gamma[0]
    alpha = alpha[0]
    dtype = targets.dtype
    device = targets.device
    class_range = torch.arange(1, num_classes + 1, dtype=dtype, device=device).unsqueeze(0)

    t = targets.unsqueeze(1)
    p = torch.sigmoid(logits)
    term1 = (1 - p) ** gamma * torch.log(p)
    term2 = p ** gamma * torch.log(1 - p)
    return -(t == class_range).float() * term1 * alpha - \
            ((t != class_range) * (t >= 0)).float() * term2 * (1 - alpha)


class SigmoidFocalLoss(nn.Module):

    def __init__(self, gamma, alpha):
        super(SigmoidFocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(self, logits, targets):
        # device = logits.device
        if logits.is_cuda:
            loss_func = sigmoid_focal_loss_cuda
        else:
            loss_func = sigmoid_focal_loss_cpu

        loss = loss_func(logits, targets, self.gamma, self.alpha)
        return loss.sum()

    def __repr__(self):
        tmpstr = self.__class__.__name__ + "("
        tmpstr += "gamma=" + str(self.gamma)
        tmpstr += ", alpha=" + str(self.alpha)
        tmpstr += ")"
        return tmpstr
```

#### cvpods/modeling/losses/focal_loss.py

```python
#!/usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.

import torch
from torch.nn import functional as F


def sigmoid_focal_loss(
    logits,
    targets,
    alpha: float = -1,
    gamma: float = 2,
    reduction: str = "none",
):
    """
    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.
    Args:
        logits: A float tensor of arbitrary shape.
                The predictions for each example.
        targets: A float tensor with the same shape as logits. Stores the binary
                 classification label for each element in logits
                (0 for the negative class and 1 for the positive class).
        alpha: (optional) Weighting factor in range (0,1) to balance
                positive vs negative examples. Default = -1 (no weighting).
        gamma: Exponent of the modulating factor (1 - p_t) to
               balance easy vs hard examples.
        reduction: 'none' | 'mean' | 'sum'
                 'none': No reduction will be applied to the output.
                 'mean': The output will be averaged.
                 'sum': The output will be summed.
    Returns:
        Loss tensor with the reduction option applied.
    """
    p = torch.sigmoid(logits)
    ce_loss = F.binary_cross_entropy_with_logits(
        logits, targets, reduction="none"
    )
    p_t = p * targets + (1 - p) * (1 - targets)
    loss = ce_loss * ((1 - p_t) ** gamma)

    if alpha >= 0:
        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)
        loss = alpha_t * loss

    if reduction == "mean":
        loss = loss.mean()
    elif reduction == "sum":
        loss = loss.sum()

    return loss


sigmoid_focal_loss_jit = torch.jit.script(
    sigmoid_focal_loss
)  # type: torch.jit.ScriptModule


def sigmoid_focal_loss_star(
    logits,
    targets,
    alpha: float = -1,
    gamma: float = 1,
    reduction: str = "none",
):
    """
    FL* described in RetinaNet paper Appendix: https://arxiv.org/abs/1708.02002.
    Args:
        logits: A float tensor of arbitrary shape.
                The predictions for each example.
        targets: A float tensor with the same shape as logits. Stores the binary
                 classification label for each element in logits
                (0 for the negative class and 1 for the positive class).
        alpha: (optional) Weighting factor in range (0,1) to balance
                positive vs negative examples. Default = -1 (no weighting).
        gamma: Gamma parameter described in FL*. Default = 1 (no weighting).
        reduction: 'none' | 'mean' | 'sum'
                 'none': No reduction will be applied to the output.
                 'mean': The output will be averaged.
                 'sum': The output will be summed.
    Returns:
        Loss tensor with the reduction option applied.
    """
    shifted_logits = gamma * (logits * (2 * targets - 1))
    loss = -F.logsigmoid(shifted_logits) / gamma

    if alpha >= 0:
        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)
        loss *= alpha_t

    if reduction == "mean":
        loss = loss.mean()  # pyre-ignore
    elif reduction == "sum":
        loss = loss.sum()  # pyre-ignore

    return loss


sigmoid_focal_loss_star_jit = torch.jit.script(
    sigmoid_focal_loss_star
)  # type: torch.jit.ScriptModule
```

#### cvpods/modeling/losses/reg_l1_loss.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import torch.nn as nn
import torch.nn.functional as F

from cvpods.modeling.nn_utils.feature_utils import gather_feature


class reg_l1_loss(nn.Module):

    def __init__(self):
        super(reg_l1_loss, self).__init__()

    def forward(self, output, mask, index, target):
        pred = gather_feature(output, index, use_transform=True)
        mask = mask.unsqueeze(dim=2).expand_as(pred).float()
        # loss = F.l1_loss(pred * mask, target * mask, reduction='elementwise_mean')
        loss = F.l1_loss(pred * mask, target * mask, reduction='sum')
        loss = loss / (mask.sum() + 1e-4)
        return loss
```

#### cvpods/modeling/losses/label_smooth_ce_loss.py

```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import torch
import torch.nn as nn


class LabelSmoothCELoss(nn.Module):
    """
    Cross-entrophy loss with label smooth.

    Args:
        epsilon: Smoothing level. Use one-hot label when set to 0, use uniform label when set to 1.
    """
    def __init__(self, epsilon):
        super(LabelSmoothCELoss, self).__init__()
        self.epsilon = epsilon
        self.logsoftmax = nn.LogSoftmax(dim=1)

    def forward(self, logits, targets):
        """
        Args:
            logits: A float tensor of shape: (minibatch, C).
            targets: A float tensor of shape: (minibatch,). Stores the class indices
                    in range `[0, C - 1]`.

        Returns:
            A scalar tensor.
        """
        log_probs = self.logsoftmax(logits)
        targets = torch.zeros_like(log_probs).scatter_(1, targets.unsqueeze(1), 1)
        targets = (1 - self.epsilon) * targets + self.epsilon / logits.shape[1]
        loss = (-targets * log_probs).mean(0).sum()
        return loss


def label_smooth_ce_loss(logits, targets, epsilon):
    """
    Cross-entrophy loss with label smooth.

    Args:
        logits: A float tensor of shape: (minibatch, C).
        targets: A float tensor of shape: (minibatch,). Stores the class indices
                 in range `[0, C - 1]`.
        epsilon: Smoothing level. Use one-hot label when set to 0, use uniform label when set to 1.

    Returns:
        A scalar tensor.
    """
    log_probs = nn.functional.log_softmax(logits, dim=1)
    targets = torch.zeros_like(log_probs).scatter_(1, targets.unsqueeze(1), 1)
    targets = (1 - epsilon) * targets + epsilon / logits.shape[1]
    loss = (-targets * log_probs).mean(0).sum()
    return loss
```

#### cvpods/modeling/losses/iou_loss.py

```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import torch
from torch import nn


class IOULoss(nn.Module):
    """
    Computing the IoU loss between a set of predicted bboxes and target bboxes.
    The loss is calculated as negative log of IoU.

    TODO: Add formulation. @zhubenjin
    """
    def __init__(self, loss_type="iou"):
        """
        Args:
            loss_type (str): candidates are [iou, giou]
        """
        super(IOULoss, self).__init__()
        self.loss_type = loss_type

    def forward(self, pred, target, weight=None):
        pred_left = pred[:, 0]
        pred_top = pred[:, 1]
        pred_right = pred[:, 2]
        pred_bottom = pred[:, 3]

        target_left = target[:, 0]
        target_top = target[:, 1]
        target_right = target[:, 2]
        target_bottom = target[:, 3]

        target_area = (target_left + target_right) * \
                      (target_top + target_bottom)
        pred_area = (pred_left + pred_right) * \
                    (pred_top + pred_bottom)

        w_intersect = torch.min(pred_left, target_left) + torch.min(pred_right, target_right)
        g_w_intersect = torch.max(pred_left, target_left) + torch.max(
            pred_right, target_right)
        h_intersect = torch.min(pred_bottom, target_bottom) + torch.min(pred_top, target_top)
        g_h_intersect = torch.max(pred_bottom, target_bottom) + torch.max(pred_top, target_top)
        ac_uion = g_w_intersect * g_h_intersect + 1e-7
        area_intersect = w_intersect * h_intersect
        area_union = target_area + pred_area - area_intersect
        ious = (area_intersect + 1.0) / (area_union + 1.0)
        gious = ious - (ac_uion - area_union) / ac_uion
        if self.loss_type == 'iou':
            losses = -torch.log(ious)
        elif self.loss_type == 'linear_iou':
            losses = 1 - ious
        elif self.loss_type == 'giou':
            losses = 1 - gious
        else:
            raise NotImplementedError

        if weight is not None and weight.sum() > 0:
            return (losses * weight).sum()
        else:
            assert losses.numel() != 0
            return losses.sum()


def iou_loss(
    inputs,
    targets,
    weight=None,
    box_mode="xyxy",
    loss_type="iou",
    smooth=False,
    reduction="none"
):
    """
    Compute iou loss of type ['iou', 'giou', 'linear_iou']

    Args:
        inputs (tensor): pred values
        targets (tensor): target values
        weight (tensor): loss weight
        box_mode (str): 'xyxy' or 'ltrb', 'ltrb' is currently supported.
        loss_type (str): 'giou' or 'iou' or 'linear_iou'
        reduction (str): reduction manner

    Returns:
        loss (tensor): computed iou loss.
    """
    if box_mode == "ltrb":
        inputs = torch.cat((-inputs[..., :2], inputs[..., 2:]), dim=-1)
        targets = torch.cat((-targets[..., :2], targets[..., 2:]), dim=-1)
    elif box_mode != "xyxy":
        raise NotImplementedError

    eps = torch.finfo(torch.float32).eps

    inputs_area = (inputs[..., 2] - inputs[..., 0]).clamp_(min=0) \
        * (inputs[..., 3] - inputs[..., 1]).clamp_(min=0)
    targets_area = (targets[..., 2] - targets[..., 0]).clamp_(min=0) \
        * (targets[..., 3] - targets[..., 1]).clamp_(min=0)

    w_intersect = (torch.min(inputs[..., 2], targets[..., 2])
                   - torch.max(inputs[..., 0], targets[..., 0])).clamp_(min=0)
    h_intersect = (torch.min(inputs[..., 3], targets[..., 3])
                   - torch.max(inputs[..., 1], targets[..., 1])).clamp_(min=0)

    area_intersect = w_intersect * h_intersect
    area_union = targets_area + inputs_area - area_intersect
    if smooth:
        ious = (area_intersect + 1) / (area_union + 1)
    else:
        ious = area_intersect / area_union.clamp(min=eps)

    if loss_type == "iou":
        loss = -ious.clamp(min=eps).log()
    elif loss_type == "linear_iou":
        loss = 1 - ious
    elif loss_type == "giou":
        g_w_intersect = torch.max(inputs[..., 2], targets[..., 2]) \
            - torch.min(inputs[..., 0], targets[..., 0])
        g_h_intersect = torch.max(inputs[..., 3], targets[..., 3]) \
            - torch.min(inputs[..., 1], targets[..., 1])
        ac_uion = g_w_intersect * g_h_intersect
        gious = ious - (ac_uion - area_union) / ac_uion.clamp(min=eps)
        loss = 1 - gious
    else:
        raise NotImplementedError
    if weight is not None:
        loss = loss * weight.view(loss.size())
        if reduction == "mean":
            loss = loss.sum() / max(weight.sum().item(), eps)
    else:
        if reduction == "mean":
            loss = loss.mean()
    if reduction == "sum":
        loss = loss.sum()

    return loss
```

#### cvpods/modeling/losses/smooth_l1_loss.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
import torch


def smooth_l1_loss(input,
                   target,
                   beta: float,
                   reduction: str = "none",
                   size_average=False):
    """
    Smooth L1 loss defined in the Fast R-CNN paper as:

                  | 0.5 * x ** 2 / beta   if abs(x) < beta
    smoothl1(x) = |
                  | abs(x) - 0.5 * beta   otherwise,

    where x = input - target.

    Smooth L1 loss is related to Huber loss, which is defined as:

                | 0.5 * x ** 2                  if abs(x) < beta
     huber(x) = |
                | beta * (abs(x) - 0.5 * beta)  otherwise

    Smooth L1 loss is equal to huber(x) / beta. This leads to the following
    differences:

     - As beta -> 0, Smooth L1 loss converges to L1 loss, while Huber loss
       converges to a constant 0 loss.
     - As beta -> +inf, Smooth L1 converges to a constant 0 loss, while Huber loss
       converges to L2 loss.
     - For Smooth L1 loss, as beta varies, the L1 segment of the loss has a constant
       slope of 1. For Huber loss, the slope of the L1 segment is beta.

    Smooth L1 loss can be seen as exactly L1 loss, but with the abs(x) < beta
    portion replaced with a quadratic function such that at abs(x) = beta, its
    slope is 1. The quadratic segment smooths the L1 loss near x = 0.

    Args:
        input (Tensor): input tensor of any shape
        target (Tensor): target value tensor with the same shape as input
        beta (float): L1 to L2 change point.
            For beta values < 1e-5, L1 loss is computed.
        reduction: 'none' | 'mean' | 'sum'
                 'none': No reduction will be applied to the output.
                 'mean': The output will be averaged.
                 'sum': The output will be summed.

    Returns:
        The loss with the reduction option applied.

    Note:
        PyTorch's builtin "Smooth L1 loss" implementation does not actually
        implement Smooth L1 loss, nor does it implement Huber loss. It implements
        the special case of both in which they are equal (beta=1).
        See: https://pytorch.org/docs/stable/nn.html#torch.nn.SmoothL1Loss.
     """
    if beta < 1e-5:
        # if beta == 0, then torch.where will result in nan gradients when
        # the chain rule is applied due to pytorch implementation details
        # (the False branch "0.5 * n ** 2 / 0" has an incoming gradient of
        # zeros, rather than "no gradient"). To avoid this issue, we define
        # small values of beta to be exactly l1 loss.
        loss = torch.abs(input - target)
    else:
        n = torch.abs(input - target)
        cond = n < beta
        loss = torch.where(cond, 0.5 * n**2 / beta, n - 0.5 * beta)

    if reduction == "mean" or size_average:
        loss = loss.mean()
    elif reduction == "sum":
        loss = loss.sum()

    return loss
```

#### cvpods/modeling/nn_utils/flop_count.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.

import logging
import typing
from collections import defaultdict

import torch.nn as nn

from .jit_handles import (
    addmm_flop_jit,
    conv_flop_jit,
    einsum_flop_jit,
    get_jit_model_analysis,
    matmul_flop_jit
)

# A dictionary that maps supported operations to their flop count jit handles.
_SUPPORTED_OPS: typing.Dict[str, typing.Callable] = {
    "aten::addmm": addmm_flop_jit,
    "aten::_convolution": conv_flop_jit,
    "aten::einsum": einsum_flop_jit,
    "aten::matmul": matmul_flop_jit,
}


def flop_count(
    model: nn.Module,
    inputs: typing.Tuple[object, ...],
    supported_ops: typing.Union[typing.Dict[str, typing.Callable], None] = None,
) -> typing.Tuple[typing.DefaultDict[str, float], typing.Counter[str]]:
    """
    Given a model and an input to the model, compute the Gflops of the given
    model. Note the input should have a batch size of 1.

    Args:
        model (nn.Module): The model to compute flop counts.
        inputs (tuple): Inputs that are passed to `model` to count flops.
            Inputs need to be in a tuple.
        supported_ops (dict(str,Callable) or None) : By default, we count flops
            for convolution layers, fully connected layers, torch.matmul and
            torch.einsum operations. We define a FLOP as a single atomic
            Multiply-Add. Users can provide customized supported_ops for
            counting flops if desired.

    Returns:
        tuple[defaultdict, Counter]: A dictionary that records the number of
            gflops for each operation and a Counter that records the number of
            skipped operations.
    """
    assert isinstance(inputs, tuple), "Inputs need to be in a tuple."
    if not supported_ops:
        supported_ops = _SUPPORTED_OPS.copy()

    # Run flop count.
    total_flop_counter, skipped_ops = get_jit_model_analysis(
        model, inputs, supported_ops
    )

    # Log for skipped operations.
    if len(skipped_ops) > 0:
        for op, freq in skipped_ops.items():
            logging.warning("Skipped operation {} {} time(s)".format(op, freq))

    # Convert flop count to gigaflops.
    final_count = defaultdict(float)
    for op in total_flop_counter:
        final_count[op] = total_flop_counter[op] / 1e9

    return final_count, skipped_ops
```

#### cvpods/modeling/nn_utils/__init__.py

```python
```

#### cvpods/modeling/nn_utils/activation_count.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.

import logging
import typing
from collections import defaultdict

import torch.nn as nn

from .jit_handles import generic_activation_jit, get_jit_model_analysis

# A dictionary that maps supported operations to their activation count handles.
_SUPPORTED_OPS: typing.Dict[str, typing.Callable] = {
    "aten::_convolution": generic_activation_jit("conv"),
    "aten::addmm": generic_activation_jit("addmm"),
}


def activation_count(
    model: nn.Module,
    inputs: typing.Tuple[object, ...],
    supported_ops: typing.Union[typing.Dict[str, typing.Callable], None] = None,
) -> typing.Tuple[typing.DefaultDict[str, float], typing.Counter[str]]:
    """
    Given a model and an input to the model, compute the total number of
    activations of the model. Note the input should have a batch size of 1.

    Args:
        model (nn.Module): The model to compute activation counts.
        inputs (tuple): Inputs that are passed to `model` to count activations.
            Inputs need to be in a tuple.
        supported_ops (dict(str,Callable) or None) : By default, we count
            activation for convolution and fully connected layers. Users can
            provide customized supported_ops if desired.

    Returns:
        tuple[defaultdict, Counter]: A dictionary that records the number of
            activation (mega) for each operation and a Counter that records the
            number of skipped operations.
    """
    assert isinstance(inputs, tuple), "Inputs need to be in a tuple."
    if not supported_ops:
        supported_ops = _SUPPORTED_OPS.copy()

    # Run activation count.
    total_activation_count, skipped_ops = get_jit_model_analysis(
        model, inputs, supported_ops
    )

    # Log for skipped operations.
    if len(skipped_ops) > 0:
        for op, freq in skipped_ops.items():
            logging.warning("Skipped operation {} {} time(s)".format(op, freq))

    # Convert activation count to mega count.
    final_count = defaultdict(float)
    for op in total_activation_count:
        final_count[op] = total_activation_count[op] / 1e6

    return final_count, skipped_ops
```

#### cvpods/modeling/nn_utils/module_converter.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import logging

from torch import nn

from cvpods.layers import batch_norm
from cvpods.utils.distributed import get_world_size

SYNC_BN_MODULE = (
    nn.SyncBatchNorm,
    batch_norm.NaiveSyncBatchNorm,
    batch_norm.NaiveSyncBatchNorm1d,
)


def maybe_convert_module(model):
    if get_world_size() == 1:
        logger = logging.getLogger(__name__)
        logger.warning("SyncBN used with 1GPU, auto convert to BatchNorm")
        model = convert_syncbn(model)

    return model


def convert_syncbn(module):
    model = module

    if isinstance(module, SYNC_BN_MODULE):
        if isinstance(module, batch_norm.NaiveSyncBatchNorm1d):
            model = nn.BatchNorm1d(module.num_features)
        else:
            model = nn.BatchNorm2d(module.num_features)

        if module.affine:
            model.weight.data = module.weight.data.clone().detach()
            model.bias.data = module.bias.data.clone().detach()
        model.running_mean.data = module.running_mean.data
        model.running_var.data = module.running_var.data
        model.eps = module.eps
    else:  # convert syncbn to bn recurrisvely
        for name, child in module.named_children():
            new_child = convert_syncbn(child)
            if new_child is not child:
                model.add_module(name, new_child)

    return model
```

#### cvpods/modeling/nn_utils/precise_bn.py

```python
#!/usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.

import itertools

import torch

BN_MODULE_TYPES = (
    torch.nn.BatchNorm1d,
    torch.nn.BatchNorm2d,
    torch.nn.BatchNorm3d,
    torch.nn.SyncBatchNorm,
)


@torch.no_grad()
def update_bn_stats(model, data_loader, num_iters: int = 200):
    """
    Recompute and update the batch norm stats to make them more precise. During
    training both BN stats and the weight are changing after every iteration, so
    the running average can not precisely reflect the actual stats of the
    current model.
    In this function, the BN stats are recomputed with fixed weights, to make
    the running average more precise. Specifically, it computes the true average
    of per-batch mean/variance instead of the running average.

    Args:
        model (nn.Module): the model whose bn stats will be recomputed.

            Note that:

            1. This function will not alter the training mode of the given model.
               Users are responsible for setting the layers that needs
               precise-BN to training mode, prior to calling this function.

            2. Be careful if your models contain other stateful layers in
               addition to BN, i.e. layers whose state can change in forward
               iterations.  This function will alter their state. If you wish
               them unchanged, you need to either pass in a submodule without
               those layers, or backup the states.
        data_loader (iterator): an iterator. Produce data as inputs to the model.
        num_iters (int): number of iterations to compute the stats.
    """
    bn_layers = get_bn_modules(model)

    if len(bn_layers) == 0:
        return

    # In order to make the running stats only reflect the current batch, the
    # momentum is disabled.
    # bn.running_mean = (1 - momentum) * bn.running_mean + momentum * batch_mean
    # Setting the momentum to 1.0 to compute the stats without momentum.
    momentum_actual = [bn.momentum for bn in bn_layers]
    for bn in bn_layers:
        bn.momentum = 1.0

    # Note that running_var actually means "running average of variance"
    running_mean = [torch.zeros_like(bn.running_mean) for bn in bn_layers]
    running_var = [torch.zeros_like(bn.running_var) for bn in bn_layers]

    for ind, inputs in enumerate(itertools.islice(data_loader, num_iters)):
        model(inputs)

        for i, bn in enumerate(bn_layers):
            # Accumulates the bn stats.
            running_mean[i] += (bn.running_mean - running_mean[i]) / (ind + 1)
            running_var[i] += (bn.running_var - running_var[i]) / (ind + 1)
            # We compute the "average of variance" across iterations.
    assert ind == num_iters - 1, (
        "update_bn_stats is meant to run for {} iterations, "
        "but the dataloader stops at {} iterations.".format(num_iters, ind)
    )

    for i, bn in enumerate(bn_layers):
        # Sets the precise bn stats.
        bn.running_mean = running_mean[i]
        bn.running_var = running_var[i]
        bn.momentum = momentum_actual[i]


def get_bn_modules(model):
    """
    Find all BatchNorm (BN) modules that are in training mode. See
    cvpods.modeling.nn_utils.precise_bn.BN_MODULE_TYPES for a list of all modules that are
    included in this search.

    Args:
        model (nn.Module): a model possibly containing BN modules.

    Returns:
        list[nn.Module]: all BN modules in the model.
    """
    # Finds all the bn layers.
    bn_layers = [
        m
        for m in model.modules()
        if m.training and isinstance(m, BN_MODULE_TYPES)
    ]
    return bn_layers
```

#### cvpods/modeling/nn_utils/feature_utils.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved


def gather_feature(fmap, index, mask=None, use_transform=False):
    """
    used for Centernet
    """
    if use_transform:
        # change a (N, C, H, W) tenor to (N, HxW, C) shape
        batch, channel = fmap.shape[:2]
        fmap = fmap.view(batch, channel, -1).permute((0, 2, 1)).contiguous()

    dim = fmap.size(-1)
    index  = index.unsqueeze(len(index.shape)).expand(*index.shape, dim)
    fmap = fmap.gather(dim=1, index=index)
    if mask is not None:
        # this part is not called in Res18 dcn COCO
        mask = mask.unsqueeze(2).expand_as(fmap)
        fmap = fmap[mask]
        fmap = fmap.reshape(-1, dim)
    return fmap
```

#### cvpods/modeling/nn_utils/jit_handles.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

import typing
from collections import Counter, OrderedDict

from numpy import prod

import torch
import torch.nn as nn

# A list that contains ignored operations.
_IGNORED_OPS: typing.List[str] = [
    "aten::Int",
    "aten::__and__",
    "aten::arange",
    "aten::cat",
    "aten::clamp",
    "aten::clamp_",
    "aten::contiguous",
    "aten::copy_",
    "aten::detach",
    "aten::empty",
    "aten::eq",
    "aten::expand",
    "aten::flatten",
    "aten::floor",
    "aten::full",
    "aten::gt",
    "aten::index",
    "aten::index_put_",
    "aten::max",
    "aten::nonzero",
    "aten::permute",
    "aten::remainder",
    "aten::reshape",
    "aten::select",
    "aten::size",
    "aten::slice",
    "aten::split_with_sizes",
    "aten::squeeze",
    "aten::t",
    "aten::to",
    "aten::transpose",
    "aten::unsqueeze",
    "aten::view",
    "aten::zeros",
    "aten::zeros_like",
    "prim::Constant",
    "prim::Int",
    "prim::ListConstruct",
    "prim::ListUnpack",
    "prim::NumToTensor",
    "prim::TupleConstruct",
]


def get_jit_model_analysis(
    model: nn.Module,
    inputs: typing.Tuple[object, ...],
    ops_handles: typing.Dict[str, typing.Callable],
) -> typing.Tuple[typing.Counter[str], typing.Counter[str]]:
    """
    Given a model, the inputs and the handles for each operation, return the
    results for the model analysis.

    Args:
        model (nn.Module): The model for torch script to trace.
        inputs (tuple): Inputs that are passed to `model` to trace. Inputs need
            to be in a tuple.
        ops_handles (typing.Dict[str, typing.Callable]): A dictionary of handles
            for model analysis.

    Returns:
        typing.Tuple[typing.Counter[str], typing.Counter[str]]: A counter that
            contains the results of per operation analysis of the model and a
            Counter of ignored operations.
    """
    # Torch script does not support parallel torch models.
    if isinstance(
        model, (nn.parallel.distributed.DistributedDataParallel, nn.DataParallel)
    ):
        model = model.module  # pyre-ignore

    # Compatibility with torch.jit.
    if hasattr(torch.jit, "get_trace_graph"):
        trace, _ = torch.jit.get_trace_graph(model, inputs)
        trace_nodes = trace.graph().nodes()
    else:
        trace, _ = torch.jit._get_trace_graph(model, inputs)
        trace_nodes = trace.nodes()

    skipped_ops = Counter()
    total_count = Counter()

    for node in trace_nodes:
        kind = node.kind()
        if kind not in ops_handles.keys():
            # If the operation is not in _IGNORED_OPS, count skipped operations.
            if kind not in _IGNORED_OPS:
                skipped_ops[kind] += 1
            continue

        handle_count = ops_handles.get(kind, None)
        if handle_count is None:
            continue
        # pyre-ignore
        inputs, outputs = list(node.inputs()), list(node.outputs())
        op_count = handle_count(inputs, outputs)
        total_count += op_count
    return total_count, skipped_ops


def generic_activation_jit(
    op_name: str
) -> typing.Callable[[typing.List[object], typing.List[object]], typing.Counter[str]]:
    """
    This method return a handle that counts the number of activation from the
    output shape for the specified operation.

    Args:
        op_name (str): The name of the operation.

    Returns:
        typing.Callable: An activation handle for the given operation.
    """

    def _generic_activation_jit(outputs: typing.List[object]) -> int:
        """
        This is a generic jit handle that counts the number of activations for any
        operation given the output shape.

        Args:
            outputs (list(torch._C.Value)): The output shape in the form of a list
                of jit object.

        Returns:
            int: Total number of activations for each operation.
        """
        out_shape = get_shape(outputs[0])
        ac_count = prod(out_shape)
        return ac_count

    return lambda inputs, outputs: Counter({op_name: _generic_activation_jit(outputs)})


def get_shape(val: object) -> typing.List[int]:
    """
    Get the shapes from a jit value object.

    Args:
        val (torch._C.Value): jit value object.

    Returns:
        list(int): return a list of ints.
    """
    if val.isCompleteTensor():  # pyre-ignore
        return val.type().sizes()  # pyre-ignore
    else:
        raise ValueError()


def addmm_flop_jit(
    inputs: typing.List[object], outputs: typing.List[object]
) -> typing.Counter[str]:
    """
    This method counts the flops for fully connected layers with torch script.

    Args:
        inputs (list(torch._C.Value)): The input shape in the form of a list of
            jit object.
        outputs (list(torch._C.Value)): The output shape in the form of a list
            of jit object.

    Returns:
        Counter: A Counter dictionary that records the number of flops for each
            operation.
    """
    # Count flop for nn.Linear
    # inputs is a list of length 3.
    input_shapes = [get_shape(v) for v in inputs[1:3]]
    # input_shapes[0]: [batch size, input feature dimension]
    # input_shapes[1]: [batch size, output feature dimension]
    assert len(input_shapes[0]) == 2
    assert len(input_shapes[1]) == 2
    batch_size, input_dim = input_shapes[0]
    output_dim = input_shapes[1][1]
    flop = batch_size * input_dim * output_dim
    flop_counter = Counter({"addmm": flop})
    return flop_counter


def conv_flop_count(
    x_shape: typing.List[int], w_shape: typing.List[int], out_shape: typing.List[int]
) -> typing.Counter[str]:
    """
    This method counts the flops for convolution. Note only multiplication is
    counted. Computation for addition and bias is ignored.

    Args:
        x_shape (list(int)): The input shape before convolution.
        w_shape (list(int)): The filter shape.
        out_shape (list(int)): The output shape after convolution.

    Returns:
        Counter: A Counter dictionary that records the number of flops for each
            operation.
    """
    batch_size, Cin_dim, Cout_dim = x_shape[0], w_shape[1], out_shape[1]
    out_size = prod(out_shape[2:])
    kernel_size = prod(w_shape[2:])
    flop = batch_size * out_size * Cout_dim * Cin_dim * kernel_size
    flop_counter = Counter({"conv": flop})
    return flop_counter


def conv_flop_jit(
    inputs: typing.List[object], outputs: typing.List[object]
) -> typing.Counter[str]:
    """
    This method counts the flops for convolution using torch script.

    Args:
        inputs (list(torch._C.Value)): The input shape in the form of a list of
            jit object before convolution.
        outputs (list(torch._C.Value)): The output shape in the form of a list
            of jit object after convolution.

    Returns:
        Counter: A Counter dictionary that records the number of flops for each
            operation.
    """
    # Inputs of Convolution should be a list of length 12. They represent:
    # 0) input tensor, 1) convolution filter, 2) bias, 3) stride, 4) padding,
    # 5) dilation, 6) transposed, 7) out_pad, 8) groups, 9) benchmark_cudnn,
    # 10) deterministic_cudnn and 11) user_enabled_cudnn.
    assert len(inputs) == 12
    x, w = inputs[:2]
    x_shape, w_shape, out_shape = (get_shape(x), get_shape(w), get_shape(outputs[0]))
    return conv_flop_count(x_shape, w_shape, out_shape)


def einsum_flop_jit(
    inputs: typing.List[object], outputs: typing.List[object]
) -> typing.Counter[str]:
    """
    This method counts the flops for the einsum operation. We currently support
    two einsum operations: "nct,ncp->ntp" and "ntg,ncg->nct".

    Args:
        inputs (list(torch._C.Value)): The input shape in the form of a list of
            jit object before einsum.
        outputs (list(torch._C.Value)): The output shape in the form of a list
            of jit object after einsum.

    Returns:
        Counter: A Counter dictionary that records the number of flops for each
            operation.
    """
    # Inputs of einsum should be a list of length 2.
    # Inputs[0] stores the equation used for einsum.
    # Inputs[1] stores the list of input shapes.
    assert len(inputs) == 2
    equation = inputs[0].toIValue()  # pyre-ignore
    # Get rid of white space in the equation string.
    equation = equation.replace(" ", "")
    # Re-map equation so that same equation with different alphabet
    # representations will look the same.
    letter_order = OrderedDict((k, 0) for k in equation if k.isalpha()).keys()
    mapping = {ord(x): 97 + i for i, x in enumerate(letter_order)}
    equation = equation.translate(mapping)
    input_shapes_jit = inputs[1].node().inputs()  # pyre-ignore
    input_shapes = [get_shape(v) for v in input_shapes_jit]

    if equation == "abc,abd->acd":
        n, c, t = input_shapes[0]
        p = input_shapes[-1][-1]
        flop = n * c * t * p
        flop_counter = Counter({"einsum": flop})
        return flop_counter

    elif equation == "abc,adc->adb":
        n, t, g = input_shapes[0]
        c = input_shapes[-1][1]
        flop = n * t * g * c
        flop_counter = Counter({"einsum": flop})
        return flop_counter

    else:
        raise NotImplementedError("Unsupported einsum operation.")


def matmul_flop_jit(
    inputs: typing.List[object], outputs: typing.List[object]
) -> typing.Counter[str]:
    """
    This method counts the flops for matmul.

    Args:
        inputs (list(torch._C.Value)): The input shape in the form of a list of
            jit object before matmul.
        outputs (list(torch._C.Value)): The output shape in the form of a list
            of jit object after matmul.

    Returns:
        Counter: A Counter dictionary that records the number of flops for each
            operation.
    """
    # Inputs should be a list of length 2.
    # Inputs contains the shapes of two matrices.
    input_shapes = [get_shape(v) for v in inputs]
    assert len(input_shapes) == 2
    assert len(input_shapes[1]) == 2
    assert input_shapes[0][-1] == input_shapes[1][0]
    batch_dim = input_shapes[0][0]
    m1_dim, m2_dim = input_shapes[1]
    flop = m1_dim * m2_dim * batch_dim
    flop_counter = Counter({"matmul": flop})
    return flop_counter


def batchnorm_flop_jit(
    inputs: typing.List[object], outputs: typing.List[object]
) -> typing.Counter[str]:
    """
    This method counts the flops for batch norm.

    Args:
        inputs (list(torch._C.Value)): The input shape in the form of a list of
            jit object before batch norm.
        outputs (list(torch._C.Value)): The output shape in the form of a list
            of jit object after batch norm.

    Returns:
        Counter: A Counter dictionary that records the number of flops for each
            operation.
    """
    # Inputs[0] contains the shape of the input.
    input_shape = get_shape(inputs[0])
    assert 2 <= len(input_shape) <= 5
    flop = prod(input_shape) * 4
    flop_counter = Counter({"batchnorm": flop})
    return flop_counter
```

#### cvpods/modeling/nn_utils/scale_grad.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved
from torch.autograd.function import Function


class _ScaleGradient(Function):

    @staticmethod
    def forward(ctx, input, scale):
        ctx.scale = scale
        return input

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output * ctx.scale, None
```

#### cvpods/modeling/nn_utils/weight_init.py

```python
#!/usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
import math

import torch.nn as nn


def constant_init(module, val, bias=0):
    nn.init.constant_(module.weight, val)
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)


def xavier_init(module, gain=1, bias=0, distribution='normal'):
    assert distribution in ['uniform', 'normal']
    if distribution == 'uniform':
        nn.init.xavier_uniform_(module.weight, gain=gain)
    else:
        nn.init.xavier_normal_(module.weight, gain=gain)
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)


def normal_init(module, mean=0, std=1, bias=0):
    nn.init.normal_(module.weight, mean, std)
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)


def uniform_init(module, a=0, b=1, bias=0):
    nn.init.uniform_(module.weight, a, b)
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)


def kaiming_init(module,
                 a=0,
                 mode='fan_out',
                 nonlinearity='relu',
                 bias=0,
                 distribution='normal'):
    assert distribution in ['uniform', 'normal']
    if distribution == 'uniform':
        nn.init.kaiming_uniform_(module.weight,
                                 a=a,
                                 mode=mode,
                                 nonlinearity=nonlinearity)
    else:
        nn.init.kaiming_normal_(module.weight,
                                a=a,
                                mode=mode,
                                nonlinearity=nonlinearity)
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)


def caffe2_xavier_init(module, bias=0):
    # `XavierFill` in Caffe2 corresponds to `kaiming_uniform_` in PyTorch
    # Acknowledgment to FAIR's internal code
    kaiming_init(module,
                 a=1,
                 mode='fan_in',
                 nonlinearity='leaky_relu',
                 bias=bias,
                 distribution='uniform')


def c2_xavier_fill(module: nn.Module):
    """
    Initialize `module.weight` using the "XavierFill" implemented in Caffe2.
    Also initializes `module.bias` to 0.

    Args:
        module (torch.nn.Module): module to initialize.
    """
    # Caffe2 implementation of XavierFill in fact
    # corresponds to kaiming_uniform_ in PyTorch
    nn.init.kaiming_uniform_(module.weight, a=1)
    if module.bias is not None:
        nn.init.constant_(module.bias, 0)


def c2_msra_fill(module: nn.Module):
    """
    Initialize `module.weight` using the "MSRAFill" implemented in Caffe2.
    Also initializes `module.bias` to 0.

    Args:
        module (torch.nn.Module): module to initialize.
    """
    nn.init.kaiming_normal_(module.weight, mode="fan_out", nonlinearity="relu")
    if module.bias is not None:
        nn.init.constant_(module.bias, 0)


def init_weights(m: nn.Module, zero_init_final_gamma=False):
    """Performs ResNet-style weight initialization."""
    if isinstance(m, nn.Conv2d):
        # Note that there is no bias due to BN
        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
        m.weight.data.normal_(mean=0.0, std=math.sqrt(2.0 / fan_out))
    elif isinstance(m, nn.BatchNorm2d):
        zero_init_gamma = (
            hasattr(m, "final_bn") and m.final_bn and zero_init_final_gamma
        )
        m.weight.data.fill_(0.0 if zero_init_gamma else 1.0)
        m.bias.data.zero_()
    elif isinstance(m, nn.Linear):
        m.weight.data.normal_(mean=0.0, std=0.01)
        m.bias.data.zero_()
```

#### cvpods/modeling/nn_utils/parameter_count.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

import typing
from collections import defaultdict
import tabulate

from torch import nn


def parameter_count(model: nn.Module) -> typing.DefaultDict[str, int]:
    """
    Count parameters of a model and its submodules.

    Args:
        model: a torch module

    Returns:
        dict (str-> int): the key is either a parameter name or a module name.
            The value is the number of elements in the parameter, or in all
            parameters of the module. The key "" corresponds to the total
            number of parameters of the model.
    """
    r = defaultdict(int)
    for name, prm in model.named_parameters():
        size = prm.numel()
        name = name.split(".")
        for k in range(0, len(name) + 1):
            prefix = ".".join(name[:k])
            r[prefix] += size
    return r


def parameter_count_table(model: nn.Module, max_depth: int = 3) -> str:
    """
    Format the parameter count of the model (and its submodules or parameters)
    in a nice table.

    Args:
        model: a torch module
        max_depth (int): maximum depth to recursively print submodules or
            parameters

    Returns:
        str: the table to be printed
    """
    count: typing.DefaultDict[str, int] = parameter_count(model)
    param_shape: typing.Dict[str, typing.Tuple] = {
        k: tuple(v.shape) for k, v in model.named_parameters()
    }

    table: typing.List[typing.Tuple] = []

    def format_size(x: int) -> str:
        if x > 1e5:
            return "{:.1f}M".format(x / 1e6)
        if x > 1e2:
            return "{:.1f}K".format(x / 1e3)
        return str(x)

    def fill(lvl: int, prefix: str) -> None:
        if lvl >= max_depth:
            return
        for name, v in count.items():
            if name.count(".") == lvl and name.startswith(prefix):
                indent = " " * (lvl + 1)
                if name in param_shape:
                    table.append((indent + name, indent + str(param_shape[name])))
                else:
                    table.append((indent + name, indent + format_size(v)))
                    fill(lvl + 1, name + ".")

    table.append(("model", format_size(count.pop(""))))
    fill(0, "")

    old_ws = tabulate.PRESERVE_WHITESPACE
    tabulate.PRESERVE_WHITESPACE = True
    tab = tabulate.tabulate(
        table, headers=["name", "#elements or shape"], tablefmt="pipe"
    )
    tabulate.PRESERVE_WHITESPACE = old_ws
    return tab
```

#### cvpods/modeling/basenet/__init__.py

```python
from .basenet import basenet
```

#### cvpods/modeling/basenet/basenet.py

```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import os
from collections import defaultdict

import cv2
import numpy as np
from PIL import Image

import torch

from cvpods.configs.base_config import config as cfg
from cvpods.utils import Visualizer
from cvpods.utils.visualizer.show import visualize_feature_maps


def basenet(cls):

    def data_analyze_on(self):
        if not hasattr(cls, 'analyze_buffer'):
            cls.analyze_buffer = defaultdict(list)

    cls.data_analyze_on = data_analyze_on

    def visualize_data(self, per_image, save_to_file=False):
        """
        Visualize data from batch_inputs of dataloader.

        Args:
            per_image (dict): a dict that contains:
                * image: Tensor, image in (C, H, W) format.
                * instances: Instances
                Other information that's included in the original dicts, such as:
                * "height", "width" (int): the output resolution of the model, used in inference.
                    See :meth:`postprocess` for details.
            save_to_file: whether save img to disk.

        Example:
            >>> self.visualize_data(batch_inputs[0])
        """
        metadata = self.data_meta

        def output(vis, fname):
            if not save_to_file:
                print(fname)
                cv2.imshow("window", vis.get_image()[:, :, ::-1])
                cv2.waitKey()
            else:
                filepath = os.path.join("./", fname)
                print("Saving to {} ...".format(filepath))
                vis.save(filepath)

        scale = 1.0
        # Pytorch tensor is in (C, H, W) format
        img = per_image["image"].permute(1, 2, 0)
        if cfg.INPUT.FORMAT == "BGR":
            img = img[:, :, [2, 1, 0]]
        else:
            img = np.asarray(Image.fromarray(img, mode=cfg.INPUT.FORMAT).convert("RGB"))

        visualizer = Visualizer(img, metadata=metadata, scale=scale)
        target_fields = per_image["instances"].get_fields()
        labels = [metadata.thing_classes[i] for i in target_fields["gt_classes"]]
        vis = visualizer.overlay_instances(
            labels=labels,
            boxes=target_fields.get("gt_boxes", None),
            masks=target_fields.get("gt_masks", None),
            keypoints=target_fields.get("gt_keypoints", None),
        )
        output(vis, str(per_image["image_id"]) + ".jpg")

    cls.visualize_data = visualize_data

    def visualize_predict_data(self, per_image, per_instalce, save_to_file=False):
        metadata = self.data_meta

        def output(vis, fname):
            if not save_to_file:
                print(fname)
                cv2.imshow("window", vis.get_image()[:, :, ::-1])
                cv2.waitKey()
            else:
                filepath = os.path.join("./", fname)
                print("Saving to {} ...".format(filepath))
                vis.save(filepath)

        scale = 1.0
        # Pytorch tensor is in (C, H, W) format
        img = per_image["image"].permute(1, 2, 0)
        if cfg.INPUT.FORMAT == "BGR":
            img = img[:, :, [2, 1, 0]]
        else:
            img = np.asarray(Image.fromarray(img, mode=cfg.INPUT.FORMAT).convert("RGB"))

        visualizer = Visualizer(img, metadata=metadata, scale=scale)
        vis = visualizer.draw_instance_predictions(per_instalce)
        output(vis, str(per_image["image_id"]) + ".jpg")

    cls.visualize_predict_data = visualize_predict_data

    def visualize_feature_map(self, feature_map, per_image=None, stride=8,
                              save_name=0, with_img=True, channelwise=False):
        """
        Visualize feature map with (optional) gt boxes

        Args:
            feature_map (torch.Tensor): C x H x W
            per_image (dict): batch_inputs[i]
            stride (int): down sample ratio of current feature_map
            save_name (int or str): feature map figure name
            with_img (bool): weather visualize corresponding image data
            channelwise (bool): visualize feature map mean or all channels

        Examples::
            >>> level = 1
            >>> self.visualize_feature_map(features[level][0],
            >>>                        per_image=batched_inputs[level],
            >>>                        stride=self.fpn_strides[level],
            >>>                        save_name=1,
            >>>                        with_img=False,
            >>>                        channelwise=False)
        """
        if with_img and save_name == 0:
            self.visualize_data(per_image)

        with torch.no_grad():
            if "instances" in per_image:
                instance = per_image["instances"]
                gts = instance.gt_boxes.tensor.cpu().numpy()
                l = gts[:, 0:1]  # noqa:E741
                t = gts[:, 1:2]
                r = gts[:, 2:3]
                b = gts[:, 3:4]
                boxes = (
                    np.concatenate([l, t, l, b, r, b, r, t], axis=1)
                    .reshape(-1, 4, 2)
                    .transpose(0, 2, 1)
                )
            else:
                boxes = []
            if not channelwise:
                fm = feature_map.permute(1, 2, 0).mean(dim=-1, keepdim=True)
            else:
                fm = feature_map.permute(1, 2, 0)
            # visualize_feature_maps(fm.sigmoid().cpu().numpy(),
            visualize_feature_maps(
                fm.cpu().numpy(),
                boxes=boxes,
                stride=stride,
                save_filename=f"feature_map_{save_name}.png",
            )

    cls.visualize_feature_map = visualize_feature_map

    return cls
```

#### cvpods/modeling/meta_arch/rcnn.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
# Modified by BaseDetection, Inc. and its affiliates. All Rights Reserved
import logging

import numpy as np

import torch
from torch import nn

from cvpods.layers import ShapeSpec
from cvpods.structures import ImageList
from cvpods.utils import get_event_storage, log_first_n

from ..postprocessing import detector_postprocess

__all__ = ["GeneralizedRCNN", "ProposalNetwork"]


class GeneralizedRCNN(nn.Module):
    """
    Generalized R-CNN. Any models that contains the following three components:
    1. Per-image feature extraction (aka backbone)
    2. Region proposal generation
    3. Per-region feature extraction and prediction
    """
    def __init__(self, cfg):
        super().__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)
        self.backbone = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))
        self.proposal_generator = cfg.build_proposal_generator(cfg, self.backbone.output_shape())
        self.roi_heads = cfg.build_roi_heads(cfg, self.backbone.output_shape())
        self.vis_period = cfg.VIS_PERIOD
        self.input_format = cfg.INPUT.FORMAT

        assert len(cfg.MODEL.PIXEL_MEAN) == len(cfg.MODEL.PIXEL_STD)
        num_channels = len(cfg.MODEL.PIXEL_MEAN)
        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(
            num_channels, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(
            num_channels, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std
        self.to(self.device)

    def visualize_training(self, batched_inputs, proposals):
        """
        A function used to visualize images and proposals. It shows ground truth
        bounding boxes on the original image and up to 20 predicted object
        proposals on the original image. Users can implement different
        visualization functions for different models.
        Args:
            batched_inputs (list): a list that contains input to the model.
            proposals (list): a list that contains predicted proposals. Both
            batched_inputs and proposals should have the same length.
        """
        from cvpods.utils import Visualizer
        storage = get_event_storage()
        max_vis_prop = 20

        for input, prop in zip(batched_inputs, proposals):
            img = input["image"].cpu().numpy()
            assert img.shape[0] == 3, "Images should have 3 channels."
            if self.input_format == "BGR":
                img = img[::-1, :, :]
            img = img.transpose(1, 2, 0)
            v_gt = Visualizer(img, None)
            v_gt = v_gt.overlay_instances(boxes=input["instances"].gt_boxes)
            anno_img = v_gt.get_image()
            box_size = min(len(prop.proposal_boxes), max_vis_prop)
            v_pred = Visualizer(img, None)
            v_pred = v_pred.overlay_instances(
                boxes=prop.proposal_boxes[0:box_size].tensor.cpu().numpy()
            )
            prop_img = v_pred.get_image()
            vis_img = np.concatenate((anno_img, prop_img), axis=1)
            vis_img = vis_img.transpose(2, 0, 1)
            vis_name = " 1. GT bounding boxes  2. Predicted proposals"
            storage.put_image(vis_name, vis_img)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
                Each item in the list contains the inputs for one image.
                For now, each item in the list is a dict that contains:

                * image: Tensor, image in (C, H, W) format.
                * instances (optional): groundtruth :class:`Instances`
                * proposals (optional): :class:`Instances`, precomputed proposals.

                Other information that's included in the original dicts, such as:

                * "height", "width" (int): the output resolution of the model, used in inference.
                  See :meth:`postprocess` for details.

        Returns:
            list[dict]:
                Each dict is the output for one input image.
                The dict contains one key "instances" whose value is a :class:`Instances`.
                The :class:`Instances` object has the following keys:
                "pred_boxes", "pred_classes", "scores", "pred_masks", "pred_keypoints"
        """
        if not self.training:
            return self.inference(batched_inputs)

        images = self.preprocess_image(batched_inputs)
        if "instances" in batched_inputs[0]:
            gt_instances = [
                x["instances"].to(self.device) for x in batched_inputs
            ]
        elif "targets" in batched_inputs[0]:
            log_first_n(
                logging.WARN,
                "'targets' in the model inputs is now renamed to 'instances'!",
                n=10)
            gt_instances = [
                x["targets"].to(self.device) for x in batched_inputs
            ]
        else:
            gt_instances = None

        features = self.backbone(images.tensor)

        if self.proposal_generator:
            proposals, proposal_losses = self.proposal_generator(
                images, features, gt_instances)
        else:
            assert "proposals" in batched_inputs[0]
            proposals = [
                x["proposals"].to(self.device) for x in batched_inputs
            ]
            proposal_losses = {}

        _, detector_losses = self.roi_heads(images, features, proposals,
                                            gt_instances)
        if self.vis_period > 0:
            storage = get_event_storage()
            if storage.iter % self.vis_period == 0:
                self.visualize_training(batched_inputs, proposals)

        losses = {}
        losses.update(detector_losses)
        losses.update(proposal_losses)
        return losses

    def inference(self,
                  batched_inputs,
                  detected_instances=None,
                  do_postprocess=True):
        """
        Run inference on the given inputs.

        Args:
            batched_inputs (list[dict]): same as in :meth:`forward`
            detected_instances (None or list[Instances]): if not None, it
                contains an `Instances` object per image. The `Instances`
                object contains "pred_boxes" and "pred_classes" which are
                known boxes in the image.
                The inference will then skip the detection of bounding boxes,
                and only predict other per-ROI outputs.
            do_postprocess (bool): whether to apply post-processing on the outputs.

        Returns:
            same as in :meth:`forward`.
        """
        assert not self.training

        images = self.preprocess_image(batched_inputs)
        features = self.backbone(images.tensor)

        if detected_instances is None:
            if self.proposal_generator:
                proposals, _ = self.proposal_generator(images, features, None)
            else:
                assert "proposals" in batched_inputs[0]
                proposals = [
                    x["proposals"].to(self.device) for x in batched_inputs
                ]

            results, _ = self.roi_heads(images, features, proposals, None)
        else:
            detected_instances = [
                x.to(self.device) for x in detected_instances
            ]
            results = self.roi_heads.forward_with_given_boxes(
                features, detected_instances)

        if do_postprocess:
            return GeneralizedRCNN._postprocess(results, batched_inputs, images.image_sizes)
        else:
            return results

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images,
                                        self.backbone.size_divisibility)
        return images

    @staticmethod
    def _postprocess(instances, batched_inputs, image_sizes):
        """
        Rescale the output instances to the target size.
        """
        # note: private function; subject to changes
        processed_results = []
        for results_per_image, input_per_image, image_size in zip(
            instances, batched_inputs, image_sizes
        ):
            height = input_per_image.get("height", image_size[0])
            width = input_per_image.get("width", image_size[1])
            r = detector_postprocess(results_per_image, height, width)
            processed_results.append({"instances": r})
        return processed_results


class ProposalNetwork(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.device = torch.device(cfg.MODEL.DEVICE)

        self.backbone = cfg.build_backbone(cfg)
        self.proposal_generator = cfg.build_proposal_generator(cfg, self.backbone.output_shape())

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(
            -1, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(
            -1, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std
        self.to(self.device)

    def forward(self, batched_inputs):
        """
        Args:
            Same as in :class:`GeneralizedRCNN.forward`

        Returns:
            list[dict]:
                Each dict is the output for one input image.
                The dict contains one key "proposals" whose value is a
                :class:`Instances` with keys "proposal_boxes" and "objectness_logits".
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images,
                                        self.backbone.size_divisibility)
        features = self.backbone(images.tensor)

        if "instances" in batched_inputs[0]:
            gt_instances = [
                x["instances"].to(self.device) for x in batched_inputs
            ]
        elif "targets" in batched_inputs[0]:
            log_first_n(
                logging.WARN,
                "'targets' in the model inputs is now renamed to 'instances'!",
                n=10)
            gt_instances = [
                x["targets"].to(self.device) for x in batched_inputs
            ]
        else:
            gt_instances = None
        proposals, proposal_losses = self.proposal_generator(
            images, features, gt_instances)
        # In training, the proposals are not useful at all but we generate them anyway.
        # This makes RPN-only models about 5% slower.
        if self.training:
            return proposal_losses

        processed_results = []
        for results_per_image, input_per_image, image_size in zip(
                proposals, batched_inputs, images.image_sizes):
            height = input_per_image.get("height", image_size[0])
            width = input_per_image.get("width", image_size[1])
            r = detector_postprocess(results_per_image, height, width)
            processed_results.append({"proposals": r})
        return processed_results
```

#### cvpods/modeling/meta_arch/panoptic_fpn.py

```python
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

import torch
from torch import nn

from cvpods.layers import ShapeSpec
from cvpods.structures import ImageList

from ..postprocessing import detector_postprocess, sem_seg_postprocess

__all__ = ["PanopticFPN"]


class PanopticFPN(nn.Module):
    """
    Main class for Panoptic FPN architectures (see https://arxiv.org/abd/1901.02446).
    """

    def __init__(self, cfg):
        super().__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)

        self.instance_loss_weight = cfg.MODEL.PANOPTIC_FPN.INSTANCE_LOSS_WEIGHT

        # options when combining instance & semantic outputs
        self.combine_on = cfg.MODEL.PANOPTIC_FPN.COMBINE.ENABLED
        self.combine_overlap_threshold = cfg.MODEL.PANOPTIC_FPN.COMBINE.OVERLAP_THRESH
        self.combine_stuff_area_limit = cfg.MODEL.PANOPTIC_FPN.COMBINE.STUFF_AREA_LIMIT
        self.combine_instances_confidence_threshold = (
            cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH
        )

        self.backbone = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))
        self.proposal_generator = cfg.build_proposal_generator(cfg, self.backbone.output_shape())
        self.roi_heads = cfg.build_roi_heads(cfg, self.backbone.output_shape())
        self.sem_seg_head = cfg.build_sem_seg_head(cfg, self.backbone.output_shape())

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(3, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(3, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std
        self.to(self.device)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper`.
                Each item in the list contains the inputs for one image.

                For now, each item in the list is a dict that contains:

                * "image": Tensor, image in (C, H, W) format.
                * "instances": Instances
                * "sem_seg": semantic segmentation ground truth.
                * Other information that's included in the original dicts, such as:
                  "height", "width" (int): the output resolution of the model, used in inference.
                  See :meth:`postprocess` for details.

        Returns:
            list[dict]:
                each dict is the results for one image. The dict contains the following keys:

                * "instances": see :meth:`GeneralizedRCNN.forward` for its format.
                * "sem_seg": see :meth:`SemanticSegmentor.forward` for its format.
                * "panoptic_seg": available when `PANOPTIC_FPN.COMBINE.ENABLED`.
                  See the return value of
                  :func:`combine_semantic_and_instance_outputs` for its format.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images, self.backbone.size_divisibility)
        features = self.backbone(images.tensor)

        if "proposals" in batched_inputs[0]:
            proposals = [x["proposals"].to(self.device) for x in batched_inputs]
            proposal_losses = {}

        if "sem_seg" in batched_inputs[0]:
            gt_sem_seg = [x["sem_seg"].to(self.device) for x in batched_inputs]
            gt_sem_seg = ImageList.from_tensors(
                gt_sem_seg, self.backbone.size_divisibility,
                pad_value=self.sem_seg_head.ignore_value
            ).tensor
        else:
            gt_sem_seg = None
        sem_seg_results, sem_seg_losses = self.sem_seg_head(features, gt_sem_seg)

        if "instances" in batched_inputs[0]:
            gt_instances = [x["instances"].to(self.device) for x in batched_inputs]
        else:
            gt_instances = None
        if self.proposal_generator:
            proposals, proposal_losses = self.proposal_generator(images, features, gt_instances)
        detector_results, detector_losses = self.roi_heads(
            images, features, proposals, gt_instances
        )

        if self.training:
            losses = {}
            losses.update(sem_seg_losses)
            losses.update({k: v * self.instance_loss_weight for k, v in detector_losses.items()})
            losses.update(proposal_losses)
            return losses

        processed_results = []
        for sem_seg_result, detector_result, input_per_image, image_size in zip(
            sem_seg_results, detector_results, batched_inputs, images.image_sizes
        ):
            height = input_per_image.get("height", image_size[0])
            width = input_per_image.get("width", image_size[1])
            sem_seg_r = sem_seg_postprocess(sem_seg_result, image_size, height, width)
            detector_r = detector_postprocess(detector_result, height, width)

            processed_results.append({"sem_seg": sem_seg_r, "instances": detector_r})

            if self.combine_on:
                panoptic_r = combine_semantic_and_instance_outputs(
                    detector_r,
                    sem_seg_r.argmax(dim=0),
                    self.combine_overlap_threshold,
                    self.combine_stuff_area_limit,
                    self.combine_instances_confidence_threshold,
                )
                processed_results[-1]["panoptic_seg"] = panoptic_r
        return processed_results


def combine_semantic_and_instance_outputs(
    instance_results,
    semantic_results,
    overlap_threshold,
    stuff_area_limit,
    instances_confidence_threshold,
):
    """
    Implement a simple combining logic following
    "combine_semantic_and_instance_predictions.py" in panopticapi
    to produce panoptic segmentation outputs.

    Args:
        instance_results: output of :func:`detector_postprocess`.
        semantic_results: an (H, W) tensor, each is the contiguous semantic
            category id

    Returns:
        panoptic_seg (Tensor): of shape (height, width) where the values are ids for each segment.
        segments_info (list[dict]): Describe each segment in `panoptic_seg`.
            Each dict contains keys "id", "category_id", "isthing".
    """
    panoptic_seg = torch.zeros_like(semantic_results, dtype=torch.int32)

    # sort instance outputs by scores
    sorted_inds = torch.argsort(-instance_results.scores)

    current_segment_id = 0
    segments_info = []

    instance_masks = instance_results.pred_masks.to(dtype=torch.bool, device=panoptic_seg.device)

    # Add instances one-by-one, check for overlaps with existing ones
    for inst_id in sorted_inds:
        score = instance_results.scores[inst_id].item()
        if score < instances_confidence_threshold:
            break
        mask = instance_masks[inst_id]  # H,W
        mask_area = mask.sum().item()

        if mask_area == 0:
            continue

        intersect = (mask > 0) & (panoptic_seg > 0)
        intersect_area = intersect.sum().item()

        if intersect_area * 1.0 / mask_area > overlap_threshold:
            continue

        if intersect_area > 0:
            mask = mask & (panoptic_seg == 0)

        current_segment_id += 1
        panoptic_seg[mask] = current_segment_id
        segments_info.append(
            {
                "id": current_segment_id,
                "isthing": True,
                "score": score,
                "category_id": instance_results.pred_classes[inst_id].item(),
                "instance_id": inst_id.item(),
            }
        )

    # Add semantic results to remaining empty areas
    semantic_labels = torch.unique(semantic_results).cpu().tolist()
    for semantic_label in semantic_labels:
        if semantic_label == 0:  # 0 is a special "thing" class
            continue
        mask = (semantic_results == semantic_label) & (panoptic_seg == 0)
        mask_area = mask.sum().item()
        if mask_area < stuff_area_limit:
            continue

        current_segment_id += 1
        panoptic_seg[mask] = current_segment_id
        segments_info.append(
            {
                "id": current_segment_id,
                "isthing": False,
                "category_id": semantic_label,
                "area": mask_area,
            }
        )

    return panoptic_seg, segments_info
```

#### cvpods/modeling/meta_arch/detr.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

"""
DETR model and criterion classes.
"""
import torch
import torch.nn.functional as F
from torch import nn

from cvpods.layers import ShapeSpec, position_encoding_dict
from cvpods.modeling.backbone import Transformer
from cvpods.modeling.matcher import HungarianMatcher
from cvpods.structures import Boxes, ImageList, Instances
from cvpods.structures import boxes as box_ops
from cvpods.structures.boxes import generalized_box_iou
from cvpods.utils import comm
from cvpods.utils.metrics import accuracy


class DETR(nn.Module):
    def __init__(self, cfg):
        super(DETR, self).__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)

        # Build Backbone
        self.backbone = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))
        )

        # Build Transformer
        self.transformer = Transformer(cfg)

        self.aux_loss = not cfg.MODEL.DETR.NO_AUX_LOSS
        self.num_classes = cfg.MODEL.DETR.NUM_CLASSES
        self.num_queries = cfg.MODEL.DETR.NUM_QUERIES
        hidden_dim = self.transformer.d_model

        # Build FFN
        self.class_embed = nn.Linear(hidden_dim, self.num_classes + 1)
        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)
        # Build Object Queries
        self.query_embed = nn.Embedding(self.num_queries, hidden_dim)

        backbone_out_shapes = self.backbone.output_shape()["res5"]
        self.input_proj = nn.Conv2d(backbone_out_shapes.channels, hidden_dim, kernel_size=1)

        self.position_embedding = position_encoding_dict[cfg.MODEL.DETR.POSITION_EMBEDDING](
            num_pos_feats=hidden_dim // 2,
            temperature=cfg.MODEL.DETR.TEMPERATURE,
            normalize=True if cfg.MODEL.DETR.POSITION_EMBEDDING == "sine" else False,
            scale=None,
        )

        self.weight_dict = {
            "loss_ce": cfg.MODEL.DETR.CLASS_LOSS_COEFF,
            "loss_bbox": cfg.MODEL.DETR.BBOX_LOSS_COEFF,
            "loss_giou": cfg.MODEL.DETR.GIOU_LOSS_COEFF,
        }

        if self.aux_loss:
            self.aux_weight_dict = {}
            for i in range(cfg.MODEL.DETR.TRANSFORMER.NUM_DEC_LAYERS - 1):
                self.aux_weight_dict.update({k + f"_{i}": v for k, v in self.weight_dict.items()})
            self.weight_dict.update(self.aux_weight_dict)

        losses = ["labels", "boxes", "cardinality"]

        matcher = HungarianMatcher(
            cost_class=cfg.MODEL.DETR.COST_CLASS,
            cost_bbox=cfg.MODEL.DETR.COST_BBOX,
            cost_giou=cfg.MODEL.DETR.COST_GIOU,
        )

        self.criterion = SetCriterion(
            self.num_classes,
            matcher=matcher,
            weight_dict=self.weight_dict,
            eos_coef=cfg.MODEL.DETR.EOS_COEFF,
            losses=losses,
        )

        self.post_processors = {"bbox": PostProcess()}

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(3, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(3, 1, 1)

        if not cfg.MODEL.RESNETS.STRIDE_IN_1X1:
            # Custom or torch pretrain weights
            self.normalizer = lambda x: (x / 255.0 - pixel_mean) / pixel_std
        else:
            # MSRA pretrain weights
            self.normalizer = lambda x: (x - pixel_mean) / pixel_std

        self.to(self.device)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
                Each item in the list contains the inputs for one image.
                For now, each item in the list is a dict that contains:
                * image: Tensor, image in (C, H, W) format.
                * instances: Instances
                Other information that's included in the original dicts, such as:
                * "height", "width" (int): the output resolution of the model, used in inference.
                  See :meth:`postprocess` for details.
        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a tensor storing the loss. Used during training only.
        """
        images = self.preprocess_image(batched_inputs)

        B, C, H, W = images.tensor.shape
        device = images.tensor.device

        mask = torch.ones((B, H, W), dtype=torch.bool, device=device)
        for img_shape, m in zip(images.image_sizes, mask):
            m[: img_shape[0], : img_shape[1]] = False

        src = self.backbone(images.tensor)["res5"]
        mask = F.interpolate(mask[None].float(), size=src.shape[-2:]).bool()[0]
        pos = self.position_embedding(src, mask)

        hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos)[0]

        outputs_class = self.class_embed(hs)
        outputs_coord = self.bbox_embed(hs).sigmoid()
        out = {"pred_logits": outputs_class[-1], "pred_boxes": outputs_coord[-1]}

        if self.training:

            targets = self.convert_anno_format(batched_inputs)

            if self.aux_loss:
                out["aux_outputs"] = [
                    {"pred_logits": a, "pred_boxes": b}
                    for a, b in zip(outputs_class[:-1], outputs_coord[:-1])
                ]
            loss_dict = self.criterion(out, targets)
            for k, v in loss_dict.items():
                loss_dict[k] = v * self.weight_dict[k] if k in self.weight_dict else v
            return loss_dict
        else:
            target_sizes = torch.stack(
                [
                    torch.tensor([
                        bi.get("height", img_size[0]),
                        bi.get("width", img_size[1])],
                        device=self.device)
                    for bi, img_size in zip(batched_inputs, images.image_sizes)
                ]
            )
            res = self.post_processors["bbox"](out, target_sizes)

            processed_results = []
            # for results_per_image, input_per_image, image_size in zip(
            for results_per_image, _, image_size in zip(res, batched_inputs, images.image_sizes):
                result = Instances(image_size)
                result.pred_boxes = Boxes(results_per_image["boxes"].float())
                result.scores = results_per_image["scores"].float()
                result.pred_classes = results_per_image["labels"]
                processed_results.append({"instances": result})

            return processed_results

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        images = [x["image"].float().to(self.device) for x in batched_inputs]
        images = [self.normalizer(img) for img in images]
        images = ImageList.from_tensors(images, self.backbone.size_divisibility)
        return images

    def convert_anno_format(self, batched_inputs):
        targets = []
        for bi in batched_inputs:
            target = {}
            h, w = bi["image"].shape[-2:]
            boxes = box_ops.box_xyxy_to_cxcywh(
                bi["instances"].gt_boxes.tensor / torch.tensor([w, h, w, h], dtype=torch.float32)
            )
            target["boxes"] = boxes.to(self.device)
            target["area"] = bi["instances"].gt_boxes.area().to(self.device)
            target["labels"] = bi["instances"].gt_classes.to(self.device)
            if hasattr(bi["instances"], "gt_masks"):
                target["masks"] = bi["instances"].gt_masks
            target["iscrowd"] = torch.zeros_like(target["labels"], device=self.device)
            target["orig_size"] = torch.tensor([bi["height"], bi["width"]], device=self.device)
            target["size"] = torch.tensor([h, w], device=self.device)
            target["image_id"] = torch.tensor(bi["image_id"], device=self.device)
            targets.append(target)

        return targets


class SetCriterion(nn.Module):
    """ This class computes the loss for DETR.
    The process happens in two steps:
        1) we compute hungarian assignment between ground truth boxes and the outputs of the model
        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)
    """

    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):
        """ Create the criterion.
        Parameters:
            num_classes: number of object categories, omitting the special no-object category
            matcher: module able to compute a matching between targets and proposals
            weight_dict: dict containing as key the names of the losses and as values their
                        relative weight.
            eos_coef: relative classification weight applied to the no-object category
            losses: list of all the losses to be applied. See get_loss for list of available losses.
        """
        super().__init__()
        self.num_classes = num_classes
        self.matcher = matcher
        self.weight_dict = weight_dict
        self.eos_coef = eos_coef
        self.losses = losses
        empty_weight = torch.ones(self.num_classes + 1)
        empty_weight[-1] = self.eos_coef
        self.register_buffer("empty_weight", empty_weight)

    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):
        """Classification loss (NLL)
        targets dicts must contain the key "labels" containing a tensor of dim [nb_target_boxes]
        """
        assert "pred_logits" in outputs
        del num_boxes

        src_logits = outputs["pred_logits"]

        idx = self._get_src_permutation_idx(indices)
        target_classes_o = torch.cat([t["labels"][J] for t, (_, J) in zip(targets, indices)])
        target_classes = torch.full(
            src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device
        )
        target_classes[idx] = target_classes_o

        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)
        losses = {"loss_ce": loss_ce}

        if log:
            # TODO this should probably be a separate loss, not hacked in this one here
            losses["class_error"] = 100 - accuracy(src_logits[idx], target_classes_o)[0]

        return losses

    @torch.no_grad()
    def loss_cardinality(self, outputs, targets, indices, num_boxes):
        """
        Compute the cardinality error, ie the absolute error in the number of predicted non-empty
        boxes. This is not really a loss, it is intended for logging purposes only. It doesn't
        propagate gradients
        """
        del indices
        del num_boxes
        pred_logits = outputs["pred_logits"]
        device = pred_logits.device
        tgt_lengths = torch.as_tensor([len(v["labels"]) for v in targets], device=device)
        # Count the number of predictions that are NOT "no-object" (which is the last class)
        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)
        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())
        losses = {"cardinality_error": card_err}
        return losses

    def loss_boxes(self, outputs, targets, indices, num_boxes):
        """
        Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss
        targets dicts must contain the key "boxes" containing a tensor of dim [nb_target_boxes, 4]
        The target boxes are expected in format (center_x, center_y, h, w), normalized by the
        image size.
        """
        assert "pred_boxes" in outputs
        idx = self._get_src_permutation_idx(indices)
        src_boxes = outputs["pred_boxes"][idx]
        target_boxes = torch.cat([t["boxes"][i] for t, (_, i) in zip(targets, indices)], dim=0)

        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction="none")

        losses = {}
        losses["loss_bbox"] = loss_bbox.sum() / num_boxes

        loss_giou = 1 - torch.diag(
            generalized_box_iou(
                box_ops.box_cxcywh_to_xyxy(src_boxes), box_ops.box_cxcywh_to_xyxy(target_boxes)
            )
        )
        losses["loss_giou"] = loss_giou.sum() / num_boxes
        return losses

    def _get_src_permutation_idx(self, indices):
        # permute predictions following indices
        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])
        src_idx = torch.cat([src for (src, _) in indices])
        return batch_idx, src_idx

    def _get_tgt_permutation_idx(self, indices):
        # permute targets following indices
        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])
        tgt_idx = torch.cat([tgt for (_, tgt) in indices])
        return batch_idx, tgt_idx

    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):
        loss_map = {
            "labels": self.loss_labels,
            "cardinality": self.loss_cardinality,
            "boxes": self.loss_boxes,
        }
        assert loss in loss_map, f"do you really want to compute {loss} loss?"
        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)

    def forward(self, outputs, targets):
        """
        This performs the loss computation.

        Parameters:
            outputs: dict of tensors, see the output specification of the model for the format
            targets: list of dicts, such that len(targets) == batch_size.
                      The expected keys in each dict depends on the losses applied, see each
                      loss' doc
        """
        outputs_without_aux = {k: v for k, v in outputs.items() if k != "aux_outputs"}

        # Retrieve the matching between the outputs of the last layer and the targets
        indices = self.matcher(outputs_without_aux, targets)

        # Compute the average number of target boxes accross all nodes, for normalization purposes
        num_boxes = sum(len(t["labels"]) for t in targets)
        num_boxes = torch.as_tensor(
            [num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device
        )

        if comm.get_world_size() > 1:
            torch.distributed.all_reduce(num_boxes)
        num_boxes = torch.clamp(num_boxes / comm.get_world_size(), min=1).item()

        # Compute all the requested losses
        losses = {}
        for loss in self.losses:
            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))

        # In case of auxiliary losses, we repeat this process with the output of
        # each intermediate layer.
        if "aux_outputs" in outputs:
            for i, aux_outputs in enumerate(outputs["aux_outputs"]):
                indices = self.matcher(aux_outputs, targets)
                for loss in self.losses:
                    if loss == "masks":
                        # Intermediate masks losses are too costly to compute, we ignore them.
                        continue
                    kwargs = {}
                    if loss == "labels":
                        # Logging is enabled only for the last layer
                        kwargs = {"log": False}
                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_boxes, **kwargs)
                    l_dict = {k + f"_{i}": v for k, v in l_dict.items()}
                    losses.update(l_dict)

        return losses


class PostProcess(nn.Module):
    """ This module converts the model's output into the format expected by the coco api"""

    @torch.no_grad()
    def forward(self, outputs, target_sizes):
        """
        Perform the computation
        Parameters:
            outputs: raw outputs of the model
            target_sizes: tensor of dimension [batch_size x 2] containing the size of each images
                        of the batch
                For evaluation, this must be the original image size (before any data augmentation)
                For visualization, this should be the image size after data augment,
                but before padding
        """
        out_logits, out_bbox = outputs["pred_logits"], outputs["pred_boxes"]

        assert len(out_logits) == len(target_sizes)
        assert target_sizes.shape[1] == 2

        prob = F.softmax(out_logits, -1)
        scores, labels = prob[..., :-1].max(-1)

        # convert to [x0, y0, x1, y1] format
        boxes = box_ops.box_cxcywh_to_xyxy(out_bbox)
        # and from relative [0, 1] to absolute [0, height] coordinates
        img_h, img_w = target_sizes.unbind(1)
        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)
        boxes = boxes * scale_fct[:, None, :]

        results = [{"scores": s, "labels": l, "boxes": b} for s, l, b in zip(scores, labels, boxes)]

        return results


class MLP(nn.Module):
    """ Very simple multi-layer perceptron (also called FFN)"""

    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super().__init__()
        self.num_layers = num_layers
        h = [hidden_dim] * (num_layers - 1)
        self.layers = nn.ModuleList(
            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])
        )

    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)
        return x
```

#### cvpods/modeling/meta_arch/yolov3.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.

import logging
import random
from collections import OrderedDict

import numpy as np

import torch
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F

from cvpods.layers import ShapeSpec, generalized_batched_nms
from cvpods.modeling.basenet import basenet
from cvpods.modeling.postprocessing import detector_postprocess
from cvpods.structures import Boxes, ImageList, Instances
from cvpods.utils import comm, log_first_n


@basenet
class YOLOv3(nn.Module):
    """
    YOLOv3 model. Darknet 53 is the default backbone of this model.
    """
    def __init__(self, cfg):
        super(YOLOv3, self).__init__()
        self.device = torch.device(cfg.MODEL.DEVICE)

        self.num_classes = cfg.MODEL.YOLO.CLASSES

        self.backbone = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))
        backbone_shape = self.backbone.output_shape
        self.in_features = cfg.MODEL.YOLO.IN_FEATURES

        # out 0
        out_filter_0 = len(
            cfg.MODEL.YOLO.ANCHORS[0]) * (5 + cfg.MODEL.YOLO.CLASSES)
        self.out0 = self._make_embedding(
            [512, 1024], backbone_shape[-1], out_filter_0)

        # out 1
        out_filter_1 = len(
            cfg.MODEL.YOLO.ANCHORS[1]) * (5 + cfg.MODEL.YOLO.CLASSES)
        self.out1_cbl = self._make_cbl(512, 256, 1)
        self.out1_upsample = nn.Upsample(scale_factor=2, mode='nearest')
        self.out1 = self._make_embedding(
            [256, 512], backbone_shape[-2] + 256, out_filter_1)

        # out 2
        out_filter_2 = len(
            cfg.MODEL.YOLO.ANCHORS[2]) * (5 + cfg.MODEL.YOLO.CLASSES)
        self.out2_cbl = self._make_cbl(256, 128, 1)
        self.out2_upsample = nn.Upsample(scale_factor=2, mode='nearest')
        self.out2 = self._make_embedding(
            [128, 256], backbone_shape[-3] + 128, out_filter_2)

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(
            3, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(
            3, 1, 1)
        self.normalizer = lambda x: (x / 255. - pixel_mean) / pixel_std

        self.loss_evaluators = [
            YOLOHead(cfg, anchor, level) for level, anchor in enumerate(cfg.MODEL.YOLO.ANCHORS)]

        self.conf_threshold = cfg.MODEL.YOLO.CONF_THRESHOLD
        self.nms_threshold = cfg.MODEL.YOLO.NMS_THRESHOLD
        self.nms_type = cfg.MODEL.NMS_TYPE

        self.size = 512
        self.multi_size = [320, 352, 384, 416, 448, 480, 512, 544, 576, 608]
        self.change_iter = 10
        self.iter = 0
        self.max_iter = cfg.SOLVER.LR_SCHEDULER.MAX_ITER

        self.to(self.device)

    def _make_cbl(self, _in, _out, ks):
        ''' cbl = conv + batch_norm + leaky_relu
        '''
        pad = (ks - 1) // 2 if ks else 0
        return nn.Sequential(OrderedDict([
            ("conv", nn.Conv2d(_in, _out, kernel_size=ks,
                               stride=1, padding=pad, bias=False)),
            ("bn", nn.BatchNorm2d(_out)),
            ("relu", nn.LeakyReLU(0.1)),
        ]))

    def _make_embedding(self, filters_list, in_filters, out_filter):
        m = nn.ModuleList([
            self._make_cbl(in_filters, filters_list[0], 1),
            self._make_cbl(filters_list[0], filters_list[1], 3),
            self._make_cbl(filters_list[1], filters_list[0], 1),
            self._make_cbl(filters_list[0], filters_list[1], 3),
            self._make_cbl(filters_list[1], filters_list[0], 1),
            self._make_cbl(filters_list[0], filters_list[1], 3)])
        m.add_module("conv_out", nn.Conv2d(filters_list[1], out_filter, kernel_size=1,
                                           stride=1, padding=0, bias=True))
        return m

    def preprocess_image(self, batched_inputs, training):
        """
        Normalize, pad and batch the input images.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        bs = len(images)
        images = [self.normalizer(x) for x in images]

        images = ImageList.from_tensors(
            images, size_divisibility=0, pad_ref_long=True)

        # sync image size for all gpus
        comm.synchronize()
        if training and self.iter % self.change_iter == 0:
            if self.iter < self.max_iter - 20000:
                meg = torch.LongTensor(1).to(self.device)
                comm.synchronize()
                if comm.is_main_process():
                    size = np.random.choice(self.multi_size)
                    meg.fill_(size)

                if comm.get_world_size() > 1:
                    comm.synchronize()
                    dist.broadcast(meg, 0)
                self.size = meg.item()

                comm.synchronize()
            else:
                self.size = 608

        if training:

            # resize image inputs
            modes = ['bilinear', 'nearest', 'bicubic', 'area']
            mode = modes[random.randrange(4)]
            if mode == 'bilinear' or mode == 'bicubic':
                images.tensor = F.interpolate(
                    images.tensor, size=[self.size, self.size], mode=mode, align_corners=False)
            else:
                images.tensor = F.interpolate(images.tensor, size=[self.size, self.size], mode=mode)

            if "instances" in batched_inputs[0]:
                gt_instances = [
                    x["instances"].to(self.device) for x in batched_inputs
                ]
            elif "targets" in batched_inputs[0]:
                log_first_n(
                    logging.WARN,
                    "'targets' in the model inputs is now renamed to 'instances'!",
                    n=10)
                gt_instances = [
                    x["targets"].to(self.device) for x in batched_inputs
                ]
            else:
                gt_instances = None

            targets = [
                torch.cat(
                    [instance.gt_classes.float().unsqueeze(-1), instance.gt_boxes.tensor], dim=-1
                )
                for instance in gt_instances
            ]
            labels = torch.zeros((bs, 100, 5))
            for i, target in enumerate(targets):
                labels[i][:target.shape[0]] = target
            labels[:, :, 1:] = labels[:, :, 1:] / 512. * self.size
        else:
            labels = None

        self.iter += 1
        return images, labels

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
                Each item in the list contains the inputs for one image.
                For now, each item in the list is a dict that contains:

                * image: Tensor, image in (C, H, W) format.
                * instances: Instances

                Other information that's included in the original dicts, such as:

                * "height", "width" (int): the output resolution of the model, used in inference.
                    See :meth:`postprocess` for details.
        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a tensor storing the loss. Used during training only.
        """

        images, labels = self.preprocess_image(batched_inputs, self.training)

        # batched_inputs[0]['image'] = images.tensor[0].cpu() * 255
        # self.visualize_data(batched_inputs[0])

        x = images.tensor
        img_size = x.shape[-2:]

        def _branch(_embedding, _in):
            for i, e in enumerate(_embedding):
                _in = e(_in)
                if i == 4:
                    out_branch = _in
            return _in, out_branch

        #  backbone
        # x2, x1, x0 = self.backbone(x)
        out_features = self.backbone(x)
        features = [out_features[f] for f in self.in_features]
        [x2, x1, x0] = features
        #  yolo branch 0
        out0, out0_branch = _branch(self.out0, x0)
        #  yolo branch 1
        x1_in = self.out1_cbl(out0_branch)
        x1_in = self.out1_upsample(x1_in)
        x1_in = torch.cat([x1_in, x1], 1)
        out1, out1_branch = _branch(self.out1, x1_in)
        #  yolo branch 2
        x2_in = self.out2_cbl(out1_branch)
        x2_in = self.out2_upsample(x2_in)
        x2_in = torch.cat([x2_in, x2], 1)
        out2, out2_branch = _branch(self.out2, x2_in)

        outputs = [out0, out1, out2]

        if self.training:
            losses = [
                loss_evaluator(out, labels, img_size) for out, loss_evaluator in zip(
                    outputs, self.loss_evaluators)
            ]
            keys = ["loss_x", "loss_y", "loss_w",
                    "loss_h", "loss_conf", "loss_cls"]
            losses_dict = {}
            for key in keys:
                losses_dict[key] = sum([loss[key] for loss in losses])
            return losses_dict
        else:
            predictions_list = [loss_evaluator(out, labels, img_size) for
                                out, loss_evaluator in zip(outputs, self.loss_evaluators)]

            predictions = torch.cat(predictions_list, 1)
            detections = postprocess(predictions,
                                     self.num_classes,
                                     self.conf_threshold,
                                     self.nms_threshold,
                                     nms_type=self.nms_type)

            results = []
            for idx, out in enumerate(detections):
                if out is None:
                    out = x.new_zeros((0, 7))
                # image_size = images.image_sizes[idx]
                image_size = img_size
                result = Instances(image_size)
                result.pred_boxes = Boxes(out[:, :4])
                result.scores = out[:, 5] * out[:, 4]
                result.pred_classes = out[:, -1]
                results.append(result)

            processed_results = []
            for results_per_image, input_per_image, image_size in zip(
                    results, batched_inputs, images.image_sizes):
                height = input_per_image.get("height", image_size[0])
                width = input_per_image.get("width", image_size[1])
                r = detector_postprocess(results_per_image, height, width)
                processed_results.append({"instances": r})

            return processed_results


class YOLOHead(nn.Module):
    def __init__(self, cfg, anchors, level):
        super(YOLOHead, self).__init__()
        self.level = level
        self.all_anchors = np.array(cfg.MODEL.YOLO.ANCHORS).reshape([-1, 2])
        self.anchors = anchors
        self.ref_anchors = np.zeros((len(self.all_anchors), 4))
        self.ref_anchors[:, 2:] = self.all_anchors
        self.ref_anchors = torch.from_numpy(self.ref_anchors)

        self.num_anchors = len(anchors)
        self.num_classes = cfg.MODEL.YOLO.CLASSES
        self.bbox_attrs = 5 + self.num_classes

        self.ignore_threshold = cfg.MODEL.YOLO.IGNORE_THRESHOLD
        self.lambda_xy = 1.0
        self.lambda_wh = 1.0
        self.lambda_conf = 1.0
        self.lambda_cls = 1.0

        self.mse_loss = nn.MSELoss(reduction="none")
        self.l1_loss = nn.L1Loss(reduction="none")
        self.bce_loss = nn.BCELoss(reduction="none")

    def forward(self, input, targets=None, image_size=(416, 416)):

        bs = input.size(0)
        in_h = input.size(2)
        in_w = input.size(3)
        stride_h = image_size[1] / in_h
        stride_w = image_size[0] / in_w
        # scaled_anchors = [(a_w / stride_w, a_h / stride_h)
        #                  for a_w, a_h in self.anchors]

        scaled_anchors = [(a_w, a_h)
                          for a_w, a_h in self.anchors]

        prediction = input.view(bs, self.num_anchors,
                                self.bbox_attrs, in_h, in_w).permute(0, 1, 3, 4, 2).contiguous()

        # Get outputs
        x = torch.sigmoid(prediction[..., 0])          # Center x
        y = torch.sigmoid(prediction[..., 1])          # Center y
        w = prediction[..., 2]                         # Width
        h = prediction[..., 3]                         # Height
        conf = torch.sigmoid(prediction[..., 4])       # Conf
        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.

        FloatTensor = lambda x: torch.FloatTensor(x).to(pred_cls.device) # noqa
        LongTensor = lambda x: torch.LongTensor(x).to(pred_cls.device) # noqa

        # Calculate offsets for each grid
        grid_x = FloatTensor(torch.linspace(0, in_w - 1, in_w).repeat(in_h, 1).repeat(
            bs * self.num_anchors, 1, 1).view(x.shape))
        grid_y = FloatTensor(torch.linspace(0, in_h - 1, in_h).repeat(in_w, 1).t().repeat(
            bs * self.num_anchors, 1, 1).view(y.shape))
        # Calculate anchor w, h
        anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0]))
        anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1]))
        anchor_w = anchor_w.repeat(bs, 1).repeat(
            1, 1, in_h * in_w).view(w.shape)
        anchor_h = anchor_h.repeat(bs, 1).repeat(
            1, 1, in_h * in_w).view(h.shape)
        # Add offset and scale with anchors
        pred_boxes = prediction[..., :4].clone()
        pred_boxes[..., 0] = x.data + grid_x
        pred_boxes[..., 1] = y.data + grid_y
        pred_boxes[..., 2] = torch.exp(w.data) * anchor_w
        pred_boxes[..., 3] = torch.exp(h.data) * anchor_h
        pred_boxes[..., 0] *= stride_w
        pred_boxes[..., 1] *= stride_h
        pred_boxes = pred_boxes.data

        if targets is not None:
            #  build target
            mask, obj_mask, \
                tx, ty, tw, th, \
                tgt_scale, tcls = self.get_target(targets, pred_boxes, image_size,
                                                  in_w, in_h,
                                                  stride_w, stride_h,
                                                  self.ignore_threshold)

            mask, obj_mask = mask.cuda(), obj_mask.cuda()
            tx, ty, tw, th = tx.cuda(), ty.cuda(), tw.cuda(), th.cuda()
            tgt_scale, tcls = tgt_scale.cuda(), tcls.cuda()

            loss_x = (mask * tgt_scale * self.bce_loss(x * mask, tx * mask)).sum() / bs
            loss_y = (mask * tgt_scale * self.bce_loss(y * mask, ty * mask)).sum() / bs
            loss_w = (mask * tgt_scale * self.l1_loss(w * mask, tw * mask)).sum() / bs
            loss_h = (mask * tgt_scale * self.l1_loss(h * mask, th * mask)).sum() / bs

            loss_conf = (obj_mask * self.bce_loss(conf, mask)).sum() / bs

            loss_cls = self.bce_loss(pred_cls[mask == 1], tcls[mask == 1]).sum() / bs

            #  total loss = losses * weight
            loss = {
                "loss_x": loss_x * self.lambda_xy,
                "loss_y": loss_y * self.lambda_xy,
                "loss_w": loss_w * self.lambda_wh,
                "loss_h": loss_h * self.lambda_wh,
                "loss_conf": loss_conf * self.lambda_conf,
                "loss_cls": loss_cls * self.lambda_cls,
            }
            return loss
        else:
            # Results
            output = torch.cat((pred_boxes.view(bs, -1, 4),
                                conf.view(bs, -1, 1), pred_cls.view(bs, -1, self.num_classes)), -1)
            return output.data

    def get_target(self, target, pred_boxes, img_size,
                   in_w, in_h, stride_w, stride_h, ignore_threshold):

        FloatTensor = lambda x: torch.FloatTensor(x).to(pred_boxes.device)  # noqa

        bs = target.size(0)

        mask = torch.zeros(bs, self.num_anchors, in_h,
                           in_w, requires_grad=False)
        obj_mask = torch.ones(bs, self.num_anchors,
                              in_h, in_w, requires_grad=False)
        tx = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False)
        ty = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False)
        tw = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False)
        th = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False)
        tgt_scale = torch.zeros(bs, self.num_anchors, in_h, in_w, requires_grad=False)

        tcls = torch.zeros(bs, self.num_anchors, in_h, in_w,
                           self.num_classes, requires_grad=False)
        nlabel = (target.sum(dim=2) > 0).sum(dim=1)
        gx_all = (target[:, :, 1] + target[:, :, 3]) / 2.0   # center x
        gy_all = (target[:, :, 2] + target[:, :, 4]) / 2.0  # center y
        gw_all = (target[:, :, 3] - target[:, :, 1])        # width
        gh_all = (target[:, :, 4] - target[:, :, 2])        # height
        gi_all = (gx_all / stride_w).to(torch.int16)
        gj_all = (gy_all / stride_h).to(torch.int16)

        for b in range(bs):
            n = int(nlabel[b])
            if n == 0:
                continue

            truth_box = FloatTensor(np.zeros((n, 4)))
            truth_box[:, 2] = gw_all[b, :n]
            truth_box[:, 3] = gh_all[b, :n]
            truth_i = gi_all[b, :n]
            truth_j = gj_all[b, :n]

            anchor_ious_all = bboxes_iou(truth_box.cpu(),
                                         self.ref_anchors.type_as(truth_box.cpu()), xyxy=False)
            best_n_all = np.argmax(anchor_ious_all, axis=1)
            best_n = best_n_all % 3
            best_n_mask = ((best_n_all // 3) == self.level)

            truth_box[:n, 0] = gx_all[b, :n]
            truth_box[:n, 1] = gy_all[b, :n]
            pred_box = pred_boxes[b]

            pred_ious = bboxes_iou(pred_box.view(-1, 4),
                                   truth_box, xyxy=False)

            pred_best_iou, _ = pred_ious.max(dim=1)
            pred_best_iou = (pred_best_iou > ignore_threshold)
            pred_best_iou = pred_best_iou.view(pred_box.shape[:3])
            obj_mask[b] = ~pred_best_iou

            if sum(best_n_mask) == 0:
                continue

            for t in range(best_n.shape[0]):
                if best_n_mask[t] == 1:
                    gi, gj = truth_i[t], truth_j[t]
                    gx, gy = gx_all[b, t], gy_all[b, t]
                    gw, gh = gw_all[b, t], gh_all[b, t]

                    a = best_n[t]

                    # Masks
                    mask[b, a, gj, gi] = 1
                    obj_mask[b, a, gj, gi] = 1

                    # Coordinates
                    tx[b, a, gj, gi] = gx / stride_w - gi
                    ty[b, a, gj, gi] = gy / stride_h - gj
                    # Width and height
                    tw[b, a, gj, gi] = torch.log(gw / self.anchors[a][0] + 1e-16)
                    th[b, a, gj, gi] = torch.log(gh / self.anchors[a][1] + 1e-16)

                    tgt_scale[b, a, gj, gi] = 2.0 - gw * gh / (img_size[0] * img_size[1])
                    # One-hot encoding of label
                    tcls[b, a, gj, gi, int(target[b, t, 0])] = 1

        return mask, obj_mask, tx, ty, tw, th, tgt_scale, tcls


def bboxes_iou(bboxes_a, bboxes_b, xyxy=True):
    """Calculate the Intersection of Unions (IoUs) between bounding boxes.
    IoU is calculated as a ratio of area of the intersection
    and area of the union.

    Args:
        bbox_a (array): An array whose shape is :math:`(N, 4)`.
            :math:`N` is the number of bounding boxes.
            The dtype should be :obj:`numpy.float32`.
        bbox_b (array): An array similar to :obj:`bbox_a`,
            whose shape is :math:`(K, 4)`.
            The dtype should be :obj:`numpy.float32`.
    Returns:
        array:
        An array whose shape is :math:`(N, K)`. \
        An element at index :math:`(n, k)` contains IoUs between \
        :math:`n` th bounding box in :obj:`bbox_a` and :math:`k` th bounding \
        box in :obj:`bbox_b`.

    from: https://github.com/chainer/chainercv
    """
    if bboxes_a.shape[1] != 4 or bboxes_b.shape[1] != 4:
        raise IndexError

    if xyxy:
        tl = torch.max(bboxes_a[:, None, :2], bboxes_b[:, :2])
        br = torch.min(bboxes_a[:, None, 2:], bboxes_b[:, 2:])
        area_a = torch.prod(bboxes_a[:, 2:] - bboxes_a[:, :2], 1)
        area_b = torch.prod(bboxes_b[:, 2:] - bboxes_b[:, :2], 1)
    else:
        tl = torch.max((bboxes_a[:, None, :2] - bboxes_a[:, None, 2:] / 2),
                       (bboxes_b[:, :2] - bboxes_b[:, 2:] / 2))
        br = torch.min((bboxes_a[:, None, :2] + bboxes_a[:, None, 2:] / 2),
                       (bboxes_b[:, :2] + bboxes_b[:, 2:] / 2))

        area_a = torch.prod(bboxes_a[:, 2:], 1)
        area_b = torch.prod(bboxes_b[:, 2:], 1)
    en = (tl < br).type(tl.type()).prod(dim=2)
    area_i = torch.prod(br - tl, 2) * en  # * ((tl < br).all())

    return area_i / (area_a[:, None] + area_b - area_i)


def postprocess(prediction, num_classes, conf_thre=0.7, nms_thre=0.45, nms_type='normal'):
    """
    Postprocess for the output of YOLO model
    perform box transformation, specify the class for each detection,
    and perform class-wise non-maximum suppression.
    Args:
        prediction (torch tensor): The shape is :math:`(N, B, 4)`.
            :math:`N` is the number of predictions,
            :math:`B` the number of boxes. The last axis consists of
            :math:`xc, yc, w, h` where `xc` and `yc` represent a center
            of a bounding box.
        num_classes (int):
            number of dataset classes.
        conf_thre (float):
            confidence threshold ranging from 0 to 1,
            which is defined in the config file.
        nms_thre (float):
            IoU threshold of non-max suppression ranging from 0 to 1.

    Returns:
        output (list of torch tensor):

    """
    box_corner = prediction.new(prediction.shape)
    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2
    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2
    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2
    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2
    prediction[:, :, :4] = box_corner[:, :, :4]

    output = [None for _ in range(len(prediction))]
    for i, image_pred in enumerate(prediction):

        # If none are remaining => process next image
        if not image_pred.size(0):
            continue
        # Get score and class with highest confidence
        class_conf, class_pred = torch.max(
            image_pred[:, 5:5 + num_classes], 1, keepdim=True)

        conf_mask = (image_pred[:, 4] * class_conf.squeeze() >= conf_thre).squeeze()
        # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)
        detections = torch.cat(
            (image_pred[:, :5], class_conf, class_pred.float()), 1)
        detections = detections[conf_mask]
        if not detections.size(0):
            continue

        confidence = detections[:, 4] * detections[:, 5]
        nms_out_index = generalized_batched_nms(detections[:, :4], confidence,
                                                detections[:, -1], nms_thre,
                                                nms_type=nms_type)
        detections[:, 4] = confidence / detections[:, 5]

        detections = detections[nms_out_index]

        # Iterate through all predicted classes
        unique_labels = detections[:, -1].unique()

        for c in unique_labels:
            # Get the detections with the particular class
            detections_class = detections[detections[:, -1] == c]
            if output[i] is None:
                output[i] = detections_class
            else:
                output[i] = torch.cat((output[i], detections_class))

    return output
```

#### cvpods/modeling/meta_arch/tensormask.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

import copy
import logging
import math
from typing import List

import torch
import torch.nn.functional as F
from torch import nn

from cvpods.layers import (
    ShapeSpec,
    SwapAlign2Nat,
    cat,
    generalized_batched_nms,
    paste_masks_in_image
)
from cvpods.modeling.anchor_generator import DefaultAnchorGenerator
from cvpods.modeling.box_regression import Box2BoxTransform
from cvpods.modeling.losses import sigmoid_focal_loss_star_jit, smooth_l1_loss
from cvpods.modeling.meta_arch.retinanet import (
    permute_all_cls_and_box_to_N_HWA_K_and_concat,
    permute_to_N_HWA_K
)
from cvpods.structures import Boxes, ImageList, Instances
from cvpods.utils import log_first_n

__all__ = ["TensorMask"]


def _assignment_rule(
    gt_boxes,
    anchor_boxes,
    unit_lengths,
    min_anchor_size,
    scale_thresh=2.0,
    spatial_thresh=1.0,
    uniqueness_on=True,
):
    """
    Given two lists of boxes of N ground truth boxes and M anchor boxes,
    compute the assignment between the two, following the assignment rules in
    https://arxiv.org/abs/1903.12174.
    The box order must be (xmin, ymin, xmax, ymax), so please make sure to
    convert to BoxMode.XYXY_ABS before calling this function.
    Args:
        gt_boxes, anchor_boxes (Boxes): two Boxes. Contains N & M boxes/anchors
            , respectively. unit_lengths (Tensor): Contains the unit lengths of M
            anchor boxes.
        min_anchor_size (float): Minimum size of the anchor, in pixels
        scale_thresh (float): The `scale` threshold: the maximum size of the anchor
                              should not be greater than scale_thresh x max(h, w) of
                              the ground truth box.
        spatial_thresh (float): The `spatial` threshold: the l2 distance between the
                              center of the anchor and the ground truth box should not
                              be greater than spatial_thresh x u where u is the unit length.
    Returns:
        matches (Tensor[int64]): a vector of length M, where matches[i] is a matched
                ground-truth index in [0, N)
        match_labels (Tensor[int8]): a vector of length M, where pred_labels[i] indicates
            whether a prediction is a true or false positive or ignored
    """
    gt_boxes, anchor_boxes = gt_boxes.tensor, anchor_boxes.tensor
    N = gt_boxes.shape[0]
    M = anchor_boxes.shape[0]
    if N == 0 or M == 0:
        return (
            gt_boxes.new_full((N,), 0, dtype=torch.int64),
            gt_boxes.new_full((N,), -1, dtype=torch.int8),
        )

    # Containment rule
    lt = torch.min(gt_boxes[:, None, :2], anchor_boxes[:, :2])  # [N,M,2]
    rb = torch.max(gt_boxes[:, None, 2:], anchor_boxes[:, 2:])  # [N,M,2]
    union = cat([lt, rb], dim=2)  # [N,M,4]

    dummy_gt_boxes = torch.zeros_like(gt_boxes)
    anchor = dummy_gt_boxes[:, None, :] + anchor_boxes[:, :]  # [N,M,4]

    contain_matrix = torch.all(union == anchor, dim=2)  # [N,M]

    # Centrality rule, scale
    gt_size_lower = torch.max(gt_boxes[:, 2:] - gt_boxes[:, :2], dim=1)[0]  # [N]
    gt_size_upper = gt_size_lower * scale_thresh  # [N]
    # Fall back for small objects
    gt_size_upper[gt_size_upper < min_anchor_size] = min_anchor_size
    # Due to sampling of locations, the anchor sizes are deducted with sampling strides
    anchor_size = (
        torch.max(anchor_boxes[:, 2:] - anchor_boxes[:, :2], dim=1)[0] - unit_lengths
    )  # [M]

    size_diff_upper = gt_size_upper[:, None] - anchor_size  # [N,M]
    scale_matrix = size_diff_upper >= 0  # [N,M]

    # Centrality rule, spatial
    gt_center = (gt_boxes[:, 2:] + gt_boxes[:, :2]) / 2  # [N,2]
    anchor_center = (anchor_boxes[:, 2:] + anchor_boxes[:, :2]) / 2  # [M,2]
    offset_center = gt_center[:, None, :] - anchor_center[:, :]  # [N,M,2]
    offset_center /= unit_lengths[:, None]  # [N,M,2]
    spatial_square = spatial_thresh * spatial_thresh
    spatial_matrix = torch.sum(offset_center * offset_center, dim=2) <= spatial_square

    assign_matrix = (contain_matrix & scale_matrix & spatial_matrix).int()

    # assign_matrix is N (gt) x M (predicted)
    # Max over gt elements (dim 0) to find best gt candidate for each prediction
    matched_vals, matches = assign_matrix.max(dim=0)
    match_labels = matches.new_full(matches.size(), 1, dtype=torch.int8)

    match_labels[matched_vals == 0] = 0
    match_labels[matched_vals == 1] = 1

    # find all the elements that match to ground truths multiple times
    not_unique_idxs = assign_matrix.sum(dim=0) > 1
    if uniqueness_on:
        match_labels[not_unique_idxs] = 0
    else:
        match_labels[not_unique_idxs] = -1

    return matches, match_labels


def _paste_mask_lists_in_image(masks, boxes, image_shape, threshold=0.5):
    """
    Paste a list of masks that are of various resolutions (e.g., 28 x 28) into an image.
    The location, height, and width for pasting each mask is determined by their
    corresponding bounding boxes in boxes.
    Args:
        masks (list(Tensor)): A list of Tensor of shape (1, Hmask_i, Wmask_i).
                            Values are in [0, 1]. The list length, Bimg, is the
                            number of detected object instances in the image.
        boxes (Boxes): A Boxes of length Bimg. boxes.tensor[i] and masks[i] correspond
                            to the same object instance.
        image_shape (tuple): height, width
        threshold (float): A threshold in [0, 1] for converting the (soft) masks to
            binary masks.
    Returns:
        img_masks (Tensor): A tensor of shape (Bimg, Himage, Wimage), where Bimg is the
        number of detected object instances and Himage, Wimage are the image width
        and height. img_masks[i] is a binary mask for object instance i.
    """
    if len(masks) == 0:
        return torch.empty((0, 1) + image_shape, dtype=torch.uint8)

    # Loop over masks groups. Each group has the same mask prediction size.
    img_masks = []
    ind_masks = []
    mask_sizes = torch.tensor([m.shape[-1] for m in masks])
    unique_sizes = torch.unique(mask_sizes)
    for msize in unique_sizes.tolist():
        cur_ind = torch.where(mask_sizes == msize)[0]
        ind_masks.append(cur_ind)

        cur_masks = cat([masks[i] for i in cur_ind])
        cur_boxes = boxes[cur_ind]
        img_masks.append(
            paste_masks_in_image(cur_masks, cur_boxes, image_shape, threshold)
        )

    img_masks = cat(img_masks)
    ind_masks = cat(ind_masks)

    img_masks_out = torch.empty_like(img_masks)
    img_masks_out[ind_masks, :, :] = img_masks

    return img_masks_out


def _postprocess(
    results, result_mask_info, output_height, output_width, mask_threshold=0.5
):
    """
    Post-process the output boxes for TensorMask.
    The input images are often resized when entering an object detector.
    As a result, we often need the outputs of the detector in a different
    resolution from its inputs.
    This function will postprocess the raw outputs of TensorMask
    to produce outputs according to the desired output resolution.
    Args:
        results (Instances): the raw outputs from the detector.
            `results.image_size` contains the input image resolution the detector sees.
            This object might be modified in-place. Note that it does not contain the field
            `pred_masks`, which is provided by another input `result_masks`.
        result_mask_info (list[Tensor], Boxes): a pair of two items for mask related results.
                The first item is a list of #detection tensors, each is the predicted masks.
                The second item is the anchors corresponding to the predicted masks.
        output_height, output_width: the desired output resolution.
    Returns:
        Instances: the postprocessed output from the model, based on the output resolution
    """
    scale_x, scale_y = (
        output_width / results.image_size[1],
        output_height / results.image_size[0],
    )
    results = Instances((output_height, output_width), **results.get_fields())

    output_boxes = results.pred_boxes
    output_boxes.tensor[:, 0::2] *= scale_x
    output_boxes.tensor[:, 1::2] *= scale_y
    output_boxes.clip(results.image_size)

    inds_nonempty = output_boxes.nonempty()
    results = results[inds_nonempty]
    result_masks, result_anchors = result_mask_info
    if result_masks:
        result_anchors.tensor[:, 0::2] *= scale_x
        result_anchors.tensor[:, 1::2] *= scale_y
        result_masks = [x for (i, x) in zip(inds_nonempty.tolist(), result_masks) if i]
        results.pred_masks = _paste_mask_lists_in_image(
            result_masks,
            result_anchors[inds_nonempty],
            results.image_size,
            threshold=mask_threshold,
        )
    return results


class TensorMaskAnchorGenerator(DefaultAnchorGenerator):
    """
    For a set of image sizes and feature maps, computes a set of anchors for TensorMask.
    It also computes the unit lengths and indexes for each anchor box.
    """

    def grid_anchors_with_unit_lengths_and_indexes(self, grid_sizes):
        anchors = []
        unit_lengths = []
        indexes = []
        for lvl, (size, stride, base_anchors) in enumerate(
            zip(grid_sizes, self.strides, self.cell_anchors)
        ):
            grid_height, grid_width = size
            device = base_anchors.device
            shifts_x = torch.arange(
                0, grid_width * stride, step=stride, dtype=torch.float32, device=device
            )
            shifts_y = torch.arange(
                0, grid_height * stride, step=stride, dtype=torch.float32, device=device
            )
            shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)
            shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=2)
            # Stack anchors in shapes of (HWA, 4)
            cur_anchor = (shifts[:, :, None, :] + base_anchors.view(1, 1, -1, 4)).view(
                -1, 4
            )
            anchors.append(cur_anchor)
            unit_lengths.append(
                torch.full(
                    (cur_anchor.shape[0],), stride, dtype=torch.float32, device=device
                )
            )
            # create mask indexes using mesh grid
            shifts_l = torch.full((1,), lvl, dtype=torch.int64, device=device)
            shifts_i = torch.zeros((1,), dtype=torch.int64, device=device)
            shifts_h = torch.arange(0, grid_height, dtype=torch.int64, device=device)
            shifts_w = torch.arange(0, grid_width, dtype=torch.int64, device=device)
            shifts_a = torch.arange(
                0, base_anchors.shape[0], dtype=torch.int64, device=device
            )
            grids = torch.meshgrid(shifts_l, shifts_i, shifts_h, shifts_w, shifts_a)

            indexes.append(torch.stack(grids, dim=5).view(-1, 5))

        return anchors, unit_lengths, indexes

    def forward(self, features):
        """
        Returns:
            list[list[Boxes]]: a list of #image elements. Each is a list of #feature level Boxes.
                The Boxes contains anchors of this image on the specific feature level.
            list[list[Tensor]]: a list of #image elements. Each is a list of #feature level tensors.
                The tensor contains strides, or unit lengths for the anchors.
            list[list[Tensor]]: a list of #image elements. Each is a list of #feature level tensors.
                The Tensor contains indexes for the anchors, with the last dimension meaning
                (L, N, H, W, A), where L is level, I is image (not set yet), H is height,
                W is width, and A is anchor.
        """
        num_images = len(features[0])
        grid_sizes = [feature_map.shape[-2:] for feature_map in features]
        (
            anchors_list,
            lengths_list,
            indexes_list,
        ) = self.grid_anchors_with_unit_lengths_and_indexes(grid_sizes)

        # Convert anchors from Tensor to Boxes
        anchors_per_im = [Boxes(x) for x in anchors_list]

        anchors = [copy.deepcopy(anchors_per_im) for _ in range(num_images)]
        unit_lengths = [copy.deepcopy(lengths_list) for _ in range(num_images)]
        indexes = [copy.deepcopy(indexes_list) for _ in range(num_images)]

        return anchors, unit_lengths, indexes


class TensorMask(nn.Module):
    """
    TensorMask model. Creates FPN backbone, anchors and a head for classification
    and box regression. Calculates and applies proper losses to class, box, and
    masks.
    """

    def __init__(self, cfg):
        super().__init__()

        # get the deice of the model
        self.device = torch.device(cfg.MODEL.DEVICE)

        # fmt: off
        self.num_classes = cfg.MODEL.TENSOR_MASK.NUM_CLASSES
        self.in_features = cfg.MODEL.TENSOR_MASK.IN_FEATURES
        self.anchor_sizes = cfg.MODEL.ANCHOR_GENERATOR.SIZES
        self.num_levels = len(cfg.MODEL.ANCHOR_GENERATOR.SIZES)
        # Loss parameters:
        self.focal_loss_alpha = cfg.MODEL.TENSOR_MASK.FOCAL_LOSS_ALPHA
        self.focal_loss_gamma = cfg.MODEL.TENSOR_MASK.FOCAL_LOSS_GAMMA
        # Inference parameters:
        self.score_threshold = cfg.MODEL.TENSOR_MASK.SCORE_THRESH_TEST
        self.topk_candidates = cfg.MODEL.TENSOR_MASK.TOPK_CANDIDATES_TEST
        self.nms_threshold = cfg.MODEL.TENSOR_MASK.NMS_THRESH_TEST
        self.nms_type = cfg.MODEL.NMS_TYPE
        self.detections_im = cfg.TEST.DETECTIONS_PER_IMAGE
        # Mask parameters:
        self.mask_on = cfg.MODEL.MASK_ON
        self.mask_loss_weight = cfg.MODEL.TENSOR_MASK.MASK_LOSS_WEIGHT
        self.mask_pos_weight = torch.tensor(cfg.MODEL.TENSOR_MASK.POSITIVE_WEIGHT,
                                            dtype=torch.float32,
                                            device=self.device)
        self.bipyramid_on = cfg.MODEL.TENSOR_MASK.BIPYRAMID_ON
        # fmt: on

        # build the backbone
        self.backbone = cfg.build_backbone(cfg)

        backbone_shape = self.backbone.output_shape()
        feature_shapes = [backbone_shape[f] for f in self.in_features]
        feature_strides = [x.stride for x in feature_shapes]
        # build anchors
        self.anchor_generator = TensorMaskAnchorGenerator(cfg, feature_shapes)
        self.num_anchors = self.anchor_generator.num_cell_anchors[0]
        anchors_min_level = cfg.MODEL.ANCHOR_GENERATOR.SIZES[0]
        self.mask_sizes = [size // feature_strides[0] for size in anchors_min_level]
        self.min_anchor_size = min(anchors_min_level) - feature_strides[0]

        # head of the TensorMask
        self.head = TensorMaskHead(
            cfg, self.num_levels, self.num_anchors, self.mask_sizes, feature_shapes
        )
        # box transform
        self.box2box_transform = Box2BoxTransform(
            weights=cfg.MODEL.TENSOR_MASK.BBOX_REG_WEIGHTS
        )
        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(3, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(3, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std
        self.to(self.device)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DetectionTransform` .
                Each item in the list contains the inputs for one image.
            For now, each item in the list is a dict that contains:
                image: Tensor, image in (C, H, W) format.
                instances: Instances
                Other information that's included in the original dicts, such as:
                    "height", "width" (int): the output resolution of the model, used in inference.
                        See :meth:`postprocess` for details.
         Returns:
            losses (dict[str: Tensor]): mapping from a named loss to a tensor
                storing the loss. Used during training only.
        """
        images = self.preprocess_image(batched_inputs)
        if "instances" in batched_inputs[0]:
            gt_instances = [x["instances"].to(self.device) for x in batched_inputs]
        elif "targets" in batched_inputs[0]:
            log_first_n(
                logging.WARN,
                "'targets' in the model inputs is now renamed to 'instances'!",
                n=10,
            )
            gt_instances = [x["targets"].to(self.device) for x in batched_inputs]
        else:
            gt_instances = None

        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]
        # apply the TensorMask head
        pred_logits, pred_deltas, pred_masks = self.head(features)
        # generate anchors based on features, is it image specific?
        anchors, unit_lengths, indexes = self.anchor_generator(features)

        if self.training:
            # get ground truths for class labels and box targets, it will label each anchor
            gt_class_info, gt_delta_info, gt_mask_info, num_fg = self.get_ground_truth(
                anchors, unit_lengths, indexes, gt_instances
            )
            # compute the loss
            return self.losses(
                gt_class_info,
                gt_delta_info,
                gt_mask_info,
                num_fg,
                pred_logits,
                pred_deltas,
                pred_masks,
            )
        else:
            # do inference to get the output
            results = self.inference(
                pred_logits, pred_deltas, pred_masks, anchors, indexes, images
            )
            processed_results = []
            for results_im, input_im, image_size in zip(
                results, batched_inputs, images.image_sizes
            ):
                height = input_im.get("height", image_size[0])
                width = input_im.get("width", image_size[1])
                # this is to do post-processing with the image size
                result_box, result_mask = results_im
                r = _postprocess(result_box, result_mask, height, width)
                processed_results.append({"instances": r})
            return processed_results

    def losses(
        self,
        gt_class_info,
        gt_delta_info,
        gt_mask_info,
        num_fg,
        pred_logits,
        pred_deltas,
        pred_masks,
    ):
        """
        Args:
            For `gt_class_info`, `gt_delta_info`, `gt_mask_info` and `num_fg` parameters, see
                :meth:`TensorMask.get_ground_truth`.
            For `pred_logits`, `pred_deltas` and `pred_masks`, see
                :meth:`TensorMaskHead.forward`.
        Returns:
            losses (dict[str: Tensor]): mapping from a named loss to a scalar tensor
                storing the loss. Used during training only. The potential dict keys are:
                "loss_cls", "loss_box_reg" and "loss_mask".
        """
        gt_classes_target, gt_valid_inds = gt_class_info
        gt_deltas, gt_fg_inds = gt_delta_info
        gt_masks, gt_mask_inds = gt_mask_info
        loss_normalizer = torch.tensor(
            max(1, num_fg), dtype=torch.float32, device=self.device
        )

        # classification and regression
        pred_logits, pred_deltas = permute_all_cls_and_box_to_N_HWA_K_and_concat(
            pred_logits, pred_deltas, self.num_classes
        )
        loss_cls = (
            sigmoid_focal_loss_star_jit(
                pred_logits[gt_valid_inds],
                gt_classes_target[gt_valid_inds],
                alpha=self.focal_loss_alpha,
                gamma=self.focal_loss_gamma,
                reduction="sum",
            )
            / loss_normalizer
        )

        if num_fg == 0:
            loss_box_reg = pred_deltas.sum() * 0
        else:
            loss_box_reg = (
                smooth_l1_loss(
                    pred_deltas[gt_fg_inds], gt_deltas, beta=0.0, reduction="sum"
                )
                / loss_normalizer
            )
        losses = {"loss_cls": loss_cls, "loss_box_reg": loss_box_reg}

        # mask prediction
        if self.mask_on:
            loss_mask = 0
            for lvl in range(self.num_levels):
                cur_level_factor = 2 ** lvl if self.bipyramid_on else 1
                for anc in range(self.num_anchors):
                    cur_gt_mask_inds = gt_mask_inds[lvl][anc]
                    if cur_gt_mask_inds is None:
                        loss_mask += pred_masks[lvl][anc][0, 0, 0, 0] * 0
                    else:
                        cur_mask_size = self.mask_sizes[anc] * cur_level_factor
                        # TODO maybe there are numerical issues when mask sizes are large
                        cur_size_divider = torch.tensor(
                            self.mask_loss_weight / (cur_mask_size ** 2),
                            dtype=torch.float32,
                            device=self.device,
                        )

                        cur_pred_masks = pred_masks[lvl][anc][
                            cur_gt_mask_inds[:, 0],  # N
                            :,  # V x U
                            cur_gt_mask_inds[:, 1],  # H
                            cur_gt_mask_inds[:, 2],  # W
                        ]

                        loss_mask += F.binary_cross_entropy_with_logits(
                            # V, U
                            cur_pred_masks.view(-1, cur_mask_size, cur_mask_size),
                            gt_masks[lvl][anc].to(dtype=torch.float32),
                            reduction="sum",
                            weight=cur_size_divider,
                            pos_weight=self.mask_pos_weight,
                        )
            losses["loss_mask"] = loss_mask / loss_normalizer
        return losses

    @torch.no_grad()
    def get_ground_truth(self, anchors, unit_lengths, indexes, targets):
        """
        Args:
            anchors (list[list[Boxes]]): a list of N=#image elements. Each is a
                list of #feature level Boxes. The Boxes contains anchors of
                this image on the specific feature level.
            unit_lengths (list[list[Tensor]]): a list of N=#image elements. Each is a
                list of #feature level Tensor. The tensor contains unit lengths for anchors of
                this image on the specific feature level.
            indexes (list[list[Tensor]]): a list of N=#image elements. Each is a
                list of #feature level Tensor. The tensor contains the 5D index of
                each anchor, the second dimension means (L, N, H, W, A), where L
                is level, I is image, H is height, W is width, and A is anchor.
            targets (list[Instances]): a list of N `Instances`s. The i-th
                `Instances` contains the ground-truth per-instance annotations
                for the i-th input image.  Specify `targets` during training only.
        Returns:
            gt_class_info (Tensor, Tensor): A pair of two tensors for classification.
                The first one is an integer tensor of shape (R, #classes) storing ground-truth
                labels for each anchor. R is the total number of anchors in the batch.
                The second one is an integer tensor of shape (R,), to indicate which
                anchors are valid for loss computation, which anchors are not.
            gt_delta_info (Tensor, Tensor): A pair of two tensors for boxes.
                The first one, of shape (F, 4). F=#foreground anchors.
                The last dimension represents ground-truth box2box transform
                targets (dx, dy, dw, dh) that map each anchor to its matched ground-truth box.
                Only foreground anchors have values in this tensor. Could be `None` if F=0.
                The second one, of shape (R,), is an integer tensor indicating which anchors
                are foreground ones used for box regression. Could be `None` if F=0.
            gt_mask_info (list[list[Tensor]], list[list[Tensor]]): A pair of two lists for masks.
                The first one is a list of P=#feature level elements. Each is a
                list of A=#anchor tensors. Each tensor contains the ground truth
                masks of the same size and for the same feature level. Could be `None`.
                The second one is a list of P=#feature level elements. Each is a
                list of A=#anchor tensors. Each tensor contains the location of the ground truth
                masks of the same size and for the same feature level. The second dimension means
                (N, H, W), where N is image, H is height, and W is width. Could be `None`.
            num_fg (int): F=#foreground anchors, used later for loss normalization.
        """
        gt_classes = []
        gt_deltas = []
        gt_masks = [
            [[] for _ in range(self.num_anchors)] for _ in range(self.num_levels)
        ]
        gt_mask_inds = [
            [[] for _ in range(self.num_anchors)] for _ in range(self.num_levels)
        ]

        anchors = [Boxes.cat(anchors_i) for anchors_i in anchors]
        unit_lengths = [cat(unit_lengths_i) for unit_lengths_i in unit_lengths]
        indexes = [cat(indexes_i) for indexes_i in indexes]

        num_fg = 0
        for i, (anchors_im, unit_lengths_im, indexes_im, targets_im) in enumerate(
            zip(anchors, unit_lengths, indexes, targets)
        ):
            # Initialize all
            gt_classes_i = torch.full_like(
                unit_lengths_im, self.num_classes, dtype=torch.int64, device=self.device
            )
            # Ground truth classes
            has_gt = len(targets_im) > 0
            if has_gt:
                # Compute the pairwise matrix
                gt_matched_inds, anchor_labels = _assignment_rule(
                    targets_im.gt_boxes,
                    anchors_im,
                    unit_lengths_im,
                    self.min_anchor_size,
                )
                # Find the foreground instances
                fg_inds = anchor_labels == 1
                fg_anchors = anchors_im[fg_inds]
                num_fg += len(fg_anchors)
                # Find the ground truths for foreground instances
                gt_fg_matched_inds = gt_matched_inds[fg_inds]
                # Assign labels for foreground instances
                gt_classes_i[fg_inds] = targets_im.gt_classes[gt_fg_matched_inds]
                # Anchors with label -1 are ignored, others are left as negative
                gt_classes_i[anchor_labels == -1] = -1

                # Boxes
                # Ground truth box regression, only for foregrounds
                matched_gt_boxes = targets_im[gt_fg_matched_inds].gt_boxes
                # Compute box regression offsets for foregrounds only
                gt_deltas_i = self.box2box_transform.get_deltas(
                    fg_anchors.tensor, matched_gt_boxes.tensor
                )
                gt_deltas.append(gt_deltas_i)

                # Masks
                if self.mask_on:
                    # Compute masks for each level and each anchor
                    matched_indexes = indexes_im[fg_inds, :]
                    for lvl in range(self.num_levels):
                        ids_lvl = matched_indexes[:, 0] == lvl
                        if torch.any(ids_lvl):
                            cur_level_factor = 2 ** lvl if self.bipyramid_on else 1
                            for anc in range(self.num_anchors):
                                ids_lvl_anchor = ids_lvl & (
                                    matched_indexes[:, 4] == anc
                                )
                                if torch.any(ids_lvl_anchor):
                                    gt_masks[lvl][anc].append(
                                        targets_im[
                                            gt_fg_matched_inds[ids_lvl_anchor]
                                        ].gt_masks.crop_and_resize(
                                            fg_anchors[ids_lvl_anchor].tensor,
                                            self.mask_sizes[anc] * cur_level_factor,
                                        )
                                    )
                                    # Select (N, H, W) dimensions
                                    gt_mask_inds_lvl_anc = matched_indexes[
                                        ids_lvl_anchor, 1:4
                                    ]
                                    # Set the image index to the current image
                                    gt_mask_inds_lvl_anc[:, 0] = i
                                    gt_mask_inds[lvl][anc].append(gt_mask_inds_lvl_anc)
            gt_classes.append(gt_classes_i)

        # Classes and boxes
        gt_classes = cat(gt_classes)
        gt_valid_inds = gt_classes >= 0
        gt_fg_inds = gt_valid_inds & (gt_classes < self.num_classes)
        gt_classes_target = torch.zeros(
            (gt_classes.shape[0], self.num_classes),
            dtype=torch.float32,
            device=self.device,
        )
        gt_classes_target[gt_fg_inds, gt_classes[gt_fg_inds]] = 1
        gt_deltas = cat(gt_deltas) if gt_deltas else None

        # Masks
        gt_masks = [[cat(mla) if mla else None for mla in ml] for ml in gt_masks]
        gt_mask_inds = [
            [cat(ila) if ila else None for ila in il] for il in gt_mask_inds
        ]
        return (
            (gt_classes_target, gt_valid_inds),
            (gt_deltas, gt_fg_inds),
            (gt_masks, gt_mask_inds),
            num_fg,
        )

    def inference(self, pred_logits, pred_deltas, pred_masks, anchors, indexes, images):
        """
        Arguments:
            pred_logits, pred_deltas, pred_masks: Same as the output of:
                meth:`TensorMaskHead.forward`
            anchors, indexes: Same as the input of meth:`TensorMask.get_ground_truth`
            images (ImageList): the input images
        Returns:
            results (List[Instances]): a list of #images elements.
        """
        assert len(anchors) == len(images)
        results = []

        pred_logits = [permute_to_N_HWA_K(x, self.num_classes) for x in pred_logits]
        pred_deltas = [permute_to_N_HWA_K(x, 4) for x in pred_deltas]

        pred_logits = cat(pred_logits, dim=1)
        pred_deltas = cat(pred_deltas, dim=1)

        for img_idx, (anchors_im, indexes_im) in enumerate(zip(anchors, indexes)):
            # Get the size of the current image
            image_size = images.image_sizes[img_idx]

            logits_im = pred_logits[img_idx]
            deltas_im = pred_deltas[img_idx]

            if self.mask_on:
                masks_im = [[mla[img_idx] for mla in ml] for ml in pred_masks]
            else:
                masks_im = [None] * self.num_levels
            results_im = self.inference_single_image(
                logits_im,
                deltas_im,
                masks_im,
                Boxes.cat(anchors_im),
                cat(indexes_im),
                tuple(image_size),
            )
            results.append(results_im)
        return results

    def inference_single_image(
        self, pred_logits, pred_deltas, pred_masks, anchors, indexes, image_size
    ):
        """
        Single-image inference. Return bounding-box detection results by thresholding
        on scores and applying non-maximum suppression (NMS).
        Arguments:
            pred_logits (list[Tensor]): list of #feature levels. Each entry contains
                tensor of size (AxHxW, K)
            pred_deltas (list[Tensor]): Same shape as 'pred_logits' except that K becomes 4.
            pred_masks (list[list[Tensor]]): List of #feature levels, each is a list of #anchors.
                Each entry contains tensor of size (M_i*M_i, H, W). `None` if mask_on=False.
            anchors (list[Boxes]): list of #feature levels. Each entry contains
                a Boxes object, which contains all the anchors for that
                image in that feature level.
            image_size (tuple(H, W)): a tuple of the image height and width.
        Returns:
            Same as `inference`, but for only one image.
        """
        pred_logits = pred_logits.flatten().sigmoid_()
        # We get top locations across all levels to accelerate the inference speed,
        # which does not seem to affect the accuracy.
        # First select values above the threshold
        logits_top_idxs = torch.where(pred_logits > self.score_threshold)[0]
        # Then get the top values
        num_topk = min(self.topk_candidates, logits_top_idxs.shape[0])
        pred_prob, topk_idxs = pred_logits[logits_top_idxs].sort(descending=True)
        # Keep top k scoring values
        pred_prob = pred_prob[:num_topk]
        # Keep top k values
        top_idxs = logits_top_idxs[topk_idxs[:num_topk]]

        # class index
        cls_idxs = top_idxs % self.num_classes
        # HWA index
        top_idxs //= self.num_classes
        # predict boxes
        pred_boxes = self.box2box_transform.apply_deltas(
            pred_deltas[top_idxs], anchors[top_idxs].tensor
        )

        # apply nms
        keep = generalized_batched_nms(pred_boxes, pred_prob, cls_idxs,
                                       self.nms_threshold, nms_type=self.nms_type)
        # pick the top ones
        keep = keep[: self.detections_im]

        results = Instances(image_size)
        results.pred_boxes = Boxes(pred_boxes[keep])
        results.scores = pred_prob[keep]
        results.pred_classes = cls_idxs[keep]

        # deal with masks
        result_masks, result_anchors = [], None
        if self.mask_on:
            # index and anchors, useful for masks
            top_indexes = indexes[top_idxs]
            top_anchors = anchors[top_idxs]
            result_indexes = top_indexes[keep]
            result_anchors = top_anchors[keep]
            # Get masks and do sigmoid
            for lvl, _, h, w, anc in result_indexes.tolist():
                cur_size = self.mask_sizes[anc] * (2 ** lvl if self.bipyramid_on else 1)
                result_masks.append(
                    torch.sigmoid(
                        pred_masks[lvl][anc][:, h, w].view(1, cur_size, cur_size)
                    )
                )

        return results, (result_masks, result_anchors)

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images, self.backbone.size_divisibility)
        return images


class TensorMaskHead(nn.Module):
    def __init__(
        self, cfg, num_levels, num_anchors, mask_sizes, input_shape: List[ShapeSpec]
    ):
        """
        TensorMask head.
        """
        super().__init__()
        # fmt: off
        self.in_features = cfg.MODEL.TENSOR_MASK.IN_FEATURES
        in_channels = input_shape[0].channels
        num_classes = cfg.MODEL.TENSOR_MASK.NUM_CLASSES
        cls_channels = cfg.MODEL.TENSOR_MASK.CLS_CHANNELS
        num_convs = cfg.MODEL.TENSOR_MASK.NUM_CONVS
        # box parameters
        bbox_channels = cfg.MODEL.TENSOR_MASK.BBOX_CHANNELS
        # mask parameters
        self.mask_on = cfg.MODEL.MASK_ON
        self.mask_sizes = mask_sizes
        mask_channels = cfg.MODEL.TENSOR_MASK.MASK_CHANNELS
        self.align_on = cfg.MODEL.TENSOR_MASK.ALIGNED_ON
        self.bipyramid_on = cfg.MODEL.TENSOR_MASK.BIPYRAMID_ON
        # fmt: on

        # class subnet
        cls_subnet = []
        cur_channels = in_channels
        for _ in range(num_convs):
            cls_subnet.append(
                nn.Conv2d(
                    cur_channels, cls_channels, kernel_size=3, stride=1, padding=1
                )
            )
            cur_channels = cls_channels
            cls_subnet.append(nn.ReLU())

        self.cls_subnet = nn.Sequential(*cls_subnet)
        self.cls_score = nn.Conv2d(
            cur_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1
        )
        modules_list = [self.cls_subnet, self.cls_score]

        # box subnet
        bbox_subnet = []
        cur_channels = in_channels
        for _ in range(num_convs):
            bbox_subnet.append(
                nn.Conv2d(
                    cur_channels, bbox_channels, kernel_size=3, stride=1, padding=1
                )
            )
            cur_channels = bbox_channels
            bbox_subnet.append(nn.ReLU())

        self.bbox_subnet = nn.Sequential(*bbox_subnet)
        self.bbox_pred = nn.Conv2d(
            cur_channels, num_anchors * 4, kernel_size=3, stride=1, padding=1
        )
        modules_list.extend([self.bbox_subnet, self.bbox_pred])

        # mask subnet
        if self.mask_on:
            mask_subnet = []
            cur_channels = in_channels
            for _ in range(num_convs):
                mask_subnet.append(
                    nn.Conv2d(
                        cur_channels, mask_channels, kernel_size=3, stride=1, padding=1
                    )
                )
                cur_channels = mask_channels
                mask_subnet.append(nn.ReLU())

            self.mask_subnet = nn.Sequential(*mask_subnet)
            modules_list.append(self.mask_subnet)
            for mask_size in self.mask_sizes:
                cur_mask_module = "mask_pred_%02d" % mask_size
                self.add_module(
                    cur_mask_module,
                    nn.Conv2d(
                        cur_channels,
                        mask_size * mask_size,
                        kernel_size=1,
                        stride=1,
                        padding=0,
                    ),
                )
                modules_list.append(getattr(self, cur_mask_module))
            if self.align_on:
                if self.bipyramid_on:
                    for lvl in range(num_levels):
                        cur_mask_module = "align2nat_%02d" % lvl
                        lambda_val = 2 ** lvl
                        setattr(self, cur_mask_module, SwapAlign2Nat(lambda_val))
                    # Also the fusing layer, stay at the same channel size
                    mask_fuse = [
                        nn.Conv2d(
                            cur_channels,
                            cur_channels,
                            kernel_size=3,
                            stride=1,
                            padding=1,
                        ),
                        nn.ReLU(),
                    ]
                    self.mask_fuse = nn.Sequential(*mask_fuse)
                    modules_list.append(self.mask_fuse)
                else:
                    self.align2nat = SwapAlign2Nat(1)

        # Initialization
        for modules in modules_list:
            for layer in modules.modules():
                if isinstance(layer, nn.Conv2d):
                    torch.nn.init.normal_(layer.weight, mean=0, std=0.01)
                    torch.nn.init.constant_(layer.bias, 0)

        # Use prior in model initialization to improve stability
        bias_value = -math.log((1 - 0.01) / 0.01)
        torch.nn.init.constant_(self.cls_score.bias, bias_value)

    def forward(self, features):
        """
        Arguments:
            features (list[Tensor]): FPN feature map tensors in high to low resolution.
                Each tensor in the list correspond to different feature levels.
        Returns:
            pred_logits (list[Tensor]): #lvl tensors, each has shape (N, AxK, Hi, Wi).
                The tensor predicts the classification probability
                at each spatial position for each of the A anchors and K object
                classes.
            pred_deltas (list[Tensor]): #lvl tensors, each has shape (N, Ax4, Hi, Wi).
                The tensor predicts 4-vector (dx,dy,dw,dh) box
                regression values for every anchor. These values are the
                relative offset between the anchor and the ground truth box.
            pred_masks (list(list[Tensor])): #lvl list of tensors, each is a list of
                A tensors of shape (N, M_{i,a}, Hi, Wi).
                The tensor predicts a dense set of M_ixM_i masks at every location.
        """
        pred_logits = [self.cls_score(self.cls_subnet(x)) for x in features]
        pred_deltas = [self.bbox_pred(self.bbox_subnet(x)) for x in features]

        pred_masks = None
        if self.mask_on:
            mask_feats = [self.mask_subnet(x) for x in features]

            if self.bipyramid_on:
                mask_feat_high_res = mask_feats[0]
                H, W = mask_feat_high_res.shape[-2:]
                mask_feats_up = []
                for lvl, mask_feat in enumerate(mask_feats):
                    lambda_val = 2.0 ** lvl
                    mask_feat_up = mask_feat
                    if lvl > 0:
                        mask_feat_up = F.interpolate(
                            mask_feat,
                            scale_factor=lambda_val,
                            mode="bilinear",
                            align_corners=False,
                        )
                    mask_feats_up.append(
                        self.mask_fuse(mask_feat_up[:, :, :H, :W] + mask_feat_high_res)
                    )
                mask_feats = mask_feats_up

            pred_masks = []
            for lvl, mask_feat in enumerate(mask_feats):
                cur_masks = []
                for mask_size in self.mask_sizes:
                    cur_mask_module = getattr(self, "mask_pred_%02d" % mask_size)
                    cur_mask = cur_mask_module(mask_feat)
                    if self.align_on:
                        if self.bipyramid_on:
                            cur_mask_module = getattr(self, "align2nat_%02d" % lvl)
                            cur_mask = cur_mask_module(cur_mask)
                        else:
                            cur_mask = self.align2nat(cur_mask)
                    cur_masks.append(cur_mask)
                pred_masks.append(cur_masks)
        return pred_logits, pred_deltas, pred_masks
```

#### cvpods/modeling/meta_arch/fcn.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.

from typing import Dict

import numpy as np

import torch
import torch.nn.functional as F
from torch import nn

from cvpods.layers import Conv2d, ConvTranspose2d, ShapeSpec


class FCNHead(nn.Module):
    """
    The head used in FCN for Semantic Segmentation.
    See: https://arxiv.org/abs/1605.06211 for more details.
    """

    def __init__(self, cfg, input_shape: Dict[str, ShapeSpec]):
        super().__init__()

        self.in_features = cfg.MODEL.SEM_SEG_HEAD.IN_FEATURES
        feature_strides = {k: v.stride for k, v in input_shape.items()}
        feature_channels = {k: v.channels for k, v in input_shape.items()}
        self.ignore_value = cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE
        num_classes = cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES
        self.loss_weight = cfg.MODEL.SEM_SEG_HEAD.LOSS_WEIGHT

        upsampling_strides = []
        feature_strides_list = list(feature_strides.values())
        upsampling_strides.append(feature_strides_list[0])
        feature_strides_list = feature_strides_list[::-1]
        for s1, s2 in zip(feature_strides_list[:], feature_strides_list[1:]):
            upsampling_strides.append(s1 // s2)
        assert len(upsampling_strides) == len(self.in_features)

        score_convs = []
        upsampling_convs = []
        for idx, in_feature in enumerate(self.in_features):
            ch = feature_channels[in_feature]
            score_convs.append(
                Conv2d(ch, num_classes, kernel_size=1)
            )
            stride = upsampling_strides[idx]
            upsampling_convs.append(
                ConvTranspose2d(
                    num_classes,
                    num_classes,
                    kernel_size=stride * 2,
                    stride=stride,
                    padding=1,
                    bias=False,
                )
            )
        self.score_convs = nn.ModuleList(score_convs)
        self.upsampling_convs = nn.ModuleList(upsampling_convs)
        self._initialize_weights()

    def _initialize_weights(self):
        # Ref: https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/surgery.py
        def get_upsampling_weight(in_channels, out_channels, kernel_size):
            """
            Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size.
            """
            factor = (kernel_size + 1) // 2
            if kernel_size % 2 == 1:
                center = factor - 1
            else:
                center = factor - 0.5
            og = np.ogrid[:kernel_size, :kernel_size]
            filt = (1 - abs(og[0] - center) / factor) * \
                (1 - abs(og[1] - center) / factor)
            weight = np.zeros(
                (in_channels, out_channels, kernel_size, kernel_size),
                dtype=np.float64
            )
            weight[range(in_channels), range(out_channels), :, :] = filt
            return torch.from_numpy(weight).float()

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                m.weight.data.zero_()
                if m.bias is not None:
                    m.bias.data.zero_()
            if isinstance(m, nn.ConvTranspose2d):
                assert m.kernel_size[0] == m.kernel_size[1]
                initial_weight = get_upsampling_weight(
                    m.in_channels, m.out_channels, m.kernel_size[0])
                m.weight.data.copy_(initial_weight)

    def forward(self, features, targets=None):
        """
        Returns:
            In training, returns (None, dict of losses)
            In inference, returns (CxHxW logits, {})
        """
        x = self.layers(features, ori_shape=targets.shape[-2:])
        if self.training:
            return None, self.losses(x, targets)
        else:
            return x, {}

    def layers(self, features, ori_shape):
        # NOTE The compute order is from back to front
        for i, f in zip(range(-1, -len(features) - 1, -1), self.in_features[::-1]):
            if i == -1:
                x = self.score_convs[i](features[f])
                pre = self.upsampling_convs[i](x)
            else:
                x = self.score_convs[i](features[f])
                # Crop
                h, w = pre.shape[-2:]
                crop_offset_h = (x.size(-2) - pre.size(-2)) // 2
                crop_offset_w = (x.size(-1) - pre.size(-1)) // 2
                cur = x[:, :, crop_offset_h: crop_offset_h + h, crop_offset_w: crop_offset_w + w]
                # Fuse
                x = pre + cur
                pre = self.upsampling_convs[i](x)

        h, w = ori_shape[-2:]
        crop_offset_h = (pre.size(-2) - ori_shape[-2]) // 2
        crop_offset_w = (pre.size(-1) - ori_shape[-1]) // 2
        x = pre[:, :, crop_offset_h: crop_offset_h + h, crop_offset_w: crop_offset_w + w]

        return x

    def losses(self, predictions, targets):
        loss = F.cross_entropy(
            predictions, targets, reduction="mean", ignore_index=self.ignore_value
        )
        losses = {"loss_sem_seg": loss * self.loss_weight}
        return losses
```

#### cvpods/modeling/meta_arch/pointrend.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

from typing import Dict, List, Optional, Tuple

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

from cvpods.layers import Conv2d, ShapeSpec, cat, interpolate
from cvpods.modeling.nn_utils import weight_init
from cvpods.modeling.roi_heads import StandardROIHeads, select_foreground_proposals
from cvpods.modeling.roi_heads.mask_head import mask_rcnn_inference, mask_rcnn_loss
from cvpods.structures import BitMasks, Boxes, ImageList, Instances
from cvpods.utils import get_event_storage


"""
Shape shorthand in this module:

    N: minibatch dimension size, i.e. the number of RoIs for instance segmenation or the
        number of images for semantic segmenation.
    R: number of ROIs, combined over all images, in the minibatch
    P: number of points
"""


def point_sample(input, point_coords, **kwargs):
    """
    A wrapper around :function:`torch.nn.functional.grid_sample` to support 3D point_coords tensors.
    Unlike :function:`torch.nn.functional.grid_sample` it assumes `point_coords` to lie inside
    [0, 1] x [0, 1] square.

    Args:
        input (Tensor): A tensor of shape (N, C, H, W) that contains features map on a H x W grid.
        point_coords (Tensor): A tensor of shape (N, P, 2) or (N, Hgrid, Wgrid, 2) that contains
        [0, 1] x [0, 1] normalized point coordinates.

    Returns:
        output (Tensor): A tensor of shape (N, C, P) or (N, C, Hgrid, Wgrid) that contains
            features for points in `point_coords`. The features are obtained via bilinear
            interplation from `input` the same way as :function:`torch.nn.functional.grid_sample`.
    """
    add_dim = False
    if point_coords.dim() == 3:
        add_dim = True
        point_coords = point_coords.unsqueeze(2)
    output = F.grid_sample(input, 2.0 * point_coords - 1.0, **kwargs)
    if add_dim:
        output = output.squeeze(3)
    return output


def generate_regular_grid_point_coords(R, side_size, device):
    """
    Generate regular square grid of points in [0, 1] x [0, 1] coordinate space.

    Args:
        R (int): The number of grids to sample, one for each region.
        side_size (int): The side size of the regular grid.
        device (torch.device): Desired device of returned tensor.

    Returns:
        (Tensor): A tensor of shape (R, side_size^2, 2) that contains coordinates
            for the regular grids.
    """
    aff = torch.tensor([[[0.5, 0, 0.5], [0, 0.5, 0.5]]], device=device)
    r = F.affine_grid(aff, torch.Size((1, 1, side_size, side_size)), align_corners=False)
    return r.view(1, -1, 2).expand(R, -1, -1)


def get_uncertain_point_coords_with_randomness(
    coarse_logits, uncertainty_func, num_points, oversample_ratio, importance_sample_ratio
):
    """
    Sample points in [0, 1] x [0, 1] coordinate space based on their uncertainty. The unceratinties
        are calculated for each point using 'uncertainty_func' function that takes point's logit
        prediction as input.
    See PointRend paper for details.

    Args:
        coarse_logits (Tensor): A tensor of shape (N, C, Hmask, Wmask) or (N, 1, Hmask, Wmask) for
            class-specific or class-agnostic prediction.
        uncertainty_func: A function that takes a Tensor of shape (N, C, P) or (N, 1, P) that
            contains logit predictions for P points and returns their uncertainties as a Tensor of
            shape (N, 1, P).
        num_points (int): The number of points P to sample.
        oversample_ratio (int): Oversampling parameter.
        importance_sample_ratio (float): Ratio of points that are sampled via importnace sampling.

    Returns:
        point_coords (Tensor): A tensor of shape (N, P, 2) that contains the coordinates of P
            sampled points.
    """
    assert oversample_ratio >= 1
    assert importance_sample_ratio <= 1 and importance_sample_ratio >= 0
    num_boxes = coarse_logits.shape[0]
    num_sampled = int(num_points * oversample_ratio)
    point_coords = torch.rand(num_boxes, num_sampled, 2, device=coarse_logits.device)
    point_logits = point_sample(coarse_logits, point_coords, align_corners=False)
    # It is crucial to calculate uncertainty based on the sampled prediction value for the points.
    # Calculating uncertainties of the coarse predictions first and sampling them for points leads
    # to incorrect results.
    # To illustrate this: assume uncertainty_func(logits)=-abs(logits), a sampled point between
    # two coarse predictions with -1 and 1 logits has 0 logits, and therefore 0 uncertainty value.
    # However, if we calculate uncertainties for the coarse predictions first,
    # both will have -1 uncertainty, and the sampled point will get -1 uncertainty.
    point_uncertainties = uncertainty_func(point_logits)
    num_uncertain_points = int(importance_sample_ratio * num_points)
    num_random_points = num_points - num_uncertain_points
    idx = torch.topk(point_uncertainties[:, 0, :], k=num_uncertain_points, dim=1)[1]
    shift = num_sampled * torch.arange(num_boxes, dtype=torch.long, device=coarse_logits.device)
    idx += shift[:, None]
    point_coords = point_coords.view(-1, 2)[idx.view(-1), :].view(
        num_boxes, num_uncertain_points, 2
    )
    if num_random_points > 0:
        point_coords = cat(
            [
                point_coords,
                torch.rand(num_boxes, num_random_points, 2, device=coarse_logits.device),
            ],
            dim=1,
        )
    return point_coords


def get_uncertain_point_coords_on_grid(uncertainty_map, num_points):
    """
    Find `num_points` most uncertain points from `uncertainty_map` grid.

    Args:
        uncertainty_map (Tensor): A tensor of shape (N, 1, H, W) that contains uncertainty
            values for a set of points on a regular H x W grid.
        num_points (int): The number of points P to select.

    Returns:
        point_indices (Tensor): A tensor of shape (N, P) that contains indices from
            [0, H x W) of the most uncertain points.
        point_coords (Tensor): A tensor of shape (N, P, 2) that contains [0, 1] x [0, 1] normalized
            coordinates of the most uncertain points from the H x W grid.
    """
    R, _, H, W = uncertainty_map.shape
    h_step = 1.0 / float(H)
    w_step = 1.0 / float(W)

    num_points = min(H * W, num_points)
    point_indices = torch.topk(uncertainty_map.view(R, H * W), k=num_points, dim=1)[1]
    point_coords = torch.zeros(R, num_points, 2, dtype=torch.float, device=uncertainty_map.device)
    point_coords[:, :, 0] = w_step / 2.0 + (point_indices % W).to(torch.float) * w_step
    point_coords[:, :, 1] = h_step / 2.0 + (point_indices // W).to(torch.float) * h_step
    return point_indices, point_coords


def point_sample_fine_grained_features(features_list, feature_scales, boxes, point_coords):
    """
    Get features from feature maps in `features_list` that correspond to specific point coordinates
        inside each bounding box from `boxes`.

    Args:
        features_list (list[Tensor]): A list of feature map tensors to get features from.
        feature_scales (list[float]): A list of scales for tensors in `features_list`.
        boxes (list[Boxes]): A list of I Boxes  objects that contain R_1 + ... + R_I = R boxes all
            together.
        point_coords (Tensor): A tensor of shape (R, P, 2) that contains
            [0, 1] x [0, 1] box-normalized coordinates of the P sampled points.

    Returns:
        point_features (Tensor): A tensor of shape (R, C, P) that contains features sampled
            from all features maps in feature_list for P sampled points for all R boxes in `boxes`.
        point_coords_wrt_image (Tensor): A tensor of shape (R, P, 2) that contains image-level
            coordinates of P points.
    """
    cat_boxes = Boxes.cat(boxes)
    num_boxes = [len(b) for b in boxes]

    point_coords_wrt_image = get_point_coords_wrt_image(cat_boxes.tensor, point_coords)
    split_point_coords_wrt_image = torch.split(point_coords_wrt_image, num_boxes)

    point_features = []
    for idx_img, point_coords_wrt_image_per_image in enumerate(split_point_coords_wrt_image):
        point_features_per_image = []
        for idx_feature, feature_map in enumerate(features_list):
            h, w = feature_map.shape[-2:]
            scale = torch.tensor([w, h], device=feature_map.device) / feature_scales[idx_feature]
            point_coords_scaled = point_coords_wrt_image_per_image / scale
            point_features_per_image.append(
                point_sample(
                    feature_map[idx_img].unsqueeze(0),
                    point_coords_scaled.unsqueeze(0),
                    align_corners=False,
                )
                .squeeze(0)
                .transpose(1, 0)
            )
        point_features.append(cat(point_features_per_image, dim=1))

    return cat(point_features, dim=0), point_coords_wrt_image


def get_point_coords_wrt_image(boxes_coords, point_coords):
    """
    Convert box-normalized [0, 1] x [0, 1] point cooordinates to image-level coordinates.

    Args:
        boxes_coords (Tensor): A tensor of shape (R, 4) that contains bounding boxes.
            coordinates.
        point_coords (Tensor): A tensor of shape (R, P, 2) that contains
            [0, 1] x [0, 1] box-normalized coordinates of the P sampled points.

    Returns:
        point_coords_wrt_image (Tensor): A tensor of shape (R, P, 2) that contains
            image-normalized coordinates of P sampled points.
    """
    with torch.no_grad():
        point_coords_wrt_image = point_coords.clone()
        point_coords_wrt_image[:, :, 0] = point_coords_wrt_image[:, :, 0] * (
            boxes_coords[:, None, 2] - boxes_coords[:, None, 0]
        )
        point_coords_wrt_image[:, :, 1] = point_coords_wrt_image[:, :, 1] * (
            boxes_coords[:, None, 3] - boxes_coords[:, None, 1]
        )
        point_coords_wrt_image[:, :, 0] += boxes_coords[:, None, 0]
        point_coords_wrt_image[:, :, 1] += boxes_coords[:, None, 1]
    return point_coords_wrt_image


def calculate_uncertainty_ins_seg(logits, classes):
    """
    We estimate uncerainty as L1 distance between 0.0 and the logit prediction in 'logits' for the
        foreground class in `classes`.

    Args:
        logits (Tensor): A tensor of shape (R, C, ...) or (R, 1, ...) for class-specific or
            class-agnostic, where R is the total number of predicted masks in all images and C is
            the number of foreground classes. The values are logits.
        classes (list): A list of length R that contains either predicted of ground truth class
            for eash predicted mask.

    Returns:
        scores (Tensor): A tensor of shape (R, 1, ...) that contains uncertainty scores with
            the most uncertain locations having the highest uncertainty score.
    """
    if logits.shape[1] == 1:
        gt_class_logits = logits.clone()
    else:
        gt_class_logits = logits[
            torch.arange(logits.shape[0], device=logits.device), classes
        ].unsqueeze(1)
    return -(torch.abs(gt_class_logits))


def roi_mask_point_loss(mask_logits, instances, points_coord):
    """
    Compute the point-based loss for instance segmentation mask predictions.

    Args:
        mask_logits (Tensor): A tensor of shape (R, C, P) or (R, 1, P) for class-specific or
            class-agnostic, where R is the total number of predicted masks in all images, C is the
            number of foreground classes, and P is the number of points sampled for each mask.
            The values are logits.
        instances (list[Instances]): A list of N Instances, where N is the number of images
            in the batch. These instances are in 1:1 correspondence with the `mask_logits`. So, i_th
            elememt of the list contains R_i objects and R_1 + ... + R_N is equal to R.
            The ground-truth labels (class, box, mask, ...) associated with each instance are stored
            in fields.
        points_coords (Tensor): A tensor of shape (R, P, 2), where R is the total number of
            predicted masks and P is the number of points for each mask. The coordinates are in
            the image pixel coordinate space, i.e. [0, H] x [0, W].
    Returns:
        point_loss (Tensor): A scalar tensor containing the loss.
    """
    with torch.no_grad():
        cls_agnostic_mask = mask_logits.size(1) == 1
        total_num_masks = mask_logits.size(0)

        gt_classes = []
        gt_mask_logits = []
        idx = 0
        for instances_per_image in instances:
            if len(instances_per_image) == 0:
                continue
            assert isinstance(
                instances_per_image.gt_masks, BitMasks
            ), "Point head works with GT in 'bitmask' format. Set INPUT.MASK_FORMAT to 'bitmask'."

            if not cls_agnostic_mask:
                gt_classes_per_image = instances_per_image.gt_classes.to(dtype=torch.int64)
                gt_classes.append(gt_classes_per_image)

            gt_bit_masks = instances_per_image.gt_masks.tensor
            h, w = instances_per_image.gt_masks.image_size
            scale = torch.tensor([w, h], dtype=torch.float, device=gt_bit_masks.device)
            points_coord_grid_sample_format = (
                points_coord[idx: idx + len(instances_per_image)] / scale
            )
            idx += len(instances_per_image)
            gt_mask_logits.append(
                point_sample(
                    gt_bit_masks.to(torch.float32).unsqueeze(1),
                    points_coord_grid_sample_format,
                    align_corners=False,
                ).squeeze(1)
            )

    if len(gt_mask_logits) == 0:
        return mask_logits.sum() * 0

    gt_mask_logits = cat(gt_mask_logits)
    assert gt_mask_logits.numel() > 0, gt_mask_logits.shape

    if cls_agnostic_mask:
        mask_logits = mask_logits[:, 0]
    else:
        indices = torch.arange(total_num_masks)
        gt_classes = cat(gt_classes, dim=0)
        mask_logits = mask_logits[indices, gt_classes]

    # Log the training accuracy (using gt classes and 0.0 threshold for the logits)
    mask_accurate = (mask_logits > 0.0) == gt_mask_logits.to(dtype=torch.uint8)
    mask_accuracy = mask_accurate.nonzero(as_tuple=False).size(0) / mask_accurate.numel()
    get_event_storage().put_scalar("point_rend/accuracy", mask_accuracy)

    point_loss = F.binary_cross_entropy_with_logits(
        mask_logits, gt_mask_logits.to(dtype=torch.float32), reduction="mean"
    )
    return point_loss


class PointRendROIHeads(StandardROIHeads):
    """
    The RoI heads class for PointRend instance segmentation models.

    In this class we redefine the mask head of `StandardROIHeads` leaving all other heads intact.
    To avoid namespace conflict with other heads we use names starting from `mask_` for all
    variables that correspond to the mask head in the class's namespace.
    """

    def __init__(self, cfg, input_shape):
        # TODO use explicit args style
        super().__init__(cfg, input_shape)
        self._init_mask_head(cfg)

    def _init_mask_head(self, cfg):
        # fmt: off
        self.mask_on                 = cfg.MODEL.MASK_ON
        if not self.mask_on:
            return
        self.mask_coarse_in_features = cfg.MODEL.ROI_MASK_HEAD.IN_FEATURES
        self.mask_coarse_side_size   = cfg.MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION
        self._feature_scales         = {k: 1.0 / v for k, v in self.feature_strides.items()}
        # fmt: on

        in_channels = np.sum([self.feature_channels[f] for f in self.mask_coarse_in_features])
        self.mask_coarse_head = cfg.build_mask_head(
            cfg,
            ShapeSpec(
                channels=in_channels,
                width=self.mask_coarse_side_size,
                height=self.mask_coarse_side_size,
            ),
        )
        self._init_point_head(cfg)

    def _init_point_head(self, cfg):
        # fmt: off
        self.mask_point_on                      = cfg.MODEL.ROI_MASK_HEAD.POINT_HEAD_ON
        if not self.mask_point_on:
            return
        assert cfg.MODEL.ROI_HEADS.NUM_CLASSES == cfg.MODEL.POINT_HEAD.NUM_CLASSES
        self.mask_point_in_features             = cfg.MODEL.POINT_HEAD.IN_FEATURES
        self.mask_point_train_num_points        = cfg.MODEL.POINT_HEAD.TRAIN_NUM_POINTS
        self.mask_point_oversample_ratio        = cfg.MODEL.POINT_HEAD.OVERSAMPLE_RATIO
        self.mask_point_importance_sample_ratio = cfg.MODEL.POINT_HEAD.IMPORTANCE_SAMPLE_RATIO
        # next two parameters are use in the adaptive subdivions inference procedure
        self.mask_point_subdivision_steps       = cfg.MODEL.POINT_HEAD.SUBDIVISION_STEPS
        self.mask_point_subdivision_num_points  = cfg.MODEL.POINT_HEAD.SUBDIVISION_NUM_POINTS
        # fmt: on

        in_channels = np.sum([self.feature_channels[f] for f in self.mask_point_in_features])
        self.mask_point_head = cfg.build_point_head(
            cfg, ShapeSpec(channels=in_channels, width=1, height=1)
        )

    def forward(
        self,
        images: ImageList,
        features: Dict[str, torch.Tensor],
        proposals: List[Instances],
        targets: Optional[List[Instances]] = None,
    ) -> Tuple[List[Instances], Dict[str, torch.Tensor]]:
        """
        See :class:`ROIHeads.forward`.
        """
        del images
        if self.training:
            assert targets
            proposals = self.label_and_sample_proposals(proposals, targets)
        del targets

        features_list = [features[f] for f in self.in_features]

        if self.training:
            losses = self._forward_box(features_list, proposals)
            # Usually the original proposals used by the box head are used by the mask, keypoint
            # heads. But when `self.train_on_pred_boxes is True`, proposals will contain boxes
            # predicted by the box head.
            losses.update(self._forward_mask(features, proposals))
            losses.update(self._forward_keypoint(features_list, proposals))
            return proposals, losses
        else:
            pred_instances = self._forward_box(features_list, proposals)
            # During inference cascaded prediction is used: the mask and keypoints heads are only
            # applied to the top scoring box detections.
            pred_instances = self.forward_with_given_boxes(features, pred_instances)
            return pred_instances, {}

    def forward_with_given_boxes(
        self, features: Dict[str, torch.Tensor], instances: List[Instances]
    ) -> List[Instances]:
        """
        Use the given boxes in `instances` to produce other (non-box) per-ROI outputs.

        This is useful for downstream tasks where a box is known, but need to obtain
        other attributes (outputs of other heads).
        Test-time augmentation also uses this.

        Args:
            features: same as in `forward()`
            instances (list[Instances]): instances to predict other outputs. Expect the keys
                pred_boxes" and "pred_classes" to exist.

        Returns:
            instances (list[Instances]):
                the same `Instances` objects, with extra
                fields such as `pred_masks` or `pred_keypoints`.
        """
        assert not self.training
        assert instances[0].has("pred_boxes") and instances[0].has("pred_classes")
        features_list = [features[f] for f in self.in_features]

        instances = self._forward_mask(features, instances)
        instances = self._forward_keypoint(features_list, instances)
        return instances

    def _forward_mask(self, features, instances):
        """
        Forward logic of the mask prediction branch.

        Args:
            features (dict[str, Tensor]): #level input features for mask prediction
            instances (list[Instances]): the per-image instances to train/predict masks.
                In training, they can be the proposals.
                In inference, they can be the predicted boxes.

        Returns:
            In training, a dict of losses.
            In inference, update `instances` with new fields "pred_masks" and return it.
        """
        if not self.mask_on:
            return {} if self.training else instances

        if self.training:
            proposals, _ = select_foreground_proposals(instances, self.num_classes)
            proposal_boxes = [x.proposal_boxes for x in proposals]
            mask_coarse_logits = self._forward_mask_coarse(features, proposal_boxes)

            losses = {"loss_mask": mask_rcnn_loss(mask_coarse_logits, proposals)}
            losses.update(self._forward_mask_point(features, mask_coarse_logits, proposals))
            return losses
        else:
            pred_boxes = [x.pred_boxes for x in instances]
            mask_coarse_logits = self._forward_mask_coarse(features, pred_boxes)

            mask_logits = self._forward_mask_point(features, mask_coarse_logits, instances)
            mask_rcnn_inference(mask_logits, instances)
            return instances

    def _forward_mask_coarse(self, features, boxes):
        """
        Forward logic of the coarse mask head.
        """
        point_coords = generate_regular_grid_point_coords(
            np.sum(len(x) for x in boxes), self.mask_coarse_side_size, boxes[0].device
        )
        mask_coarse_features_list = [features[k] for k in self.mask_coarse_in_features]
        features_scales = [self._feature_scales[k] for k in self.mask_coarse_in_features]
        # For regular grids of points, this function is equivalent to `len(features_list)' calls
        # of `ROIAlign` (with `SAMPLING_RATIO=2`), and concat the results.
        mask_features, _ = point_sample_fine_grained_features(
            mask_coarse_features_list, features_scales, boxes, point_coords
        )
        return self.mask_coarse_head(mask_features)

    def _forward_mask_point(self, features, mask_coarse_logits, instances):
        """
        Forward logic of the mask point head.
        """
        if not self.mask_point_on:
            return {} if self.training else mask_coarse_logits

        mask_features_list = [features[k] for k in self.mask_point_in_features]
        features_scales = [self._feature_scales[k] for k in self.mask_point_in_features]

        if self.training:
            proposal_boxes = [x.proposal_boxes for x in instances]
            gt_classes = cat([x.gt_classes for x in instances])
            with torch.no_grad():
                point_coords = get_uncertain_point_coords_with_randomness(
                    mask_coarse_logits,
                    lambda logits: calculate_uncertainty_ins_seg(logits, gt_classes),
                    self.mask_point_train_num_points,
                    self.mask_point_oversample_ratio,
                    self.mask_point_importance_sample_ratio,
                )

            fine_grained_features, point_coords_wrt_image = point_sample_fine_grained_features(
                mask_features_list, features_scales, proposal_boxes, point_coords
            )
            coarse_features = point_sample(mask_coarse_logits, point_coords, align_corners=False)
            point_logits = self.mask_point_head(fine_grained_features, coarse_features)
            return {
                "loss_mask_point": roi_mask_point_loss(
                    point_logits, instances, point_coords_wrt_image
                )
            }
        else:
            pred_boxes = [x.pred_boxes for x in instances]
            pred_classes = cat([x.pred_classes for x in instances])
            # The subdivision code will fail with the empty list of boxes
            if len(pred_classes) == 0:
                return mask_coarse_logits

            mask_logits = mask_coarse_logits.clone()
            for subdivions_step in range(self.mask_point_subdivision_steps):
                mask_logits = interpolate(
                    mask_logits, scale_factor=2, mode="bilinear", align_corners=False
                )
                # If `mask_point_subdivision_num_points` is larger or equal to the
                # resolution of the next step, then we can skip this step
                H, W = mask_logits.shape[-2:]
                if (
                    self.mask_point_subdivision_num_points >= 4 * H * W
                    and subdivions_step < self.mask_point_subdivision_steps - 1
                ):
                    continue
                uncertainty_map = calculate_uncertainty_ins_seg(mask_logits, pred_classes)
                point_indices, point_coords = get_uncertain_point_coords_on_grid(
                    uncertainty_map, self.mask_point_subdivision_num_points
                )
                fine_grained_features, _ = point_sample_fine_grained_features(
                    mask_features_list, features_scales, pred_boxes, point_coords
                )
                coarse_features = point_sample(
                    mask_coarse_logits, point_coords, align_corners=False
                )
                point_logits = self.mask_point_head(fine_grained_features, coarse_features)

                # put mask point predictions to the right places on the upsampled grid.
                R, C, H, W = mask_logits.shape
                point_indices = point_indices.unsqueeze(1).expand(-1, C, -1)
                mask_logits = (
                    mask_logits.reshape(R, C, H * W)
                    .scatter_(2, point_indices, point_logits)
                    .view(R, C, H, W)
                )
            return mask_logits


class CoarseMaskHead(nn.Module):
    """
    A mask head with fully connected layers. Given pooled features it first reduces channels and
    spatial dimensions with conv layers and then uses FC layers to predict coarse masks analogously
    to the standard box head.
    """

    def __init__(self, cfg, input_shape: ShapeSpec):
        """
        The following attributes are parsed from config:
            conv_dim: the output dimension of the conv layers
            fc_dim: the feature dimenstion of the FC layers
            num_fc: the number of FC layers
            output_side_resolution: side resolution of the output square mask prediction
        """
        super(CoarseMaskHead, self).__init__()

        # fmt: off
        self.num_classes            = cfg.MODEL.ROI_HEADS.NUM_CLASSES
        conv_dim                    = cfg.MODEL.ROI_MASK_HEAD.CONV_DIM
        self.fc_dim                 = cfg.MODEL.ROI_MASK_HEAD.FC_DIM
        num_fc                      = cfg.MODEL.ROI_MASK_HEAD.NUM_FC
        self.output_side_resolution = cfg.MODEL.ROI_MASK_HEAD.OUTPUT_SIDE_RESOLUTION
        self.input_channels         = input_shape.channels
        self.input_h                = input_shape.height
        self.input_w                = input_shape.width
        # fmt: on

        self.conv_layers = []
        if self.input_channels > conv_dim:
            self.reduce_channel_dim_conv = Conv2d(
                self.input_channels,
                conv_dim,
                kernel_size=1,
                stride=1,
                padding=0,
                bias=True,
                activation=F.relu,
            )
            self.conv_layers.append(self.reduce_channel_dim_conv)

        self.reduce_spatial_dim_conv = Conv2d(
            conv_dim, conv_dim, kernel_size=2, stride=2, padding=0, bias=True, activation=F.relu
        )
        self.conv_layers.append(self.reduce_spatial_dim_conv)

        input_dim = conv_dim * self.input_h * self.input_w
        input_dim //= 4

        self.fcs = []
        for k in range(num_fc):
            fc = nn.Linear(input_dim, self.fc_dim)
            self.add_module("coarse_mask_fc{}".format(k + 1), fc)
            self.fcs.append(fc)
            input_dim = self.fc_dim

        output_dim = self.num_classes * self.output_side_resolution * self.output_side_resolution

        self.prediction = nn.Linear(self.fc_dim, output_dim)
        # use normal distribution initialization for mask prediction layer
        nn.init.normal_(self.prediction.weight, std=0.001)
        nn.init.constant_(self.prediction.bias, 0)

        for layer in self.conv_layers:
            weight_init.c2_msra_fill(layer)
        for layer in self.fcs:
            weight_init.c2_xavier_fill(layer)

    def forward(self, x):
        # unlike BaseMaskRCNNHead, this head only outputs intermediate
        # features, because the features will be used later by PointHead.
        N = x.shape[0]
        x = x.view(N, self.input_channels, self.input_h, self.input_w)
        for layer in self.conv_layers:
            x = layer(x)
        x = torch.flatten(x, start_dim=1)
        for layer in self.fcs:
            x = F.relu(layer(x))
        return self.prediction(x).view(
            N, self.num_classes, self.output_side_resolution, self.output_side_resolution
        )


class StandardPointHead(nn.Module):
    """
    A point head multi-layer perceptron which we model with conv1d layers with kernel 1. The head
    takes both fine-grained and coarse prediction features as its input.
    """

    def __init__(self, cfg, input_shape: ShapeSpec):
        """
        The following attributes are parsed from config:
            fc_dim: the output dimension of each FC layers
            num_fc: the number of FC layers
            coarse_pred_each_layer: if True, coarse prediction features are concatenated to each
                layer's input
        """
        super(StandardPointHead, self).__init__()
        # fmt: off
        num_classes = cfg.MODEL.POINT_HEAD.NUM_CLASSES
        fc_dim = cfg.MODEL.POINT_HEAD.FC_DIM
        num_fc = cfg.MODEL.POINT_HEAD.NUM_FC
        cls_agnostic_mask = cfg.MODEL.POINT_HEAD.CLS_AGNOSTIC_MASK
        self.coarse_pred_each_layer = cfg.MODEL.POINT_HEAD.COARSE_PRED_EACH_LAYER
        input_channels = input_shape.channels
        # fmt: on

        fc_dim_in = input_channels + num_classes
        self.fc_layers = []
        for k in range(num_fc):
            fc = nn.Conv1d(fc_dim_in, fc_dim, kernel_size=1, stride=1, padding=0, bias=True)
            self.add_module("fc{}".format(k + 1), fc)
            self.fc_layers.append(fc)
            fc_dim_in = fc_dim
            fc_dim_in += num_classes if self.coarse_pred_each_layer else 0

        num_mask_classes = 1 if cls_agnostic_mask else num_classes
        self.predictor = nn.Conv1d(fc_dim_in, num_mask_classes, kernel_size=1, stride=1, padding=0)

        for layer in self.fc_layers:
            weight_init.c2_msra_fill(layer)
        # use normal distribution initialization for mask prediction layer
        nn.init.normal_(self.predictor.weight, std=0.001)
        if self.predictor.bias is not None:
            nn.init.constant_(self.predictor.bias, 0)

    def forward(self, fine_grained_features, coarse_features):
        x = torch.cat((fine_grained_features, coarse_features), dim=1)
        for layer in self.fc_layers:
            x = F.relu(layer(x))
            if self.coarse_pred_each_layer:
                x = cat((x, coarse_features), dim=1)
        return self.predictor(x)


# =============== For Semantic Segmentation Task ===============

def calculate_uncertainty_sem_seg(sem_seg_logits):
    """
    For each location of the prediction `sem_seg_logits` we estimate uncerainty as the
        difference between top first and top second predicted logits.
    Args:
        mask_logits (Tensor): A tensor of shape (N, C, ...), where N is the minibatch size and
            C is the number of foreground classes. The values are logits.
    Returns:
        scores (Tensor): A tensor of shape (N, 1, ...) that contains uncertainty scores with
            the most uncertain locations having the highest uncertainty score.
    """
    top2_scores = torch.topk(sem_seg_logits, k=2, dim=1)[0]
    return (top2_scores[:, 1] - top2_scores[:, 0]).unsqueeze(1)


class PointRendSemSegHead(nn.Module):
    """
    A semantic segmentation head that combines a head set in `POINT_HEAD.COARSE_SEM_SEG_HEAD_NAME`
        and a point head set in `MODEL.POINT_HEAD.NAME`.
    """

    def __init__(self, cfg, input_shape: Dict[str, ShapeSpec]):
        super().__init__()

        self.ignore_value = cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE

        self.coarse_sem_seg_head = cfg.build_coarse_sem_seg_head(cfg, input_shape)
        self._init_point_head(cfg, input_shape)

    def _init_point_head(self, cfg, input_shape: Dict[str, ShapeSpec]):
        # fmt: off
        assert cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES == cfg.MODEL.POINT_HEAD.NUM_CLASSES
        feature_channels = {k: v.channels for k, v in input_shape.items()}
        self.in_features = cfg.MODEL.POINT_HEAD.IN_FEATURES
        self.train_num_points = cfg.MODEL.POINT_HEAD.TRAIN_NUM_POINTS
        self.oversample_ratio = cfg.MODEL.POINT_HEAD.OVERSAMPLE_RATIO
        self.importance_sample_ratio = cfg.MODEL.POINT_HEAD.IMPORTANCE_SAMPLE_RATIO
        self.subdivision_steps = cfg.MODEL.POINT_HEAD.SUBDIVISION_STEPS
        self.subdivision_num_points = cfg.MODEL.POINT_HEAD.SUBDIVISION_NUM_POINTS
        # fmt: on

        in_channels = np.sum([feature_channels[f] for f in self.in_features])
        self.point_head = cfg.build_point_head(
            cfg, ShapeSpec(channels=in_channels, width=1, height=1))

    def forward(self, features, targets=None):
        coarse_sem_seg_logits = self.coarse_sem_seg_head.layers(features)

        if self.training:
            losses = self.coarse_sem_seg_head.losses(coarse_sem_seg_logits, targets)

            with torch.no_grad():
                point_coords = get_uncertain_point_coords_with_randomness(
                    coarse_sem_seg_logits,
                    calculate_uncertainty_sem_seg,
                    self.train_num_points,
                    self.oversample_ratio,
                    self.importance_sample_ratio,
                )
            coarse_features = point_sample(coarse_sem_seg_logits, point_coords, align_corners=False)

            fine_grained_features = cat(
                [
                    point_sample(features[in_feature], point_coords, align_corners=False)
                    for in_feature in self.in_features
                ]
            )
            point_logits = self.point_head(fine_grained_features, coarse_features)
            point_targets = (
                point_sample(
                    targets.unsqueeze(1).to(torch.float),
                    point_coords,
                    mode="nearest",
                    align_corners=False,
                )
                .squeeze(1)
                .to(torch.long)
            )
            losses["loss_sem_seg_point"] = F.cross_entropy(
                point_logits, point_targets, reduction="mean", ignore_index=self.ignore_value
            )
            return None, losses
        else:
            sem_seg_logits = coarse_sem_seg_logits.clone()
            for _ in range(self.subdivision_steps):
                sem_seg_logits = F.interpolate(
                    sem_seg_logits, scale_factor=2, mode="bilinear", align_corners=False
                )
                uncertainty_map = calculate_uncertainty_sem_seg(sem_seg_logits)
                point_indices, point_coords = get_uncertain_point_coords_on_grid(
                    uncertainty_map, self.subdivision_num_points
                )
                fine_grained_features = cat(
                    [
                        point_sample(features[in_feature], point_coords, align_corners=False)
                        for in_feature in self.in_features
                    ]
                )
                coarse_features = point_sample(
                    coarse_sem_seg_logits, point_coords, align_corners=False
                )
                point_logits = self.point_head(fine_grained_features, coarse_features)

                # put sem seg point predictions to the right places on the upsampled grid.
                N, C, H, W = sem_seg_logits.shape
                point_indices = point_indices.unsqueeze(1).expand(-1, C, -1)
                sem_seg_logits = (
                    sem_seg_logits.reshape(N, C, H * W)
                    .scatter_(2, point_indices, point_logits)
                    .view(N, C, H, W)
                )
            return sem_seg_logits, {}
```

#### cvpods/modeling/meta_arch/efficientdet.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import logging
import math
from typing import List

import torch
import torch.nn as nn

from cvpods.layers import (
    MemoryEfficientSwish,
    SeparableConvBlock,
    ShapeSpec,
    Swish,
    cat,
    generalized_batched_nms,
    get_norm
)
from cvpods.modeling.box_regression import Box2BoxTransform
from cvpods.modeling.losses import sigmoid_focal_loss_jit, smooth_l1_loss
from cvpods.modeling.matcher import Matcher
from cvpods.modeling.meta_arch.retinanet import (
    permute_all_cls_and_box_to_N_HWA_K_and_concat,
    permute_to_N_HWA_K
)
from cvpods.modeling.postprocessing import detector_postprocess
from cvpods.structures import Boxes, ImageList, Instances, pairwise_iou
from cvpods.utils import log_first_n


class EfficientDet(nn.Module):
    """
    Implement EfficientDet(https://arxiv.org/abs/1911.09070).
    See: https://arxiv.org/pdf/1911.09070.pdf for more details.
    """

    def __init__(self, cfg):
        super().__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)

        self.num_classes = cfg.MODEL.EFFICIENTDET.NUM_CLASSES
        self.in_features = cfg.MODEL.EFFICIENTDET.IN_FEATURES
        self.freeze_bn = cfg.MODEL.EFFICIENTDET.FREEZE_BN
        self.freeze_backbone = cfg.MODEL.EFFICIENTDET.FREEZE_BACKBONE
        self.input_size = cfg.MODEL.BIFPN.INPUT_SIZE
        # Loss parameters:
        self.focal_loss_alpha = cfg.MODEL.EFFICIENTDET.FOCAL_LOSS_ALPHA
        self.focal_loss_gamma = cfg.MODEL.EFFICIENTDET.FOCAL_LOSS_GAMMA
        self.smooth_l1_loss_beta = cfg.MODEL.EFFICIENTDET.SMOOTH_L1_LOSS_BETA
        self.box_loss_weight = cfg.MODEL.EFFICIENTDET.BOX_LOSS_WEIGHT
        self.regress_norm = cfg.MODEL.EFFICIENTDET.REG_NORM
        # Inference parameters:
        self.score_threshold = cfg.MODEL.EFFICIENTDET.SCORE_THRESH_TEST
        self.topk_candidates = cfg.MODEL.EFFICIENTDET.TOPK_CANDIDATES_TEST
        self.nms_threshold = cfg.MODEL.EFFICIENTDET.NMS_THRESH_TEST
        self.nms_type = cfg.MODEL.NMS_TYPE
        self.max_detections_per_image = cfg.TEST.DETECTIONS_PER_IMAGE

        self.backbone = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))

        backbone_shape = self.backbone.output_shape()
        feature_shapes = [backbone_shape[f] for f in self.in_features]
        self.head = EfficientDetHead(cfg, feature_shapes)
        self.anchor_generator = cfg.build_anchor_generator(cfg, feature_shapes)

        # Matching and loss
        self.box2box_transform = Box2BoxTransform(
            weights=cfg.MODEL.EFFICIENTDET.BBOX_REG_WEIGHTS)
        self.matcher = Matcher(
            cfg.MODEL.EFFICIENTDET.IOU_THRESHOLDS,
            cfg.MODEL.EFFICIENTDET.IOU_LABELS,
            allow_low_quality_matches=False,
        )

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(
            3, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(
            3, 1, 1)
        self.normalizer = lambda x: (x / 255. - pixel_mean) / pixel_std

        if self.freeze_bn:
            for layer in self.modules():
                if isinstance(layer, nn.BatchNorm2d):
                    layer.eval()

        if self.freeze_backbone:
            for name, params in self.named_parameters():
                if name.startswith("backbone.bottom_up"):
                    params.requires_grad = False

        self.to(self.device)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
                Each item in the list contains the inputs for one image.
                For now, each item in the list is a dict that contains:

                * image: Tensor, image in (C, H, W) format.
                * instances: Instances

                Other information that's included in the original dicts, such as:

                * "height", "width" (int): the output resolution of the model, used in inference.
                    See :meth:`postprocess` for details.
        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a tensor storing the loss. Used during training only.
        """
        images = self.preprocess_image(batched_inputs)

        if "instances" in batched_inputs[0]:
            gt_instances = [
                x["instances"].to(self.device) for x in batched_inputs
            ]
        elif "targets" in batched_inputs[0]:
            log_first_n(logging.WARN,
                        "'targets' in the model inputs is now renamed to 'instances'!",
                        n=10)
            gt_instances = [
                x["targets"].to(self.device) for x in batched_inputs
            ]
        else:
            gt_instances = None

        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]
        box_cls, box_delta = self.head(features)
        anchors = self.anchor_generator(features)

        if self.training:
            gt_classes, gt_anchors_reg_deltas = self.get_ground_truth(
                anchors, gt_instances)
            return self.losses(gt_classes, gt_anchors_reg_deltas, box_cls,
                               box_delta)
        else:
            results = self.inference(box_cls, box_delta, anchors, images)
            processed_results = []
            for results_per_image, input_per_image, image_size in zip(
                    results, batched_inputs, images.image_sizes):
                height = input_per_image.get("height", image_size[0])
                width = input_per_image.get("width", image_size[1])
                r = detector_postprocess(results_per_image, height, width)
                processed_results.append({"instances": r})
            return processed_results

    def losses(self, gt_classes, gt_anchors_deltas, pred_class_logits,
               pred_anchor_deltas):
        """
        Args:
            For `gt_classes` and `gt_anchors_deltas` parameters, see
                :meth:`EfficientDet.get_ground_truth`.
            Their shapes are (N, R) and (N, R, 4), respectively, where R is
            the total number of anchors across levels, i.e. sum(Hi x Wi x A)
            For `pred_class_logits` and `pred_anchor_deltas`, see
                :meth:`EfficientDetHead.forward`.

        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a scalar tensor
                storing the loss. Used during training only. The dict keys are:
                "loss_cls" and "loss_box_reg"
        """
        pred_class_logits, pred_anchor_deltas = permute_all_cls_and_box_to_N_HWA_K_and_concat(
            pred_class_logits, pred_anchor_deltas, self.num_classes
        )  # Shapes: (N x R, K) and (N x R, 4), respectively.

        gt_classes = gt_classes.flatten()
        gt_anchors_deltas = gt_anchors_deltas.view(-1, 4)

        valid_idxs = gt_classes >= 0
        foreground_idxs = (gt_classes >= 0) & (gt_classes != self.num_classes)
        num_foreground = foreground_idxs.sum()

        gt_classes_target = torch.zeros_like(pred_class_logits)
        gt_classes_target[foreground_idxs, gt_classes[foreground_idxs]] = 1

        # Classification loss
        loss_cls = sigmoid_focal_loss_jit(
            pred_class_logits[valid_idxs],
            gt_classes_target[valid_idxs],
            alpha=self.focal_loss_alpha,
            gamma=self.focal_loss_gamma,
            reduction="sum",
        ) / max(1, num_foreground)

        # Regression loss, refer to the official released code.
        # See: https://github.com/google/automl/blob/master/efficientdet/det_model_fn.py
        loss_box_reg = self.box_loss_weight * self.smooth_l1_loss_beta * smooth_l1_loss(
            pred_anchor_deltas[foreground_idxs],
            gt_anchors_deltas[foreground_idxs],
            beta=self.smooth_l1_loss_beta,
            reduction="sum",
        ) / max(1, num_foreground * self.regress_norm)

        return {"loss_cls": loss_cls, "loss_box_reg": loss_box_reg}

    @torch.no_grad()
    def get_ground_truth(self, anchors, targets):
        """
        Args:
            anchors (list[list[Boxes]]): a list of N=#image elements. Each is a
                list of #feature level Boxes. The Boxes contains anchors of
                this image on the specific feature level.
            targets (list[Instances]): a list of N `Instances`s. The i-th
                `Instances` contains the ground-truth per-instance annotations
                for the i-th input image.  Specify `targets` during training only.

        Returns:
            gt_classes (Tensor):
                An integer tensor of shape (N, R) storing ground-truth
                labels for each anchor.
                R is the total number of anchors, i.e. the sum of Hi x Wi x A for all levels.
                Anchors with an IoU with some target higher than the foreground threshold
                are assigned their corresponding label in the [0, K-1] range.
                Anchors whose IoU are below the background threshold are assigned
                the label "K". Anchors whose IoU are between the foreground and background
                thresholds are assigned a label "-1", i.e. ignore.
            gt_anchors_deltas (Tensor):
                Shape (N, R, 4).
                The last dimension represents ground-truth box2box transform
                targets (dx, dy, dw, dh) that map each anchor to its matched ground-truth box.
                The values in the tensor are meaningful only when the corresponding
                anchor is labeled as foreground.
        """
        gt_classes = []
        gt_anchors_deltas = []
        anchors = [Boxes.cat(anchors_i) for anchors_i in anchors]
        # list[Tensor(R, 4)], one for each image

        for anchors_per_image, targets_per_image in zip(anchors, targets):
            match_quality_matrix = pairwise_iou(targets_per_image.gt_boxes,
                                                anchors_per_image)
            gt_matched_idxs, anchor_labels = self.matcher(match_quality_matrix)

            has_gt = len(targets_per_image) > 0
            if has_gt:
                # ground truth box regression
                matched_gt_boxes = targets_per_image.gt_boxes[gt_matched_idxs]
                gt_anchors_reg_deltas_i = self.box2box_transform.get_deltas(
                    anchors_per_image.tensor, matched_gt_boxes.tensor
                )

                gt_classes_i = targets_per_image.gt_classes[gt_matched_idxs]
                # Anchors with label 0 are treated as background.
                gt_classes_i[anchor_labels == 0] = self.num_classes
                # Anchors with label -1 are ignored.
                gt_classes_i[anchor_labels == -1] = -1
            else:
                gt_classes_i = torch.zeros_like(
                    gt_matched_idxs) + self.num_classes
                gt_anchors_reg_deltas_i = torch.zeros_like(
                    anchors_per_image.tensor)

            gt_classes.append(gt_classes_i)
            gt_anchors_deltas.append(gt_anchors_reg_deltas_i)

        return torch.stack(gt_classes), torch.stack(gt_anchors_deltas)

    def inference(self, box_cls, box_delta, anchors, images):
        """
        Args:
            box_cls, box_delta: same as the output of :meth:`EfficientDetHead.forward`
            anchors (list[list[Boxes]]): a list of #images elements. Each is a
                list of #feature level Boxes. The Boxes contain anchors of this
                image on the specific feature level.
            images (ImageList): the input images.

        Returns:
            results (List[Instances]): a list of #images elements.
        """
        assert len(anchors) == len(images)
        results = []

        box_cls = [permute_to_N_HWA_K(x, self.num_classes) for x in box_cls]
        box_delta = [permute_to_N_HWA_K(x, 4) for x in box_delta]
        # list[Tensor], one per level, each has shape (N, Hi x Wi x A, K or 4)

        for img_idx, anchors_per_image in enumerate(anchors):
            image_size = images.image_sizes[img_idx]
            box_cls_per_image = [
                box_cls_per_level[img_idx] for box_cls_per_level in box_cls
            ]
            box_reg_per_image = [
                box_reg_per_level[img_idx] for box_reg_per_level in box_delta
            ]
            results_per_image = self.inference_single_image(
                box_cls_per_image, box_reg_per_image, anchors_per_image,
                tuple(image_size))
            results.append(results_per_image)
        return results

    def inference_single_image(self, box_cls, box_delta, anchors, image_size):
        """
        Single-image inference. Return bounding-box detection results by thresholding
        on scores and applying non-maximum suppression (NMS).

        Arguments:
            box_cls (list[Tensor]): list of #feature levels. Each entry contains
                tensor of size (H x W x A, K)
            box_delta (list[Tensor]): Same shape as 'box_cls' except that K becomes 4.
            anchors (list[Boxes]): list of #feature levels. Each entry contains
                a Boxes object, which contains all the anchors for that
                image in that feature level.
            image_size (tuple(H, W)): a tuple of the image height and width.

        Returns:
            Same as `inference`, but for only one image.
        """
        boxes_all = []
        scores_all = []
        class_idxs_all = []

        # Iterate over every feature level
        for box_cls_i, box_reg_i, anchors_i in zip(box_cls, box_delta, anchors):
            # (HxWxAxK,)
            box_cls_i = box_cls_i.flatten().sigmoid_()

            # Keep top k top scoring indices only.
            num_topk = min(self.topk_candidates, box_reg_i.size(0))
            # torch.sort is actually faster than .topk (at least on GPUs)
            predicted_prob, topk_idxs = box_cls_i.sort(descending=True)
            predicted_prob = predicted_prob[:num_topk]
            topk_idxs = topk_idxs[:num_topk]

            # filter out the proposals with low confidence score
            keep_idxs = predicted_prob > self.score_threshold
            predicted_prob = predicted_prob[keep_idxs]
            topk_idxs = topk_idxs[keep_idxs]

            anchor_idxs = topk_idxs // self.num_classes
            classes_idxs = topk_idxs % self.num_classes

            box_reg_i = box_reg_i[anchor_idxs]
            anchors_i = anchors_i[anchor_idxs]
            # predict boxes
            predicted_boxes = self.box2box_transform.apply_deltas(
                box_reg_i, anchors_i.tensor)

            boxes_all.append(predicted_boxes)
            scores_all.append(predicted_prob)
            class_idxs_all.append(classes_idxs)

        boxes_all, scores_all, class_idxs_all = [
            cat(x) for x in [boxes_all, scores_all, class_idxs_all]
        ]

        keep = generalized_batched_nms(boxes_all, scores_all, class_idxs_all,
                                       self.nms_threshold, nms_type=self.nms_type)
        keep = keep[:self.max_detections_per_image]

        result = Instances(image_size)
        result.pred_boxes = Boxes(boxes_all[keep])
        result.scores = scores_all[keep]
        result.pred_classes = class_idxs_all[keep]
        return result

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images,
                                        self.backbone.size_divisibility,
                                        pad_ref_long=True,
                                        pad_value=0.0)
        return images


class EfficientDetHead(nn.Module):
    """
    The head used in EfficientDet for object classification and box regression.
    It has two subnets for the two tasks, with a common structure but separate parameters.
    """

    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super().__init__()
        in_channels = input_shape[0].channels
        num_classes = cfg.MODEL.EFFICIENTDET.NUM_CLASSES
        norm = cfg.MODEL.EFFICIENTDET.HEAD.NORM
        bn_momentum = cfg.MODEL.EFFICIENTDET.HEAD.BN_MOMENTUM
        bn_eps = cfg.MODEL.EFFICIENTDET.HEAD.BN_EPS
        prior_prob = cfg.MODEL.EFFICIENTDET.HEAD.PRIOR_PROB
        memory_efficient = cfg.MODEL.EFFICIENTDET.HEAD.MEMORY_EFFICIENT_SWISH
        num_conv_layers = cfg.MODEL.EFFICIENTDET.HEAD.NUM_CONV
        num_anchors = cfg.build_anchor_generator(
            cfg, input_shape).num_cell_anchors

        self.bn_momentum = bn_momentum
        self.bn_eps = bn_eps
        self.prior_prob = prior_prob

        assert (
            len(set(num_anchors)) == 1
        ), "Using different number of anchors between levels is not currently supported!"

        num_anchors = num_anchors[0]
        self.cls_subnet = nn.ModuleList([])
        self.bbox_subnet = nn.ModuleList([])
        for _ in range(num_conv_layers):
            self.cls_subnet.append(
                SeparableConvBlock(in_channels, in_channels, kernel_size=3, padding="SAME"))
            self.bbox_subnet.append(
                SeparableConvBlock(in_channels, in_channels, kernel_size=3, padding="SAME"))

        num_levels = len(input_shape)
        self.bn_cls_subnet = nn.ModuleList()
        self.bn_bbox_subnet = nn.ModuleList()
        for _ in range(num_levels):
            self.bn_cls_subnet.append(
                nn.ModuleList([
                    get_norm(norm, in_channels)
                    for _ in range(num_conv_layers)
                ])
            )
            self.bn_bbox_subnet.append(
                nn.ModuleList([
                    get_norm(norm, in_channels)
                    for _ in range(num_conv_layers)
                ])
            )

        self.cls_score = SeparableConvBlock(in_channels,
                                            num_anchors * num_classes,
                                            kernel_size=3,
                                            padding="SAME")
        self.bbox_pred = SeparableConvBlock(in_channels,
                                            num_anchors * 4,
                                            kernel_size=3,
                                            padding="SAME")
        self.act = MemoryEfficientSwish() if memory_efficient else Swish()
        self._init_weights()

    def _init_weights(self):
        """
        Weight initialization as per Tensorflow official implementations.
        See: https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/ops/init_ops.py
             #L437
        """
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                fan_in = m.kernel_size[0] * m.kernel_size[1] * m.in_channels
                stddev = math.sqrt(1. / max(1., fan_in))
                m.weight.data.normal_(0, stddev)
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                if self.bn_momentum is not None and self.bn_eps is not None:
                    m.momentum = self.bn_momentum
                    m.eps = self.bn_eps
                m.weight.data.fill_(1)
                m.bias.data.zero_()

        # Use prior in model initialization to improve stability
        bias_value = -math.log((1 - self.prior_prob) / self.prior_prob)
        torch.nn.init.constant_(self.cls_score.bias, bias_value)

    def forward(self, features):
        """
        Arguments:
            features (list[Tensor]): FPN feature map tensors in high to low resolution.
                Each tensor in the list correspond to different feature levels.

        Returns:
            #lvl tensors, each has shape (N, AxK, Hi, Wi).
            logits (list[Tensor]):
                The tensor predicts the classification probability
                at each spatial position for each of the A anchors and K object
                classes.
            #lvl tensors, each has shape (N, Ax4, Hi, Wi).
            bbox_reg (list[Tensor]):
                The tensor predicts 4-vector (dx,dy,dw,dh) box
                regression values for every anchor. These values are the
                relative offset between the anchor and the ground truth box.
        """
        logits = []
        bbox_reg = []
        # each level
        for feature_i, bn_cls_level_i, bn_bbox_level_i in zip(
                features, self.bn_cls_subnet, self.bn_bbox_subnet):
            feature_i_cls = feature_i
            feature_i_bbox = feature_i
            for bn_cls_level_i_depth_i, bn_bbox_level_i_depth_i, cls_subnet_i, bbox_subnet_i in zip(
                    bn_cls_level_i, bn_bbox_level_i, self.cls_subnet, self.bbox_subnet):
                feature_i_cls = self.act(
                    bn_cls_level_i_depth_i(cls_subnet_i(feature_i_cls)))
                feature_i_bbox = self.act(
                    bn_bbox_level_i_depth_i(bbox_subnet_i(feature_i_bbox)))
            logits.append(self.cls_score(feature_i_cls))
            bbox_reg.append(self.bbox_pred(feature_i_bbox))

        return logits, bbox_reg
```

#### cvpods/modeling/meta_arch/ssd.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.

import logging
import math
from functools import partial
from itertools import product as product
from math import sqrt as sqrt
from typing import List

import torch
import torch.nn as nn
import torch.nn.functional as F

from cvpods.layers import ShapeSpec, cat, generalized_batched_nms
from cvpods.modeling.box_regression import Box2BoxTransform
from cvpods.modeling.matcher import Matcher
from cvpods.modeling.postprocessing import detector_postprocess
from cvpods.structures import Boxes, ImageList, Instances, pairwise_iou
from cvpods.utils import log_first_n


def multi_apply(func, *args, **kwargs):
    pfunc = partial(func, **kwargs) if kwargs else func
    map_results = map(pfunc, *args)
    return tuple(map(list, zip(*map_results)))


class SSD(nn.Module):
    """
    Implement SSD (https://arxiv.org/abs/1512.02325).
    See: https://arxiv.org/pdf/1512.02325.pdf for more details.
    """

    def __init__(self, cfg):
        super().__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)

        # fmt: off
        self.image_size = cfg.MODEL.SSD.IMAGE_SIZE
        self.num_classes = cfg.MODEL.SSD.NUM_CLASSES
        self.in_features = cfg.MODEL.SSD.IN_FEATURES
        self.extra_layer_arch = cfg.MODEL.SSD.EXTRA_LAYER_ARCH[str(self.image_size)]
        self.l2norm_scale = cfg.MODEL.SSD.L2NORM_SCALE
        # Loss parameters:
        self.loss_alpha = cfg.MODEL.SSD.LOSS_ALPHA
        self.smooth_l1_loss_beta = cfg.MODEL.SSD.SMOOTH_L1_LOSS_BETA
        self.negative_positive_ratio = cfg.MODEL.SSD.NEGATIVE_POSITIVE_RATIO
        # Inference parameters:
        self.score_threshold = cfg.MODEL.SSD.SCORE_THRESH_TEST
        self.nms_threshold = cfg.MODEL.SSD.NMS_THRESH_TEST
        self.nms_type = cfg.MODEL.NMS_TYPE
        self.max_detections_per_image = cfg.TEST.DETECTIONS_PER_IMAGE
        # fmt: on

        self.backbone = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))
        backbone_shape = self.backbone.output_shape()
        feature_shapes = [backbone_shape[f] for f in self.in_features]

        # build extra layers
        self.extra_layers = self._make_extra_layers(
            feature_shapes[-1].channels, self.extra_layer_arch)
        extra_layer_channels = [c for c in self.extra_layer_arch if isinstance(c, int)]
        feature_shapes += [ShapeSpec(channels=c) for c in extra_layer_channels[1::2]]

        # ssd head
        self.head = SSDHead(cfg, feature_shapes)
        self.l2norm = L2Norm(512, self.l2norm_scale)
        self.default_box_generator = cfg.build_default_box_generator(cfg)
        self.default_boxes = self.default_box_generator()

        # Matching and loss
        self.box2box_transform = Box2BoxTransform(
            weights=cfg.MODEL.SSD.BBOX_REG_WEIGHTS)
        self.matcher = Matcher(
            cfg.MODEL.SSD.IOU_THRESHOLDS,
            cfg.MODEL.SSD.IOU_LABELS,
            allow_low_quality_matches=False,
        )

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(
            3, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(
            3, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std
        self.to(self.device)

        # Initialization
        self._init_weights()

    def _init_weights(self):
        # extra layers param init
        for layer in self.extra_layers:
            for param in layer.parameters():
                if param.dim() > 1:
                    nn.init.xavier_uniform_(param)

        # l2 norm param init
        for param in self.l2norm.parameters():
            torch.nn.init.constant_(param, self.l2norm_scale)

    def _make_extra_layers(self, in_channels, extra_arch):
        extra_layers = list()
        flag = False  # kernel size flag
        for idx, v in enumerate(extra_arch):
            if in_channels != 'S':
                if v == 'S':
                    extra_layers += [nn.Conv2d(in_channels, extra_arch[idx + 1],
                                               kernel_size=(1, 3)[flag], stride=2, padding=1)]
                else:
                    extra_layers += [nn.Conv2d(in_channels,
                                               v, kernel_size=(1, 3)[flag])]
                flag = not flag
            in_channels = v
        if self.image_size == 512:
            extra_layers[-1] = nn.Conv2d(extra_arch[-2], extra_arch[-1], kernel_size=4, padding=1)

        return nn.ModuleList(extra_layers)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
                Each item in the list contains the inputs for one image.
                For now, each item in the list is a dict that contains:

                * image: Tensor, image in (C, H, W) format.
                * instances: Instances

                Other information that's included in the original dicts, such as:

                * "height", "width" (int): the output resolution of the model, used in inference.
                    See :meth:`postprocess` for details.
        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a tensor storing the loss. Used during training only.
        """
        images = self.preprocess_image(batched_inputs)

        if "instances" in batched_inputs[0]:
            gt_instances = [
                x["instances"].to(self.device) for x in batched_inputs
            ]
        elif "targets" in batched_inputs[0]:
            log_first_n(logging.WARN,
                        "'targets' in the model inputs is now renamed to 'instances'!",
                        n=10)
            gt_instances = [
                x["targets"].to(self.device) for x in batched_inputs
            ]
        else:
            gt_instances = None

        # vgg feature maps: ['Conv4_3', 'Conv7']
        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]

        # featrue map: Conv4_3
        # Conv4_3 has a different feature scale compared to the other layers, we use
        # the L2 normalization technique to scale the feature norm at each location
        # in the feature map to 20 and learn the scale during back propagation.
        features[0] = self.l2norm(features[0])

        # Conv7
        x = features[-1]
        # compute featrue maps: conv8_2, conv9_2, conv10_2, and conv11_2
        for idx, extra_layer in enumerate(self.extra_layers):
            x = F.relu(extra_layer(x), inplace=True)
            if idx % 2 == 1:
                features.append(x)

        conf_pred, loc_pred = self.head(features)

        if self.training:
            gt_conf, gt_default_boxes_deltas = self.get_ground_truth(
                self.default_boxes, gt_instances)
            return self.losses(gt_conf, gt_default_boxes_deltas, conf_pred, loc_pred)
        else:
            results = self.inference(
                conf_pred, loc_pred, self.default_boxes, images)
            processed_results = []
            for results_per_image, input_per_image, image_size in zip(
                    results, batched_inputs, images.image_sizes):
                height = input_per_image.get("height", image_size[0])
                width = input_per_image.get("width", image_size[1])
                r = detector_postprocess(results_per_image, height, width)
                processed_results.append({"instances": r})
            return processed_results

    def losses(self, gt_conf, gt_default_boxes_deltas, conf_pred, loc_pred):
        """
        SSD Weighted Loss Function:
            L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N
            Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss
            weighted by α, which is set to 1 by cross val.
            where:
                c: class confidences,
                l: predicted boxes,
                g: ground truth boxes
                N: number of matched default boxes
            See: https://arxiv.org/pdf/1512.02325.pdf for more details.

        Args:
            For `gt_conf` and `gt_default_boxes_deltas` parameters, see
                :method:`get_ground_truth`.
                Their concatenated shapes are [N, R] and [N, R, 4] respectively, where the R
                is the total number of default box, i.e. sum(Hi x Wi x D) for all levels, the
                C is the total number of class, the D is the number of default box in each location.
            For `conf_pred` and `loc_pred`, see: method:`SSDHead.forward`.
                Their shapes are [N, R, C] and [N, R,, 4] respectively.

        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a scalar tensor storing the loss.
                Used during training only. The dict keys are: "loss_conf" and "loss_loc".
        """
        # shape=[#batch_size, #default_boxes, #num_classes] and [#batch_size, #default_boxes, 4]
        conf_pred = cat(conf_pred, dim=1)
        loc_pred = cat(loc_pred, dim=1)

        # filter out the negative samples
        positive_mask = gt_conf < 80

        # the number of matched default box
        num_pos_samples = positive_mask.sum()

        loss_conf, loss_loc = multi_apply(
            self.loss_single,
            conf_pred,
            loc_pred,
            gt_conf,
            gt_default_boxes_deltas,
            num_total_samples=num_pos_samples
        )
        return {"loss_conf": sum(loss_conf), "loss_loc": sum(loss_loc)}

    def loss_single(self,
                    conf_pred_i,
                    loc_pred_i,
                    gt_conf_i,
                    gt_default_boxes_deltas_i,
                    num_total_samples):
        """
        Calculate the loss of a single image.

        Args:
            conf_pred_i (Tensor): see: method: `losses`.
            loc_pred_i (Tensor): see: method: `losses`.
            gt_conf_i (Tensor): see: method: `losses`.
            gt_default_boxes_deltas_i (Tensor): see: method: `losses`.
            Their shapes are [R, C], [R, 4], [R] and [R, 4] respectively.
            num_total_samples (int): the number of matched default box.
        """
        # confidence loss
        loss_conf_all = F.cross_entropy(
            conf_pred_i, gt_conf_i, reduction='none')
        pos_idxs = (gt_conf_i < self.num_classes).nonzero(as_tuple=False).view(-1)
        neg_idxs = (gt_conf_i == self.num_classes).nonzero(as_tuple=False).view(-1)

        num_pos_samples = pos_idxs.size(0)
        num_neg_samples = int(self.negative_positive_ratio * num_pos_samples)
        if num_neg_samples > neg_idxs.size(0):
            num_neg_samples = neg_idxs.size(0)
        topk_loss_conf_neg, _ = loss_conf_all[neg_idxs].topk(num_neg_samples)
        loss_conf_pos = loss_conf_all[pos_idxs].sum()
        loss_con_neg = topk_loss_conf_neg.sum()
        # confidence loss including positive and negative samples
        loss_conf = (loss_conf_pos + loss_con_neg) / num_total_samples

        # localization loss
        loss_loc = F.smooth_l1_loss(
            loc_pred_i, gt_default_boxes_deltas_i, reduction='none').sum(dim=-1)
        loss_loc = loss_loc[pos_idxs].sum() / num_total_samples

        return loss_conf, loss_loc

    @torch.no_grad()
    def get_ground_truth(self, default_boxes, targets):
        """
        Args:
            default_boxes (list[Boxes]): a list of 'Boxes' elements.
                The Boxes contains default boxes of one image on the specific feature level.
            targets (list[Instances]): a list of N `Instances`s. The i-th
                `Instances` contains the ground-truth per-instance annotations
                for the i-th input image.  Specify `targets` during training only.

        Returns:
            gt_conf (Tensor):
                An integer tensor of shape [N, R] storing ground-truth labels for each default box.
                R is the total number of default box, i.e. the sum of Hi x Wi x D for all levels.

                * Default box with an IoU with some target higher than the foreground threshold
                are assigned their corresponding label in the [0, C-1] range.
                * Default box whose IoU are below the background threshold are assigned
                the label "C".
                * Default box whose IoU are between the foreground and background
                thresholds are assigned a label "-1", i.e. ignore.

            gt_default_boxes_deltas (Tensor): Shape [N, R, 4].
                The last dimension represents ground-truth box2box transform targets
                (g^cx, g^cy, g^w, g^h)that map each default box to its matched ground-truth box.
                The values in the tensor are meaningful only when the corresponding default box
                is labeled as foreground.
        """
        gt_conf = list()
        gt_default_boxes_deltas = list()
        # list[Tensor(R, 4)], one for each image
        default_boxes_per_image = Boxes.cat(default_boxes)

        # each Instances (for one image)
        for targets_per_image in targets:
            match_quality_matrix = pairwise_iou(
                targets_per_image.gt_boxes, default_boxes_per_image)  # M * N
            gt_matched_idxs, default_box_labels = self.matcher(
                match_quality_matrix)

            has_gt = len(targets_per_image) > 0
            if has_gt:
                # ground truth box regression
                matched_gt_boxes = targets_per_image.gt_boxes[gt_matched_idxs]

                # meaningful only when the corresponding default box is labeled as foreground.
                gt_default_boxes_deltas_i = self.box2box_transform.get_deltas(
                    default_boxes_per_image.tensor, matched_gt_boxes.tensor
                )

                gt_conf_i = targets_per_image.gt_classes[gt_matched_idxs]
                # Anchors with label 0 are treated as background.
                gt_conf_i[default_box_labels == 0] = self.num_classes
                # Anchors with label -1 are ignored.
                gt_conf_i[default_box_labels == -1] = -1
            else:
                gt_conf_i = torch.zeros_like(
                    gt_matched_idxs) + self.num_classes
                gt_default_boxes_deltas_i = torch.zeros_like(
                    default_boxes_per_image.tensor)

            gt_conf.append(gt_conf_i)
            gt_default_boxes_deltas.append(gt_default_boxes_deltas_i)

        return torch.stack(gt_conf), torch.stack(gt_default_boxes_deltas)

    def inference(self, conf_pred, loc_pred, default_boxes, images):
        """
        Args:
            conf_pred, loc_pred: Same as the output of :meth:`SSDHead.forward`
                shape = [N, Hi x Wi x D, 4] and [N, Hi x Wi x D, C].
            default_boxes (list['Boxes']):  a list of 'Boxes' elements.
                The Boxes contains default boxes of one image on the specific feature level.
            images (ImageList): the input images.

        Returns:
            results (List[Instances]): a list of #images elements.
        """
        results = list()

        for img_idx in range(len(conf_pred[0])):
            image_size = images.image_sizes[img_idx]
            conf_pred_per_image = [
                conf_pred_per_level[img_idx] for conf_pred_per_level in conf_pred
            ]
            loc_pred_per_image = [
                loc_pred_per_level[img_idx] for loc_pred_per_level in loc_pred
            ]
            results_per_image = self.inference_single_image(
                conf_pred_per_image, loc_pred_per_image, default_boxes,
                tuple(image_size))
            results.append(results_per_image)

        return results

    def inference_single_image(self,
                               conf_pred_per_image,
                               loc_pred_per_image,
                               default_boxes,
                               image_size):
        """
        Single-image inference. Return bounding-box detection results by thresholding
        on scores and applying non-maximum suppression (NMS).

        Args:
            conf_pred_per_image (list[Tensor]): list of #feature levels. Each entry contains
                tensor of size [Hi x Wi x D, C].
            loc_pred_per_image (list[Tensor]): same shape as 'conf_pred_per_image' except
                that C becomes 4.
            default_boxes (list['Boxes']):  a list of 'Boxes' elements.
                The Boxes contains default boxes of one image on the specific feature level.
            image_size (tuple(H, W)): a tuple of the image height and width.

        Returns:
            Same as `inference`, but for only one image.
        """
        # predict confidence
        conf_pred = torch.cat(conf_pred_per_image, dim=0)  # [R, C]
        conf_pred = conf_pred.softmax(dim=1)

        # predict boxes
        loc_pred = torch.cat(loc_pred_per_image, dim=0)  # [R, 4]
        default_boxes = Boxes.cat(default_boxes)  # [R, 4]
        boxes_pred = self.box2box_transform.apply_deltas(
            loc_pred, default_boxes.tensor)

        num_boxes, num_classes = conf_pred.shape
        boxes_pred = boxes_pred.view(num_boxes, 1, 4).expand(
            num_boxes, num_classes, 4)  # [R, C, 4]
        labels = torch.arange(num_classes, device=self.device)  # [0, ..., C]
        labels = labels.view(1, num_classes).expand_as(conf_pred)  # [R, C]

        # remove predictions with the background label
        boxes_pred = boxes_pred[:, :-1]
        conf_pred = conf_pred[:, :-1]
        labels = labels[:, :-1]

        # batch everything, by making every class prediction be a separate instance
        boxes_pred = boxes_pred.reshape(-1, 4)
        conf_pred = conf_pred.reshape(-1)
        labels = labels.reshape(-1)

        # remove low scoring boxes
        indices = torch.nonzero(conf_pred > self.score_threshold, as_tuple=False).squeeze(1)
        boxes_pred, conf_pred, labels = boxes_pred[indices], conf_pred[indices], labels[indices]

        keep = generalized_batched_nms(boxes_pred, conf_pred, labels,
                                       self.nms_threshold, nms_type=self.nms_type)

        keep = keep[:self.max_detections_per_image]
        result = Instances(image_size)
        result.pred_boxes = Boxes(boxes_pred[keep])
        result.scores = conf_pred[keep]
        result.pred_classes = labels[keep]
        return result

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images,
                                        self.backbone.size_divisibility)
        return images


class SSDHead(nn.Module):
    """
    The head used in SSD for object classification and box regression.
    """

    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super().__init__()

        self.num_classes = cfg.MODEL.SSD.NUM_CLASSES
        self.default_box_aspect_ratios = cfg.MODEL.SSD.DEFAULT_BOX.ASPECT_RATIOS

        # build classification subnet and localization subnet
        # number of boxes per feature map location
        mbox = [(len(a_r) + 1) * 2 for a_r in self.default_box_aspect_ratios]
        self.cls_subnet = nn.ModuleList()
        self.bbox_subnet = nn.ModuleList()
        for i, m in zip(input_shape, mbox):
            self.cls_subnet.append(
                nn.Conv2d(i.channels, m * (self.num_classes + 1),
                          kernel_size=3, padding=1)
            )
            self.bbox_subnet.append(
                nn.Conv2d(i.channels, m * 4, kernel_size=3, padding=1)
            )

        # Initialization
        self._init_weights()

    def _init_weights(self):
        for layer in [*self.cls_subnet, *self.bbox_subnet]:
            for param in layer.parameters():
                if param.dim() > 1:
                    nn.init.xavier_uniform_(param)

    def forward(self, features):
        """
        Args:
            features (list[Tensor]): VGG16-D feature map tensors.
                We use conv4_3, conv7(fc7), conv8_2, conv9_2, conv10_2, and conv11_2 to predict
                both location and confidences.
        Returns:
            conf_pred (list[Tensor]): a list of tensors, each has shape (N, HWD, K).
                These tensors predicts the classification confidences of default box at each
                feature map.
            loc_pred (list[Tensor]): a list of tensors, each has shape (N, HWD, 4).
                The tensor predicts 4-vector (g^cx, g^cy, g^w, g^h) box regression values for
                every default box.
        """
        # compute confidences and location
        conf_pred = list()
        loc_pred = list()
        for feature, cls_module, bbox_module in zip(features, self.cls_subnet, self.bbox_subnet):
            # permute: conf_pred[i].shape from [N, C, Hi, Wi] to [N, Hi, Wi, C]
            conf_pred.append(cls_module(feature).permute(
                0, 2, 3, 1).contiguous())
            loc_pred.append(bbox_module(feature).permute(
                0, 2, 3, 1).contiguous())

        # resize to (N, HWD, 4) and (N, HWD, K).
        conf_pred = [
            result.view(result.size(0), -1, (self.num_classes + 1)) for result in conf_pred
        ]
        loc_pred = [
            result.view(result.size(0), -1, 4) for result in loc_pred
        ]

        return conf_pred, loc_pred


class L2Norm(nn.Module):

    def __init__(self, n_dims, scale=20., eps=1e-10):
        super(L2Norm, self).__init__()
        self.n_dims = n_dims
        self.weight = nn.Parameter(torch.Tensor(self.n_dims))
        self.eps = eps
        self.scale = scale

    def forward(self, x):
        x_float = x.float()
        norm = x_float.pow(2).sum(1, keepdim=True).sqrt() + self.eps
        return (
            self.weight[None, :, None, None].float().expand_as(x_float) * x_float / norm
        ).type_as(x)


class DefaultBox:
    """Compute default box coordinates (xmin, ymin, xmax, ymax) for each feature map.
    Returns:
        default_boxes (list['Boxes']): a list of 'Boxes' elements.
            The Boxes contains default box of this image on the specific feature level.
    """

    def __init__(self, cfg):
        super(DefaultBox, self).__init__()
        self.device = torch.device(cfg.MODEL.DEVICE)
        self.feature_map_size = cfg.MODEL.SSD.FEATURE_MAP_SIZE
        self.image_size = cfg.MODEL.SSD.IMAGE_SIZE
        self.conv4_3_scale = cfg.MODEL.SSD.DEFAULT_BOX.SCALE.CONV4_3_SCALE
        self.s_min = cfg.MODEL.SSD.DEFAULT_BOX.SCALE.S_MIN
        self.s_max = cfg.MODEL.SSD.DEFAULT_BOX.SCALE.S_MAX
        self.aspect_ratios = cfg.MODEL.SSD.DEFAULT_BOX.ASPECT_RATIOS
        self.clip = cfg.MODEL.SSD.DEFAULT_BOX.CLIP

    def __call__(self):
        # compute box_size
        m = len(self.feature_map_size) - 1
        size_stride = math.floor(
            (math.floor(self.s_max * 100) - math.floor(self.s_min * 100)
             ) / (m - 1))
        bbox_size = [self.conv4_3_scale * self.image_size]
        bbox_size += [
            (self.s_min + i * size_stride / 100) * self.image_size
            for i in range(m)
        ]
        bbox_size += [1.05 * self.image_size]

        self.widths = [[] for _ in self.aspect_ratios]
        self.heights = [[] for _ in self.aspect_ratios]

        # each a_r denotes the aspect ratios of one feature map
        for i, a_rs in enumerate(self.aspect_ratios):
            # ratio = 1
            a_r = 1
            self.widths[i].append(bbox_size[i] * sqrt(a_r))
            self.heights[i].append(bbox_size[i] / sqrt(a_r))
            self.widths[i].append(
                sqrt(bbox_size[i] * bbox_size[i + 1]) * sqrt(a_r))
            self.heights[i].append(
                sqrt(bbox_size[i] * bbox_size[i + 1]) / sqrt(a_r))

            # other ratios
            for a_r in a_rs:
                self.widths[i].append(bbox_size[i] * sqrt(a_r))
                self.heights[i].append(bbox_size[i] / sqrt(a_r))
                a_r = 1 / a_r
                self.widths[i].append(bbox_size[i] * sqrt(a_r))
                self.heights[i].append(bbox_size[i] / sqrt(a_r))

        # compute center of default boxes
        self.center_xs = [[] for _ in self.feature_map_size]
        self.center_ys = [[] for _ in self.feature_map_size]
        for k, f_k in enumerate(self.feature_map_size):
            for i, j in product(range(f_k), repeat=2):
                # bbox center x, y
                cx = (j + 0.5) / f_k * self.image_size
                cy = (i + 0.5) / f_k * self.image_size
                self.center_xs[k].append(cx)
                self.center_ys[k].append(cy)

        default_boxes = []
        for i, cxs, cys in zip(range(len(self.feature_map_size)), self.center_xs, self.center_ys):
            one_feature_map_boxes = []
            widths = self.widths[i]
            heights = self.heights[i]
            for cx, cy in zip(cxs, cys):
                for w, h in zip(widths, heights):
                    (xmin, ymin, xmax, ymax) = cx - 0.5 * \
                        w, cy - 0.5 * h, cx + 0.5 * w, cy + 0.5 * h
                    one_feature_map_boxes.append([xmin, ymin, xmax, ymax])
            one_feature_map_boxes = torch.tensor(
                one_feature_map_boxes, device=self.device)
            if self.clip:
                one_feature_map_boxes = one_feature_map_boxes.clamp_(
                    max=self.image_size, min=0)
            default_boxes.append(Boxes(one_feature_map_boxes))
        return default_boxes
```

#### cvpods/modeling/meta_arch/centernet.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.

import math

import cv2
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

from cvpods.data.transforms.transform_gen import CenterAffine
from cvpods.layers import DeformConvWithOff, ModulatedDeformConvWithOff, ShapeSpec
from cvpods.modeling.losses import reg_l1_loss
from cvpods.modeling.nn_utils.feature_utils import gather_feature
from cvpods.structures import Boxes, ImageList, Instances


class DeconvLayer(nn.Module):

    def __init__(
        self, in_planes,
        out_planes, deconv_kernel,
        deconv_stride=2, deconv_pad=1,
        deconv_out_pad=0, modulate_deform=True,
    ):
        super(DeconvLayer, self).__init__()
        if modulate_deform:
            self.dcn = ModulatedDeformConvWithOff(
                in_planes, out_planes,
                kernel_size=3, deformable_groups=1,
            )
        else:
            self.dcn = DeformConvWithOff(
                in_planes, out_planes,
                kernel_size=3, deformable_groups=1,
            )

        self.dcn_bn = nn.BatchNorm2d(out_planes)
        self.up_sample = nn.ConvTranspose2d(
            in_channels=out_planes,
            out_channels=out_planes,
            kernel_size=deconv_kernel,
            stride=deconv_stride, padding=deconv_pad,
            output_padding=deconv_out_pad,
            bias=False,
        )
        self._deconv_init()
        self.up_bn = nn.BatchNorm2d(out_planes)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.dcn(x)
        x = self.dcn_bn(x)
        x = self.relu(x)
        x = self.up_sample(x)
        x = self.up_bn(x)
        x = self.relu(x)
        return x

    def _deconv_init(self):
        w = self.up_sample.weight.data
        f = math.ceil(w.size(2) / 2)
        c = (2 * f - 1 - f % 2) / (2. * f)
        for i in range(w.size(2)):
            for j in range(w.size(3)):
                w[0, 0, i, j] = \
                    (1 - math.fabs(i / f - c)) * (1 - math.fabs(j / f - c))
        for c in range(1, w.size(0)):
            w[c, 0, :, :] = w[0, 0, :, :]


class CenternetDeconv(nn.Module):

    def __init__(self, cfg):
        super(CenternetDeconv, self).__init__()
        # modify into config
        channels = cfg.MODEL.CENTERNET.DECONV_CHANNEL
        deconv_kernel = cfg.MODEL.CENTERNET.DECONV_KERNEL
        modulate_deform = cfg.MODEL.CENTERNET.MODULATE_DEFORM
        self.deconv1 = DeconvLayer(
            channels[0], channels[1],
            deconv_kernel=deconv_kernel[0],
            modulate_deform=modulate_deform,
        )
        self.deconv2 = DeconvLayer(
            channels[1], channels[2],
            deconv_kernel=deconv_kernel[1],
            modulate_deform=modulate_deform,
        )
        self.deconv3 = DeconvLayer(
            channels[2], channels[3],
            deconv_kernel=deconv_kernel[2],
            modulate_deform=modulate_deform,
        )

    def forward(self, x):
        x = self.deconv1(x)
        x = self.deconv2(x)
        x = self.deconv3(x)
        return x


class CenterNet(nn.Module):
    r"""
    Implement CenterNet (https://arxiv.org/abs/1904.07850).
    """
    def __init__(self, cfg):
        super().__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)
        self.cfg = cfg

        # fmt: off
        self.num_classes = cfg.MODEL.CENTERNET.NUM_CLASSES
        # Loss parameters:
        # Inference parameters:
        self.max_detections_per_image = cfg.TEST.DETECTIONS_PER_IMAGE
        # fmt: on
        self.backbone = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))
        )
        self.upsample = CenternetDeconv(cfg)
        self.head = CenternetHead(cfg)
        self.reg_loss = reg_l1_loss()

        self.mean, self.std = cfg.MODEL.PIXEL_MEAN, cfg.MODEL.PIXEL_STD
        pixel_mean = torch.Tensor(self.mean).to(self.device).view(3, 1, 1)
        pixel_std = torch.Tensor(self.std).to(self.device).view(3, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std
        self.to(self.device)

    def forward(self, batched_inputs):
        r"""
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
                Each item in the list contains the inputs for one image.
                For now, each item in the list is a dict that contains:

                * image: Tensor, image in (C, H, W) format.
                * instances: Instances

                Other information that's included in the original dicts, such as:

                * "height", "width" (int): the output resolution of the model, used in inference.
                    See :meth:`postprocess` for details.

        Returns:
            dict[str: Tensor]:
        """
        images = self.preprocess_image(batched_inputs)

        if not self.training:
            return self.inference(images)

        features = self.backbone(images.tensor)
        up_fmap = self.upsample(features["res5"])
        pred_dict = self.head(up_fmap)

        gt_dict = self.get_ground_truth(batched_inputs)

        return self.losses(pred_dict, gt_dict)

    def losses(self, pred_dict, gt_dict):
        r"""
        calculate losses of pred and gt

        Args:
            gt_dict (dict): a dict contains all information of gt
                gt_dict = {
                    "score_map": gt scoremap,
                    "wh": gt width and height of boxes,
                    "reg": gt regression of box center point,
                    "reg_mask": mask of regression,
                    "index": gt index,
                }
            pred (dict): a dict contains all information of prediction
                pred = {
                    "cls": predicted score map,
                    "reg": predcited regression,
                    "wh": predicted width and height of box,
                }
        """
        # scoremap loss
        pred_score = pred_dict['cls']
        cur_device = pred_score.device
        for k in gt_dict:
            gt_dict[k] = gt_dict[k].to(cur_device)

        loss_cls = _modified_focal_loss(pred_score, gt_dict['score_map'])

        mask = gt_dict['reg_mask']
        index = gt_dict['index']
        index = index.to(torch.long)
        # width and height loss, better version
        loss_wh = self.reg_loss(pred_dict['wh'], mask, index, gt_dict['wh'])

        # regression loss
        loss_reg = self.reg_loss(pred_dict['reg'], mask, index, gt_dict['reg'])

        loss_cls *= self.cfg.MODEL.LOSS.CLS_WEIGHT
        loss_wh *= self.cfg.MODEL.LOSS.WH_WEIGHT
        loss_reg *= self.cfg.MODEL.LOSS.REG_WEIGHT

        loss = {
            "loss_cls": loss_cls,
            "loss_box_wh": loss_wh,
            "loss_center_reg": loss_reg,
        }
        return loss

    @torch.no_grad()
    def get_ground_truth(self, batched_inputs):
        return CenterNetGT.generate(self.cfg, batched_inputs)

    @torch.no_grad()
    def inference(self, images):
        r"""
        image(tensor): ImageList in cvpods.structures
        """
        n, c, h, w = images.tensor.shape
        new_h, new_w = (h | 31) + 1, (w | 31) + 1
        center_wh = np.array([w // 2, h // 2], dtype=np.float32)
        size_wh = np.array([new_w, new_h], dtype=np.float32)
        down_scale = self.cfg.MODEL.CENTERNET.DOWN_SCALE
        img_info = dict(center=center_wh, size=size_wh,
                        height=new_h // down_scale,
                        width=new_w // down_scale)

        pad_value = [-x / y for x, y in zip(self.mean, self.std)]
        aligned_img = torch.Tensor(pad_value).reshape((1, -1, 1, 1)).expand(n, c, new_h, new_w)
        aligned_img = aligned_img.to(images.tensor.device)

        pad_w, pad_h = math.ceil((new_w - w) / 2), math.ceil((new_h - h) / 2)
        aligned_img[..., pad_h:h + pad_h, pad_w:w + pad_w] = images.tensor

        features = self.backbone(aligned_img)
        up_fmap = self.upsample(features["res5"])
        pred_dict = self.head(up_fmap)
        results = self.decode_prediction(pred_dict, img_info)

        ori_w, ori_h = img_info['center'] * 2
        det_instance = Instances((int(ori_h), int(ori_w)), **results)

        return [{"instances": det_instance}]

    def decode_prediction(self, pred_dict, img_info):
        r"""
        Args:
            pred_dict (dict): a dict contains all information of prediction
            img_info (dict): a dict contains needed information of origin image
        """
        fmap = pred_dict["cls"]
        reg = pred_dict["reg"]
        wh = pred_dict["wh"]

        boxes, scores, classes = CenterNetDecoder.decode(fmap, wh, reg)
        # boxes = Boxes(boxes.reshape(boxes.shape[-2:]))
        scores = scores.reshape(-1)
        classes = classes.reshape(-1).to(torch.int64)

        # dets = CenterNetDecoder.decode(fmap, wh, reg)
        boxes = CenterNetDecoder.transform_boxes(boxes, img_info)
        boxes = Boxes(boxes)
        return dict(pred_boxes=boxes, scores=scores, pred_classes=classes)

    def preprocess_image(self, batched_inputs):
        r"""
        Normalize, pad and batch the input images.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        # images = [img / 255 for img in images]
        images = [self.normalizer(img / 255.0) for img in images]
        images = ImageList.from_tensors(images,
                                        self.backbone.size_divisibility)
        return images


class CenterNetDecoder(object):

    @staticmethod
    def decode(fmap, wh, reg=None, cat_spec_wh=False, K=100):
        r"""
        decode feature maps, width height, regression to detections results.

        Args:
            fmap (Tensor): input feature map.
            wh (Tensor): tensor represents (width, height).
            reg (Tensor): tensor represents regression.
            cat_spec_wh (bool): whether reshape wh tensor.
            K (int): top k value in score map.
        """
        batch, channel, height, width = fmap.shape

        fmap = CenterNetDecoder.pseudo_nms(fmap)

        scores, index, clses, ys, xs = CenterNetDecoder.topk_score(fmap, K=K)
        if reg is not None:
            reg = gather_feature(reg, index, use_transform=True)
            reg = reg.reshape(batch, K, 2)
            xs = xs.view(batch, K, 1) + reg[:, :, 0:1]
            ys = ys.view(batch, K, 1) + reg[:, :, 1:2]
        else:
            xs = xs.view(batch, K, 1) + 0.5
            ys = ys.view(batch, K, 1) + 0.5
        wh = gather_feature(wh, index, use_transform=True)

        if cat_spec_wh:
            wh = wh.view(batch, K, channel, 2)
            clses_ind = clses.view(batch, K, 1, 1).expand(batch, K, 1, 2).long()
            wh = wh.gather(2, clses_ind).reshape(batch, K, 2)
        else:
            wh = wh.reshape(batch, K, 2)

        clses  = clses.reshape(batch, K, 1).float()
        scores = scores.reshape(batch, K, 1)

        half_w = wh[..., 0:1] / 2
        half_h = wh[..., 1:2] / 2
        bboxes = torch.cat([xs - half_w, ys - half_h,
                            xs + half_w, ys + half_h], dim=2)

        detections = (bboxes, scores, clses)

        return detections

    @staticmethod
    def transform_boxes(boxes, img_info):
        r"""
        transform predicted boxes to target boxes

        Args:
            boxes (Tensor): torch Tensor with (Batch, N, 4) shape
            img_info (dict): dict contains all information of original image
        """
        boxes = boxes.cpu().numpy().reshape(-1, 4)

        center = img_info['center']
        size = img_info['size']
        output_size = (img_info['width'], img_info['height'])
        src, dst = CenterAffine.generate_src_and_dst(center, size, output_size)
        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))

        coords = boxes.reshape(-1, 2)
        aug_coords = np.column_stack((coords, np.ones(coords.shape[0])))
        target_boxes = np.dot(aug_coords, trans.T).reshape(-1, 4)
        return target_boxes

    @staticmethod
    def pseudo_nms(fmap, pool_size=3):
        r"""
        apply maxpooling instead of NMS.

        Args:
            fmap (Tensor): output feature maps.
            pool_size (int): max pooling window size.
        """
        pad = (pool_size - 1) // 2
        fmap_max = F.max_pool2d(fmap, pool_size, stride=1, padding=pad)
        keep = (fmap_max == fmap).float()
        return fmap * keep

    @staticmethod
    def topk_score(scores, K=40):
        r"""
        get top point in score map.

        Args:
            scores (Tensor): scores map.
            K (int): top K in scores map.
        """
        batch, channel, height, width = scores.shape

        # get topk score and its index in every H x W(channel dim) feature map
        topk_scores, topk_inds = torch.topk(scores.reshape(batch, channel, -1), K)

        topk_inds = topk_inds % (height * width)
        topk_ys = (topk_inds / width).int().float()
        topk_xs = (topk_inds % width).int().float()

        # get all topk in in a batch
        topk_score, index = torch.topk(topk_scores.reshape(batch, -1), K)
        # div by K because index is grouped by K(C x K shape)
        topk_clses = (index / K).int()
        topk_inds = gather_feature(topk_inds.view(batch, -1, 1), index).reshape(batch, K)
        topk_ys = gather_feature(topk_ys.reshape(batch, -1, 1), index).reshape(batch, K)
        topk_xs = gather_feature(topk_xs.reshape(batch, -1, 1), index).reshape(batch, K)

        return topk_score, topk_inds, topk_clses, topk_ys, topk_xs


class CenterNetGT(object):

    @staticmethod
    def generate(config, batched_input):
        r"""
        genterate ground truth for CenterNet
        """
        box_scale = 1 / config.MODEL.CENTERNET.DOWN_SCALE
        num_classes = config.MODEL.CENTERNET.NUM_CLASSES
        output_size = config.INPUT.OUTPUT_SIZE
        min_overlap = config.MODEL.CENTERNET.MIN_OVERLAP
        tensor_dim = config.MODEL.CENTERNET.TENSOR_DIM

        scoremap_list, wh_list, reg_list, reg_mask_list, index_list = [[] for i in range(5)]
        for data in batched_input:
            # img_size = (data['height'], data['width'])

            bbox_dict = data['instances'].get_fields()

            # init gt tensors
            gt_scoremap = torch.zeros(num_classes, *output_size)
            gt_wh = torch.zeros(tensor_dim, 2)
            gt_reg = torch.zeros_like(gt_wh)
            reg_mask = torch.zeros(tensor_dim)
            gt_index = torch.zeros(tensor_dim)
            # pass

            boxes, classes = bbox_dict['gt_boxes'], bbox_dict['gt_classes']
            num_boxes = boxes.tensor.shape[0]
            boxes.scale(box_scale, box_scale)

            centers = boxes.get_centers()
            centers_int = centers.to(torch.int32)
            gt_index[:num_boxes] = centers_int[..., 1] * output_size[1] + centers_int[..., 0]
            gt_reg[:num_boxes] = centers - centers_int
            reg_mask[:num_boxes] = 1

            wh = torch.zeros_like(centers)
            box_tensor = boxes.tensor
            wh[..., 0] = box_tensor[..., 2] - box_tensor[..., 0]
            wh[..., 1] = box_tensor[..., 3] - box_tensor[..., 1]
            CenterNetGT.generate_score_map(
                gt_scoremap, classes, wh,
                centers_int, min_overlap,
            )
            gt_wh[:num_boxes] = wh

            scoremap_list.append(gt_scoremap)
            wh_list.append(gt_wh)
            reg_list.append(gt_reg)
            reg_mask_list.append(reg_mask)
            index_list.append(gt_index)

        gt_dict = {
            "score_map": torch.stack(scoremap_list, dim=0),
            "wh": torch.stack(wh_list, dim=0),
            "reg": torch.stack(reg_list, dim=0),
            "reg_mask": torch.stack(reg_mask_list, dim=0),
            "index": torch.stack(index_list, dim=0),
        }
        return gt_dict

    @staticmethod
    def generate_score_map(fmap, gt_class, gt_wh, centers_int, min_overlap):
        r"""
        generate score map

        Args:
            fmap (Tensor): input feature map.
            gt_class (Tensor): tensor represents ground truth classes.
            gt_wh (Tensor): ground truth width and height value.
            centers_int (Tensor): ground truth int value of centers.
            min_overlap (float): IoU threshold.
        """
        radius = CenterNetGT.get_gaussian_radius(gt_wh, min_overlap)
        radius = torch.clamp_min(radius, 0)
        radius = radius.type(torch.int).cpu().numpy()
        for i in range(gt_class.shape[0]):
            channel_index = gt_class[i]
            CenterNetGT.draw_gaussian(fmap[channel_index], centers_int[i], radius[i])

    @staticmethod
    def get_gaussian_radius(box_size, min_overlap):
        r"""
        get gaussian radius according to box size and IoU threshold, copyed from CornerNet.

        box_size: (w, h) information. Could be a torch.Tensor, numpy.ndarray, list or tuple.
        NOTE: we are using a bug-version, please refer to fix bug version in CornerNet.
        """
        box_tensor = torch.Tensor(box_size)
        width, height = box_tensor[..., 0], box_tensor[..., 1]

        a1  = 1
        b1  = (height + width)
        c1  = width * height * (1 - min_overlap) / (1 + min_overlap)
        sq1 = torch.sqrt(b1 ** 2 - 4 * a1 * c1)
        r1  = (b1 + sq1) / 2

        a2  = 4
        b2  = 2 * (height + width)
        c2  = (1 - min_overlap) * width * height
        sq2 = torch.sqrt(b2 ** 2 - 4 * a2 * c2)
        r2  = (b2 + sq2) / 2

        a3  = 4 * min_overlap
        b3  = -2 * min_overlap * (height + width)
        c3  = (min_overlap - 1) * width * height
        sq3 = torch.sqrt(b3 ** 2 - 4 * a3 * c3)
        r3  = (b3 + sq3) / 2

        return torch.min(r1, torch.min(r2, r3))

    @staticmethod
    def gaussian2D(radius, sigma=1):
        r"""
        generate guassian distribution according to gaussian radius and sigma.

        Args:
            radius (Tensor): radius of gaussian radius.
            sigma (int): sigma in gaussian.
        """
        m, n = radius
        y, x = np.ogrid[-m:m + 1, -n:n + 1]

        gauss = np.exp(-(x * x + y * y) / (2 * sigma * sigma))
        gauss[gauss < np.finfo(gauss.dtype).eps * gauss.max()] = 0
        return gauss

    @staticmethod
    def draw_gaussian(fmap, center, radius, k=1):
        r"""
        generate ground truth for CenterNet

        Args:
            fmap (Tensor): output feature map
            center (Tensor): gaussian center
            radius (Tensor): gaussian radius
            k (int): topk
        """
        diameter = 2 * radius + 1
        gaussian = CenterNetGT.gaussian2D((radius, radius), sigma=diameter / 6)
        gaussian = torch.Tensor(gaussian)
        x, y = int(center[0]), int(center[1])
        height, width = fmap.shape[:2]

        left, right = min(x, radius), min(width - x, radius + 1)
        top, bottom = min(y, radius), min(height - y, radius + 1)

        masked_fmap  = fmap[y - top:y + bottom, x - left:x + right]
        masked_gaussian = gaussian[radius - top:radius + bottom, radius - left:radius + right]
        if min(masked_gaussian.shape) > 0 and min(masked_fmap.shape) > 0:
            masked_fmap = torch.max(masked_fmap, masked_gaussian * k)
            fmap[y - top:y + bottom, x - left:x + right] = masked_fmap
        # return fmap


def _modified_focal_loss(pred, gt):
    r"""
    focal loss used for CenterNet, modified from focal loss.
    but this function is a numeric stable version implementation.
    """
    pos_inds = gt.eq(1).float()
    neg_inds = gt.lt(1).float()

    neg_weights = torch.pow(1 - gt, 4)
    pred = torch.max(pred, torch.ones_like(pred) * 1e-12)

    pos_loss = torch.log(pred) * torch.pow(1 - pred, 2) * pos_inds
    neg_loss = torch.log(1 - pred) * torch.pow(pred, 2) * neg_weights * neg_inds

    num_pos  = pos_inds.float().sum()
    pos_loss = pos_loss.sum()
    neg_loss = neg_loss.sum()

    if num_pos == 0:
        loss = -neg_loss
    else:
        loss = -(pos_loss + neg_loss) / num_pos
    return loss


class SingleHead(nn.Module):
    r"""
    Single head used in CenterNet Head.
    """

    def __init__(self, in_channel, out_channel, bias_fill=False, bias_value=0):
        super(SingleHead, self).__init__()
        self.feat_conv = nn.Conv2d(in_channel, in_channel, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.out_conv = nn.Conv2d(in_channel, out_channel, kernel_size=1)
        if bias_fill:
            self.out_conv.bias.data.fill_(bias_value)

    def forward(self, x):
        x = self.feat_conv(x)
        x = self.relu(x)
        x = self.out_conv(x)
        return x


class CenternetHead(nn.Module):
    r"""
    The head used in CenterNet for object classification and box regression.
    It has three single heads, with a common structure but separate parameters.
    """

    def __init__(self, cfg):
        super(CenternetHead, self).__init__()
        self.cls_head = SingleHead(
            64,
            cfg.MODEL.CENTERNET.NUM_CLASSES,
            bias_fill=True,
            bias_value=cfg.MODEL.CENTERNET.BIAS_VALUE,
        )
        self.wh_head = SingleHead(64, 2)
        self.reg_head = SingleHead(64, 2)

    def forward(self, x):
        cls = self.cls_head(x)
        cls = torch.sigmoid(cls)
        wh = self.wh_head(x)
        reg = self.reg_head(x)
        pred = {
            'cls': cls,
            'wh': wh,
            'reg': reg
        }
        return pred
```

#### cvpods/modeling/meta_arch/imagenet.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import torch
from torch import nn

from cvpods.layers import ShapeSpec
from cvpods.structures import ImageList


def accuracy(output, target, topk=(1,)):
    """Computes the accuracy over the k top predictions for the specified values of k"""
    with torch.no_grad():
        maxk = max(topk)
        batch_size = target.size(0)

        _, pred = output.topk(maxk, 1, True, True)
        pred = pred.t()
        correct = pred.eq(target.view(1, -1).expand_as(pred))

        res = []
        for k in topk:
            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)
            res.append(correct_k.mul_(100.0 / batch_size))
        return res


class Classification(nn.Module):
    """
    ImageNet classification module.
    Weights of this model can be used as pretrained weights of any models in cvpods.
    """
    def __init__(self, cfg):
        super(Classification, self).__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)

        self.network = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))

        self.loss_evaluator = nn.CrossEntropyLoss()

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(
            3, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(
            3, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std

        self.to(self.device)

    def forward(self, batched_inputs):
        images = self.preprocess_image(batched_inputs)

        preds = self.network(images.tensor)["linear"]

        if self.training:
            labels = torch.tensor([gi["category_id"] for gi in batched_inputs]).cuda()
            losses = self.loss_evaluator(preds, labels)
            acc1, acc5 = accuracy(preds, labels, topk=(1, 5))

            return {
                "loss_cls": losses,
                "Acc@1": acc1,
                "Acc@5": acc5,
            }
        else:
            return preds

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        images = [x["image"].float().to(self.device) for x in batched_inputs]
        images = [self.normalizer(x.div(255)) for x in images]
        images = ImageList.from_tensors(images, self.network.size_divisibility)
        return images
```

#### cvpods/modeling/meta_arch/solo.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.

import logging
import math
from functools import partial
from typing import List

import numpy as np
from PIL import Image
from scipy import ndimage

import torch
import torch.nn as nn
import torch.nn.functional as F

from cvpods.layers import ShapeSpec, get_norm, matrix_nms
from cvpods.modeling.losses import dice_loss, sigmoid_focal_loss_jit
from cvpods.modeling.nn_utils.weight_init import normal_init
from cvpods.structures import BitMasks, ImageList, Instances
from cvpods.utils import log_first_n


def multi_apply(func, *args, **kwargs):
    pfunc = partial(func, **kwargs) if kwargs else func
    map_results = map(pfunc, *args)
    return tuple(map(list, zip(*map_results)))


def points_nms(heat, kernel=2):
    # kernel must be 2
    hmax = nn.functional.max_pool2d(
        heat, (kernel, kernel), stride=1, padding=1)
    keep = (hmax[:, :, :-1, :-1] == heat).float()
    return heat * keep


class SOLO(nn.Module):
    """
    Implement SOLO: Segmenting Objects by Locations.
    See: https://arxiv.org/pdf/1512.02325.pdf for more details.
    """

    def __init__(self, cfg):
        super().__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)

        self.num_classes = cfg.MODEL.SOLO.NUM_CLASSES
        self.in_features = cfg.MODEL.SOLO.IN_FEATURES
        self.seg_num_grids = cfg.MODEL.SOLO.NUM_GRIDS
        self.head_type = cfg.MODEL.SOLO.HEAD.TYPE
        self.scale_ranges = cfg.MODEL.SOLO.SCALE_RANGES
        self.feature_strides = cfg.MODEL.SOLO.FEATURE_STRIDES
        self.sigma = cfg.MODEL.SOLO.SIGMA
        # Loss parameters:
        # category loss
        self.loss_ins_type = cfg.MODEL.SOLO.LOSS_INS.TYPE
        self.loss_ins_weight = cfg.MODEL.SOLO.LOSS_INS.LOSS_WEIGHT
        # mask loss
        self.loss_cat_type = cfg.MODEL.SOLO.LOSS_CAT.TYPE
        self.loss_cat_weight = cfg.MODEL.SOLO.LOSS_CAT.LOSS_WEIGHT
        self.loss_cat_gamma = cfg.MODEL.SOLO.LOSS_CAT.GAMMA
        self.loss_cat_alpha = cfg.MODEL.SOLO.LOSS_CAT.ALPHA
        # Inference parameters:
        self.score_threshold = cfg.MODEL.SOLO.SCORE_THRESH_TEST
        self.mask_threshold = cfg.MODEL.SOLO.MASK_THRESH_TEST
        self.nms_per_image = cfg.MODEL.SOLO.NMS_PER_IMAGE
        self.nms_kernel = cfg.MODEL.SOLO.NMS_KERNEL
        self.nms_sigma = cfg.MODEL.SOLO.NMS_SIGMA
        self.update_threshold = cfg.MODEL.SOLO.UPDATE_THRESH
        self.max_detections_per_image = cfg.TEST.DETECTIONS_PER_IMAGE
        self.backbone = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))

        backbone_shape = self.backbone.output_shape()
        feature_shapes = [backbone_shape[f] for f in self.in_features]
        self._init_head(cfg, feature_shapes)

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(
            3, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(
            3, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std
        self.to(self.device)

    def _init_head(self, cfg, feature_shapes):
        assert self.head_type == "SOLOHead"
        self.head = SOLOHead(cfg, feature_shapes)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
                Each item in the list contains the inputs for one image.
                For now, each item in the list is a dict that contains:

                * image: Tensor, image in (C, H, W) format.
                * instances: Instances

                Other information that's included in the original dicts, such as:

                * "height", "width" (int): the output resolution of the model, used in inference.
                    See :meth:`postprocess` for details.
        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a tensor storing the loss. Used during training only.
        """
        images = self.preprocess_image(batched_inputs)
        if "instances" in batched_inputs[0]:
            gt_instances = [
                x["instances"].to(self.device) for x in batched_inputs
            ]
        elif "targets" in batched_inputs[0]:
            log_first_n(
                logging.WARN,
                "'targets' in the model inputs is now renamed to 'instances'!",
                n=10)
            gt_instances = [
                x["targets"].to(self.device) for x in batched_inputs
            ]
        else:
            gt_instances = None

        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]

        if self.training:
            ins_preds, cate_preds = self.head(features, eval=False)
            featmap_sizes = [featmap.size()[-2:] for featmap in ins_preds]
            ins_label_list, cate_label_list, ins_ind_label_list = self.get_ground_truth(
                gt_instances, featmap_sizes)
            return self.losses(
                ins_preds, cate_preds, ins_label_list, cate_label_list, ins_ind_label_list)
        else:
            ins_preds, cate_preds = self.head(features, eval=True)
            results = self.inference(ins_preds, cate_preds, batched_inputs)
            processed_results = [{"instances": r} for r in results]
            return processed_results

    def losses(self,
               ins_preds,
               cate_preds,
               ins_label_list,
               cate_label_list,
               ins_ind_label_list):
        """
        Compute losses:

            L = L_cate + λ * L_mask

        Args:
            ins_preds (list[Tensor]): each element in the list is mask prediction results
                of one level, and the shape of each element is [N, G*G, H, W], where:
                * N is the number of images per mini-batch
                * G is the side length of each level of the grids
                * H and W is the height and width of the predicted mask

            cate_preds (list[Tensor]): each element in the list is category prediction results
                of one level, and the shape of each element is [#N, #C, #G, #G], where:
                * C is the number of classes

            ins_label_list (list[list[Tensor]]): each element in the list is mask ground truth
                of one image, and each element is a list which contains mask tensors per level
                with shape [H, W], where:
                * H and W is the ground truth mask size per level (same as `ins_preds`)

            cate_label_list (list[list[Tensor]]): each element in the list is category ground truth
                of one image, and each element is a list which contains tensors with shape [G, G]
                per level.

            ins_ind_label_list (list[list[Tensor]]):  used to indicate which grids contain objects,
                these grids need to calculate mask loss. Each element in the list is indicator
                of one image, and each element is a list which contains tensors with shape [G*G]
                per level。

        Returns:
            dict[str -> Tensor]: losses.
        """
        # ins, per level
        ins_preds_valid = []
        ins_labels_valid = []
        cate_labels_valid = []
        num_images = len(ins_label_list)
        num_levels = len(ins_label_list[0])
        for level_idx in range(num_levels):
            ins_preds_per_level = []
            ins_labels_per_level = []
            cate_labels_per_level = []
            for img_idx in range(num_images):
                valid_ins_inds = ins_ind_label_list[img_idx][level_idx]
                ins_preds_per_level.append(
                    ins_preds[level_idx][img_idx][valid_ins_inds, ...]
                )
                ins_labels_per_level.append(
                    ins_label_list[img_idx][level_idx][valid_ins_inds, ...]
                )
                cate_labels_per_level.append(
                    cate_label_list[img_idx][level_idx].flatten()
                )
            ins_preds_valid.append(torch.cat(ins_preds_per_level))
            ins_labels_valid.append(torch.cat(ins_labels_per_level))
            cate_labels_valid.append(torch.cat(cate_labels_per_level))

        # dice loss, per_level
        loss_ins = []
        for input, target in zip(ins_preds_valid, ins_labels_valid):
            if input.size()[0] == 0:
                continue
            input = torch.sigmoid(input)
            target = target.float() / 255.
            loss_ins.append(dice_loss(input, target))
        # loss_ins (list[Tensor]): each element's shape is [#Ins, #H*#W]
        loss_ins = torch.cat(loss_ins).mean()
        loss_ins = loss_ins * self.loss_ins_weight

        # cate
        cate_preds = [
            cate_pred.permute(0, 2, 3, 1).reshape(-1, self.num_classes)
            for cate_pred in cate_preds
        ]
        cate_preds = torch.cat(cate_preds)

        flatten_cate_labels = torch.cat(cate_labels_valid)
        foreground_idxs = flatten_cate_labels != self.num_classes
        cate_labels = torch.zeros_like(cate_preds)
        cate_labels[foreground_idxs, flatten_cate_labels[foreground_idxs]] = 1
        num_ins = foreground_idxs.sum()

        loss_cate = self.loss_cat_weight * sigmoid_focal_loss_jit(
            cate_preds,
            cate_labels,
            alpha=self.loss_cat_alpha,
            gamma=self.loss_cat_gamma,
            reduction="sum",
        ) / max(1, num_ins)
        return dict(loss_ins=loss_ins, loss_cate=loss_cate)

    @torch.no_grad()
    def get_ground_truth(self, gt_instances, featmap_sizes):
        """
        Args:
            gt_instances (list[Instances]): a list of N `Instances`s. The i-th
                `Instances` contains the ground-truth per-instance annotations
                for the i-th input image.  Specify `targets` during training only.
            featmap_sizes (list[]): a list of #level elements. Each is a
                tuple of #feature level feature map size.

        Returns:
            ins_label_list, cate_label_list, ins_ind_label_list: See: method: `losses`.
        """
        ins_label_list, cate_label_list, ins_ind_label_list = multi_apply(
            self.solo_target_single_image,
            gt_instances,
            featmap_sizes=featmap_sizes)
        return ins_label_list, cate_label_list, ins_ind_label_list

    @torch.no_grad()
    def solo_target_single_image(self, gt_instance, featmap_sizes):
        """
        Prepare ground truth for single image.

        Args:
            gt_instance, featmap_sizes: See: method: `get_ground_truth`.

        Returns:
            ins_label_list, cate_label_list, ins_ind_label_list: See: method: `losses`.
        """
        device = self.device
        gt_bboxes_raw = gt_instance.gt_boxes
        gt_labels_raw = gt_instance.gt_classes
        gt_masks_raw = gt_instance.gt_masks

        # ins
        gt_areas = torch.sqrt(gt_bboxes_raw.area())

        ins_label_list = []  # per level
        cate_label_list = []  # per level
        ins_ind_label_list = []  # per level
        for (lower_bound, upper_bound), stride, featmap_size, num_grid in zip(
                self.scale_ranges, self.feature_strides, featmap_sizes, self.seg_num_grids):
            ins_label = torch.zeros([num_grid ** 2, featmap_size[0],
                                     featmap_size[1]], dtype=torch.uint8, device=device)
            cate_label = torch.full(
                [num_grid, num_grid], self.num_classes, dtype=torch.int64, device=device)
            ins_ind_label = torch.zeros([num_grid ** 2], dtype=torch.bool, device=device)

            hit_indices = ((gt_areas >= lower_bound) & (
                gt_areas <= upper_bound)).nonzero(as_tuple=False).flatten()

            if len(hit_indices) == 0:
                ins_label_list.append(ins_label)
                cate_label_list.append(cate_label)
                ins_ind_label_list.append(ins_ind_label)
                continue

            gt_bboxes = gt_bboxes_raw[hit_indices]
            gt_labels = gt_labels_raw[hit_indices]
            gt_masks = gt_masks_raw[hit_indices]
            # move mask to cpu and convert to ndarray for compute gt ins center
            gt_masks = gt_masks.tensor.to("cpu").numpy()

            half_ws = 0.5 * (gt_bboxes.tensor[:, 2] - gt_bboxes.tensor[:, 0]) * self.sigma
            half_hs = 0.5 * (gt_bboxes.tensor[:, 3] - gt_bboxes.tensor[:, 1]) * self.sigma

            output_stride = stride / 2
            # For each mask
            for seg_mask, gt_label, half_h, half_w in zip(gt_masks, gt_labels, half_hs, half_ws):
                if seg_mask.sum() < 10:
                    continue
                # mass center
                upsampled_size = (featmap_sizes[0][0] * 4, featmap_sizes[0][1] * 4)
                center_h, center_w = ndimage.measurements.center_of_mass(seg_mask)
                coord_w = int((center_w / upsampled_size[1]) // (1. / num_grid))
                coord_h = int((center_h / upsampled_size[0]) // (1. / num_grid))

                # left, top, right, down
                top_box = max(0, int(((center_h - half_h) / upsampled_size[0]) // (1. / num_grid)))
                down_box = min(
                    num_grid - 1, int(((center_h + half_h) / upsampled_size[0]) // (1. / num_grid))
                )
                left_box = max(0, int(((center_w - half_w) / upsampled_size[1]) // (1. / num_grid)))
                right_box = min(
                    num_grid - 1, int(((center_w + half_w) / upsampled_size[1]) // (1. / num_grid))
                )

                top = max(top_box, coord_h - 1)
                down = min(down_box, coord_h + 1)
                left = max(coord_w - 1, left_box)
                right = min(right_box, coord_w + 1)

                cate_label[top:(down + 1), left:(right + 1)] = gt_label
                # ins
                scale = 1. / output_stride
                h, w = seg_mask.shape[-2:]
                new_h, new_w = int(h * scale + 0.5), int(w * scale + 0.5)

                seg_mask = Image.fromarray(seg_mask)
                seg_mask = seg_mask.resize((new_w, new_h), Image.BILINEAR)
                seg_mask = np.array(seg_mask)
                seg_mask = torch.from_numpy(seg_mask)
                for i in range(top, down + 1):
                    for j in range(left, right + 1):
                        label = int(i * num_grid + j)
                        ins_label[label, :seg_mask.shape[0], :seg_mask.shape[1]] = seg_mask
                        ins_ind_label[label] = True
            ins_label_list.append(ins_label)
            cate_label_list.append(cate_label)
            ins_ind_label_list.append(ins_ind_label)
        return ins_label_list, cate_label_list, ins_ind_label_list

    @torch.no_grad()
    def inference(self, seg_preds, cate_preds, batched_inputs):
        """
        Args:
            seg_preds (list[Tensor]): predicted mask results, each element's
                shape is [N, G*G, H, W].
            cate_preds (list[Tensor]): predicted category results, each element's
                shape is [N, C, G, G].
                N, G, H, W: See: method: `losses`.
        Returns:
            results (list[Instance]): predicted results after post-processing.
        """
        assert len(seg_preds) == len(cate_preds)
        num_levels = len(cate_preds)
        featmap_size = seg_preds[0].size()[-2:]

        results = []
        for img_id, batched_input in enumerate(batched_inputs):
            cate_pred_list = []
            seg_pred_list = []
            for i in range(num_levels):
                cate_pred_list.append(
                    cate_preds[i][img_id].view(-1, self.num_classes).detach()
                )
                seg_pred_list.append(
                    seg_preds[i][img_id].detach()
                )
            cate_pred_list = torch.cat(cate_pred_list, dim=0)
            seg_pred_list = torch.cat(seg_pred_list, dim=0)

            img_shape = batched_input["instances"].image_size
            ori_shape = (batched_input["height"], batched_input["width"])

            results_per_image = self.inference_single_image(
                cate_pred_list, seg_pred_list, featmap_size, img_shape, ori_shape)
            results.append(results_per_image)
        return results

    @torch.no_grad()
    def inference_single_image(self,
                               cate_preds,
                               seg_preds,
                               featmap_size,
                               img_shape,
                               ori_shape):
        """
        Args:
            cate_preds, seg_preds: see: method: `inference`.
            featmap_size (list[tuple]): feature map size per level.
            img_shape (tuple): the size of the image fed into the model (height and width).
            ori_shape (tuple): original image shape (height and width).

        Returns:
            result (Instances): predicted results of single image after post-processing.
        """
        assert len(cate_preds) == len(seg_preds)
        result = Instances(ori_shape)

        # overall info.
        h, w = img_shape
        upsampled_size_out = (featmap_size[0] * 4, featmap_size[1] * 4)

        # process.
        inds = (cate_preds > self.score_threshold)
        # category scores.
        cate_scores = cate_preds[inds]
        if len(cate_scores) == 0:
            return result
        # category labels.
        inds = inds.nonzero(as_tuple=False)
        cate_labels = inds[:, 1]

        # strides.
        size_trans = cate_labels.new_tensor(self.seg_num_grids).pow(
            2).cumsum(0)  # [1600, 2896, 3472, 3728, 3872]
        strides = cate_scores.new_ones(size_trans[-1])
        n_stage = len(self.seg_num_grids)
        strides[:size_trans[0]] *= self.feature_strides[0]
        for ind_ in range(1, n_stage):
            strides[size_trans[ind_ - 1]:size_trans[ind_]] *= self.feature_strides[ind_]
        strides = strides[inds[:, 0]]

        # masks.
        seg_preds = seg_preds[inds[:, 0]]
        seg_masks = seg_preds > self.mask_threshold
        sum_masks = seg_masks.sum((1, 2)).float()

        # filter.
        keep = sum_masks > strides
        if keep.sum() == 0:
            return result

        seg_masks = seg_masks[keep, ...]
        seg_preds = seg_preds[keep, ...]
        sum_masks = sum_masks[keep]
        cate_scores = cate_scores[keep]
        cate_labels = cate_labels[keep]

        # mask scoring.
        seg_scores = (seg_preds * seg_masks.float()).sum((1, 2)) / sum_masks
        cate_scores *= seg_scores

        # sort and keep top nms_pre
        sort_inds = torch.argsort(cate_scores, descending=True)
        if len(sort_inds) > self.nms_per_image:
            sort_inds = sort_inds[: self.nms_per_image]
        seg_masks = seg_masks[sort_inds, :, :]
        seg_preds = seg_preds[sort_inds, :, :]
        sum_masks = sum_masks[sort_inds]
        cate_scores = cate_scores[sort_inds]
        cate_labels = cate_labels[sort_inds]

        # Matrix NMS
        cate_scores = matrix_nms(seg_masks, cate_labels, cate_scores,
                                 kernel=self.nms_kernel, sigma=self.nms_sigma, sum_masks=sum_masks)

        # filter.
        keep = cate_scores >= self.update_threshold
        if keep.sum() == 0:
            return result
        seg_preds = seg_preds[keep, :, :]
        cate_scores = cate_scores[keep]
        cate_labels = cate_labels[keep]

        # sort and keep top_k
        sort_inds = torch.argsort(cate_scores, descending=True)
        if len(sort_inds) > self.max_detections_per_image:
            sort_inds = sort_inds[: self.max_detections_per_image]
        seg_preds = seg_preds[sort_inds, :, :]
        cate_scores = cate_scores[sort_inds]
        cate_labels = cate_labels[sort_inds]

        seg_preds = F.interpolate(seg_preds.unsqueeze(0),
                                  size=upsampled_size_out,
                                  mode='bilinear')[:, :, :h, :w]
        seg_masks = F.interpolate(seg_preds,
                                  size=ori_shape,
                                  mode='bilinear').squeeze(0)
        seg_masks = seg_masks > self.mask_threshold

        seg_masks = BitMasks(seg_masks)
        result.pred_masks = seg_masks
        result.pred_boxes = seg_masks.get_bounding_boxes()
        result.scores = cate_scores
        result.pred_classes = cate_labels
        return result

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images,
                                        self.backbone.size_divisibility)
        return images


class SOLOHead(nn.Module):
    """
    The head used in SOLO for instance segmentation.
    """

    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super().__init__()

        self.num_classes = cfg.MODEL.SOLO.NUM_CLASSES
        self.seg_num_grids = cfg.MODEL.SOLO.NUM_GRIDS
        self.in_channels = input_shape[0].channels
        self.seg_feat_channels = cfg.MODEL.SOLO.HEAD.SEG_FEAT_CHANNELS
        self.stacked_convs = cfg.MODEL.SOLO.HEAD.STACKED_CONVS
        self.prior_prob = cfg.MODEL.SOLO.HEAD.PRIOR_PROB
        self.norm = cfg.MODEL.SOLO.HEAD.NORM
        # Initialization
        self._init_layers()
        self._init_weights()

    def _init_layers(self):
        ins_convs = []
        cate_convs = []
        for i in range(self.stacked_convs):
            # Mask branch
            chn = self.in_channels + 2 if i == 0 else self.seg_feat_channels
            ins_convs.append(
                nn.Conv2d(chn,
                          self.seg_feat_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1,
                          bias=False if self.norm else True)
            )
            if self.norm:
                ins_convs.append(get_norm(self.norm, self.seg_feat_channels))
            ins_convs.append(nn.ReLU(inplace=True))

            # Category branch
            chn = self.in_channels if i == 0 else self.seg_feat_channels
            cate_convs.append(
                nn.Conv2d(chn,
                          self.seg_feat_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1,
                          bias=False if self.norm else True)
            )
            if self.norm:
                cate_convs.append(get_norm(self.norm, self.seg_feat_channels))
            cate_convs.append(nn.ReLU(inplace=True))

        self.ins_convs = nn.Sequential(*ins_convs)
        self.cate_convs = nn.Sequential(*cate_convs)

        self.solo_ins_list = nn.ModuleList()
        for seg_num_grid in self.seg_num_grids:
            self.solo_ins_list.append(
                nn.Conv2d(
                    self.seg_feat_channels,
                    seg_num_grid ** 2,
                    kernel_size=1)
            )
        self.solo_cate = nn.Conv2d(
            self.seg_feat_channels,
            self.num_classes,
            kernel_size=3,
            stride=1,
            padding=1,
        )

    def _init_weights(self):
        for modules in [self.ins_convs, self.cate_convs]:
            for m in modules.modules():
                if isinstance(m, nn.Conv2d):
                    normal_init(m, std=0.01)
        # Use prior in model initialization to improve stability
        bias_value = -math.log((1 - self.prior_prob) / self.prior_prob)
        for modules in [self.solo_ins_list, self.solo_cate]:
            for m in modules.modules():
                if isinstance(m, nn.Conv2d):
                    normal_init(m, std=0.01, bias=bias_value)

    def split_features(self, features):
        return (
            F.interpolate(features[0], scale_factor=0.5, mode='bilinear'),
            features[1],
            features[2],
            features[3],
            F.interpolate(features[4], size=features[3].shape[-2:], mode='bilinear')
        )

    def forward(self, features, eval=False):
        new_features = self.split_features(features)
        featmap_sizes = [featmap.size()[-2:] for featmap in new_features]
        upsampled_size = (featmap_sizes[0][0] * 2, featmap_sizes[0][1] * 2)
        ins_pred, cate_pred = multi_apply(
            self.forward_single_level,
            new_features,
            list(range(len(self.seg_num_grids))),
            eval=eval,
            upsampled_size=upsampled_size
        )
        return ins_pred, cate_pred

    def forward_single_level(self, x, idx, eval=False, upsampled_size=None):
        ins_feat = x
        cate_feat = x
        # Ins branch
        # concat coord
        x_range = torch.linspace(-1, 1, ins_feat.shape[-1], device=ins_feat.device)
        y_range = torch.linspace(-1, 1, ins_feat.shape[-2], device=ins_feat.device)
        y, x = torch.meshgrid(y_range, x_range)
        y = y.expand([ins_feat.shape[0], 1, -1, -1])
        x = x.expand([ins_feat.shape[0], 1, -1, -1])
        coord_feat = torch.cat([x, y], 1)
        ins_feat = torch.cat([ins_feat, coord_feat], 1)

        ins_feat = self.ins_convs(ins_feat)
        ins_feat = F.interpolate(ins_feat, scale_factor=2, mode='bilinear')
        ins_pred = self.solo_ins_list[idx](ins_feat)

        # Cate branch
        seg_num_grid = self.seg_num_grids[idx]
        # Align
        cate_feat = F.interpolate(cate_feat, size=seg_num_grid, mode='bilinear')
        cate_feat = self.cate_convs(cate_feat)
        cate_pred = self.solo_cate(cate_feat)

        if eval:
            ins_pred = F.interpolate(ins_pred.sigmoid(), size=upsampled_size, mode='bilinear')
            cate_pred = points_nms(cate_pred.sigmoid(), kernel=2).permute(0, 2, 3, 1)
        return ins_pred, cate_pred
```

#### cvpods/modeling/meta_arch/__init__.py

```python
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

# import all the meta_arch, so they will be registered

from .auto_assign import AutoAssign
from .borderdet import BorderDet
from .centernet import CenterNet
from .dynamic4seg import DynamicNet4Seg
from .efficientdet import EfficientDet
from .fcn import FCNHead
from .fcos import FCOS
from .free_anchor import FreeAnchor
from .panoptic_fpn import PanopticFPN
from .pointrend import CoarseMaskHead, PointRendROIHeads, PointRendSemSegHead, StandardPointHead
from .rcnn import GeneralizedRCNN, ProposalNetwork
from .reppoints import RepPoints
from .retinanet import RetinaNet
from .semantic_seg import SemanticSegmentor, SemSegFPNHead
from .ssd import SSD
from .tensormask import TensorMask
from .yolov3 import YOLOv3
```

#### cvpods/modeling/meta_arch/dynamic4seg.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.

# network file -> build basic pipline and decoder for Dynamic Network
from typing import Dict

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

from cvpods.layers import Conv2d, ShapeSpec, get_norm
from cvpods.modeling.backbone.dynamic_arch import cal_op_flops
from cvpods.modeling.nn_utils import weight_init
from cvpods.modeling.postprocessing import sem_seg_postprocess
from cvpods.structures import ImageList

__all__ = ["DynamicNet4Seg", "SemSegDecoderHead", "BudgetConstraint"]


class DynamicNet4Seg(nn.Module):
    """
    This module implements Dynamic Network for Semantic Segmentation.
    """
    def __init__(self, cfg):
        super().__init__()
        self.constrain_on = cfg.MODEL.BUDGET.CONSTRAIN
        self.unupdate_rate = cfg.MODEL.BUDGET.UNUPDATE_RATE
        self.device = torch.device(cfg.MODEL.DEVICE)
        self.backbone = cfg.build_backbone(cfg)
        self.sem_seg_head = SemSegDecoderHead(
            cfg, self.backbone.output_shape())
        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(
            -1, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(
            -1, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std
        self.budget_constrint = BudgetConstraint(cfg)
        self.iter = 0
        self.max_iter = cfg.SOLVER.LR_SCHEDULER.MAX_ITER
        self.to(self.device)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
                Each item in the list contains the inputs for one image.
        For now, each item in the list is a dict that contains:
            image: Tensor, image in (C, H, W) format.
            sem_seg: semantic segmentation ground truth
            Other information that's included in the original dicts, such as:
                "height", "width" (int): the output resolution of the model, used in inference.
                    See :meth:`postprocess` for details.
        Returns:
            list[dict]: Each dict is the output for one input image.
                The dict contains one key "sem_seg" whose value is a
                Tensor of the output resolution that represents the
                per-pixel segmentation prediction.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images,
                                        self.backbone.size_divisibility)

        # step_rate: a float, calculated by current_step/total_step,
        #         This parameter is used for Scheduled Drop Path.
        step_rate = self.iter * 1.0 / self.max_iter
        self.iter += 1
        features, expt_flops, real_flops = self.backbone(
            images.tensor, step_rate)

        if "sem_seg" in batched_inputs[0]:
            targets = [x["sem_seg"].to(self.device) for x in batched_inputs]
            targets = ImageList.from_tensors(
                targets, self.backbone.size_divisibility, False,
                self.sem_seg_head.ignore_value).tensor
        else:
            targets = None

        results, losses = self.sem_seg_head(features, targets)
        # calculate flops
        real_flops += self.sem_seg_head.flops
        # remove grad, avoid adding flops to the loss sum
        real_flops = real_flops.detach().requires_grad_(False)
        expt_flops = expt_flops.detach().requires_grad_(False)
        flops = {'real_flops': real_flops, 'expt_flops': expt_flops}
        # use budget constraint for training
        if self.training:
            if self.constrain_on and step_rate >= self.unupdate_rate:
                warm_up_rate = min(
                    1.0, (step_rate - self.unupdate_rate) / 0.02
                )
                loss_budget = self.budget_constrint(
                    expt_flops, warm_up_rate=warm_up_rate
                )
                losses.update({'loss_budget': loss_budget})

            losses.update(flops)
            return losses

        processed_results = []
        for result, input_per_image, image_size in zip(results, batched_inputs,
                                                       images.image_sizes):
            height = input_per_image.get("height")
            width = input_per_image.get("width")
            r = sem_seg_postprocess(result, image_size, height, width)
            processed_results.append({"sem_seg": r, "flops": flops})
        return processed_results


class SemSegDecoderHead(nn.Module):
    """
    This module implements simple decoder head for Semantic Segmentation.
    It creats decoder on top of the dynamic backbone.
    """
    def __init__(self, cfg, input_shape: Dict[str, ShapeSpec]):
        super().__init__()
        # fmt: off
        self.in_features = cfg.MODEL.SEM_SEG_HEAD.IN_FEATURES
        feature_strides = {k: v.stride for k, v in input_shape.items()}  # noqa:F841
        feature_channels = {k: v.channels for k, v in input_shape.items()}
        feature_resolution = {
            k: np.array([v.height, v.width])
            for k, v in input_shape.items()
        }
        self.ignore_value = cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE
        num_classes = cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES
        norm = cfg.MODEL.SEM_SEG_HEAD.NORM
        self.loss_weight = cfg.MODEL.SEM_SEG_HEAD.LOSS_WEIGHT
        self.cal_flops = cfg.MODEL.CAL_FLOPS
        self.real_flops = 0.0
        # fmt: on

        self.layer_decoder_list = nn.ModuleList()
        # set affine in BatchNorm
        if 'Sync' in norm:
            affine = True
        else:
            affine = False
        # use simple decoder
        for _feat in self.in_features:
            res_size = feature_resolution[_feat]
            in_channel = feature_channels[_feat]
            if _feat == 'layer_0':
                out_channel = in_channel
            else:
                out_channel = in_channel // 2
            conv_1x1 = Conv2d(in_channel,
                              out_channel,
                              kernel_size=1,
                              stride=1,
                              padding=0,
                              bias=False,
                              norm=get_norm(norm, out_channel),
                              activation=nn.ReLU())
            self.real_flops += cal_op_flops.count_ConvBNReLU_flop(
                res_size[0],
                res_size[1],
                in_channel,
                out_channel, [1, 1],
                is_affine=affine)
            self.layer_decoder_list.append(conv_1x1)
        # using Kaiming init
        for layer in self.layer_decoder_list:
            for m in layer.modules():
                if isinstance(m, nn.Conv2d):
                    weight_init.kaiming_init(m, mode='fan_in')
                elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):
                    if m.weight is not None:
                        nn.init.constant_(m.weight, 1)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
        in_channel = feature_channels['layer_0']
        # the output layer
        self.predictor = Conv2d(in_channels=in_channel,
                                out_channels=num_classes,
                                kernel_size=3,
                                stride=1,
                                padding=1)
        self.real_flops += cal_op_flops.count_Conv_flop(
            feature_resolution['layer_0'][0], feature_resolution['layer_0'][1],
            in_channel, num_classes, [3, 3])
        # using Kaiming init
        for m in self.predictor.modules():
            if isinstance(m, nn.Conv2d):
                weight_init.kaiming_init(m, mode='fan_in')
            elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):
                if m.weight is not None:
                    nn.init.constant_(m.weight, 1)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, features, targets=None):
        pred, pred_output = None, None
        for _index in range(len(self.in_features)):
            out_index = len(self.in_features) - _index - 1
            out_feat = features[self.in_features[out_index]]
            if out_index <= 2:
                out_feat = pred + out_feat
            pred = self.layer_decoder_list[out_index](out_feat)
            if out_index > 0:
                pred = F.interpolate(input=pred,
                                     scale_factor=2,
                                     mode='bilinear',
                                     align_corners=False)
            else:
                pred_output = pred
        # pred output
        pred_output = self.predictor(pred_output)
        pred_output = F.interpolate(input=pred_output,
                                    scale_factor=4,
                                    mode='bilinear',
                                    align_corners=False)

        if self.training:
            losses = {}
            losses["loss_sem_seg"] = (
                F.cross_entropy(
                    pred_output, targets, reduction="mean",
                    ignore_index=self.ignore_value
                ) * self.loss_weight
            )
            return [], losses
        else:
            return pred_output, {}

    @property
    def flops(self):
        return self.real_flops


class BudgetConstraint(nn.Module):
    """
    Given budget constraint to reduce expected inference FLOPs in the Dynamic Network.
    """
    def __init__(self, cfg):
        super().__init__()
        # fmt: off
        self.loss_weight = cfg.MODEL.BUDGET.LOSS_WEIGHT
        self.loss_mu = cfg.MODEL.BUDGET.LOSS_MU
        self.flops_all = cfg.MODEL.BUDGET.FLOPS_ALL
        self.warm_up = cfg.MODEL.BUDGET.WARM_UP
        # fmt: on

    def forward(self, flops_expt, warm_up_rate=1.0):
        if self.warm_up:
            warm_up_rate = min(1.0, warm_up_rate)
        else:
            warm_up_rate = 1.0
        losses = self.loss_weight * warm_up_rate * (
            (flops_expt / self.flops_all - self.loss_mu)**2
        )
        return losses
```

#### cvpods/modeling/meta_arch/reppoints.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.

import logging
import math
from typing import List

import numpy as np

import torch
import torch.nn as nn

from cvpods.layers import ShapeSpec, cat, generalized_batched_nms
from cvpods.layers.deform_conv import DeformConv
from cvpods.modeling.losses import sigmoid_focal_loss_jit, smooth_l1_loss
from cvpods.modeling.postprocessing import detector_postprocess
from cvpods.structures import Boxes, ImageList, Instances, pairwise_iou
from cvpods.utils import log_first_n


class RepPoints(nn.Module):
    """
    Implement RepPoints (https://arxiv.org/abs/1904.11490)
    """
    def __init__(self, cfg):
        super().__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)

        model_params = cfg.MODEL.REPPOINTS
        # pyramid feature:
        self.num_classes = model_params.NUM_CLASSES
        self.in_features = model_params.IN_FEATURES
        self.fpn_strides = model_params.FPN_STRIDES

        # loss parameters:
        self.focal_loss_gamma = model_params.FOCAL_LOSS_GAMMA
        self.focal_loss_alpha = model_params.FOCAL_LOSS_ALPHA
        self.loss_cls_weight = model_params.LOSS_CLS_WEIGHT
        self.loss_bbox_init_weight = model_params.LOSS_BBOX_INIT_WEIGHT
        self.loss_bbox_refine_weight = model_params.LOSS_BBOX_REFINE_WEIGHT

        # reppoints parameters:
        self.point_base_scale = model_params.POINT_BASE_SCALE
        self.num_points = model_params.NUM_POINTS
        self.transform_method = model_params.TRANSFORM_METHOD
        self.moment_mul = model_params.MOMENT_MUL

        if self.transform_method == 'moment':
            self.moment_transfer = nn.Parameter(data=torch.zeros(2),
                                                requires_grad=True)

        self.backbone = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))
        backbone_shape = self.backbone.output_shape()
        feature_shapes = [backbone_shape[f] for f in self.in_features]
        self.head = RepPointsHead(cfg, feature_shapes)
        self.shift_generator = cfg.build_shift_generator(cfg, feature_shapes)

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(
            3, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(
            3, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std
        self.to(self.device)

        # inference parameters:
        self.score_threshold = model_params.SCORE_THRESH_TEST
        self.topk_candidates = model_params.TOPK_CANDIDATES_TEST
        self.nms_threshold = model_params.NMS_THRESH_TEST
        self.nms_type = cfg.MODEL.NMS_TYPE
        self.max_detections_per_image = cfg.TEST.DETECTIONS_PER_IMAGE

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of : class:`DatasetMapper`.
            Each item in the list contains the input for one image.
            For now, each item in the list is a dict that contains:
             * images: Tensor, image in (C, H, W) format.
             * instances: Instances.
             Other information that' s included in the original dict ,such as:
             * "height", "width"(int): the output resolution of the model,
             used in inference.See  `postprocess` for detail
        Return:
            dict[str: Tensor]:
                mapping from a named loss to a tensor storing the loss, Used
                during training only.
                At inference stage, return predicted bboxes.
        """
        images = self.preprocess_image(batched_inputs)
        if "instances" in batched_inputs[0]:
            gt_instances = [
                x['instances'].to(self.device) for x in batched_inputs
            ]
        elif "targets" in batched_inputs[0]:
            log_first_n(logging.WARN,
                        "'targets' in the model inputs is \
                            now renamed to 'instances'!",
                        n=10)
            gt_instances = [
                x['instances'].to(self.device) for x in batched_inputs
            ]
        else:
            gt_instances = None

        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]
        cls_outs, pts_outs_init, pts_outs_refine = self.head(features)
        center_pts = self.shift_generator(features)

        if self.training:
            return self.losses(center_pts, cls_outs, pts_outs_init,
                               pts_outs_refine, gt_instances)
        else:
            results = self.inference(center_pts, cls_outs, pts_outs_init,
                                     pts_outs_refine, images)
            processed_results = []
            for results_per_image, input_per_image, image_size in zip(
                    results, batched_inputs, images.image_sizes):
                height = input_per_image.get("height", image_size[0])
                width = input_per_image.get("width", image_size[1])
                r = detector_postprocess(results_per_image, height, width)
                processed_results.append({"instances": r})
            return processed_results

    def losses(self, center_pts, cls_outs, pts_outs_init, pts_outs_refine,
               targets):
        """
        Args:
            center_pts: (list[list[Tensor]]): a list of N=#image elements. Each
                is a list of #feature level tensors. The tensors contains
                shifts of this image on the specific feature level.
            cls_outs: List[Tensor], each item in list with
                shape:[N, num_classes, H, W]
            pts_outs_init: List[Tensor], each item in list with
                shape:[N, num_points*2, H, W]
            pts_outs_refine: List[Tensor], each item in list with
            shape:[N, num_points*2, H, W]
            targets: (list[Instances]): a list of N `Instances`s. The i-th
                `Instances` contains the ground-truth per-instance annotations
                for the i-th input image.
                Specify `targets` during training only.

        Returns:
            dict[str:Tensor]:
                mapping from a named loss to scalar tensor
        """
        featmap_sizes = [featmap.size()[-2:] for featmap in cls_outs]
        assert len(featmap_sizes) == len(center_pts[0])

        pts_dim = 2 * self.num_points

        cls_outs = [
            cls_out.permute(0, 2, 3, 1).reshape(cls_out.size(0), -1,
                                                self.num_classes)
            for cls_out in cls_outs
        ]
        pts_outs_init = [
            pts_out_init.permute(0, 2, 3, 1).reshape(pts_out_init.size(0), -1,
                                                     pts_dim)
            for pts_out_init in pts_outs_init
        ]
        pts_outs_refine = [
            pts_out_refine.permute(0, 2, 3, 1).reshape(pts_out_refine.size(0),
                                                       -1, pts_dim)
            for pts_out_refine in pts_outs_refine
        ]

        cls_outs = torch.cat(cls_outs, dim=1)
        pts_outs_init = torch.cat(pts_outs_init, dim=1)
        pts_outs_refine = torch.cat(pts_outs_refine, dim=1)

        pts_strides = []
        for i, s in enumerate(center_pts[0]):
            pts_strides.append(
                cls_outs.new_full((s.size(0), ), self.fpn_strides[i]))
        pts_strides = torch.cat(pts_strides, dim=0)

        center_pts = [
            torch.cat(c_pts, dim=0).to(self.device) for c_pts in center_pts
        ]

        pred_cls = []
        pred_init = []
        pred_refine = []

        target_cls = []
        target_init = []
        target_refine = []

        num_pos_init = 0
        num_pos_refine = 0

        for img, (per_center_pts, cls_prob, pts_init, pts_refine,
                  per_targets) in enumerate(
                      zip(center_pts, cls_outs, pts_outs_init, pts_outs_refine,
                          targets)):
            assert per_center_pts.shape[:-1] == cls_prob.shape[:-1]

            gt_bboxes = per_targets.gt_boxes.to(cls_prob.device)
            gt_labels = per_targets.gt_classes.to(cls_prob.device)

            pts_init_bbox_targets, pts_init_labels_targets = \
                self.point_targets(per_center_pts,
                                   pts_strides,
                                   gt_bboxes.tensor,
                                   gt_labels)

            # per_center_pts, shape:[N, 18]
            per_center_pts_repeat = per_center_pts.repeat(1, self.num_points)

            normalize_term = self.point_base_scale * pts_strides
            normalize_term = normalize_term.reshape(-1, 1)

            # bbox_center = torch.cat([per_center_pts, per_center_pts], dim=1)
            per_pts_strides = pts_strides.reshape(-1, 1)
            pts_init_coordinate = pts_init * per_pts_strides + \
                per_center_pts_repeat
            init_bbox_pred = self.pts_to_bbox(pts_init_coordinate)

            foreground_idxs = (pts_init_labels_targets >= 0) & \
                (pts_init_labels_targets != self.num_classes)

            pred_init.append(init_bbox_pred[foreground_idxs]
                             / normalize_term[foreground_idxs])
            target_init.append(pts_init_bbox_targets[foreground_idxs]
                               / normalize_term[foreground_idxs])
            num_pos_init += foreground_idxs.sum()

            # A another way to convert predicted offset to bbox
            # bbox_pred_init = self.pts_to_bbox(pts_init.detach()) * \
            #     per_pts_strides
            # init_bbox_pred = bbox_center + bbox_pred_init

            pts_refine_bbox_targets, pts_refine_labels_targets = \
                self.bbox_targets(init_bbox_pred, gt_bboxes, gt_labels)

            pts_refine_coordinate = pts_refine * per_pts_strides + \
                per_center_pts_repeat
            refine_bbox_pred = self.pts_to_bbox(pts_refine_coordinate)

            # bbox_pred_refine = self.pts_to_bbox(pts_refine) * per_pts_strides
            # refine_bbox_pred = bbox_center + bbox_pred_refine

            foreground_idxs = (pts_refine_labels_targets >= 0) & \
                (pts_refine_labels_targets != self.num_classes)

            pred_refine.append(refine_bbox_pred[foreground_idxs]
                               / normalize_term[foreground_idxs])
            target_refine.append(pts_refine_bbox_targets[foreground_idxs]
                                 / normalize_term[foreground_idxs])
            num_pos_refine += foreground_idxs.sum()

            gt_classes_target = torch.zeros_like(cls_prob)
            gt_classes_target[foreground_idxs,
                              pts_refine_labels_targets[foreground_idxs]] = 1
            pred_cls.append(cls_prob)
            target_cls.append(gt_classes_target)

        pred_cls = torch.cat(pred_cls, dim=0)
        pred_init = torch.cat(pred_init, dim=0)
        pred_refine = torch.cat(pred_refine, dim=0)

        target_cls = torch.cat(target_cls, dim=0)
        target_init = torch.cat(target_init, dim=0)
        target_refine = torch.cat(target_refine, dim=0)

        loss_cls = sigmoid_focal_loss_jit(
            pred_cls,
            target_cls,
            alpha=self.focal_loss_alpha,
            gamma=self.focal_loss_gamma,
            reduction="sum") / max(
                1, num_pos_refine.item()) * self.loss_cls_weight

        loss_pts_init = smooth_l1_loss(
            pred_init, target_init, beta=0.11, reduction='sum') / max(
                1, num_pos_init.item()) * self.loss_bbox_init_weight

        loss_pts_refine = smooth_l1_loss(
            pred_refine, target_refine, beta=0.11, reduction='sum') / max(
                1, num_pos_refine.item()) * self.loss_bbox_refine_weight

        return {
            "loss_cls": loss_cls,
            "loss_pts_init": loss_pts_init,
            "loss_pts_refine": loss_pts_refine
        }

    def pts_to_bbox(self, points):
        """
        Converting the points set into bounding box.

        :param pts: the input points sets (fields), each points
            set (fields) is represented as 2n scalar.
        :return: each points set is converting to a bbox [x1, y1, x2, y2].
        """
        pts_x = points[:, 0::2]
        pts_y = points[:, 1::2]

        if self.transform_method == "minmax":
            bbox_left = pts_x.min(dim=1, keepdim=True)[0]
            bbox_right = pts_x.max(dim=1, keepdim=True)[0]
            bbox_top = pts_y.min(dim=1, keepdim=True)[0]
            bbox_bottom = pts_y.max(dim=1, keepdim=True)[0]
            bbox = torch.cat([bbox_left, bbox_top, bbox_right, bbox_bottom],
                             dim=1)
        elif self.transform_method == "partial_minmax":
            bbox_left = pts_x[:, :4].min(dim=1, keepdim=True)[0]
            bbox_right = pts_x[:, :4].max(dim=1, keepdim=True)[0]
            bbox_top = pts_y[:, :4].min(dim=1, keepdim=True)[0]
            bbox_bottom = pts_y[:, :4].max(dim=1, keepdim=True)[0]
            bbox = torch.cat([bbox_left, bbox_top, bbox_right, bbox_bottom],
                             dim=1)
        elif self.transform_method == "moment":
            pts_x_mean = pts_x.mean(dim=1, keepdim=True)
            pts_y_mean = pts_y.mean(dim=1, keepdim=True)
            pts_x_std = pts_x.std(dim=1, keepdim=True)
            pts_y_std = pts_y.std(dim=1, keepdim=True)
            moment_transfer = self.moment_transfer * self.moment_mul + \
                self.moment_transfer.detach() * (1 - self.moment_mul)
            moment_transfer_width = moment_transfer[0]
            moment_transfer_height = moment_transfer[1]
            half_width = pts_x_std * moment_transfer_width.exp()
            half_height = pts_y_std * moment_transfer_height.exp()
            bbox = torch.cat([
                pts_x_mean - half_width, pts_y_mean - half_height,
                pts_x_mean + half_width, pts_y_mean + half_height
            ], dim=1)
        else:
            raise ValueError

        return bbox

    @torch.no_grad()
    def point_targets(self, points, pts_strides, gt_bboxes, gt_labels):
        """
        Target assign: point assign. Compute corresponding GT box and classification targets
        for proposals.

        Args:
            points: pred boxes
            pts_strides: boxes stride of current point(box)
            gt_bboxes: gt boxes
            gt_labels: gt labels

        Returns:
            assigned_bboxes, assigned_labels
        """
        if points.shape[0] == 0 or gt_bboxes.shape[0] == 0:
            raise ValueError('No gt or bboxes')
        points_lvl = torch.log2(pts_strides).int()
        lvl_min, lvl_max = points_lvl.min(), points_lvl.max()
        num_gts, num_points = gt_bboxes.shape[0], points.shape[0]

        # assign gt box
        gt_bboxes_ctr_xy = (gt_bboxes[:, :2] + gt_bboxes[:, 2:]) / 2
        gt_bboxes_wh = (gt_bboxes[:, 2:] - gt_bboxes[:, :2]).clamp(min=1e-6)

        scale = self.point_base_scale

        gt_bboxes_lvl = ((torch.log2(gt_bboxes_wh[:, 0] / scale)
                          + torch.log2(gt_bboxes_wh[:, 1] / scale)) / 2).int()
        gt_bboxes_lvl = torch.clamp(gt_bboxes_lvl, min=lvl_min, max=lvl_max)

        assigned_gt_inds = points.new_zeros((num_points, ), dtype=torch.long)
        assigned_gt_dist = points.new_full((num_points, ), float('inf'))
        points_range = torch.arange(points.shape[0])

        for idx in range(num_gts):
            gt_lvl = gt_bboxes_lvl[idx]
            lvl_idx = gt_lvl == points_lvl
            points_index = points_range[lvl_idx]
            lvl_points = points[lvl_idx, :]
            gt_point = gt_bboxes_ctr_xy[[idx], :]
            gt_wh = gt_bboxes_wh[[idx], :]

            points_gt_dist = ((lvl_points - gt_point) / gt_wh).norm(dim=1)
            min_dist, min_dist_index = torch.topk(points_gt_dist, 1, largest=False)
            min_dist_points_index = points_index[min_dist_index]
            less_than_recorded_index = min_dist < assigned_gt_dist[
                min_dist_points_index]
            min_dist_points_index = min_dist_points_index[less_than_recorded_index]

            assigned_gt_inds[min_dist_points_index] = idx + 1
            assigned_gt_dist[min_dist_points_index] = min_dist[less_than_recorded_index]

        assigned_bboxes = points.new_zeros((num_points, 4))
        assigned_labels = points.new_full((num_points, ), self.num_classes)

        pos_inds = torch.nonzero(assigned_gt_inds > 0, as_tuple=False).squeeze()
        if pos_inds.numel() > 0:
            assigned_labels[pos_inds] = (
                gt_labels[assigned_gt_inds[pos_inds] - 1].to(assigned_labels.dtype)
            )
            assigned_bboxes[pos_inds] = gt_bboxes[assigned_gt_inds[pos_inds] - 1]

        return assigned_bboxes, assigned_labels

    @torch.no_grad()
    def bbox_targets(self,
                     candidate_bboxes,
                     gt_bboxes,
                     gt_labels,
                     pos_iou_thr=0.5,
                     neg_iou_thr=0.4,
                     gt_max_matching=True):
        """
        Target assign: MaxIoU assign

        Args:
            candidate_bboxes:
            gt_bboxes:
            gt_labels:
            pos_iou_thr:
            neg_iou_thr:
            gt_max_matching:

        Returns:

        """
        if candidate_bboxes.size(0) == 0 or gt_bboxes.tensor.size(0) == 0:
            raise ValueError('No gt or anchors')

        candidate_bboxes[:, 0].clamp_(min=0)
        candidate_bboxes[:, 1].clamp_(min=0)
        candidate_bboxes[:, 2].clamp_(min=0)
        candidate_bboxes[:, 3].clamp_(min=0)

        num_candidates = candidate_bboxes.size(0)

        overlaps = pairwise_iou(Boxes(candidate_bboxes), gt_bboxes)
        assigned_labels = overlaps.new_full((overlaps.size(0), ),
                                            self.num_classes,
                                            dtype=torch.long)

        # for each anchor, which gt best overlaps with it
        # for each anchor, the max iou of all gts
        max_overlaps, argmax_overlaps = overlaps.max(dim=1)
        # for each gt, which anchor best overlaps with it
        # for each gt, the max iou of all proposals
        gt_max_overlaps, gt_argmax_overlaps = overlaps.max(dim=0)

        bg_inds = max_overlaps < neg_iou_thr
        assigned_labels[bg_inds] = self.num_classes

        fg_inds = max_overlaps >= pos_iou_thr
        assigned_labels[fg_inds] = gt_labels[argmax_overlaps[fg_inds]]

        if gt_max_matching:
            fg_inds = torch.nonzero(overlaps == gt_max_overlaps, as_tuple=False)[:, 0]
            assigned_labels[fg_inds] = gt_labels[argmax_overlaps[fg_inds]]

        assigned_bboxes = overlaps.new_zeros((num_candidates, 4))

        fg_inds = (assigned_labels >= 0) & (assigned_labels
                                            != self.num_classes)
        assigned_bboxes[fg_inds] = gt_bboxes.tensor[argmax_overlaps[fg_inds]]

        return assigned_bboxes, assigned_labels

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        images = [x['image'].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images,
                                        self.backbone.size_divisibility)

        return images

    def inference(self, center_pts, cls_outs, pts_outs_init, pts_outs_refine,
                  images):
        """
        Argumments:
            cls_outs, pts_outs_init, pts_outs_refine:
                Same as the output of :`RepPointsHead.forward`
            center_pts: (list[list[Tensor]]): a list of N=#image elements. Each
                is a list of #feature level tensors. The tensors contains
                shifts of this image on the specific feature level.
        Returns:
            results (List[Instances]): a list of #images elements
        """
        assert len(center_pts) == len(images)
        results = []
        cls_outs = [
            x.permute(0, 2, 3, 1).reshape(x.size(0), -1, self.num_classes)
            for x in cls_outs
        ]
        pts_outs_init = [
            x.permute(0, 2, 3, 1).reshape(x.size(0), -1, self.num_points * 2)
            for x in pts_outs_init
        ]
        pts_outs_refine = [
            x.permute(0, 2, 3, 1).reshape(x.size(0), -1, self.num_points * 2)
            for x in pts_outs_refine
        ]

        pts_strides = []
        for i, s in enumerate(center_pts[0]):
            pts_strides.append(cls_outs[0].new_full((s.size(0), ),
                                                    self.fpn_strides[i]))
        # pts_strides = torch.cat(pts_strides, dim=0)

        for img_idx, center_pts_per_image in enumerate(center_pts):
            image_size = images.image_sizes[img_idx]
            cls_outs_per_img = [
                cls_outs_per_level[img_idx] for cls_outs_per_level in cls_outs
            ]
            pts_outs_refine_per_img = [
                pts_outs_refine_per_level[img_idx]
                for pts_outs_refine_per_level in pts_outs_refine
            ]
            results_per_img = self.inference_single_image(
                cls_outs_per_img, pts_outs_refine_per_img, pts_strides,
                center_pts_per_image, tuple(image_size))
            results.append(results_per_img)
        return results

    def inference_single_image(self, cls_logits, pts_refine, pts_strides,
                               points, image_size):
        """
        Single-image inference. Return bounding-box detection results by
        thresholding on scores and applying non-maximum suppression (NMS).

        Arguemnts:
            cls_logits (list[Tensor]): list of #feature levels. Each entry
                contains tensor of size (H x W, K)
            pts_refine (list[Tensor]): Same shape as 'cls_logits' except that K
                becomes 2 * num_points.
            pts_strides (list(Tensor)): list of #feature levels. Each entry
                contains tensor of size (H x W, )
            points (list[Tensor]): list of #feature levels. Each entry contains
                a tensor, which contains all the points for that
                image in that feature level.
            image_size (tuple(H, W)): a tuple of the image height and width.
        Returns:
            Same as `inference`, but only for one image
        """
        assert len(cls_logits) == len(pts_refine) == len(pts_strides)
        boxes_all = []
        scores_all = []
        class_idxs_all = []

        # Iterate over every feature level
        for cls_logits_i, pts_refine_i, points_i, pts_strides_i in zip(
                cls_logits, pts_refine, points, pts_strides):

            bbox_pos_center = torch.cat([points_i, points_i], dim=1)
            bbox_pred = self.pts_to_bbox(pts_refine_i)
            bbox_pred = bbox_pred * pts_strides_i.reshape(-1,
                                                          1) + bbox_pos_center
            bbox_pred[:, 0].clamp_(min=0, max=image_size[1])
            bbox_pred[:, 1].clamp_(min=0, max=image_size[0])
            bbox_pred[:, 2].clamp_(min=0, max=image_size[1])
            bbox_pred[:, 3].clamp_(min=0, max=image_size[0])

            # (HxWxK, )
            point_cls_i = cls_logits_i.flatten().sigmoid_()

            # keep top k scoring indices only
            num_topk = min(self.topk_candidates, point_cls_i.size(0))
            # torch.sort is actually faster than .topk (at least on GPUs)
            predicted_prob, topk_idxs = point_cls_i.sort(descending=True)
            predicted_prob = predicted_prob[:num_topk]
            topk_idxs = topk_idxs[:num_topk]

            # filter out the proposals with low confidence score
            keep_idxs = predicted_prob > self.score_threshold
            predicted_prob = predicted_prob[keep_idxs]
            topk_idxs = topk_idxs[keep_idxs]

            point_idxs = topk_idxs // self.num_classes
            classes_idxs = topk_idxs % self.num_classes

            predicted_boxes = bbox_pred[point_idxs]

            boxes_all.append(predicted_boxes)
            scores_all.append(predicted_prob)
            class_idxs_all.append(classes_idxs)

        boxes_all, scores_all, class_idxs_all = [
            cat(x) for x in [boxes_all, scores_all, class_idxs_all]
        ]

        keep = generalized_batched_nms(boxes_all, scores_all, class_idxs_all,
                                       self.nms_threshold, nms_type=self.nms_type)
        keep = keep[:self.max_detections_per_image]

        result = Instances(image_size)
        result.pred_boxes = Boxes(boxes_all[keep])
        result.scores = scores_all[keep]
        result.pred_classes = class_idxs_all[keep]

        return result


class RepPointsHead(nn.Module):
    """
    The head used in RepPoints for object classification and box regression.
    It has two subnets for the two tasks, which is response for classification
    and regression respectively.
    """
    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super().__init__()
        head_params = cfg.MODEL.REPPOINTS
        self.in_channels = input_shape[0].channels
        self.num_classes = head_params.NUM_CLASSES
        self.feat_channels = head_params.FEAT_CHANNELS
        self.point_feat_channels = head_params.POINT_FEAT_CHANNELS
        self.stacked_convs = head_params.STACK_CONVS
        self.norm_mode = head_params.NORM_MODE
        self.num_points = head_params.NUM_POINTS
        self.gradient_mul = head_params.GRADIENT_MUL
        self.prior_prob = head_params.PRIOR_PROB

        self.dcn_kernel = int(np.sqrt(self.num_points))
        self.dcn_pad = int((self.dcn_kernel - 1) / 2)
        dcn_base = np.arange(-self.dcn_pad,
                             self.dcn_pad + 1).astype(np.float64)
        dcn_base_y = np.repeat(dcn_base, self.dcn_kernel)
        dcn_base_x = np.tile(dcn_base, self.dcn_kernel)
        dcn_base_offset = np.stack([dcn_base_y, dcn_base_x], axis=1).reshape(
            (-1))
        self.dcn_base_offset = torch.tensor(dcn_base_offset).view(1, -1, 1, 1)

        self.relu = nn.ReLU(inplace=True)
        self.cls_convs = nn.ModuleList()
        self.reg_convs = nn.ModuleList()

        for i in range(self.stacked_convs):
            chn = self.in_channels if i == 0 else self.feat_channels
            self.cls_convs.append(
                nn.Conv2d(chn,
                          self.feat_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1,
                          bias=False))
            if self.norm_mode == 'GN':
                self.cls_convs.append(
                    nn.GroupNorm(32 * self.feat_channels // 256,
                                 self.feat_channels))
            else:
                raise ValueError('The normalization method in reppoints \
                            head should be GN')
            self.cls_convs.append(nn.ReLU(inplace=True))

            self.reg_convs.append(
                nn.Conv2d(chn,
                          self.feat_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1,
                          bias=False))
            if self.norm_mode == 'GN':
                self.reg_convs.append(
                    nn.GroupNorm(32 * self.feat_channels // 256,
                                 self.feat_channels))
            else:
                raise ValueError('The normalization method in reppoints \
                            head should be GN')
            self.reg_convs.append(nn.ReLU(inplace=True))

        point_out_dim = 2 * self.num_points
        self.reppoints_cls_conv = DeformConv(self.feat_channels,
                                             self.point_feat_channels,
                                             self.dcn_kernel, 1, self.dcn_pad)
        self.reppoints_cls_out = nn.Conv2d(self.feat_channels,
                                           self.num_classes, 1, 1, 0)
        self.reppoints_pts_init_conv = nn.Conv2d(self.feat_channels,
                                                 self.point_feat_channels, 3,
                                                 1, 1)
        self.reppoints_pts_init_out = nn.Conv2d(self.point_feat_channels,
                                                point_out_dim, 1, 1, 0)
        self.reppoints_pts_refine_conv = DeformConv(self.feat_channels,
                                                    self.point_feat_channels,
                                                    self.dcn_kernel, 1,
                                                    self.dcn_pad)
        self.reppoints_pts_refine_out = nn.Conv2d(self.point_feat_channels,
                                                  point_out_dim, 1, 1, 0)
        self.init_weights()

    def init_weights(self):
        """
        Initialize model weights
        """
        for modules in [
                self.cls_convs, self.reg_convs, self.reppoints_cls_conv,
                self.reppoints_cls_out, self.reppoints_pts_init_conv,
                self.reppoints_pts_init_out, self.reppoints_pts_refine_conv,
                self.reppoints_pts_refine_out
        ]:
            for layer in modules.modules():
                if isinstance(layer, nn.Conv2d):
                    nn.init.normal_(layer.weight, mean=0, std=0.01)
                    if hasattr(layer, 'bias') and layer.bias is not None:
                        nn.init.constant_(layer.bias, 0)
                if isinstance(layer, nn.GroupNorm):
                    nn.init.constant_(layer.weight, 1)
                    nn.init.constant_(layer.bias, 0)
        bias_value = -math.log((1 - self.prior_prob) / self.prior_prob)
        nn.init.constant_(self.reppoints_cls_out.bias, bias_value)

    def forward(self, features):
        """
        Arguments:
            features (list[Tensor]): FPN feature map tensors
            in high to low resolutions.Each tensor in the list
            correspond to different feature levels.

        Returns:
            cls_outs (list[Tensor]): list of #feature levels.
            Each entry contains tensor of size (H x W, K)
            pts_outs_init (list[Tensor]): list of #feature levels,
            each entry containstensor of size (H x W, num_points * 2)
            pts_outs_refine (list[Tensor]): list of #feature levels,
            each entry contains tensor of size (H x W, num_points * 2)
        """
        dcn_base_offsets = self.dcn_base_offset.type_as(features[0])

        cls_outs = []
        pts_outs_init = []
        pts_outs_refine = []

        for feature in features:
            reg_feat = cls_feat = feature

            for cls_conv in self.cls_convs:
                cls_feat = cls_conv(cls_feat)
            for reg_conv in self.reg_convs:
                reg_feat = reg_conv(reg_feat)

            # initialize reppoints
            pts_out_init = self.reppoints_pts_init_out(
                self.relu(self.reppoints_pts_init_conv(reg_feat)))
            pts_outs_init.append(pts_out_init)
            # refine and classify reppoints
            pts_out_init_grad_mul = (1 - self.gradient_mul) * \
                pts_out_init.detach() + self.gradient_mul * pts_out_init
            dcn_offset = pts_out_init_grad_mul - dcn_base_offsets

            cls_outs.append(
                self.reppoints_cls_out(
                    self.relu(self.reppoints_cls_conv(cls_feat, dcn_offset))
                )
            )
            pts_out_refine = self.reppoints_pts_refine_out(
                self.relu(self.reppoints_pts_refine_conv(reg_feat, dcn_offset))
            )
            pts_out_refine = pts_out_refine + pts_out_init.detach()

            pts_outs_refine.append(pts_out_refine)

        return cls_outs, pts_outs_init, pts_outs_refine
```

#### cvpods/modeling/meta_arch/fcos.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.

import logging
import math
from typing import List

import torch
import torch.nn.functional as F
from torch import nn

from cvpods.layers import ShapeSpec, cat, generalized_batched_nms
from cvpods.modeling.box_regression import Shift2BoxTransform
from cvpods.modeling.losses import iou_loss, sigmoid_focal_loss_jit
from cvpods.modeling.meta_arch.retinanet import permute_to_N_HWA_K
from cvpods.modeling.postprocessing import detector_postprocess
from cvpods.structures import Boxes, ImageList, Instances
from cvpods.utils import comm, log_first_n


def permute_all_cls_and_box_to_N_HWA_K_and_concat(
    box_cls, box_delta, box_center, num_classes=80
):
    """
    Rearrange the tensor layout from the network output, i.e.:
    list[Tensor]: #lvl tensors of shape (N, A x K, Hi, Wi)
    to per-image predictions, i.e.:
    Tensor: of shape (N x sum(Hi x Wi x A), K)
    """
    # for each feature level, permute the outputs to make them be in the
    # same format as the labels. Note that the labels are computed for
    # all feature levels concatenated, so we keep the same representation
    # for the objectness, the box_delta and the centerness
    box_cls_flattened = [permute_to_N_HWA_K(x, num_classes) for x in box_cls]
    box_delta_flattened = [permute_to_N_HWA_K(x, 4) for x in box_delta]
    box_center_flattened = [permute_to_N_HWA_K(x, 1) for x in box_center]
    # concatenate on the first dimension (representing the feature levels), to
    # take into account the way the labels were generated (with all feature maps
    # being concatenated as well)
    box_cls = cat(box_cls_flattened, dim=1).view(-1, num_classes)
    box_delta = cat(box_delta_flattened, dim=1).view(-1, 4)
    box_center = cat(box_center_flattened, dim=1).view(-1, 1)
    return box_cls, box_delta, box_center


class Scale(nn.Module):
    def __init__(self, init_value=1.0):
        super(Scale, self).__init__()
        self.scale = nn.Parameter(torch.FloatTensor([init_value]))

    def forward(self, input):
        return input * self.scale


class FCOS(nn.Module):
    """
    Implement FCOS (https://arxiv.org/abs/1708.02002).
    """
    def __init__(self, cfg):
        super().__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)

        # fmt: off
        self.num_classes = cfg.MODEL.FCOS.NUM_CLASSES
        self.in_features = cfg.MODEL.FCOS.IN_FEATURES
        self.fpn_strides = cfg.MODEL.FCOS.FPN_STRIDES
        # Loss parameters:
        self.focal_loss_alpha = cfg.MODEL.FCOS.FOCAL_LOSS_ALPHA
        self.focal_loss_gamma = cfg.MODEL.FCOS.FOCAL_LOSS_GAMMA
        self.iou_loss_type = cfg.MODEL.FCOS.IOU_LOSS_TYPE
        self.center_sampling_radius = cfg.MODEL.FCOS.CENTER_SAMPLING_RADIUS
        # Inference parameters:
        self.score_threshold = cfg.MODEL.FCOS.SCORE_THRESH_TEST
        self.topk_candidates = cfg.MODEL.FCOS.TOPK_CANDIDATES_TEST
        self.nms_threshold = cfg.MODEL.FCOS.NMS_THRESH_TEST
        self.nms_type = cfg.MODEL.NMS_TYPE
        self.max_detections_per_image = cfg.TEST.DETECTIONS_PER_IMAGE
        # fmt: on

        self.backbone = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))

        backbone_shape = self.backbone.output_shape()
        feature_shapes = [backbone_shape[f] for f in self.in_features]
        self.head = FCOSHead(cfg, feature_shapes)
        self.shift_generator = cfg.build_shift_generator(cfg, feature_shapes)

        # Matching and loss
        self.shift2box_transform = Shift2BoxTransform(
            weights=cfg.MODEL.FCOS.BBOX_REG_WEIGHTS)
        self.object_sizes_of_interest = cfg.MODEL.FCOS.OBJECT_SIZES_OF_INTEREST

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(
            3, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(
            3, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std
        self.to(self.device)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
                Each item in the list contains the inputs for one image.
                For now, each item in the list is a dict that contains:

                * image: Tensor, image in (C, H, W) format.
                * instances: Instances

                Other information that's included in the original dicts, such as:

                * "height", "width" (int): the output resolution of the model, used in inference.
                    See :meth:`postprocess` for details.
        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a tensor storing the loss. Used during training only.
        """
        images = self.preprocess_image(batched_inputs)
        if "instances" in batched_inputs[0]:
            gt_instances = [
                x["instances"].to(self.device) for x in batched_inputs
            ]
        elif "targets" in batched_inputs[0]:
            log_first_n(
                logging.WARN,
                "'targets' in the model inputs is now renamed to 'instances'!",
                n=10)
            gt_instances = [
                x["targets"].to(self.device) for x in batched_inputs
            ]
        else:
            gt_instances = None

        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]
        box_cls, box_delta, box_center = self.head(features)
        shifts = self.shift_generator(features)

        if self.training:
            gt_classes, gt_shifts_reg_deltas, gt_centerness = self.get_ground_truth(
                shifts, gt_instances)
            return self.losses(gt_classes, gt_shifts_reg_deltas, gt_centerness,
                               box_cls, box_delta, box_center)
        else:
            results = self.inference(box_cls, box_delta, box_center, shifts,
                                     images)
            processed_results = []
            for results_per_image, input_per_image, image_size in zip(
                    results, batched_inputs, images.image_sizes):
                height = input_per_image.get("height", image_size[0])
                width = input_per_image.get("width", image_size[1])
                r = detector_postprocess(results_per_image, height, width)
                processed_results.append({"instances": r})
            return processed_results

    def losses(self, gt_classes, gt_shifts_deltas, gt_centerness,
               pred_class_logits, pred_shift_deltas, pred_centerness):
        """
        Args:
            For `gt_classes`, `gt_shifts_deltas` and `gt_centerness` parameters, see
                :meth:`FCOS.get_ground_truth`.
            Their shapes are (N, R) and (N, R, 4), respectively, where R is
            the total number of shifts across levels, i.e. sum(Hi x Wi)
            For `pred_class_logits`, `pred_shift_deltas` and `pred_centerness`, see
                :meth:`FCOSHead.forward`.

        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a scalar tensor
                storing the loss. Used during training only. The dict keys are:
                "loss_cls" and "loss_box_reg"
        """
        pred_class_logits, pred_shift_deltas, pred_centerness = \
            permute_all_cls_and_box_to_N_HWA_K_and_concat(
                pred_class_logits, pred_shift_deltas, pred_centerness,
                self.num_classes
            )  # Shapes: (N x R, K) and (N x R, 4), respectively.

        gt_classes = gt_classes.flatten()
        gt_shifts_deltas = gt_shifts_deltas.view(-1, 4)
        gt_centerness = gt_centerness.view(-1, 1)

        valid_idxs = gt_classes >= 0
        foreground_idxs = (gt_classes >= 0) & (gt_classes != self.num_classes)
        num_foreground = foreground_idxs.sum()

        gt_classes_target = torch.zeros_like(pred_class_logits)
        gt_classes_target[foreground_idxs, gt_classes[foreground_idxs]] = 1

        num_foreground = comm.all_reduce(num_foreground) / float(comm.get_world_size())
        num_foreground_centerness = gt_centerness[foreground_idxs].sum()
        num_targets = comm.all_reduce(num_foreground_centerness)  / float(comm.get_world_size())

        # logits loss
        loss_cls = sigmoid_focal_loss_jit(
            pred_class_logits[valid_idxs],
            gt_classes_target[valid_idxs],
            alpha=self.focal_loss_alpha,
            gamma=self.focal_loss_gamma,
            reduction="sum",
        ) / max(1.0, num_foreground)

        # regression loss
        loss_box_reg = iou_loss(
            pred_shift_deltas[foreground_idxs],
            gt_shifts_deltas[foreground_idxs],
            gt_centerness[foreground_idxs],
            box_mode="ltrb",
            loss_type=self.iou_loss_type,
            reduction="sum",
        ) / max(1.0, num_targets)

        # centerness loss
        loss_centerness = F.binary_cross_entropy_with_logits(
            pred_centerness[foreground_idxs],
            gt_centerness[foreground_idxs],
            reduction="sum",
        ) / max(1, num_foreground)

        return {
            "loss_cls": loss_cls,
            "loss_box_reg": loss_box_reg,
            "loss_centerness": loss_centerness
        }

    @torch.no_grad()
    def get_ground_truth(self, shifts, targets):
        """
        Args:
            shifts (list[list[Tensor]]): a list of N=#image elements. Each is a
                list of #feature level tensors. The tensors contains shifts of
                this image on the specific feature level.
            targets (list[Instances]): a list of N `Instances`s. The i-th
                `Instances` contains the ground-truth per-instance annotations
                for the i-th input image.  Specify `targets` during training only.

        Returns:
            gt_classes (Tensor):
                An integer tensor of shape (N, R) storing ground-truth
                labels for each shift.
                R is the total number of shifts, i.e. the sum of Hi x Wi for all levels.
                Shifts in the valid boxes are assigned their corresponding label in the
                [0, K-1] range. Shifts in the background are assigned the label "K".
                Shifts in the ignore areas are assigned a label "-1", i.e. ignore.
            gt_shifts_deltas (Tensor):
                Shape (N, R, 4).
                The last dimension represents ground-truth shift2box transform
                targets (dl, dt, dr, db) that map each shift to its matched ground-truth box.
                The values in the tensor are meaningful only when the corresponding
                shift is labeled as foreground.
            gt_centerness (Tensor):
                An float tensor (0, 1) of shape (N, R) whose values in [0, 1]
                storing ground-truth centerness for each shift.

        """
        gt_classes = []
        gt_shifts_deltas = []
        gt_centerness = []

        for shifts_per_image, targets_per_image in zip(shifts, targets):
            object_sizes_of_interest = torch.cat([
                shifts_i.new_tensor(size).unsqueeze(0).expand(
                    shifts_i.size(0), -1) for shifts_i, size in zip(
                        shifts_per_image, self.object_sizes_of_interest)
            ], dim=0)

            shifts_over_all_feature_maps = torch.cat(shifts_per_image, dim=0)

            gt_boxes = targets_per_image.gt_boxes

            deltas = self.shift2box_transform.get_deltas(
                shifts_over_all_feature_maps, gt_boxes.tensor.unsqueeze(1))

            if self.center_sampling_radius > 0:
                centers = gt_boxes.get_centers()
                is_in_boxes = []
                for stride, shifts_i in zip(self.fpn_strides, shifts_per_image):
                    radius = stride * self.center_sampling_radius
                    center_boxes = torch.cat((
                        torch.max(centers - radius, gt_boxes.tensor[:, :2]),
                        torch.min(centers + radius, gt_boxes.tensor[:, 2:]),
                    ), dim=-1)
                    center_deltas = self.shift2box_transform.get_deltas(
                        shifts_i, center_boxes.unsqueeze(1))
                    is_in_boxes.append(center_deltas.min(dim=-1).values > 0)
                is_in_boxes = torch.cat(is_in_boxes, dim=1)
            else:
                # no center sampling, it will use all the locations within a ground-truth box
                is_in_boxes = deltas.min(dim=-1).values > 0

            max_deltas = deltas.max(dim=-1).values
            # limit the regression range for each location
            is_cared_in_the_level = \
                (max_deltas >= object_sizes_of_interest[None, :, 0]) & \
                (max_deltas <= object_sizes_of_interest[None, :, 1])

            gt_positions_area = gt_boxes.area().unsqueeze(1).repeat(
                1, shifts_over_all_feature_maps.size(0))
            gt_positions_area[~is_in_boxes] = math.inf
            gt_positions_area[~is_cared_in_the_level] = math.inf

            # if there are still more than one objects for a position,
            # we choose the one with minimal area
            positions_min_area, gt_matched_idxs = gt_positions_area.min(dim=0)

            # ground truth box regression
            gt_shifts_reg_deltas_i = self.shift2box_transform.get_deltas(
                shifts_over_all_feature_maps, gt_boxes[gt_matched_idxs].tensor)

            # ground truth classes
            has_gt = len(targets_per_image) > 0
            if has_gt:
                gt_classes_i = targets_per_image.gt_classes[gt_matched_idxs]
                # Shifts with area inf are treated as background.
                gt_classes_i[positions_min_area == math.inf] = self.num_classes
            else:
                gt_classes_i = torch.zeros_like(
                    gt_matched_idxs) + self.num_classes

            # ground truth centerness
            left_right = gt_shifts_reg_deltas_i[:, [0, 2]]
            top_bottom = gt_shifts_reg_deltas_i[:, [1, 3]]
            gt_centerness_i = torch.sqrt(
                (left_right.min(dim=-1).values / left_right.max(dim=-1).values).clamp_(min=0)
                * (top_bottom.min(dim=-1).values / top_bottom.max(dim=-1).values).clamp_(min=0)
            )

            gt_classes.append(gt_classes_i)
            gt_shifts_deltas.append(gt_shifts_reg_deltas_i)
            gt_centerness.append(gt_centerness_i)

        return torch.stack(gt_classes), torch.stack(
            gt_shifts_deltas), torch.stack(gt_centerness)

    def inference(self, box_cls, box_delta, box_center, shifts, images):
        """
        Arguments:
            box_cls, box_delta, box_center: Same as the output of :meth:`FCOSHead.forward`
            shifts (list[list[Tensor]): a list of #images elements. Each is a
                list of #feature level tensor. The tensor contain shifts of this
                image on the specific feature level.
            images (ImageList): the input images

        Returns:
            results (List[Instances]): a list of #images elements.
        """
        assert len(shifts) == len(images)
        results = []

        box_cls = [permute_to_N_HWA_K(x, self.num_classes) for x in box_cls]
        box_delta = [permute_to_N_HWA_K(x, 4) for x in box_delta]
        box_center = [permute_to_N_HWA_K(x, 1) for x in box_center]
        # list[Tensor], one per level, each has shape (N, Hi x Wi, K or 4)

        for img_idx, shifts_per_image in enumerate(shifts):
            image_size = images.image_sizes[img_idx]
            box_cls_per_image = [
                box_cls_per_level[img_idx] for box_cls_per_level in box_cls
            ]
            box_reg_per_image = [
                box_reg_per_level[img_idx] for box_reg_per_level in box_delta
            ]
            box_ctr_per_image = [
                box_ctr_per_level[img_idx] for box_ctr_per_level in box_center
            ]
            results_per_image = self.inference_single_image(
                box_cls_per_image, box_reg_per_image, box_ctr_per_image,
                shifts_per_image, tuple(image_size))
            results.append(results_per_image)
        return results

    def inference_single_image(self, box_cls, box_delta, box_center, shifts,
                               image_size):
        """
        Single-image inference. Return bounding-box detection results by thresholding
        on scores and applying non-maximum suppression (NMS).

        Arguments:
            box_cls (list[Tensor]): list of #feature levels. Each entry contains
                tensor of size (H x W, K)
            box_delta (list[Tensor]): Same shape as 'box_cls' except that K becomes 4.
            box_center (list[Tensor]): Same shape as 'box_cls' except that K becomes 1.
            shifts (list[Tensor]): list of #feature levels. Each entry contains
                a tensor, which contains all the shifts for that
                image in that feature level.
            image_size (tuple(H, W)): a tuple of the image height and width.

        Returns:
            Same as `inference`, but for only one image.
        """
        boxes_all = []
        scores_all = []
        class_idxs_all = []

        # Iterate over every feature level
        for box_cls_i, box_reg_i, box_ctr_i, shifts_i in zip(
                box_cls, box_delta, box_center, shifts):
            # (HxWxK,)
            box_cls_i = box_cls_i.flatten().sigmoid_()

            # Keep top k top scoring indices only.
            num_topk = min(self.topk_candidates, box_reg_i.size(0))
            # torch.sort is actually faster than .topk (at least on GPUs)
            predicted_prob, topk_idxs = box_cls_i.sort(descending=True)
            predicted_prob = predicted_prob[:num_topk]
            topk_idxs = topk_idxs[:num_topk]

            # filter out the proposals with low confidence score
            keep_idxs = predicted_prob > self.score_threshold
            predicted_prob = predicted_prob[keep_idxs]
            topk_idxs = topk_idxs[keep_idxs]

            shift_idxs = topk_idxs // self.num_classes
            classes_idxs = topk_idxs % self.num_classes

            box_reg_i = box_reg_i[shift_idxs]
            shifts_i = shifts_i[shift_idxs]
            # predict boxes
            predicted_boxes = self.shift2box_transform.apply_deltas(
                box_reg_i, shifts_i)

            box_ctr_i = box_ctr_i.flatten().sigmoid_()[shift_idxs]
            predicted_prob = torch.sqrt(predicted_prob * box_ctr_i)

            boxes_all.append(predicted_boxes)
            scores_all.append(predicted_prob)
            class_idxs_all.append(classes_idxs)

        boxes_all, scores_all, class_idxs_all = [
            cat(x) for x in [boxes_all, scores_all, class_idxs_all]
        ]

        keep = generalized_batched_nms(
            boxes_all, scores_all, class_idxs_all,
            self.nms_threshold, nms_type=self.nms_type
        )
        keep = keep[:self.max_detections_per_image]

        result = Instances(image_size)
        result.pred_boxes = Boxes(boxes_all[keep])
        result.scores = scores_all[keep]
        result.pred_classes = class_idxs_all[keep]
        return result

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images,
                                        self.backbone.size_divisibility)
        return images

    def _inference_for_ms_test(self, batched_inputs):
        """
        function used for multiscale test, will be refactor in the future.
        The same input with `forward` function.
        """
        assert not self.training, "inference mode with training=True"
        assert len(batched_inputs) == 1, "inference image number > 1"
        images = self.preprocess_image(batched_inputs)

        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]
        box_cls, box_delta, box_center = self.head(features)
        shifts = self.shift_generator(features)

        results = self.inference(box_cls, box_delta, box_center, shifts, images)
        for results_per_image, input_per_image, image_size in zip(
                results, batched_inputs, images.image_sizes
        ):
            height = input_per_image.get("height", image_size[0])
            width = input_per_image.get("width", image_size[1])
            processed_results = detector_postprocess(results_per_image, height, width)
        return processed_results


class FCOSHead(nn.Module):
    """
    The head used in FCOS for object classification and box regression.
    It has two subnets for the two tasks, with a common structure but separate parameters.
    """
    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super().__init__()
        # fmt: off
        in_channels = input_shape[0].channels
        num_classes = cfg.MODEL.FCOS.NUM_CLASSES
        num_convs = cfg.MODEL.FCOS.NUM_CONVS
        prior_prob = cfg.MODEL.FCOS.PRIOR_PROB
        num_shifts = cfg.build_shift_generator(cfg, input_shape).num_cell_shifts
        self.fpn_strides = cfg.MODEL.FCOS.FPN_STRIDES
        self.centerness_on_reg = cfg.MODEL.FCOS.CENTERNESS_ON_REG
        self.norm_reg_targets = cfg.MODEL.FCOS.NORM_REG_TARGETS
        # fmt: on
        assert len(set(num_shifts)) == 1, "using differenct num_shifts value is not supported"
        num_shifts = num_shifts[0]

        cls_subnet = []
        bbox_subnet = []
        for _ in range(num_convs):
            cls_subnet.append(
                nn.Conv2d(in_channels,
                          in_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1))
            cls_subnet.append(nn.GroupNorm(32, in_channels))
            cls_subnet.append(nn.ReLU())
            bbox_subnet.append(
                nn.Conv2d(in_channels,
                          in_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1))
            bbox_subnet.append(nn.GroupNorm(32, in_channels))
            bbox_subnet.append(nn.ReLU())

        self.cls_subnet = nn.Sequential(*cls_subnet)
        self.bbox_subnet = nn.Sequential(*bbox_subnet)
        self.cls_score = nn.Conv2d(in_channels,
                                   num_shifts * num_classes,
                                   kernel_size=3,
                                   stride=1,
                                   padding=1)
        self.bbox_pred = nn.Conv2d(in_channels,
                                   num_shifts * 4,
                                   kernel_size=3,
                                   stride=1,
                                   padding=1)
        self.centerness = nn.Conv2d(in_channels,
                                    num_shifts * 1,
                                    kernel_size=3,
                                    stride=1,
                                    padding=1)

        # Initialization
        for modules in [
                self.cls_subnet, self.bbox_subnet, self.cls_score,
                self.bbox_pred, self.centerness
        ]:
            for layer in modules.modules():
                if isinstance(layer, nn.Conv2d):
                    torch.nn.init.normal_(layer.weight, mean=0, std=0.01)
                    torch.nn.init.constant_(layer.bias, 0)
                if isinstance(layer, nn.GroupNorm):
                    torch.nn.init.constant_(layer.weight, 1)
                    torch.nn.init.constant_(layer.bias, 0)

        # Use prior in model initialization to improve stability
        bias_value = -math.log((1 - prior_prob) / prior_prob)
        torch.nn.init.constant_(self.cls_score.bias, bias_value)

        self.scales = nn.ModuleList(
            [Scale(init_value=1.0) for _ in range(len(self.fpn_strides))])

    def forward(self, features):
        """
        Arguments:
            features (list[Tensor]): FPN feature map tensors in high to low resolution.
                Each tensor in the list correspond to different feature levels.

        Returns:
            logits (list[Tensor]): #lvl tensors, each has shape (N, K, Hi, Wi).
                The tensor predicts the classification probability
                at each spatial position for each of the K object classes.
            bbox_reg (list[Tensor]): #lvl tensors, each has shape (N, 4, Hi, Wi).
                The tensor predicts 4-vector (dl,dt,dr,db) box
                regression values for every shift. These values are the
                relative offset between the shift and the ground truth box.
            centerness (list[Tensor]): #lvl tensors, each has shape (N, 1, Hi, Wi).
                The tensor predicts the centerness at each spatial position.
        """
        logits = []
        bbox_reg = []
        centerness = []
        for level, feature in enumerate(features):
            cls_subnet = self.cls_subnet(feature)
            bbox_subnet = self.bbox_subnet(feature)

            logits.append(self.cls_score(cls_subnet))
            if self.centerness_on_reg:
                centerness.append(self.centerness(bbox_subnet))
            else:
                centerness.append(self.centerness(cls_subnet))

            bbox_pred = self.scales[level](self.bbox_pred(bbox_subnet))
            if self.norm_reg_targets:
                bbox_reg.append(F.relu(bbox_pred) * self.fpn_strides[level])
            else:
                bbox_reg.append(torch.exp(bbox_pred))
        return logits, bbox_reg, centerness
```

#### cvpods/modeling/meta_arch/solo_decoupled.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.

import logging
import math
from typing import List

import numpy as np
from PIL import Image
from scipy import ndimage

import torch
import torch.nn as nn
import torch.nn.functional as F

from cvpods.layers import ShapeSpec, get_norm, matrix_nms
from cvpods.modeling.losses import dice_loss, sigmoid_focal_loss_jit
from cvpods.modeling.nn_utils.weight_init import normal_init
from cvpods.structures import BitMasks, ImageList, Instances
from cvpods.utils import log_first_n

from .solo import SOLO, SOLOHead, multi_apply, points_nms


class DecoupledSOLO(SOLO):
    """
    Implement Decoupled SOLO.
    See: https://arxiv.org/pdf/1512.02325.pdf for more details.
    """

    def __init__(self, cfg):
        super().__init__(cfg)

    def _init_head(self, cfg, feature_shapes):
        assert self.head_type == 'DecoupledSOLOHead'
        self.head = DecoupledSOLOHead(cfg, feature_shapes)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
                Each item in the list contains the inputs for one image.
                For now, each item in the list is a dict that contains:

                * image: Tensor, image in (C, H, W) format.
                * instances: Instances

                Other information that's included in the original dicts, such as:

                * "height", "width" (int): the output resolution of the model, used in inference.
                    See :meth:`postprocess` for details.
        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a tensor storing the loss. Used during training only.
        """
        images = self.preprocess_image(batched_inputs)
        if "instances" in batched_inputs[0]:
            gt_instances = [
                x["instances"].to(self.device) for x in batched_inputs
            ]
        elif "targets" in batched_inputs[0]:
            log_first_n(
                logging.WARN,
                "'targets' in the model inputs is now renamed to 'instances'!",
                n=10)
            gt_instances = [
                x["targets"].to(self.device) for x in batched_inputs
            ]
        else:
            gt_instances = None

        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]

        if self.training:
            ins_preds_x, ins_preds_y, cate_preds = self.head(features, eval=False)
            featmap_sizes = [featmap.size()[-2:] for featmap in ins_preds_x]
            ins_label_list, cate_label_list, ins_ind_label_list, ins_ind_label_list_xy = \
                self.get_ground_truth(gt_instances, featmap_sizes)
            return self.losses(ins_preds_x, ins_preds_y, cate_preds, ins_label_list,
                               cate_label_list, ins_ind_label_list, ins_ind_label_list_xy)
        else:
            ins_preds_x, ins_preds_y, cate_preds = self.head(features, eval=True)
            results = self.inference(ins_preds_x, ins_preds_y, cate_preds, batched_inputs)
            processed_results = [{"instances": r} for r in results]
            return processed_results

    def losses(self,
               ins_preds_x,
               ins_preds_y,
               cate_preds,
               ins_label_list,
               cate_label_list,
               ins_ind_label_list,
               ins_ind_label_list_xy):
        # ins, per level
        ins_labels = []  # per level
        for ins_labels_level, ins_ind_labels_level in \
                zip(zip(*ins_label_list), zip(*ins_ind_label_list)):
            ins_labels_per_level = []
            for ins_labels_level_img, ins_ind_labels_level_img in \
                    zip(ins_labels_level, ins_ind_labels_level):
                ins_labels_per_level.append(
                    ins_labels_level_img[ins_ind_labels_level_img, ...]
                )
            ins_labels.append(torch.cat(ins_labels_per_level))

        ins_preds_x_final = []
        for ins_preds_level_x, ins_ind_labels_level in \
                zip(ins_preds_x, zip(*ins_ind_label_list_xy)):
            ins_preds_x_final_per_level = []
            for ins_preds_level_img_x, ins_ind_labels_level_img in \
                    zip(ins_preds_level_x, ins_ind_labels_level):
                ins_preds_x_final_per_level.append(
                    ins_preds_level_img_x[ins_ind_labels_level_img[:, 1], ...]
                )
            ins_preds_x_final.append(torch.cat(ins_preds_x_final_per_level))

        ins_preds_y_final = []
        for ins_preds_level_y, ins_ind_labels_level in \
                zip(ins_preds_y, zip(*ins_ind_label_list_xy)):
            ins_preds_y_final_per_level = []
            for ins_preds_level_img_y, ins_ind_labels_level_img in \
                    zip(ins_preds_level_y, ins_ind_labels_level):
                ins_preds_y_final_per_level.append(
                    ins_preds_level_img_y[ins_ind_labels_level_img[:, 0], ...]
                )
            ins_preds_y_final.append(torch.cat(ins_preds_y_final_per_level))

        num_ins = 0.
        # dice loss, per_level
        loss_ins = []
        for input_x, input_y, target in zip(ins_preds_x_final, ins_preds_y_final, ins_labels):
            mask_n = input_x.size(0)
            if mask_n == 0:
                continue
            num_ins += mask_n
            input = (input_x.sigmoid()) * (input_y.sigmoid())
            target = target.float() / 255.
            loss_ins.append(dice_loss(input, target))

        loss_ins = torch.cat(loss_ins).mean()
        loss_ins = loss_ins * self.loss_ins_weight

        # cate
        cate_preds = [
            cate_pred.permute(0, 2, 3, 1).reshape(-1, self.num_classes)
            for cate_pred in cate_preds
        ]
        cate_preds = torch.cat(cate_preds)

        cate_labels = []
        for cate_labels_level in zip(*cate_label_list):
            cate_labels_per_level = []
            for cate_labels_level_img in cate_labels_level:
                cate_labels_per_level.append(
                    cate_labels_level_img.flatten()
                )
            cate_labels.append(torch.cat(cate_labels_per_level))
        flatten_cate_labels = torch.cat(cate_labels)
        foreground_idxs = flatten_cate_labels != self.num_classes
        cate_labels = torch.zeros_like(cate_preds)
        cate_labels[foreground_idxs, flatten_cate_labels[foreground_idxs]] = 1

        loss_cate = self.loss_cat_weight * sigmoid_focal_loss_jit(
            cate_preds,
            cate_labels,
            alpha=self.loss_cat_alpha,
            gamma=self.loss_cat_gamma,
            reduction="sum",
        ) / max(1, num_ins)
        return dict(loss_ins=loss_ins, loss_cate=loss_cate)

    @torch.no_grad()
    def get_ground_truth(self, gt_instances, featmap_sizes):
        """
        Args:
            gt_instances (list[Instances]): a list of N `Instances`s. The i-th
                `Instances` contains the ground-truth per-instance annotations
                for the i-th input image.  Specify `targets` during training only.
            featmap_sizes (list[]): a list of #level elements. Each is a
                tuple of #feature level feature map size.
        """
        ins_label_list, cate_label_list, ins_ind_label_list, ins_ind_label_list_xy = multi_apply(
            self.solo_target_single_image,
            gt_instances,
            featmap_sizes=featmap_sizes)
        return ins_label_list, cate_label_list, ins_ind_label_list, ins_ind_label_list_xy

    @torch.no_grad()
    def solo_target_single_image(self, gt_instance, featmap_sizes):
        """
        Prepare ground truth for single image.
        """
        device = self.device
        gt_bboxes_raw = gt_instance.gt_boxes
        gt_labels_raw = gt_instance.gt_classes
        gt_masks_raw = gt_instance.gt_masks

        # ins
        gt_areas = torch.sqrt(gt_bboxes_raw.area())

        ins_label_list = []  # per level
        cate_label_list = []  # per level
        ins_ind_label_list = []  # per level
        ins_ind_label_list_xy = []  # per level
        for (lower_bound, upper_bound), stride, featmap_size, num_grid in zip(
                self.scale_ranges, self.feature_strides, featmap_sizes, self.seg_num_grids):
            ins_label = torch.zeros([num_grid ** 2, featmap_size[0],
                                     featmap_size[1]], dtype=torch.uint8, device=device)
            cate_label = torch.full(
                [num_grid, num_grid], self.num_classes, dtype=torch.int64, device=device)
            ins_ind_label = torch.zeros([num_grid ** 2], dtype=torch.bool, device=device)

            hit_indices = ((gt_areas >= lower_bound) & (
                gt_areas <= upper_bound)).nonzero(as_tuple=False).flatten()

            if len(hit_indices) == 0:
                ins_label = torch.zeros([1, featmap_size[0], featmap_size[1]], dtype=torch.uint8,
                                        device=device)
                ins_label_list.append(ins_label)
                cate_label_list.append(cate_label)
                # default is False
                ins_ind_label = torch.zeros([1], dtype=torch.bool, device=device)
                ins_ind_label_list.append(ins_ind_label)
                ins_ind_label_list_xy.append(torch.zeros([0, 2], dtype=torch.int64, device=device))
                continue

            gt_bboxes = gt_bboxes_raw[hit_indices]
            gt_labels = gt_labels_raw[hit_indices]
            gt_masks = gt_masks_raw[hit_indices]
            # move mask to cpu and convert to ndarray for compute gt ins center
            gt_masks = gt_masks.tensor.to("cpu").numpy()

            half_ws = 0.5 * (gt_bboxes.tensor[:, 2] - gt_bboxes.tensor[:, 0]) * self.sigma
            half_hs = 0.5 * (gt_bboxes.tensor[:, 3] - gt_bboxes.tensor[:, 1]) * self.sigma

            output_stride = stride / 2
            # For each mask
            for seg_mask, gt_label, half_h, half_w in zip(gt_masks, gt_labels, half_hs, half_ws):
                if seg_mask.sum() < 10:
                    continue
                # mass center
                upsampled_size = (featmap_sizes[0][0] * 4, featmap_sizes[0][1] * 4)
                center_h, center_w = ndimage.measurements.center_of_mass(seg_mask)
                coord_w = int((center_w / upsampled_size[1]) // (1. / num_grid))
                coord_h = int((center_h / upsampled_size[0]) // (1. / num_grid))

                # left, top, right, down
                top_box = max(0, int(((center_h - half_h) / upsampled_size[0]) // (1. / num_grid)))
                down_box = min(
                    num_grid - 1, int(((center_h + half_h) / upsampled_size[0]) // (1. / num_grid))
                )
                left_box = max(0, int(((center_w - half_w) / upsampled_size[1]) // (1. / num_grid)))
                right_box = min(
                    num_grid - 1, int(((center_w + half_w) / upsampled_size[1]) // (1. / num_grid))
                )

                top = max(top_box, coord_h - 1)
                down = min(down_box, coord_h + 1)
                left = max(coord_w - 1, left_box)
                right = min(right_box, coord_w + 1)

                cate_label[top:(down + 1), left:(right + 1)] = gt_label
                # ins
                scale = 1. / output_stride
                h, w = seg_mask.shape[-2:]
                new_h, new_w = int(h * scale + 0.5), int(w * scale + 0.5)
                seg_mask = Image.fromarray(seg_mask)
                seg_mask = seg_mask.resize((new_w, new_h), Image.BILINEAR)
                seg_mask = np.array(seg_mask)
                seg_mask = torch.from_numpy(seg_mask)
                for i in range(top, down + 1):
                    for j in range(left, right + 1):
                        label = int(i * num_grid + j)
                        ins_label[label, :seg_mask.shape[0], :seg_mask.shape[1]] = seg_mask
                        ins_ind_label[label] = True

            # Instance mask
            ins_label = ins_label[ins_ind_label]
            ins_label_list.append(ins_label)
            # Instance category
            cate_label_list.append(cate_label)
            # Instance index
            ins_ind_label = ins_ind_label[ins_ind_label]
            ins_ind_label_list.append(ins_ind_label)
            # Instance coordinate
            foreground_idxs = (cate_label != self.num_classes)
            ins_ind_label_list_xy.append(foreground_idxs.nonzero(as_tuple=False))

        return ins_label_list, cate_label_list, ins_ind_label_list, ins_ind_label_list_xy

    @torch.no_grad()
    def inference(self, ins_preds_x, ins_preds_y, cate_preds, batched_inputs):
        assert len(ins_preds_x) == len(cate_preds)
        num_levels = len(cate_preds)
        featmap_size = ins_preds_x[0].size()[-2:]

        results = []
        for img_id, batched_input in enumerate(batched_inputs):
            cate_pred_list = []
            seg_pred_list_x = []
            seg_pred_list_y = []
            for i in range(num_levels):
                cate_pred_list.append(
                    cate_preds[i][img_id].view(-1, self.num_classes).detach()
                )
                seg_pred_list_x.append(
                    ins_preds_x[i][img_id].detach()
                )
                seg_pred_list_y.append(
                    ins_preds_y[i][img_id].detach()
                )
            cate_pred_list = torch.cat(cate_pred_list, dim=0)
            seg_pred_list_x = torch.cat(seg_pred_list_x, dim=0)
            seg_pred_list_y = torch.cat(seg_pred_list_y, dim=0)

            img_shape = batched_input["instances"].image_size
            ori_shape = (batched_input["height"], batched_input["width"])

            results_per_image = self.inference_single_image(
                cate_pred_list, seg_pred_list_x, seg_pred_list_y,
                featmap_size, img_shape, ori_shape)
            results.append(results_per_image)
        return results

    @torch.no_grad()
    def inference_single_image(self,
                               cate_preds,
                               seg_preds_x,
                               seg_preds_y,
                               featmap_size,
                               img_shape,
                               ori_shape):
        result = Instances(ori_shape)

        # overall info.
        h, w = img_shape
        upsampled_size_out = (featmap_size[0] * 4, featmap_size[1] * 4)

        # trans trans_diff.
        trans_size = torch.Tensor(self.seg_num_grids).pow(2).cumsum(0).long()
        trans_diff = torch.ones(trans_size[-1].item(), device=self.device).long()
        num_grids = torch.ones(trans_size[-1].item(), device=self.device).long()
        seg_size = torch.Tensor(self.seg_num_grids).cumsum(0).long()
        seg_diff = torch.ones(trans_size[-1].item(), device=self.device).long()
        strides = torch.ones(trans_size[-1].item(), device=self.device)

        n_stage = len(self.seg_num_grids)
        trans_diff[:trans_size[0]] *= 0
        seg_diff[:trans_size[0]] *= 0
        num_grids[:trans_size[0]] *= self.seg_num_grids[0]
        strides[:trans_size[0]] *= self.feature_strides[0]

        for ind_ in range(1, n_stage):
            trans_diff[trans_size[ind_ - 1]:trans_size[ind_]] *= trans_size[ind_ - 1]
            seg_diff[trans_size[ind_ - 1]:trans_size[ind_]] *= seg_size[ind_ - 1]
            num_grids[trans_size[ind_ - 1]:trans_size[ind_]] *= self.seg_num_grids[ind_]
            strides[trans_size[ind_ - 1]:trans_size[ind_]] *= self.feature_strides[ind_]

        # process.
        inds = (cate_preds > self.score_threshold)
        # category scores.
        cate_scores = cate_preds[inds]

        # category labels.
        inds = inds.nonzero(as_tuple=False)
        trans_diff = torch.index_select(trans_diff, dim=0, index=inds[:, 0])
        seg_diff = torch.index_select(seg_diff, dim=0, index=inds[:, 0])
        num_grids = torch.index_select(num_grids, dim=0, index=inds[:, 0])
        strides = torch.index_select(strides, dim=0, index=inds[:, 0])

        y_inds = (inds[:, 0] - trans_diff) // num_grids
        x_inds = (inds[:, 0] - trans_diff) % num_grids
        y_inds += seg_diff
        x_inds += seg_diff

        cate_labels = inds[:, 1]
        seg_masks_soft = seg_preds_x[x_inds, ...] * seg_preds_y[y_inds, ...]
        seg_masks = seg_masks_soft > self.mask_threshold
        sum_masks = seg_masks.sum((1, 2)).float()

        # filter.
        keep = sum_masks > strides
        if keep.sum() == 0:
            return result

        seg_masks_soft = seg_masks_soft[keep, ...]
        seg_masks = seg_masks[keep, ...]
        cate_scores = cate_scores[keep]
        sum_masks = sum_masks[keep]
        cate_labels = cate_labels[keep]

        # mask scoring
        seg_score = (seg_masks_soft * seg_masks.float()).sum((1, 2)) / sum_masks
        cate_scores *= seg_score

        if len(cate_scores) == 0:
            return result

        # sort and keep top nms_pre
        sort_inds = torch.argsort(cate_scores, descending=True)
        if len(sort_inds) > self.nms_per_image:
            sort_inds = sort_inds[: self.nms_per_image]
        seg_masks_soft = seg_masks_soft[sort_inds, :, :]
        seg_masks = seg_masks[sort_inds, :, :]
        cate_scores = cate_scores[sort_inds]
        sum_masks = sum_masks[sort_inds]
        cate_labels = cate_labels[sort_inds]

        # Matrix NMS
        cate_scores = matrix_nms(seg_masks, cate_labels, cate_scores,
                                 kernel=self.nms_kernel, sigma=self.nms_sigma, sum_masks=sum_masks)

        # filter.
        keep = cate_scores >= self.update_threshold
        seg_masks_soft = seg_masks_soft[keep, :, :]
        cate_scores = cate_scores[keep]
        cate_labels = cate_labels[keep]

        # sort and keep top_k
        sort_inds = torch.argsort(cate_scores, descending=True)
        if len(sort_inds) > self.max_detections_per_image:
            sort_inds = sort_inds[: self.max_detections_per_image]
        seg_masks_soft = seg_masks_soft[sort_inds, :, :]
        cate_scores = cate_scores[sort_inds]
        cate_labels = cate_labels[sort_inds]

        seg_masks_soft = F.interpolate(seg_masks_soft.unsqueeze(0),
                                       size=upsampled_size_out,
                                       mode='bilinear')[:, :, :h, :w]
        seg_masks = F.interpolate(seg_masks_soft,
                                  size=ori_shape,
                                  mode='bilinear').squeeze(0)
        seg_masks = seg_masks > self.mask_threshold

        seg_masks = BitMasks(seg_masks)
        result.pred_masks = seg_masks
        result.pred_boxes = seg_masks.get_bounding_boxes()
        result.scores = cate_scores
        result.pred_classes = cate_labels
        return result

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images,
                                        self.backbone.size_divisibility)
        return images


class DecoupledSOLOHead(SOLOHead):
    """
    The head used in SOLO for instance segmentation.
    """

    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super().__init__(cfg, input_shape)

    def _init_layers(self):
        ins_convs_x = []
        ins_convs_y = []
        cate_convs = []
        for i in range(self.stacked_convs):
            # Mask branch
            chn = self.in_channels + 1 if i == 0 else self.seg_feat_channels
            ins_convs_x.append(
                nn.Conv2d(chn,
                          self.seg_feat_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1,
                          bias=False if self.norm else True)
            )
            ins_convs_y.append(
                nn.Conv2d(chn,
                          self.seg_feat_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1,
                          bias=False if self.norm else True)
            )

            if self.norm:
                ins_convs_x.append(get_norm(self.norm, self.seg_feat_channels))
                ins_convs_y.append(get_norm(self.norm, self.seg_feat_channels))

            ins_convs_x.append(nn.ReLU(inplace=True))
            ins_convs_y.append(nn.ReLU(inplace=True))

            # Category branch
            chn = self.in_channels if i == 0 else self.seg_feat_channels
            cate_convs.append(
                nn.Conv2d(chn,
                          self.seg_feat_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1,
                          bias=False if self.norm else True)
            )
            if self.norm:
                cate_convs.append(get_norm(self.norm, self.seg_feat_channels))
            cate_convs.append(nn.ReLU(inplace=True))

        self.ins_convs_x = nn.Sequential(*ins_convs_x)
        self.ins_convs_y = nn.Sequential(*ins_convs_y)
        self.cate_convs = nn.Sequential(*cate_convs)

        self.solo_ins_list_x = nn.ModuleList()
        self.solo_ins_list_y = nn.ModuleList()
        for seg_num_grid in self.seg_num_grids:
            self.solo_ins_list_x.append(
                nn.Conv2d(
                    self.seg_feat_channels,
                    seg_num_grid,
                    kernel_size=3,
                    stride=1,
                    padding=1)
            )
            self.solo_ins_list_y.append(
                nn.Conv2d(
                    self.seg_feat_channels,
                    seg_num_grid,
                    kernel_size=3,
                    stride=1,
                    padding=1)
            )

        self.solo_cate = nn.Conv2d(
            self.seg_feat_channels,
            self.num_classes,
            kernel_size=3,
            stride=1,
            padding=1,
        )

    def _init_weights(self):
        for modules in [self.ins_convs_x, self.ins_convs_y, self.cate_convs]:
            for m in modules.modules():
                if isinstance(m, nn.Conv2d):
                    normal_init(m, std=0.01)
        # Use prior in model initialization to improve stability
        bias_value = -math.log((1 - self.prior_prob) / self.prior_prob)
        for modules in [self.solo_ins_list_x, self.solo_ins_list_y, self.solo_cate]:
            for m in modules.modules():
                if isinstance(m, nn.Conv2d):
                    normal_init(m, std=0.01, bias=bias_value)

    def forward(self, features, eval=False):
        new_features = self.split_features(features)
        featmap_sizes = [featmap.size()[-2:] for featmap in new_features]
        upsampled_size = (featmap_sizes[0][0] * 2, featmap_sizes[0][1] * 2)
        ins_pred_x, ins_pred_y, cate_pred = multi_apply(
            self.forward_single_level,
            new_features,
            list(range(len(self.seg_num_grids))),
            eval=eval,
            upsampled_size=upsampled_size
        )
        return ins_pred_x, ins_pred_y, cate_pred

    def forward_single_level(self, x, idx, eval=False, upsampled_size=None):
        ins_feat = x
        cate_feat = x
        # Ins branch
        # concat coord
        x_range = torch.linspace(-1, 1, ins_feat.shape[-1], device=ins_feat.device)
        y_range = torch.linspace(-1, 1, ins_feat.shape[-2], device=ins_feat.device)
        y, x = torch.meshgrid(y_range, x_range)
        y = y.expand([ins_feat.shape[0], 1, -1, -1])
        x = x.expand([ins_feat.shape[0], 1, -1, -1])
        ins_feat_x = torch.cat([ins_feat, x], 1)
        ins_feat_y = torch.cat([ins_feat, y], 1)

        ins_feat_x = self.ins_convs_x(ins_feat_x)
        ins_feat_y = self.ins_convs_y(ins_feat_y)

        ins_feat_x = F.interpolate(ins_feat_x, scale_factor=2, mode='bilinear')
        ins_feat_y = F.interpolate(ins_feat_y, scale_factor=2, mode='bilinear')

        ins_pred_x = self.solo_ins_list_x[idx](ins_feat_x)
        ins_pred_y = self.solo_ins_list_y[idx](ins_feat_y)

        # Cate branch
        seg_num_grid = self.seg_num_grids[idx]
        # Align
        cate_feat = F.interpolate(cate_feat, size=seg_num_grid, mode='bilinear')
        cate_feat = self.cate_convs(cate_feat)
        cate_pred = self.solo_cate(cate_feat)

        if eval:
            ins_pred_x = F.interpolate(ins_pred_x.sigmoid(), size=upsampled_size, mode='bilinear')
            ins_pred_y = F.interpolate(ins_pred_y.sigmoid(), size=upsampled_size, mode='bilinear')
            cate_pred = points_nms(cate_pred.sigmoid(), kernel=2).permute(0, 2, 3, 1)
        return ins_pred_x, ins_pred_y, cate_pred
```

#### cvpods/modeling/meta_arch/auto_assign.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.

import logging
import math
from typing import List

import torch
import torch.distributed as dist
import torch.nn.functional as F
from torch import nn

from cvpods.layers import ShapeSpec, batched_nms, cat
from cvpods.modeling.box_regression import Shift2BoxTransform
from cvpods.modeling.losses import iou_loss
from cvpods.modeling.meta_arch.fcos import Scale
from cvpods.modeling.meta_arch.retinanet import permute_to_N_HWA_K
from cvpods.modeling.postprocessing import detector_postprocess
from cvpods.structures import Boxes, ImageList, Instances, pairwise_iou
from cvpods.utils import log_first_n


def positive_bag_loss(logits, mask, gaussian_probs):
    # bag_prob = Mean-max(logits)
    weight = (3 * logits).exp() * gaussian_probs * mask
    w = weight / weight.sum(dim=1, keepdim=True).clamp(min=1e-12)
    bag_prob = (w * logits).sum(dim=1)
    return F.binary_cross_entropy(bag_prob,
                                  torch.ones_like(bag_prob),
                                  reduction='none')


def negative_bag_loss(logits, gamma):
    return logits**gamma * F.binary_cross_entropy(
        logits, torch.zeros_like(logits), reduction='none')


def normal_distribution(x, mu=0, sigma=1):
    return (-(x - mu)**2 / (2 * sigma**2)).exp()


def normalize(x):
    return (x - x.min() + 1e-12) / (x.max() - x.min() + 1e-12)


class AutoAssign(nn.Module):
    """
    Implement AutoAssign (https://arxiv.org/abs/2007.03496).
    """
    def __init__(self, cfg):
        super(AutoAssign, self).__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)

        # fmt: off
        self.num_classes = cfg.MODEL.FCOS.NUM_CLASSES
        self.in_features = cfg.MODEL.FCOS.IN_FEATURES
        self.fpn_strides = cfg.MODEL.FCOS.FPN_STRIDES
        # Loss parameters:
        self.focal_loss_alpha = cfg.MODEL.FCOS.FOCAL_LOSS_ALPHA
        self.focal_loss_gamma = cfg.MODEL.FCOS.FOCAL_LOSS_GAMMA
        self.iou_loss_type = cfg.MODEL.FCOS.IOU_LOSS_TYPE
        self.reg_weight = cfg.MODEL.FCOS.REG_WEIGHT
        # Inference parameters:
        self.score_threshold = cfg.MODEL.FCOS.SCORE_THRESH_TEST
        self.topk_candidates = cfg.MODEL.FCOS.TOPK_CANDIDATES_TEST
        self.nms_threshold = cfg.MODEL.FCOS.NMS_THRESH_TEST
        self.max_detections_per_image = cfg.TEST.DETECTIONS_PER_IMAGE
        # fmt: on

        self.backbone = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))

        backbone_shape = self.backbone.output_shape()
        feature_shapes = [backbone_shape[f] for f in self.in_features]
        self.head = AutoAssignHead(cfg, feature_shapes)
        self.shift_generator = cfg.build_shift_generator(cfg, feature_shapes)

        # Matching and loss
        self.shift2box_transform = Shift2BoxTransform(
            weights=cfg.MODEL.FCOS.BBOX_REG_WEIGHTS)
        self.mu = nn.Parameter(torch.zeros(80, 2))
        self.sigma = nn.Parameter(torch.ones(80, 2))

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(
            3, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(
            3, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std
        self.to(self.device)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
                Each item in the list contains the inputs for one image.
                For now, each item in the list is a dict that contains:

                * image: Tensor, image in (C, H, W) format.
                * instances: Instances

                Other information that's included in the original dicts, such as:

                * "height", "width" (int): the output resolution of the model, used in inference.
                    See :meth:`postprocess` for details.
        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a tensor storing the loss. Used during training only.
        """
        images = self.preprocess_image(batched_inputs)
        if "instances" in batched_inputs[0]:
            gt_instances = [
                x["instances"].to(self.device) for x in batched_inputs
            ]
        elif "targets" in batched_inputs[0]:
            log_first_n(
                logging.WARN,
                "'targets' in the model inputs is now renamed to 'instances'!",
                n=10)
            gt_instances = [
                x["targets"].to(self.device) for x in batched_inputs
            ]
        else:
            gt_instances = None

        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]
        box_cls, box_delta, box_center = self.head(features)
        shifts = self.shift_generator(features)

        if self.training:
            return self.losses(shifts, gt_instances, box_cls, box_delta,
                               box_center)
        else:
            results = self.inference(box_cls, box_delta, box_center, shifts,
                                     images)
            processed_results = []
            for results_per_image, input_per_image, image_size in zip(
                    results, batched_inputs, images.image_sizes):
                height = input_per_image.get("height", image_size[0])
                width = input_per_image.get("width", image_size[1])
                r = detector_postprocess(results_per_image, height, width)
                processed_results.append({"instances": r})
            return processed_results

    def losses(self, shifts, gt_instances, box_cls, box_delta, box_center):
        box_cls_flattened = [
            permute_to_N_HWA_K(x, self.num_classes) for x in box_cls
        ]
        box_delta_flattened = [permute_to_N_HWA_K(x, 4) for x in box_delta]
        box_center_flattened = [permute_to_N_HWA_K(x, 1) for x in box_center]
        pred_class_logits = cat(box_cls_flattened, dim=1)
        pred_shift_deltas = cat(box_delta_flattened, dim=1)
        pred_obj_logits = cat(box_center_flattened, dim=1)

        pred_class_probs = pred_class_logits.sigmoid()
        pred_obj_probs = pred_obj_logits.sigmoid()
        pred_box_probs = []
        num_foreground = pred_class_logits.new_zeros(1)
        num_background = pred_class_logits.new_zeros(1)
        positive_losses = []
        gaussian_norm_losses = []

        for shifts_per_image, gt_instances_per_image, \
            pred_class_probs_per_image, pred_shift_deltas_per_image, \
            pred_obj_probs_per_image in zip(
                shifts, gt_instances, pred_class_probs, pred_shift_deltas,
                pred_obj_probs):
            locations = torch.cat(shifts_per_image, dim=0)
            labels = gt_instances_per_image.gt_classes
            gt_boxes = gt_instances_per_image.gt_boxes

            target_shift_deltas = self.shift2box_transform.get_deltas(
                locations, gt_boxes.tensor.unsqueeze(1))
            is_in_boxes = target_shift_deltas.min(dim=-1).values > 0

            foreground_idxs = torch.nonzero(is_in_boxes, as_tuple=True)

            with torch.no_grad():
                # predicted_boxes_per_image: a_{j}^{loc}, shape: [j, 4]
                predicted_boxes_per_image = self.shift2box_transform.apply_deltas(
                    pred_shift_deltas_per_image, locations)
                # gt_pred_iou: IoU_{ij}^{loc}, shape: [i, j]
                gt_pred_iou = pairwise_iou(
                    gt_boxes, Boxes(predicted_boxes_per_image)).max(
                        dim=0, keepdim=True).values.repeat(
                            len(gt_instances_per_image), 1)

                # pred_box_prob_per_image: P{a_{j} \in A_{+}}, shape: [j, c]
                pred_box_prob_per_image = torch.zeros_like(
                    pred_class_probs_per_image)
                box_prob = 1 / (1 - gt_pred_iou[foreground_idxs]).clamp_(1e-12)
                for i in range(len(gt_instances_per_image)):
                    idxs = foreground_idxs[0] == i
                    if idxs.sum() > 0:
                        box_prob[idxs] = normalize(box_prob[idxs])
                pred_box_prob_per_image[foreground_idxs[1],
                                        labels[foreground_idxs[0]]] = box_prob
                pred_box_probs.append(pred_box_prob_per_image)

            normal_probs = []
            for stride, shifts_i in zip(self.fpn_strides, shifts_per_image):
                gt_shift_deltas = self.shift2box_transform.get_deltas(
                    shifts_i, gt_boxes.tensor.unsqueeze(1))
                distances = (gt_shift_deltas[..., :2] - gt_shift_deltas[..., 2:]) / 2
                normal_probs.append(
                    normal_distribution(distances / stride,
                                        self.mu[labels].unsqueeze(1),
                                        self.sigma[labels].unsqueeze(1)))
            normal_probs = torch.cat(normal_probs, dim=1).prod(dim=-1)

            composed_cls_prob = pred_class_probs_per_image[:, labels] * pred_obj_probs_per_image

            # matched_gt_shift_deltas: P_{ij}^{loc}
            loss_box_reg = iou_loss(pred_shift_deltas_per_image.unsqueeze(0),
                                    target_shift_deltas,
                                    box_mode="ltrb",
                                    loss_type=self.iou_loss_type,
                                    reduction="none") * self.reg_weight
            pred_reg_probs = (-loss_box_reg).exp()

            # positive_losses: { -log( Mean-max(P_{ij}^{cls} * P_{ij}^{loc}) ) }
            positive_losses.append(
                positive_bag_loss(composed_cls_prob.transpose(1, 0) * pred_reg_probs,
                                  is_in_boxes.float(), normal_probs))

            num_foreground += len(gt_instances_per_image)
            num_background += normal_probs[foreground_idxs].sum().item()

            gaussian_norm_losses.append(
                len(gt_instances_per_image) / normal_probs[foreground_idxs].sum().clamp_(1e-12))

        if dist.is_initialized():
            dist.all_reduce(num_foreground)
            num_foreground /= dist.get_world_size()
            dist.all_reduce(num_background)
            num_background /= dist.get_world_size()

        # positive_loss: \sum_{i}{ -log( Mean-max(P_{ij}^{cls} * P_{ij}^{loc}) ) } / ||B||
        positive_loss = torch.cat(positive_losses).sum() / max(1, num_foreground)

        # pred_box_probs: P{a_{j} \in A_{+}}
        pred_box_probs = torch.stack(pred_box_probs, dim=0)
        # negative_loss: \sum_{j}{ FL( (1 - P{a_{j} \in A_{+}}) * (1 - P_{j}^{bg}) ) } / n||B||
        negative_loss = negative_bag_loss(
            pred_class_probs * pred_obj_probs * (1 - pred_box_probs),
            self.focal_loss_gamma).sum() / max(1, num_background)

        loss_pos = positive_loss * self.focal_loss_alpha
        loss_neg = negative_loss * (1 - self.focal_loss_alpha)
        loss_norm = torch.stack(gaussian_norm_losses).mean() * (1 - self.focal_loss_alpha)

        return {
            "loss_pos": loss_pos,
            "loss_neg": loss_neg,
            "loss_norm": loss_norm,
        }

    def inference(self, box_cls, box_delta, box_center, shifts, images):
        """
        Arguments:
            box_cls, box_delta, box_center: Same as the output of :meth:`AutoAssignHead.forward`
            shifts (list[list[Tensor]): a list of #images elements. Each is a
                list of #feature level tensor. The tensor contain shifts of this
                image on the specific feature level.
            images (ImageList): the input images

        Returns:
            results (List[Instances]): a list of #images elements.
        """
        assert len(shifts) == len(images)
        results = []

        box_cls = [permute_to_N_HWA_K(x, self.num_classes) for x in box_cls]
        box_delta = [permute_to_N_HWA_K(x, 4) for x in box_delta]
        box_center = [permute_to_N_HWA_K(x, 1) for x in box_center]
        # list[Tensor], one per level, each has shape (N, Hi x Wi, K or 4)

        for img_idx, shifts_per_image in enumerate(shifts):
            image_size = images.image_sizes[img_idx]
            box_cls_per_image = [
                box_cls_per_level[img_idx] for box_cls_per_level in box_cls
            ]
            box_reg_per_image = [
                box_reg_per_level[img_idx] for box_reg_per_level in box_delta
            ]
            box_ctr_per_image = [
                box_ctr_per_level[img_idx] for box_ctr_per_level in box_center
            ]
            results_per_image = self.inference_single_image(
                box_cls_per_image, box_reg_per_image, box_ctr_per_image,
                shifts_per_image, tuple(image_size))
            results.append(results_per_image)
        return results

    def inference_single_image(self, box_cls, box_delta, box_center, shifts,
                               image_size):
        """
        Single-image inference. Return bounding-box detection results by thresholding
        on scores and applying non-maximum suppression (NMS).

        Arguments:
            box_cls (list[Tensor]): list of #feature levels. Each entry contains
                tensor of size (H x W, K)
            box_delta (list[Tensor]): Same shape as 'box_cls' except that K becomes 4.
            box_center (list[Tensor]): Same shape as 'box_cls' except that K becomes 1.
            shifts (list[Tensor]): list of #feature levels. Each entry contains
                a tensor, which contains all the shifts for that
                image in that feature level.
            image_size (tuple(H, W)): a tuple of the image height and width.

        Returns:
            Same as `inference`, but for only one image.
        """
        boxes_all = []
        scores_all = []
        class_idxs_all = []

        # Iterate over every feature level
        for box_cls_i, box_reg_i, box_ctr_i, shifts_i in zip(
                box_cls, box_delta, box_center, shifts):
            # (HxWxK,)
            box_cls_i = (box_cls_i.sigmoid_() * box_ctr_i.sigmoid_()).flatten()

            # Keep top k top scoring indices only.
            num_topk = min(self.topk_candidates, box_reg_i.size(0))
            # torch.sort is actually faster than .topk (at least on GPUs)
            predicted_prob, topk_idxs = box_cls_i.sort(descending=True)
            predicted_prob = predicted_prob[:num_topk]
            topk_idxs = topk_idxs[:num_topk]

            # filter out the proposals with low confidence score
            keep_idxs = predicted_prob > self.score_threshold
            predicted_prob = predicted_prob[keep_idxs]
            topk_idxs = topk_idxs[keep_idxs]

            shift_idxs = topk_idxs // self.num_classes
            classes_idxs = topk_idxs % self.num_classes

            box_reg_i = box_reg_i[shift_idxs]
            shifts_i = shifts_i[shift_idxs]
            # predict boxes
            predicted_boxes = self.shift2box_transform.apply_deltas(
                box_reg_i, shifts_i)

            boxes_all.append(predicted_boxes)
            scores_all.append(predicted_prob)
            class_idxs_all.append(classes_idxs)

        boxes_all, scores_all, class_idxs_all = [
            cat(x) for x in [boxes_all, scores_all, class_idxs_all]
        ]
        keep = batched_nms(boxes_all, scores_all, class_idxs_all,
                           self.nms_threshold)
        keep = keep[:self.max_detections_per_image]

        result = Instances(image_size)
        result.pred_boxes = Boxes(boxes_all[keep])
        result.scores = scores_all[keep]
        result.pred_classes = class_idxs_all[keep]
        return result

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images,
                                        self.backbone.size_divisibility)
        return images


class AutoAssignHead(nn.Module):
    """
    The head used in FCOS for object classification and box regression.
    It has two subnets for the two tasks, with a common structure but separate parameters.
    """
    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super(AutoAssignHead, self).__init__()
        # fmt: off
        in_channels = input_shape[0].channels
        num_classes = cfg.MODEL.FCOS.NUM_CLASSES
        num_convs = cfg.MODEL.FCOS.NUM_CONVS
        prior_prob = cfg.MODEL.FCOS.PRIOR_PROB
        self.fpn_strides = cfg.MODEL.FCOS.FPN_STRIDES
        self.norm_reg_targets = cfg.MODEL.FCOS.NORM_REG_TARGETS
        # fmt: on
        cls_subnet = []
        bbox_subnet = []
        for _ in range(num_convs):
            cls_subnet.append(
                nn.Conv2d(in_channels,
                          in_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1))
            cls_subnet.append(nn.GroupNorm(32, in_channels))
            cls_subnet.append(nn.ReLU())
            bbox_subnet.append(
                nn.Conv2d(in_channels,
                          in_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1))
            bbox_subnet.append(nn.GroupNorm(32, in_channels))
            bbox_subnet.append(nn.ReLU())

        self.cls_subnet = nn.Sequential(*cls_subnet)
        self.bbox_subnet = nn.Sequential(*bbox_subnet)
        self.cls_score = nn.Conv2d(in_channels,
                                   num_classes,
                                   kernel_size=3,
                                   stride=1,
                                   padding=1)
        self.bbox_pred = nn.Conv2d(in_channels,
                                   4,
                                   kernel_size=3,
                                   stride=1,
                                   padding=1)
        self.obj_score = nn.Conv2d(in_channels,
                                   1,
                                   kernel_size=3,
                                   stride=1,
                                   padding=1)

        # Initialization
        for modules in [
                self.cls_subnet, self.bbox_subnet, self.cls_score,
                self.bbox_pred, self.obj_score
        ]:
            for layer in modules.modules():
                if isinstance(layer, nn.Conv2d):
                    torch.nn.init.normal_(layer.weight, mean=0, std=0.01)
                    torch.nn.init.constant_(layer.bias, 0)
                if isinstance(layer, nn.GroupNorm):
                    torch.nn.init.constant_(layer.weight, 1)
                    torch.nn.init.constant_(layer.bias, 0)

        # Use prior in model initialization to improve stability
        bias_value = -math.log((1 - prior_prob) / prior_prob)
        torch.nn.init.constant_(self.cls_score.bias, bias_value)
        torch.nn.init.constant_(self.bbox_pred.bias, 4.0)

        self.scales = nn.ModuleList(
            [Scale(init_value=1.0) for _ in range(len(self.fpn_strides))])

    def forward(self, features):
        """
        Arguments:
            features (list[Tensor]): FPN feature map tensors in high to low resolution.
                Each tensor in the list correspond to different feature levels.

        Returns:
            logits (list[Tensor]): #lvl tensors, each has shape (N, K, Hi, Wi).
                The tensor predicts the classification probability
                at each spatial position for each of the K object classes.
            bbox_reg (list[Tensor]): #lvl tensors, each has shape (N, 4, Hi, Wi).
                The tensor predicts 4-vector (dl,dt,dr,db) box
                regression values for every shift. These values are the
                relative offset between the shift and the ground truth box.
        """
        logits = []
        bbox_reg = []
        obj_logits = []
        for feature, stride, scale in zip(features, self.fpn_strides,
                                          self.scales):
            cls_subnet = self.cls_subnet(feature)
            bbox_subnet = self.bbox_subnet(feature)

            logits.append(self.cls_score(cls_subnet))
            obj_logits.append(self.obj_score(bbox_subnet))

            bbox_pred = scale(self.bbox_pred(bbox_subnet))
            if self.norm_reg_targets:
                bbox_reg.append(F.relu(bbox_pred) * stride)
            else:
                bbox_reg.append(torch.exp(bbox_pred))
        return logits, bbox_reg, obj_logits
```

#### cvpods/modeling/meta_arch/borderdet.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.

import logging
import math
from typing import List

import torch
import torch.nn.functional as F
from torch import nn

from cvpods.layers import ShapeSpec, cat, generalized_batched_nms
from cvpods.layers.border_align import BorderAlign
from cvpods.modeling.box_regression import Shift2BoxTransform
from cvpods.modeling.losses import iou_loss, sigmoid_focal_loss_jit, smooth_l1_loss
from cvpods.modeling.meta_arch.retinanet import permute_to_N_HWA_K
from cvpods.modeling.postprocessing import detector_postprocess
from cvpods.structures import Boxes, ImageList, Instances, pairwise_iou
from cvpods.utils import comm, log_first_n


def permute_all_cls_and_box_to_N_HWA_K_and_concat(
        box_cls, box_delta, box_center, border_cls, border_delta, num_classes=80):
    """
    Rearrange the tensor layout from the network output, i.e.:
    list[Tensor]: #lvl tensors of shape (N, A x K, Hi, Wi)
    to per-image predictions, i.e.:
    Tensor: of shape (N x sum(Hi x Wi x A), K)
    """
    # for each feature level, permute the outputs to make them be in the
    # same format as the labels. Note that the labels are computed for
    # all feature levels concatenated, so we keep the same representation
    # for the objectness, the box_delta and the centerness
    box_cls_flattened = [permute_to_N_HWA_K(x, num_classes) for x in box_cls]
    box_delta_flattened = [permute_to_N_HWA_K(x, 4) for x in box_delta]
    box_center_flattened = [permute_to_N_HWA_K(x, 1) for x in box_center]

    border_cls_flattened = [permute_to_N_HWA_K(x, num_classes) for x in border_cls]
    border_delta_flattened = [permute_to_N_HWA_K(x, 4) for x in border_delta]
    # concatenate on the first dimension (representing the feature levels), to
    # take into account the way the labels were generated (with all feature maps
    # being concatenated as well)
    box_cls = cat(box_cls_flattened, dim=1).view(-1, num_classes)
    box_delta = cat(box_delta_flattened, dim=1).view(-1, 4)
    box_center = cat(box_center_flattened, dim=1).view(-1, 1)

    border_cls = cat(border_cls_flattened, dim=1).view(-1, num_classes)
    border_delta = cat(border_delta_flattened, dim=1).view(-1, 4)
    return box_cls, box_delta, box_center, border_cls, border_delta


class Scale(nn.Module):
    def __init__(self, init_value=1.0):
        super(Scale, self).__init__()
        self.scale = nn.Parameter(torch.FloatTensor([init_value]))

    def forward(self, input):
        return input * self.scale


class BorderDet(nn.Module):
    """
    Implement BorderDet.
    """
    def __init__(self, cfg):
        super().__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)

        # fmt: off
        self.num_classes = cfg.MODEL.FCOS.NUM_CLASSES
        self.in_features = cfg.MODEL.FCOS.IN_FEATURES
        self.fpn_strides = cfg.MODEL.FCOS.FPN_STRIDES
        # Loss Parameters:
        self.focal_loss_alpha = cfg.MODEL.FCOS.FOCAL_LOSS_ALPHA
        self.focal_loss_gamma = cfg.MODEL.FCOS.FOCAL_LOSS_GAMMA
        self.iou_loss_type = cfg.MODEL.FCOS.IOU_LOSS_TYPE
        self.center_sampling_radius = cfg.MODEL.FCOS.CENTER_SAMPLING_RADIUS
        self.border_iou_thresh = cfg.MODEL.BORDER.IOU_THRESH
        self.border_bbox_std = cfg.MODEL.BORDER.BBOX_STD
        # Inference Parameters:
        self.score_threshold  = cfg.MODEL.FCOS.SCORE_THRESH_TEST
        self.topk_candidates = cfg.MODEL.FCOS.TOPK_CANDIDATES_TEST
        self.nms_threshold = cfg.MODEL.FCOS.NMS_THRESH_TEST
        self.nms_type = cfg.MODEL.NMS_TYPE
        self.max_detections_per_image = cfg.TEST.DETECTIONS_PER_IMAGE
        # fmt: on

        # build network
        self.backbone = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))

        backbone_shape = self.backbone.output_shape()
        feature_shapes = [backbone_shape[f] for f in self.in_features]
        self.head = BorderHead(cfg, feature_shapes)
        self.shift_generator = cfg.build_shift_generator(cfg, feature_shapes)

        # Matching and Loss
        self.shift2box_transform = Shift2BoxTransform(
            weights=cfg.MODEL.FCOS.BBOX_REG_WEIGHTS)
        self.object_sizes_of_interest = cfg.MODEL.FCOS.OBJECT_SIZES_OF_INTEREST

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(3, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(3, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std
        self.to(self.device)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
                Each item in the list contains the inputs for one image.
                For now, each item in the list is a dict that contains:

                * image: Tensor, image in (C, H, W) format.
                * instances: Instances

                Other information that's included in the original dicts, such as:

                * "height", "width" (int): the output resolution of the model, used in inference.
                  See :meth:`postprocess` for details.
        Returns:
            result (list[BoxList] or dict[Tensor]): the output from the model.
                During training, it returns a dict[Tensor] which contains the losses.
                During testing, it returns list[BoxList] contains additional fields
                like `scores`, `labels` and `mask` (for Mask R-CNN models).

        """
        images = self.preprocess_image(batched_inputs)
        if "instances" in batched_inputs[0]:
            gt_instances = [x["instances"].to(self.device) for x in batched_inputs]
        elif "targets" in batched_inputs[0]:
            log_first_n(
                logging.WARN, "'targets' in the model inputs is now renamed to 'instances'!", n=10
            )
            gt_instances = [x["targets"].to(self.device) for x in batched_inputs]
        else:
            gt_instances = None

        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]
        shifts = self.shift_generator(features)
        (
            box_cls,
            box_delta,
            box_center,
            bd_box_cls,
            bd_box_delta,
            bd_based_box
        ) = self.head(features, shifts)

        if self.training:
            (
                gt_classes,
                gt_shifts_reg_deltas,
                gt_centerness,
                gt_border_classes,
                gt_border_shifts_deltas
            ) = self.get_ground_truth(shifts, gt_instances, bd_based_box)
            return self.losses(
                gt_classes,
                gt_shifts_reg_deltas,
                gt_centerness,
                gt_border_classes,
                gt_border_shifts_deltas,
                box_cls,
                box_delta,
                box_center,
                bd_box_cls,
                bd_box_delta,
            )
        else:
            results = self.inference(
                box_cls, box_center, bd_box_cls, bd_box_delta, bd_based_box, images.image_sizes
            )
            processed_results = []
            for results_per_image, input_per_image, image_size in zip(
                    results, batched_inputs, images.image_sizes):
                height = input_per_image.get("height", image_size[0])
                width = input_per_image.get("width", image_size[1])
                r = detector_postprocess(results_per_image, height, width)
                processed_results.append({"instances": r})
            return processed_results

    def losses(
            self,
            gt_classes,
            gt_shifts_deltas,
            gt_centerness,
            gt_classes_border,
            gt_deltas_border,
            pred_class_logits,
            pred_shift_deltas,
            pred_centerness,
            border_box_cls,
            border_bbox_reg,
    ):
        """
        Args:
            For `gt_classes`, `gt_shifts_deltas` and `gt_centerness` parameters, see
                :meth:`BorderDet.get_ground_truth`.
            Their shapes are (N, R) and (N, R, 4), respectively, where R is
            the total number of shifts across levels, i.e. sum(Hi x Wi)
            For `pred_class_logits`, `pred_shift_deltas` and `pred_centerness`, see
                :meth:`BorderHead.forward`.

        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a scalar tensor
                storing the loss. Used during training only. The dict keys are:
                "loss_cls" and "loss_box_reg"
        """
        (
            pred_class_logits,
            pred_shift_deltas,
            pred_centerness,
            border_class_logits,
            border_shift_deltas,
        ) = permute_all_cls_and_box_to_N_HWA_K_and_concat(
            pred_class_logits, pred_shift_deltas, pred_centerness,
            border_box_cls, border_bbox_reg, self.num_classes
        )  # Shapes: (N x R, K) and (N x R, 4), respectively.

        # fcos
        gt_classes = gt_classes.flatten()
        gt_shifts_deltas = gt_shifts_deltas.view(-1, 4)
        gt_centerness = gt_centerness.view(-1, 1)

        valid_idxs = gt_classes >= 0
        foreground_idxs = (gt_classes >= 0) & (gt_classes != self.num_classes)
        num_foreground = foreground_idxs.sum()

        gt_classes_target = torch.zeros_like(pred_class_logits)
        gt_classes_target[foreground_idxs, gt_classes[foreground_idxs]] = 1

        num_foreground = comm.all_reduce(num_foreground) / float(comm.get_world_size())
        num_foreground_centerness = gt_centerness[foreground_idxs].sum()
        num_targets = comm.all_reduce(num_foreground_centerness)  / float(comm.get_world_size())

        # logits loss
        loss_cls = sigmoid_focal_loss_jit(
            pred_class_logits[valid_idxs],
            gt_classes_target[valid_idxs],
            alpha=self.focal_loss_alpha,
            gamma=self.focal_loss_gamma,
            reduction="sum",
        ) / max(1.0, num_foreground)

        # regression loss
        loss_box_reg = iou_loss(
            pred_shift_deltas[foreground_idxs],
            gt_shifts_deltas[foreground_idxs],
            gt_centerness[foreground_idxs],
            box_mode="ltrb",
            loss_type=self.iou_loss_type,
            reduction="sum",
        ) / max(1.0, num_targets)

        # centerness loss
        loss_centerness = F.binary_cross_entropy_with_logits(
            pred_centerness[foreground_idxs],
            gt_centerness[foreground_idxs],
            reduction="sum",
        ) / max(1.0, num_foreground)

        # borderdet
        gt_classes_border = gt_classes_border.flatten()
        gt_deltas_border = gt_deltas_border.view(-1, 4)

        valid_idxs_border = gt_classes_border >= 0
        foreground_idxs_border = (gt_classes_border >= 0) & (gt_classes_border != self.num_classes)
        num_foreground_border = foreground_idxs_border.sum()

        gt_classes_border_target = torch.zeros_like(border_class_logits)
        gt_classes_border_target[
            foreground_idxs_border, gt_classes_border[foreground_idxs_border]] = 1

        num_foreground_border = (
            comm.all_reduce(num_foreground_border) / float(comm.get_world_size())
        )

        num_foreground_border = max(num_foreground_border, 1.0)
        loss_border_cls = sigmoid_focal_loss_jit(
            border_class_logits[valid_idxs_border],
            gt_classes_border_target[valid_idxs_border],
            alpha=self.focal_loss_alpha,
            gamma=self.focal_loss_gamma,
            reduction="sum",
        ) / num_foreground_border

        if foreground_idxs_border.numel() > 0:
            loss_border_reg = (
                smooth_l1_loss(
                    border_shift_deltas[foreground_idxs_border],
                    gt_deltas_border[foreground_idxs_border],
                    beta=0,
                    reduction="sum"
                ) / num_foreground_border
            )
        else:
            loss_border_reg = border_shift_deltas.sum()

        return {
            "loss_cls": loss_cls,
            "loss_box_reg": loss_box_reg,
            "loss_centerness": loss_centerness,
            "loss_border_cls": loss_border_cls,
            "loss_border_reg": loss_border_reg,
        }

    @torch.no_grad()
    def get_ground_truth(self, shifts, targets, pre_boxes_list):
        """
        Args:
            shifts (list[list[Tensor]]): a list of N=#image elements. Each is a
                list of #feature level tensors. The tensors contains shifts of
                this image on the specific feature level.
            targets (list[Instances]): a list of N `Instances`s. The i-th
                `Instances` contains the ground-truth per-instance annotations
                for the i-th input image.  Specify `targets` during training only.

        Returns:
            gt_classes (Tensor):
                An integer tensor of shape (N, R) storing ground-truth
                labels for each shift.
                R is the total number of shifts, i.e. the sum of Hi x Wi for all levels.
                Shifts in the valid boxes are assigned their corresponding label in the
                [0, K-1] range. Shifts in the background are assigned the label "K".
                Shifts in the ignore areas are assigned a label "-1", i.e. ignore.
            gt_shifts_deltas (Tensor):
                Shape (N, R, 4).
                The last dimension represents ground-truth shift2box transform
                targets (dl, dt, dr, db) that map each shift to its matched ground-truth box.
                The values in the tensor are meaningful only when the corresponding
                shift is labeled as foreground.
            gt_centerness (Tensor):
                An float tensor (0, 1) of shape (N, R) whose values in [0, 1]
                storing ground-truth centerness for each shift.
            border_classes (Tensor):
                An integer tensor of shape (N, R) storing ground-truth
                labels for each shift.
                R is the total number of shifts, i.e. the sum of Hi x Wi for all levels.
                Shifts in the valid boxes are assigned their corresponding label in the
                [0, K-1] range. Shifts in the background are assigned the label "K".
                Shifts in the ignore areas are assigned a label "-1", i.e. ignore.
            border_shifts_deltas (Tensor):
                Shape (N, R, 4).
                The last dimension represents ground-truth shift2box transform
                targets (dl, dt, dr, db) that map each shift to its matched ground-truth box.
                The values in the tensor are meaningful only when the corresponding
                shift is labeled as foreground.

        """
        gt_classes = []
        gt_shifts_deltas = []
        gt_centerness = []

        border_classes = []
        border_shifts_deltas = []

        for shifts_per_image, targets_per_image, pre_boxes in zip(shifts, targets, pre_boxes_list):
            object_sizes_of_interest = torch.cat([
                shifts_i.new_tensor(size).unsqueeze(0).expand(
                    shifts_i.size(0), -1) for shifts_i, size in zip(
                    shifts_per_image, self.object_sizes_of_interest)
            ], dim=0)

            shifts_over_all_feature_maps = torch.cat(shifts_per_image, dim=0)

            gt_boxes = targets_per_image.gt_boxes

            deltas = self.shift2box_transform.get_deltas(
                shifts_over_all_feature_maps, gt_boxes.tensor.unsqueeze(1))

            if self.center_sampling_radius > 0:
                centers = gt_boxes.get_centers()
                is_in_boxes = []
                for stride, shifts_i in zip(self.fpn_strides, shifts_per_image):
                    radius = stride * self.center_sampling_radius
                    center_boxes = torch.cat((
                        torch.max(centers - radius, gt_boxes.tensor[:, :2]),
                        torch.min(centers + radius, gt_boxes.tensor[:, 2:]),
                    ), dim=-1)
                    center_deltas = self.shift2box_transform.get_deltas(
                        shifts_i, center_boxes.unsqueeze(1))
                    is_in_boxes.append(center_deltas.min(dim=-1).values > 0)
                is_in_boxes = torch.cat(is_in_boxes, dim=1)
            else:
                # no center sampling, it will use all the locations within a ground-truth box
                is_in_boxes = deltas.min(dim=-1).values > 0

            max_deltas = deltas.max(dim=-1).values
            # limit the regression range for each location
            is_cared_in_the_level = \
                (max_deltas >= object_sizes_of_interest[None, :, 0]) & \
                (max_deltas <= object_sizes_of_interest[None, :, 1])

            gt_positions_area = gt_boxes.area().unsqueeze(1).repeat(
                1, shifts_over_all_feature_maps.size(0))
            gt_positions_area[~is_in_boxes] = math.inf
            gt_positions_area[~is_cared_in_the_level] = math.inf

            # if there are still more than one objects for a position,
            # we choose the one with minimal area
            positions_min_area, gt_matched_idxs = gt_positions_area.min(dim=0)

            # ground truth box regression
            gt_shifts_reg_deltas_i = self.shift2box_transform.get_deltas(
                shifts_over_all_feature_maps, gt_boxes[gt_matched_idxs].tensor)

            # ground truth classes
            has_gt = len(targets_per_image) > 0
            if has_gt:
                gt_classes_i = targets_per_image.gt_classes[gt_matched_idxs]
                # Shifts with area inf are treated as background.
                gt_classes_i[positions_min_area == math.inf] = self.num_classes
            else:
                gt_classes_i = torch.zeros_like(gt_matched_idxs) + self.num_classes

            # ground truth centerness
            left_right = gt_shifts_reg_deltas_i[:, [0, 2]]
            top_bottom = gt_shifts_reg_deltas_i[:, [1, 3]]
            gt_centerness_i = torch.sqrt(
                (left_right.min(dim=-1).values / left_right.max(dim=-1).values).clamp_(min=0)
                * (top_bottom.min(dim=-1).values / top_bottom.max(dim=-1).values).clamp_(min=0)
            )

            gt_classes.append(gt_classes_i)
            gt_shifts_deltas.append(gt_shifts_reg_deltas_i)
            gt_centerness.append(gt_centerness_i)

            # border
            iou = pairwise_iou(Boxes(pre_boxes), gt_boxes)
            (max_iou, argmax_iou) = iou.max(dim=1)
            invalid = max_iou < self.border_iou_thresh
            gt_target = gt_boxes[argmax_iou].tensor

            border_cls_target = targets_per_image.gt_classes[argmax_iou]
            border_cls_target[invalid] = self.num_classes

            border_bbox_std = pre_boxes.new_tensor(self.border_bbox_std)
            pre_boxes_wh = pre_boxes[:, 2:4] - pre_boxes[:, 0:2]
            pre_boxes_wh = torch.cat([pre_boxes_wh, pre_boxes_wh], dim=1)
            border_off_target = (gt_target - pre_boxes) / (pre_boxes_wh * border_bbox_std)

            border_classes.append(border_cls_target)
            border_shifts_deltas.append(border_off_target)

        return (
            torch.stack(gt_classes),
            torch.stack(gt_shifts_deltas),
            torch.stack(gt_centerness),
            torch.stack(border_classes),
            torch.stack(border_shifts_deltas),
        )

    def inference(self, box_cls, box_center, border_cls, border_delta, bd_based_box, image_sizes):
        """
        Arguments:
            box_cls, box_delta, box_center: Same as the output of :meth:`BorderHead.forward`
            shifts (list[list[Tensor]): a list of #images elements. Each is a
                list of #feature level tensor. The tensor contain shifts of this
                image on the specific feature level.
            image_sizes (ImageList): the input images

        Returns:
            results (List[Instances]): a list of #images elements.
        """
        results = []

        box_cls = [permute_to_N_HWA_K(x, self.num_classes) for x in box_cls]
        box_center = [permute_to_N_HWA_K(x, 1) for x in box_center]
        border_cls = [permute_to_N_HWA_K(x, self.num_classes) for x in border_cls]
        border_delta = [permute_to_N_HWA_K(x, 4) for x in border_delta]
        # list[Tensor], one per level, each has shape (N, Hi x Wi, K or 4)

        for img_idx, image_size_per_image in enumerate(image_sizes):
            box_cls_per_image = [box_cls_per_level[img_idx] for box_cls_per_level in box_cls]
            box_ctr_per_image = [box_ctr_per_level[img_idx] for box_ctr_per_level in box_center]
            border_cls_per_image = [
                border_cls_per_level[img_idx] for border_cls_per_level in border_cls
            ]
            border_reg_per_image = [
                border_reg_per_level[img_idx] for border_reg_per_level in border_delta
            ]
            bd_based_box_per_image = [
                box_loc_per_level[img_idx] for box_loc_per_level in bd_based_box
            ]

            results_per_image = self.inference_single_image(
                box_cls_per_image, box_ctr_per_image, border_cls_per_image,
                border_reg_per_image, bd_based_box_per_image, tuple(image_size_per_image)
            )
            results.append(results_per_image)
        return results

    def inference_single_image(
            self, box_cls, box_center, border_cls, border_delta, bd_based_box, image_size
    ):
        """
        Single-image inference. Return bounding-box detection results by thresholding
        on scores and applying non-maximum suppression (NMS).

        Arguments:
            box_cls (list[Tensor]): list of #feature levels. Each entry contains
                tensor of size (H x W, K)
            box_delta (list[Tensor]): Same shape as 'box_cls' except that K becomes 4.
            box_center (list[Tensor]): Same shape as 'box_cls' except that K becomes 1.
            shifts (list[Tensor]): list of #feature levels. Each entry contains
                a tensor, which contains all the shifts for that
                image in that feature level.
            image_size (tuple(H, W)): a tuple of the image height and width.

        Returns:
            Same as `inference`, but for only one image.
        """
        boxes_all = []
        scores_all = []
        class_idxs_all = []

        border_bbox_std = bd_based_box[0].new_tensor(self.border_bbox_std)

        # Iterate over every feature level
        for box_cls_i, box_ctr_i, bd_box_cls_i, bd_box_reg_i, bd_based_box_i in zip(
                box_cls, box_center, border_cls, border_delta, bd_based_box):
            # (HxWxK,)
            box_cls_i = box_cls_i.sigmoid_()
            box_ctr_i = box_ctr_i.sigmoid_()
            bd_box_cls_i = bd_box_cls_i.sigmoid_()

            predicted_prob = (box_cls_i * box_ctr_i).sqrt()

            # filter out the proposals with low confidence score
            keep_idxs = predicted_prob > self.score_threshold

            predicted_prob = predicted_prob * bd_box_cls_i

            predicted_prob = predicted_prob[keep_idxs]
            # Keep top k top scoring indices only.
            num_topk = min(self.topk_candidates, predicted_prob.size(0))
            # torch.sort is actually faster than .topk (at least on GPUs)
            predicted_prob, topk_idxs = predicted_prob.sort(descending=True)
            topk_idxs = topk_idxs[:num_topk]

            keep_idxs = keep_idxs.nonzero()
            keep_idxs = keep_idxs[topk_idxs]
            keep_box_idxs = keep_idxs[:, 0]
            classes_idxs = keep_idxs[:, 1]

            predicted_prob = predicted_prob[:num_topk]
            bd_box_reg_i = bd_box_reg_i[keep_box_idxs]
            bd_based_box_i = bd_based_box_i[keep_box_idxs]

            det_wh = (bd_based_box_i[..., 2:4] - bd_based_box_i[..., :2])
            det_wh = torch.cat([det_wh, det_wh], dim=1)
            predicted_boxes = bd_based_box_i + (bd_box_reg_i * border_bbox_std * det_wh)

            boxes_all.append(predicted_boxes)
            scores_all.append(predicted_prob.sqrt())
            class_idxs_all.append(classes_idxs)

        boxes_all, scores_all, class_idxs_all = [
            cat(x) for x in [boxes_all, scores_all, class_idxs_all]
        ]

        keep = generalized_batched_nms(boxes_all, scores_all, class_idxs_all,
                                       self.nms_threshold, nms_type=self.nms_type)
        boxes_all = boxes_all[keep]
        scores_all = scores_all[keep]
        class_idxs_all = class_idxs_all[keep]

        number_of_detections = len(keep)
        # Limit to max_per_image detections **over all classes**
        if number_of_detections > self.max_detections_per_image > 0:
            image_thresh, _ = torch.kthvalue(
                scores_all,
                number_of_detections - self.max_detections_per_image + 1
            )
            keep = scores_all >= image_thresh.item()
            keep = torch.nonzero(keep).squeeze(1)
            boxes_all = boxes_all[keep]
            scores_all = scores_all[keep]
            class_idxs_all = class_idxs_all[keep]

        result = Instances(image_size)
        result.pred_boxes = Boxes(boxes_all)
        result.scores = scores_all
        result.pred_classes = class_idxs_all
        return result

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images, self.backbone.size_divisibility)
        return images


class BorderHead(nn.Module):
    """
    The head used in BorderDet for object classification and box regression.
    It has two subnets for the two tasks, with a common structure but separate parameters.
    """
    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super().__init__()
        # fmt: off
        in_channels = input_shape[0].channels
        num_classes = cfg.MODEL.FCOS.NUM_CLASSES
        num_convs = cfg.MODEL.FCOS.NUM_CONVS
        prior_prob = cfg.MODEL.FCOS.PRIOR_PROB
        self.fpn_strides = cfg.MODEL.FCOS.FPN_STRIDES
        self.centerness_on_reg = cfg.MODEL.FCOS.CENTERNESS_ON_REG
        self.norm_reg_targets = cfg.MODEL.FCOS.NORM_REG_TARGETS
        # fmt: on

        cls_subnet = []
        bbox_subnet = []
        for _ in range(num_convs):
            cls_subnet.append(
                nn.Conv2d(in_channels,
                          in_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1))
            cls_subnet.append(nn.GroupNorm(32, in_channels))
            cls_subnet.append(nn.ReLU())
            bbox_subnet.append(
                nn.Conv2d(in_channels,
                          in_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1))
            bbox_subnet.append(nn.GroupNorm(32, in_channels))
            bbox_subnet.append(nn.ReLU())

        self.cls_subnet = nn.Sequential(*cls_subnet)
        self.bbox_subnet = nn.Sequential(*bbox_subnet)

        self.cls_score = nn.Conv2d(
            in_channels, num_classes, kernel_size=3, stride=1, padding=1)
        self.bbox_pred = nn.Conv2d(
            in_channels, 4, kernel_size=3, stride=1, padding=1)
        self.centerness = nn.Conv2d(
            in_channels, 1, kernel_size=3, stride=1, padding=1)

        self.add_module("border_cls_subnet", BorderBranch(in_channels, 256))
        self.add_module("border_bbox_subnet", BorderBranch(in_channels, 128))

        self.border_cls_score = nn.Conv2d(
            in_channels, num_classes, kernel_size=1, stride=1)
        self.border_bbox_pred = nn.Conv2d(
            in_channels, 4, kernel_size=1, stride=1)

        # Initialization
        for modules in [
            self.cls_subnet, self.bbox_subnet,
            self.cls_score, self.bbox_pred, self.centerness,
            self.border_cls_subnet, self.border_bbox_subnet,
            self.border_cls_score, self.border_bbox_pred
        ]:
            for layer in modules.modules():
                if isinstance(layer, nn.Conv2d):
                    torch.nn.init.normal_(layer.weight, mean=0, std=0.01)
                    torch.nn.init.constant_(layer.bias, 0)
                if isinstance(layer, nn.GroupNorm):
                    torch.nn.init.constant_(layer.weight, 1)
                    torch.nn.init.constant_(layer.bias, 0)

        # Use prior in model initialization to improve stability
        bias_value = -math.log((1 - prior_prob) / prior_prob)
        torch.nn.init.constant_(self.cls_score.bias, bias_value)
        torch.nn.init.constant_(self.border_cls_score.bias, bias_value)

        self.scales = nn.ModuleList(
            [Scale(init_value=1.0) for _ in range(len(self.fpn_strides))])

    def forward(self, features, shifts):
        """
        Arguments:
            features (list[Tensor]): FPN feature map tensors in high to low resolution.
                Each tensor in the list correspond to different feature levels.

        Returns:
            logits (list[Tensor]): #lvl tensors, each has shape (N, K, Hi, Wi).
                The tensor predicts the classification probability
                at each spatial position for each of the K object classes.
            bbox_reg (list[Tensor]): #lvl tensors, each has shape (N, 4, Hi, Wi).
                The tensor predicts 4-vector (dl,dt,dr,db) box
                regression values for every shift. These values are the
                relative offset between the shift and the ground truth box.
            centerness (list[Tensor]): #lvl tensors, each has shape (N, 1, Hi, Wi).
                The tensor predicts the centerness at each spatial position.
            border_logits (list[Tensor]): #lvl tensors, each has shape (N, K, Hi, Wi).
                The tensor predicts the border classification probability
                at each spatial position for each of the K object classes.
            border_bbox_reg (list[Tensor]): #lvl tensors, each has shape (N, 4, Hi, Wi).
                The tensor predicts 4-vector (dl,dt,dr,db) box
                regression values for every border shift. These values are the
                relative offset between the shift and the ground truth box.
            pre_bbox (list[Tensor]): #lvl tensors, each has shape (N, Hi * Wi, 4).
                The tensor predicts 4-vector (l,t,r,b) box regression values.
                These values are predicted boxes by the dense object detector.
        """
        logits = []
        bbox_reg = []
        centerness = []
        border_logits = []
        border_bbox_reg = []
        pre_bbox = []

        shifts = [
            torch.cat([shi.unsqueeze(0) for shi in shift], dim=0)
            for shift in list(zip(*shifts))
        ]

        for level, (feature, shifts_i) in enumerate(zip(features, shifts)):
            cls_subnet = self.cls_subnet(feature)
            bbox_subnet = self.bbox_subnet(feature)

            logits.append(self.cls_score(cls_subnet))
            if self.centerness_on_reg:
                centerness.append(self.centerness(bbox_subnet))
            else:
                centerness.append(self.centerness(cls_subnet))

            bbox_pred = self.scales[level](self.bbox_pred(bbox_subnet))
            if self.norm_reg_targets:
                bbox_pred = F.relu(bbox_pred) * self.fpn_strides[level]
            else:
                bbox_pred = torch.exp(bbox_pred) * self.fpn_strides[level]
            bbox_reg.append(bbox_pred)

            # border
            N, C, H, W = feature.shape
            pre_off = bbox_pred.clone().detach()
            with torch.no_grad():
                pre_off = pre_off.permute(0, 2, 3, 1).reshape(N, -1, 4)
                pre_boxes = self.compute_bbox(shifts_i, pre_off)
                align_boxes, wh = self.compute_border(pre_boxes, level, H, W)
                pre_bbox.append(pre_boxes)

            border_cls_conv = self.border_cls_subnet(cls_subnet, align_boxes, wh)
            border_cls_logits = self.border_cls_score(border_cls_conv)
            border_logits.append(border_cls_logits)

            border_reg_conv = self.border_bbox_subnet(bbox_subnet, align_boxes, wh)
            border_bbox_pred = self.border_bbox_pred(border_reg_conv)
            border_bbox_reg.append(border_bbox_pred)

        if self.training:
            pre_bbox = torch.cat(pre_bbox, dim=1)
        return (logits, bbox_reg, centerness, border_logits, border_bbox_reg, pre_bbox)

    def compute_bbox(self, location, pred_offset):
        detections = torch.stack([
            location[:, :, 0] - pred_offset[:, :, 0],
            location[:, :, 1] - pred_offset[:, :, 1],
            location[:, :, 0] + pred_offset[:, :, 2],
            location[:, :, 1] + pred_offset[:, :, 3]], dim=2)

        return detections

    def compute_border(self, _boxes, fm_i, height, width):
        """
        :param _boxes:
        :param fm_i:
        :param height:
        :param width:
        :return:
        """
        boxes = _boxes / self.fpn_strides[fm_i]
        boxes[:, :, 0].clamp_(min=0, max=width - 1)
        boxes[:, :, 1].clamp_(min=0, max=height - 1)
        boxes[:, :, 2].clamp_(min=0, max=width - 1)
        boxes[:, :, 3].clamp_(min=0, max=height - 1)

        wh = (boxes[:, :, 2:] - boxes[:, :, :2]).contiguous()
        return boxes, wh


class BorderBranch(nn.Module):
    def __init__(self, in_channels, border_channels):
        """
        :param in_channels:
        """
        super(BorderBranch, self).__init__()
        self.cur_point_conv = nn.Sequential(
            nn.Conv2d(
                in_channels,
                border_channels,
                kernel_size=1),
            nn.InstanceNorm2d(border_channels),
            nn.ReLU())

        self.ltrb_conv = nn.Sequential(
            nn.Conv2d(
                in_channels,
                border_channels * 4,
                kernel_size=1),
            nn.InstanceNorm2d(border_channels * 4),
            nn.ReLU())

        self.border_align = BorderAlign(pool_size=10)

        self.border_conv = nn.Sequential(
            nn.Conv2d(
                5 * border_channels,
                in_channels,
                kernel_size=1),
            nn.ReLU())

    def forward(self, feature, boxes):
        N, C, H, W = feature.shape

        fm_short = self.cur_point_conv(feature)
        feature = self.ltrb_conv(feature)
        ltrb_conv = self.border_align(feature, boxes)
        ltrb_conv = ltrb_conv.permute(0, 3, 1, 2).reshape(N, -1, H, W)
        align_conv = torch.cat([ltrb_conv, fm_short], dim=1)
        align_conv = self.border_conv(align_conv)
        return align_conv
```

#### cvpods/modeling/meta_arch/retinanet.py

```python
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

import logging
import math
from typing import List

import torch
from torch import nn

from cvpods.layers import ShapeSpec, cat, generalized_batched_nms
from cvpods.modeling.basenet import basenet
from cvpods.modeling.box_regression import Box2BoxTransform
from cvpods.modeling.losses import sigmoid_focal_loss_jit, smooth_l1_loss
from cvpods.modeling.matcher import Matcher
from cvpods.modeling.postprocessing import detector_postprocess
from cvpods.structures import Boxes, ImageList, Instances, pairwise_iou
from cvpods.utils import log_first_n


def permute_to_N_HWA_K(tensor, K):
    """
    Transpose/reshape a tensor from (N, (A x K), H, W) to (N, (HxWxA), K)
    """
    assert tensor.dim() == 4, tensor.shape
    N, _, H, W = tensor.shape
    tensor = tensor.view(N, -1, K, H, W)
    tensor = tensor.permute(0, 3, 4, 1, 2)
    tensor = tensor.reshape(N, -1, K)  # Size=(N,HWA,K)
    return tensor


def permute_all_cls_and_box_to_N_HWA_K_and_concat(box_cls,
                                                  box_delta,
                                                  num_classes=80):
    """
    Rearrange the tensor layout from the network output, i.e.:
    list[Tensor]: #lvl tensors of shape (N, A x K, Hi, Wi)
    to per-image predictions, i.e.:
    Tensor: of shape (N x sum(Hi x Wi x A), K)
    """
    # for each feature level, permute the outputs to make them be in the
    # same format as the labels. Note that the labels are computed for
    # all feature levels concatenated, so we keep the same representation
    # for the objectness and the box_delta
    box_cls_flattened = [permute_to_N_HWA_K(x, num_classes) for x in box_cls]
    box_delta_flattened = [permute_to_N_HWA_K(x, 4) for x in box_delta]
    # concatenate on the first dimension (representing the feature levels), to
    # take into account the way the labels were generated (with all feature maps
    # being concatenated as well)
    box_cls = cat(box_cls_flattened, dim=1).view(-1, num_classes)
    box_delta = cat(box_delta_flattened, dim=1).view(-1, 4)
    return box_cls, box_delta


@basenet
class RetinaNet(nn.Module):
    """
    Implement RetinaNet (https://arxiv.org/abs/1708.02002).
    """
    def __init__(self, cfg):
        super().__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)

        # fmt: off
        self.num_classes = cfg.MODEL.RETINANET.NUM_CLASSES
        self.in_features = cfg.MODEL.RETINANET.IN_FEATURES
        # Loss parameters:
        self.focal_loss_alpha = cfg.MODEL.RETINANET.FOCAL_LOSS_ALPHA
        self.focal_loss_gamma = cfg.MODEL.RETINANET.FOCAL_LOSS_GAMMA
        self.smooth_l1_loss_beta = cfg.MODEL.RETINANET.SMOOTH_L1_LOSS_BETA
        # Inference parameters:
        self.score_threshold = cfg.MODEL.RETINANET.SCORE_THRESH_TEST
        self.topk_candidates = cfg.MODEL.RETINANET.TOPK_CANDIDATES_TEST
        self.nms_threshold = cfg.MODEL.RETINANET.NMS_THRESH_TEST
        self.nms_type = cfg.MODEL.NMS_TYPE
        self.max_detections_per_image = cfg.TEST.DETECTIONS_PER_IMAGE
        # fmt: on

        self.backbone = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))

        backbone_shape = self.backbone.output_shape()
        feature_shapes = [backbone_shape[f] for f in self.in_features]
        self.head = RetinaNetHead(cfg, feature_shapes)
        self.anchor_generator = cfg.build_anchor_generator(cfg, feature_shapes)

        # Matching and loss
        self.box2box_transform = Box2BoxTransform(
            weights=cfg.MODEL.RETINANET.BBOX_REG_WEIGHTS)
        self.matcher = Matcher(
            cfg.MODEL.RETINANET.IOU_THRESHOLDS,
            cfg.MODEL.RETINANET.IOU_LABELS,
            allow_low_quality_matches=True,
        )

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(
            3, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(
            3, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std
        self.to(self.device)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
                Each item in the list contains the inputs for one image.
                For now, each item in the list is a dict that contains:

                * image: Tensor, image in (C, H, W) format.
                * instances: Instances

                Other information that's included in the original dicts, such as:

                * "height", "width" (int): the output resolution of the model, used in inference.
                  See :meth:`postprocess` for details.
        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a tensor storing the loss. Used during training only.
        """
        images = self.preprocess_image(batched_inputs)
        if "instances" in batched_inputs[0]:
            gt_instances = [
                x["instances"].to(self.device) for x in batched_inputs
            ]
        elif "targets" in batched_inputs[0]:
            log_first_n(
                logging.WARN,
                "'targets' in the model inputs is now renamed to 'instances'!",
                n=10)
            gt_instances = [
                x["targets"].to(self.device) for x in batched_inputs
            ]
        else:
            gt_instances = None

        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]
        box_cls, box_delta = self.head(features)
        anchors = self.anchor_generator(features)

        if self.training:
            gt_classes, gt_anchors_reg_deltas = self.get_ground_truth(
                anchors, gt_instances)
            return self.losses(gt_classes, gt_anchors_reg_deltas, box_cls,
                               box_delta)
        else:
            results = self.inference(box_cls, box_delta, anchors, images.image_sizes)
            processed_results = []
            for results_per_image, input_per_image, image_size in zip(
                    results, batched_inputs, images.image_sizes):
                height = input_per_image.get("height", image_size[0])
                width = input_per_image.get("width", image_size[1])
                r = detector_postprocess(results_per_image, height, width)
                processed_results.append({"instances": r})
            return processed_results

    def losses(self, gt_classes, gt_anchors_deltas, pred_class_logits,
               pred_anchor_deltas):
        """
        Args:
            For `gt_classes` and `gt_anchors_deltas` parameters, see
                :meth:`RetinaNet.get_ground_truth`.
            Their shapes are (N, R) and (N, R, 4), respectively, where R is
            the total number of anchors across levels, i.e. sum(Hi x Wi x A)
            For `pred_class_logits` and `pred_anchor_deltas`, see
                :meth:`RetinaNetHead.forward`.

        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a scalar tensor
                storing the loss. Used during training only. The dict keys are:
                "loss_cls" and "loss_box_reg"
        """
        pred_class_logits, pred_anchor_deltas = permute_all_cls_and_box_to_N_HWA_K_and_concat(
            pred_class_logits, pred_anchor_deltas, self.num_classes
        )  # Shapes: (N x R, K) and (N x R, 4), respectively.

        gt_classes = gt_classes.flatten()
        gt_anchors_deltas = gt_anchors_deltas.view(-1, 4)

        valid_idxs = gt_classes >= 0
        foreground_idxs = (gt_classes >= 0) & (gt_classes != self.num_classes)
        num_foreground = foreground_idxs.sum()

        gt_classes_target = torch.zeros_like(pred_class_logits)
        gt_classes_target[foreground_idxs, gt_classes[foreground_idxs]] = 1

        # logits loss
        loss_cls = sigmoid_focal_loss_jit(
            pred_class_logits[valid_idxs],
            gt_classes_target[valid_idxs],
            alpha=self.focal_loss_alpha,
            gamma=self.focal_loss_gamma,
            reduction="sum",
        ) / max(1, num_foreground)

        # regression loss
        loss_box_reg = smooth_l1_loss(
            pred_anchor_deltas[foreground_idxs],
            gt_anchors_deltas[foreground_idxs],
            beta=self.smooth_l1_loss_beta,
            reduction="sum",
        ) / max(1, num_foreground)

        return {"loss_cls": loss_cls, "loss_box_reg": loss_box_reg}

    @torch.no_grad()
    def get_ground_truth(self, anchors, targets):
        """
        Args:
            anchors (list[list[Boxes]]): a list of N=#image elements. Each is a
                list of #feature level Boxes. The Boxes contains anchors of
                this image on the specific feature level.
            targets (list[Instances]): a list of N `Instances`s. The i-th
                `Instances` contains the ground-truth per-instance annotations
                for the i-th input image.  Specify `targets` during training only.

        Returns:
            gt_classes (Tensor):
                An integer tensor of shape (N, R) storing ground-truth
                labels for each anchor.
                R is the total number of anchors, i.e. the sum of Hi x Wi x A for all levels.
                Anchors with an IoU with some target higher than the foreground threshold
                are assigned their corresponding label in the [0, K-1] range.
                Anchors whose IoU are below the background threshold are assigned
                the label "K". Anchors whose IoU are between the foreground and background
                thresholds are assigned a label "-1", i.e. ignore.
            gt_anchors_deltas (Tensor):
                Shape (N, R, 4).
                The last dimension represents ground-truth box2box transform
                targets (dx, dy, dw, dh) that map each anchor to its matched ground-truth box.
                The values in the tensor are meaningful only when the corresponding
                anchor is labeled as foreground.
        """
        gt_classes = []
        gt_anchors_deltas = []
        anchors = [Boxes.cat(anchors_i) for anchors_i in anchors]
        # list[Tensor(R, 4)], one for each image

        for anchors_per_image, targets_per_image in zip(anchors, targets):
            match_quality_matrix = pairwise_iou(targets_per_image.gt_boxes,
                                                anchors_per_image)
            gt_matched_idxs, anchor_labels = self.matcher(match_quality_matrix)

            has_gt = len(targets_per_image) > 0
            if has_gt:
                # ground truth box regression
                matched_gt_boxes = targets_per_image.gt_boxes[gt_matched_idxs]
                gt_anchors_reg_deltas_i = self.box2box_transform.get_deltas(
                    anchors_per_image.tensor, matched_gt_boxes.tensor
                )

                gt_classes_i = targets_per_image.gt_classes[gt_matched_idxs]
                # Anchors with label 0 are treated as background.
                gt_classes_i[anchor_labels == 0] = self.num_classes
                # Anchors with label -1 are ignored.
                gt_classes_i[anchor_labels == -1] = -1
            else:
                gt_classes_i = torch.zeros_like(
                    gt_matched_idxs) + self.num_classes
                gt_anchors_reg_deltas_i = torch.zeros_like(anchors_per_image.tensor)

            gt_classes.append(gt_classes_i)
            gt_anchors_deltas.append(gt_anchors_reg_deltas_i)

        return torch.stack(gt_classes), torch.stack(gt_anchors_deltas)

    def inference(self, box_cls, box_delta, anchors, image_sizes):
        """
        Arguments:
            box_cls, box_delta: Same as the output of :meth:`RetinaNetHead.forward`
            anchors (list[list[Boxes]]): a list of #images elements. Each is a
                list of #feature level Boxes. The Boxes contain anchors of this
                image on the specific feature level.
            image_sizes (List[torch.Size]): the input image sizes

        Returns:
            results (List[Instances]): a list of #images elements.
        """
        assert len(anchors) == len(image_sizes)
        results = []

        box_cls = [permute_to_N_HWA_K(x, self.num_classes) for x in box_cls]
        box_delta = [permute_to_N_HWA_K(x, 4) for x in box_delta]
        # list[Tensor], one per level, each has shape (N, Hi x Wi x A, K or 4)

        for img_idx, anchors_per_image in enumerate(anchors):
            image_size = image_sizes[img_idx]
            box_cls_per_image = [
                box_cls_per_level[img_idx] for box_cls_per_level in box_cls
            ]
            box_reg_per_image = [
                box_reg_per_level[img_idx] for box_reg_per_level in box_delta
            ]
            results_per_image = self.inference_single_image(
                box_cls_per_image, box_reg_per_image, anchors_per_image,
                tuple(image_size))
            results.append(results_per_image)
        return results

    def inference_single_image(self, box_cls, box_delta, anchors, image_size):
        """
        Single-image inference. Return bounding-box detection results by thresholding
        on scores and applying non-maximum suppression (NMS).

        Arguments:
            box_cls (list[Tensor]): list of #feature levels. Each entry contains
                tensor of size (H x W x A, K)
            box_delta (list[Tensor]): Same shape as 'box_cls' except that K becomes 4.
            anchors (list[Boxes]): list of #feature levels. Each entry contains
                a Boxes object, which contains all the anchors for that
                image in that feature level.
            image_size (tuple(H, W)): a tuple of the image height and width.

        Returns:
            Same as `inference`, but for only one image.
        """
        boxes_all = []
        scores_all = []
        class_idxs_all = []

        # Iterate over every feature level
        for box_cls_i, box_reg_i, anchors_i in zip(box_cls, box_delta,
                                                   anchors):
            # (HxWxAxK,)
            box_cls_i = box_cls_i.flatten().sigmoid_()

            # Keep top k top scoring indices only.
            num_topk = min(self.topk_candidates, box_reg_i.size(0))
            # torch.sort is actually faster than .topk (at least on GPUs)
            predicted_prob, topk_idxs = box_cls_i.sort(descending=True)
            predicted_prob = predicted_prob[:num_topk]
            topk_idxs = topk_idxs[:num_topk]

            # filter out the proposals with low confidence score
            keep_idxs = predicted_prob > self.score_threshold
            predicted_prob = predicted_prob[keep_idxs]
            topk_idxs = topk_idxs[keep_idxs]

            anchor_idxs = topk_idxs // self.num_classes
            classes_idxs = topk_idxs % self.num_classes

            box_reg_i = box_reg_i[anchor_idxs]
            anchors_i = anchors_i[anchor_idxs]
            # predict boxes
            predicted_boxes = self.box2box_transform.apply_deltas(
                box_reg_i, anchors_i.tensor)

            boxes_all.append(predicted_boxes)
            scores_all.append(predicted_prob)
            class_idxs_all.append(classes_idxs)

        boxes_all, scores_all, class_idxs_all = [
            cat(x) for x in [boxes_all, scores_all, class_idxs_all]
        ]

        keep = generalized_batched_nms(boxes_all, scores_all, class_idxs_all,
                                       self.nms_threshold, nms_type=self.nms_type)
        keep = keep[:self.max_detections_per_image]

        result = Instances(image_size)
        result.pred_boxes = Boxes(boxes_all[keep])
        result.scores = scores_all[keep]
        result.pred_classes = class_idxs_all[keep]
        return result

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images,
                                        self.backbone.size_divisibility)
        return images

    def _inference_for_ms_test(self, batched_inputs):
        """
        function used for multiscale test, will be refactor in the future.
        The same input with `forward` function.
        """
        assert not self.training, "inference mode with training=True"
        assert len(batched_inputs) == 1, "inference image number > 1"
        images = self.preprocess_image(batched_inputs)

        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]
        box_cls, box_delta = self.head(features)
        anchors = self.anchor_generator(features)

        results = self.inference(box_cls, box_delta, anchors, images.image_sizes)
        for results_per_image, input_per_image, image_size in zip(
                results, batched_inputs, images.image_sizes
        ):
            height = input_per_image.get("height", image_size[0])
            width = input_per_image.get("width", image_size[1])
            processed_results = detector_postprocess(results_per_image, height, width)
        return processed_results


class RetinaNetHead(nn.Module):
    """
    The head used in RetinaNet for object classification and box regression.
    It has two subnets for the two tasks, with a common structure but separate parameters.
    """
    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super().__init__()
        # fmt: off
        in_channels = input_shape[0].channels
        num_classes = cfg.MODEL.RETINANET.NUM_CLASSES
        num_convs = cfg.MODEL.RETINANET.NUM_CONVS
        prior_prob = cfg.MODEL.RETINANET.PRIOR_PROB
        num_anchors = cfg.build_anchor_generator(cfg, input_shape).num_cell_anchors
        # fmt: on
        assert (
            len(set(num_anchors)) == 1
        ), "Using different number of anchors between levels is not currently supported!"
        num_anchors = num_anchors[0]

        cls_subnet = []
        bbox_subnet = []
        for _ in range(num_convs):
            cls_subnet.append(
                nn.Conv2d(in_channels,
                          in_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1))
            cls_subnet.append(nn.ReLU())
            bbox_subnet.append(
                nn.Conv2d(in_channels,
                          in_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1))
            bbox_subnet.append(nn.ReLU())

        self.cls_subnet = nn.Sequential(*cls_subnet)
        self.bbox_subnet = nn.Sequential(*bbox_subnet)
        self.cls_score = nn.Conv2d(in_channels,
                                   num_anchors * num_classes,
                                   kernel_size=3,
                                   stride=1,
                                   padding=1)
        self.bbox_pred = nn.Conv2d(in_channels,
                                   num_anchors * 4,
                                   kernel_size=3,
                                   stride=1,
                                   padding=1)

        # Initialization
        for modules in [
                self.cls_subnet, self.bbox_subnet, self.cls_score,
                self.bbox_pred
        ]:
            for layer in modules.modules():
                if isinstance(layer, nn.Conv2d):
                    torch.nn.init.normal_(layer.weight, mean=0, std=0.01)
                    torch.nn.init.constant_(layer.bias, 0)

        # Use prior in model initialization to improve stability
        bias_value = -math.log((1 - prior_prob) / prior_prob)
        torch.nn.init.constant_(self.cls_score.bias, bias_value)

    def forward(self, features):
        """
        Arguments:
            features (list[Tensor]): FPN feature map tensors in high to low resolution.
                Each tensor in the list correspond to different feature levels.

        Returns:
            logits (list[Tensor]): #lvl tensors, each has shape (N, AxK, Hi, Wi).
                The tensor predicts the classification probability
                at each spatial position for each of the A anchors and K object
                classes.
            bbox_reg (list[Tensor]): #lvl tensors, each has shape (N, Ax4, Hi, Wi).
                The tensor predicts 4-vector (dx,dy,dw,dh) box
                regression values for every anchor. These values are the
                relative offset between the anchor and the ground truth box.
        """
        logits = []
        bbox_reg = []
        for feature in features:
            logits.append(self.cls_score(self.cls_subnet(feature)))
            bbox_reg.append(self.bbox_pred(self.bbox_subnet(feature)))
        return logits, bbox_reg
```

#### cvpods/modeling/meta_arch/moco.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

import torch
import torch.nn as nn

from cvpods.layers import ShapeSpec
from cvpods.structures import ImageList


def accuracy(output, target, topk=(1,)):
    """Computes the accuracy over the k top predictions for the specified values of k"""
    with torch.no_grad():
        maxk = max(topk)
        batch_size = target.size(0)

        _, pred = output.topk(maxk, 1, True, True)
        pred = pred.t()
        correct = pred.eq(target.view(1, -1).expand_as(pred))

        res = []
        for k in topk:
            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)
            res.append(correct_k.mul_(100.0 / batch_size))
        return res


class MoCo(nn.Module):
    """
    Build a MoCo model with: a query encoder, a key encoder, and a queue
    https://arxiv.org/abs/1911.05722
    """
    def __init__(self, cfg):
        """
        Args:
            cfg (BaseConfig): config
        """
        super(MoCo, self).__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)

        self.dim = cfg.MODEL.MOCO.DIM
        self.K = cfg.MODEL.MOCO.K
        self.m = cfg.MODEL.MOCO.MOMENTUM
        self.T = cfg.MODEL.MOCO.TAU
        self.mlp = cfg.MODEL.MOCO.MLP

        # create the encoders
        # num_classes is the output fc dimension
        cfg.MODEL.RESNETS.NUM_CLASSES = self.dim

        self.encoder_q = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))
        self.encoder_k = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))

        self.size_divisibility = self.encoder_q.size_divisibility

        if self.mlp:  # hack: brute-force replacement
            dim_mlp = self.encoder_q.linear.weight.shape[1]
            self.encoder_q.linear = nn.Sequential(
                nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), self.encoder_q.linear)
            self.encoder_k.linear = nn.Sequential(
                nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), self.encoder_k.linear)

        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):
            param_k.data.copy_(param_q.data)  # initialize
            param_k.requires_grad = False  # not update by gradient

        # create the queue
        self.register_buffer("queue", torch.randn(self.dim, self.K))
        self.queue = nn.functional.normalize(self.queue, dim=0)

        self.register_buffer("queue_ptr", torch.zeros(1, dtype=torch.long))

        self.loss_evaluator = nn.CrossEntropyLoss()

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(3, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(3, 1, 1)
        self.normalizer = lambda x: (x / 255.0 - pixel_mean) / pixel_std

        self.to(self.device)

    @torch.no_grad()
    def _momentum_update_key_encoder(self):
        """
        Momentum update of the key encoder
        """
        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):
            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)

    @torch.no_grad()
    def _dequeue_and_enqueue(self, keys):
        # gather keys before updating queue
        keys = concat_all_gather(keys)

        batch_size = keys.shape[0]

        ptr = int(self.queue_ptr)
        assert self.K % batch_size == 0  # for simplicity

        # replace the keys at ptr (dequeue and enqueue)
        self.queue[:, ptr:ptr + batch_size] = keys.T
        ptr = (ptr + batch_size) % self.K  # move pointer

        self.queue_ptr[0] = ptr

    @torch.no_grad()
    def _batch_shuffle_ddp(self, x):
        """
        Batch shuffle, for making use of BatchNorm.
        *** Only support DistributedDataParallel (DDP) model. ***
        """
        # gather from all gpus
        batch_size_this = x.shape[0]
        x_gather = concat_all_gather(x)
        batch_size_all = x_gather.shape[0]

        num_gpus = batch_size_all // batch_size_this

        # random shuffle index
        idx_shuffle = torch.randperm(batch_size_all).cuda()

        # broadcast to all gpus
        torch.distributed.broadcast(idx_shuffle, src=0)

        # index for restoring
        idx_unshuffle = torch.argsort(idx_shuffle)

        # shuffled index for this gpu
        gpu_idx = torch.distributed.get_rank()
        idx_this = idx_shuffle.view(num_gpus, -1)[gpu_idx]

        return x_gather[idx_this], idx_unshuffle

    @torch.no_grad()
    def _batch_unshuffle_ddp(self, x, idx_unshuffle):
        """
        Undo batch shuffle.
        *** Only support DistributedDataParallel (DDP) model. ***
        """
        # gather from all gpus
        batch_size_this = x.shape[0]
        x_gather = concat_all_gather(x)
        batch_size_all = x_gather.shape[0]

        num_gpus = batch_size_all // batch_size_this

        # restored index for this gpu
        gpu_idx = torch.distributed.get_rank()
        idx_this = idx_unshuffle.view(num_gpus, -1)[gpu_idx]

        return x_gather[idx_this]

    def forward(self, batched_inputs):
        """
        Input:
            im_q: a batch of query images
            im_k: a batch of key images
        Output:
            logits, targets
        """

        im_q = self.preprocess_image([bi["image"][0] for bi in batched_inputs]).tensor
        im_k = self.preprocess_image([bi["image"][1] for bi in batched_inputs]).tensor

        # compute query features
        q = self.encoder_q(im_q)["linear"]  # queries: NxC
        q = nn.functional.normalize(q, dim=1)

        # compute key features
        with torch.no_grad():  # no gradient to keys
            self._momentum_update_key_encoder()  # update the key encoder

            # shuffle for making use of BN
            im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)

            k = self.encoder_k(im_k)["linear"]  # keys: NxC
            k = nn.functional.normalize(k, dim=1)

            # undo shuffle
            k = self._batch_unshuffle_ddp(k, idx_unshuffle)

        # compute logits
        # Einstein sum is more intuitive
        # positive logits: Nx1
        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)
        # negative logits: NxK
        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])

        # logits: Nx(1+K)
        logits = torch.cat([l_pos, l_neg], dim=1)

        # apply temperature
        logits /= self.T

        # labels: positive key indicators
        labels = torch.zeros(logits.shape[0], dtype=torch.long).cuda()

        # dequeue and enqueue
        self._dequeue_and_enqueue(k)

        loss = self.loss_evaluator(logits, labels)
        acc1, acc5 = accuracy(logits, labels, topk=(1, 5))

        return {
            "loss_self_supervised": loss,
            "top1_acc": acc1,
            "top5_acc": acc5,
        }

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        # images = [x["image"].float().to(self.device) for x in batched_inputs]
        images = [x.float().to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images, self.size_divisibility)

        return images


# utils
@torch.no_grad()
def concat_all_gather(tensor):
    """
    Performs all_gather operation on the provided tensors.
    *** Warning ***: torch.distributed.all_gather has no gradient.
    """
    tensors_gather = [torch.ones_like(tensor)
                      for _ in range(torch.distributed.get_world_size())]
    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)

    output = torch.cat(tensors_gather, dim=0)
    return output
```

#### cvpods/modeling/meta_arch/semantic_seg.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
# Modified by BaseDetection, Inc. and its affiliates. All Rights Reserved
from typing import Dict

import numpy as np

import torch
from torch import nn
from torch.nn import functional as F

from cvpods.layers import Conv2d, ShapeSpec
from cvpods.modeling.nn_utils import weight_init
from cvpods.structures import ImageList

from ..postprocessing import sem_seg_postprocess

__all__ = ["SemanticSegmentor", "SemSegFPNHead"]


"""
Registry for semantic segmentation heads, which make semantic segmentation predictions
from feature maps.
"""


class SemanticSegmentor(nn.Module):
    """
    Main class for semantic segmentation architectures.
    """

    def __init__(self, cfg):
        super().__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)

        self.backbone = cfg.build_backbone(cfg)
        self.sem_seg_head = cfg.build_sem_seg_head(cfg, self.backbone.output_shape())

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(-1, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(-1, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std

        self.to(self.device)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
            Each item in the list contains the inputs for one image.

                For now, each item in the list is a dict that contains:

                   * "image": Tensor, image in (C, H, W) format.
                   * "sem_seg": semantic segmentation ground truth
                   * Other information that's included in the original dicts, such as:
                     "height", "width" (int): the output resolution of the model, used in inference.
                     See :meth:`postprocess` for details.

            list[dict]:
              Each dict is the output for one input image.
              The dict contains one key "sem_seg" whose value is a
              Tensor of the output resolution that represents the
              per-pixel segmentation prediction.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images, self.backbone.size_divisibility)

        features = self.backbone(images.tensor)
        if "sem_seg" in batched_inputs[0]:
            targets = [x["sem_seg"].to(self.device) for x in batched_inputs]
            targets = ImageList.from_tensors(
                targets, self.backbone.size_divisibility, pad_value=self.sem_seg_head.ignore_value
            ).tensor
        else:
            targets = None
        results, losses = self.sem_seg_head(features, targets)

        if self.training:
            return losses

        processed_results = []
        for result, input_per_image, image_size in zip(results, batched_inputs, images.image_sizes):
            height = input_per_image.get("height")
            width = input_per_image.get("width")
            r = sem_seg_postprocess(result, image_size, height, width)
            processed_results.append({"sem_seg": r})
        return processed_results


class SemSegFPNHead(nn.Module):
    """
    A semantic segmentation head described in :paper:`PanopticFPN`.
    It takes FPN features as input and merges information from all
    levels of the FPN into single output.
    """

    def __init__(self, cfg, input_shape: Dict[str, ShapeSpec]):
        super().__init__()

        # fmt: off
        self.in_features      = cfg.MODEL.SEM_SEG_HEAD.IN_FEATURES
        feature_strides       = {k: v.stride for k, v in input_shape.items()}
        feature_channels      = {k: v.channels for k, v in input_shape.items()}
        self.ignore_value     = cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE
        num_classes           = cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES
        conv_dims             = cfg.MODEL.SEM_SEG_HEAD.CONVS_DIM
        self.common_stride    = cfg.MODEL.SEM_SEG_HEAD.COMMON_STRIDE
        norm                  = cfg.MODEL.SEM_SEG_HEAD.NORM
        self.loss_weight      = cfg.MODEL.SEM_SEG_HEAD.LOSS_WEIGHT
        # fmt: on

        self.scale_heads = []
        for in_feature in self.in_features:
            head_ops = []
            head_length = max(
                1, int(np.log2(feature_strides[in_feature]) - np.log2(self.common_stride))
            )
            for k in range(head_length):
                norm_module = nn.GroupNorm(32, conv_dims) if norm == "GN" else None
                conv = Conv2d(
                    feature_channels[in_feature] if k == 0 else conv_dims,
                    conv_dims,
                    kernel_size=3,
                    stride=1,
                    padding=1,
                    bias=not norm,
                    norm=norm_module,
                    activation=F.relu,
                )
                weight_init.c2_msra_fill(conv)
                head_ops.append(conv)
                if feature_strides[in_feature] != self.common_stride:
                    head_ops.append(
                        nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)
                    )
            self.scale_heads.append(nn.Sequential(*head_ops))
            self.add_module(in_feature, self.scale_heads[-1])
        self.predictor = Conv2d(conv_dims, num_classes, kernel_size=1, stride=1, padding=0)
        weight_init.c2_msra_fill(self.predictor)

    def forward(self, features, targets=None):
        """
        Returns:
            In training, returns (None, dict of losses)
            In inference, returns (CxHxW logits, {})
        """
        x = self.layers(features)
        if self.training:
            return None, self.losses(x, targets)
        else:
            x = F.interpolate(
                x, scale_factor=self.common_stride, mode="bilinear", align_corners=False
            )
            return x, {}

    def layers(self, features):
        for i, f in enumerate(self.in_features):
            if i == 0:
                x = self.scale_heads[i](features[f])
            else:
                x = x + self.scale_heads[i](features[f])
        x = self.predictor(x)
        return x

    def losses(self, predictions, targets):
        predictions = F.interpolate(
            predictions, scale_factor=self.common_stride, mode="bilinear", align_corners=False
        )
        loss = F.cross_entropy(
            predictions, targets, reduction="mean", ignore_index=self.ignore_value
        )
        losses = {"loss_sem_seg": loss * self.loss_weight}
        return losses
```

#### cvpods/modeling/meta_arch/free_anchor.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import logging
import math
from typing import List

import torch
from torch import nn

from cvpods.layers import ShapeSpec, batched_nms, cat
from cvpods.modeling.box_regression import Box2BoxTransform
from cvpods.modeling.losses import smooth_l1_loss
from cvpods.modeling.postprocessing import detector_postprocess
from cvpods.structures import Boxes, ImageList, Instances, pairwise_iou
from cvpods.utils import log_first_n

from .retinanet import permute_to_N_HWA_K


def positive_bag_loss(logits, dim):
    # bag_prob = Mean-max(logits)
    weight = 1 / (1 - logits)
    weight /= weight.sum(dim).unsqueeze(dim=-1)
    bag_prob = (weight * logits).sum(dim)
    # positive_bag_loss is binary CE loss of (bag_prob, ones_like(bag_prob))
    return -bag_prob.log()


def negative_bag_loss(logits, gamma):
    binary_ce = -(1 - logits).log()
    return logits**gamma * binary_ce


class FreeAnchor(nn.Module):
    """
    Implement RetinaNet (https://arxiv.org/abs/1708.02002).
    """
    def __init__(self, cfg):
        super().__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)

        # fmt: off
        self.num_classes = cfg.MODEL.RETINANET.NUM_CLASSES
        self.in_features = cfg.MODEL.RETINANET.IN_FEATURES
        # Loss parameters:
        self.focal_loss_alpha = cfg.MODEL.RETINANET.FOCAL_LOSS_ALPHA
        self.focal_loss_gamma = cfg.MODEL.RETINANET.FOCAL_LOSS_GAMMA
        self.smooth_l1_loss_beta = cfg.MODEL.RETINANET.SMOOTH_L1_LOSS_BETA
        self.reg_weight = cfg.MODEL.RETINANET.REG_WEIGHT
        # Inference parameters:
        self.score_threshold = cfg.MODEL.RETINANET.SCORE_THRESH_TEST
        self.topk_candidates = cfg.MODEL.RETINANET.TOPK_CANDIDATES_TEST
        self.nms_threshold = cfg.MODEL.RETINANET.NMS_THRESH_TEST
        self.max_detections_per_image = cfg.TEST.DETECTIONS_PER_IMAGE
        # fmt: on

        self.backbone = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))

        backbone_shape = self.backbone.output_shape()
        feature_shapes = [backbone_shape[f] for f in self.in_features]
        self.head = RetinaNetHead(cfg, feature_shapes)
        self.anchor_generator = cfg.build_anchor_generator(cfg, feature_shapes)

        # Matching and loss
        self.box2box_transform = Box2BoxTransform(
            weights=cfg.MODEL.RETINANET.BBOX_REG_WEIGHTS)
        self.pos_anchor_topk = cfg.MODEL.FREE_ANCHOR.POS_ANCHOR_TOPK
        self.bbox_threshold = cfg.MODEL.FREE_ANCHOR.BBOX_THRESHOLD

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(
            3, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(
            3, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std
        self.to(self.device)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
                Each item in the list contains the inputs for one image.
                For now, each item in the list is a dict that contains:

                * image: Tensor, image in (C, H, W) format.
                * instances: Instances

                Other information that's included in the original dicts, such as:

                * "height", "width" (int): the output resolution of the model, used in inference.
                    See :meth:`postprocess` for details.

        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a tensor storing the loss. Used during training only.
        """
        images = self.preprocess_image(batched_inputs)
        if "instances" in batched_inputs[0]:
            gt_instances = [
                x["instances"].to(self.device) for x in batched_inputs
            ]
        elif "targets" in batched_inputs[0]:
            log_first_n(
                logging.WARN,
                "'targets' in the model inputs is now renamed to 'instances'!",
                n=10)
            gt_instances = [
                x["targets"].to(self.device) for x in batched_inputs
            ]
        else:
            gt_instances = None

        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]
        box_cls, box_delta = self.head(features)
        anchors = self.anchor_generator(features)

        if self.training:
            return self.losses(anchors, gt_instances, box_cls, box_delta)
        else:
            results = self.inference(box_cls, box_delta, anchors, images)
            processed_results = []
            for results_per_image, input_per_image, image_size in zip(
                    results, batched_inputs, images.image_sizes):
                height = input_per_image.get("height", image_size[0])
                width = input_per_image.get("width", image_size[1])
                r = detector_postprocess(results_per_image, height, width)
                processed_results.append({"instances": r})
            return processed_results

    def losses(self, anchors, gt_instances, box_cls, box_delta):
        anchors = [Boxes.cat(anchors_i) for anchors_i in anchors]

        box_cls_flattened = [
            permute_to_N_HWA_K(x, self.num_classes) for x in box_cls
        ]
        box_delta_flattened = [permute_to_N_HWA_K(x, 4) for x in box_delta]
        pred_class_logits = cat(box_cls_flattened, dim=1)
        pred_anchor_deltas = cat(box_delta_flattened, dim=1)

        pred_class_probs = pred_class_logits.sigmoid()
        pred_box_probs = []
        num_foreground = 0
        positive_losses = []
        for anchors_per_image, \
            gt_instances_per_image, \
            pred_class_probs_per_image, \
            pred_anchor_deltas_per_image in zip(
                anchors, gt_instances, pred_class_probs, pred_anchor_deltas):
            gt_classes_per_image = gt_instances_per_image.gt_classes

            with torch.no_grad():
                # predicted_boxes_per_image: a_{j}^{loc}, shape: [j, 4]
                predicted_boxes_per_image = self.box2box_transform.apply_deltas(
                    pred_anchor_deltas_per_image, anchors_per_image.tensor)
                # gt_pred_iou: IoU_{ij}^{loc}, shape: [i, j]
                gt_pred_iou = pairwise_iou(gt_instances_per_image.gt_boxes,
                                           Boxes(predicted_boxes_per_image))

                t1 = self.bbox_threshold
                t2 = gt_pred_iou.max(dim=1, keepdim=True).values.clamp_(
                    min=t1 + torch.finfo(torch.float32).eps)
                # gt_pred_prob: P{a_{j} -> b_{i}}, shape: [i, j]
                gt_pred_prob = ((gt_pred_iou - t1) / (t2 - t1)).clamp_(min=0, max=1)

                # pred_box_prob_per_image: P{a_{j} \in A_{+}}, shape: [j, c]
                nonzero_idxs = torch.nonzero(gt_pred_prob, as_tuple=True)
                pred_box_prob_per_image = torch.zeros_like(pred_class_probs_per_image)
                pred_box_prob_per_image[nonzero_idxs[1], gt_classes_per_image[nonzero_idxs[0]]] \
                    = gt_pred_prob[nonzero_idxs]
                pred_box_probs.append(pred_box_prob_per_image)

            # construct bags for objects
            match_quality_matrix = pairwise_iou(
                gt_instances_per_image.gt_boxes, anchors_per_image)
            _, foreground_idxs = torch.topk(match_quality_matrix,
                                            self.pos_anchor_topk,
                                            dim=1,
                                            sorted=False)

            # matched_pred_class_probs_per_image: P_{ij}^{cls}
            matched_pred_class_probs_per_image = torch.gather(
                pred_class_probs_per_image[foreground_idxs], 2,
                gt_classes_per_image.view(-1, 1, 1).repeat(1, self.pos_anchor_topk, 1)
            ).squeeze(2)

            # matched_gt_anchor_deltas_per_image: P_{ij}^{loc}
            matched_gt_anchor_deltas_per_image = self.box2box_transform.get_deltas(
                anchors_per_image.tensor[foreground_idxs],
                gt_instances_per_image.gt_boxes.tensor.unsqueeze(1))
            loss_box_reg = smooth_l1_loss(
                pred_anchor_deltas_per_image[foreground_idxs],
                matched_gt_anchor_deltas_per_image,
                beta=self.smooth_l1_loss_beta,
                reduction="none").sum(dim=-1) * self.reg_weight
            matched_pred_reg_probs_per_image = (-loss_box_reg).exp()

            # positive_losses: { -log( Mean-max(P_{ij}^{cls} * P_{ij}^{loc}) ) }
            num_foreground += len(gt_instances_per_image)
            positive_losses.append(
                positive_bag_loss(
                    matched_pred_class_probs_per_image
                    * matched_pred_reg_probs_per_image,
                    dim=1)
            )

        # positive_loss: \sum_{i}{ -log( Mean-max(P_{ij}^{cls} * P_{ij}^{loc}) ) } / ||B||
        positive_loss = torch.cat(positive_losses).sum() / max(1, num_foreground)

        # pred_box_probs: P{a_{j} \in A_{+}}
        pred_box_probs = torch.stack(pred_box_probs, dim=0)
        # negative_loss: \sum_{j}{ FL( (1 - P{a_{j} \in A_{+}}) * (1 - P_{j}^{bg}) ) } / n||B||
        negative_loss = negative_bag_loss(
            pred_class_probs * (1 - pred_box_probs),
            self.focal_loss_gamma).sum() / max(1, num_foreground * self.pos_anchor_topk)

        loss_pos = positive_loss * self.focal_loss_alpha
        loss_neg = negative_loss * (1 - self.focal_loss_alpha)

        return {"loss_pos": loss_pos, "loss_neg": loss_neg}

    def inference(self, box_cls, box_delta, anchors, images):
        """
        Arguments:
            box_cls, box_delta: Same as the output of :meth:`RetinaNetHead.forward`
            anchors (list[list[Boxes]]): a list of #images elements. Each is a
                list of #feature level Boxes. The Boxes contain anchors of this
                image on the specific feature level.
            images (ImageList): the input images

        Returns:
            results (List[Instances]): a list of #images elements.
        """
        assert len(anchors) == len(images)
        results = []

        box_cls = [permute_to_N_HWA_K(x, self.num_classes) for x in box_cls]
        box_delta = [permute_to_N_HWA_K(x, 4) for x in box_delta]
        # list[Tensor], one per level, each has shape (N, Hi x Wi x A, K or 4)

        for img_idx, anchors_per_image in enumerate(anchors):
            image_size = images.image_sizes[img_idx]
            box_cls_per_image = [
                box_cls_per_level[img_idx] for box_cls_per_level in box_cls
            ]
            box_reg_per_image = [
                box_reg_per_level[img_idx] for box_reg_per_level in box_delta
            ]
            results_per_image = self.inference_single_image(
                box_cls_per_image, box_reg_per_image, anchors_per_image,
                tuple(image_size))
            results.append(results_per_image)
        return results

    def inference_single_image(self, box_cls, box_delta, anchors, image_size):
        """
        Single-image inference. Return bounding-box detection results by thresholding
        on scores and applying non-maximum suppression (NMS).

        Arguments:
            box_cls (list[Tensor]): list of #feature levels. Each entry contains
                tensor of size (H x W x A, K)
            box_delta (list[Tensor]): Same shape as 'box_cls' except that K becomes 4.
            anchors (list[Boxes]): list of #feature levels. Each entry contains
                a Boxes object, which contains all the anchors for that
                image in that feature level.
            image_size (tuple(H, W)): a tuple of the image height and width.

        Returns:
            Same as `inference`, but for only one image.
        """
        boxes_all = []
        scores_all = []
        class_idxs_all = []

        # Iterate over every feature level
        for box_cls_i, box_reg_i, anchors_i in zip(box_cls, box_delta,
                                                   anchors):
            # (HxWxAxK,)
            box_cls_i = box_cls_i.flatten().sigmoid_()

            # Keep top k top scoring indices only.
            num_topk = min(self.topk_candidates, box_reg_i.size(0))
            # torch.sort is actually faster than .topk (at least on GPUs)
            predicted_prob, topk_idxs = box_cls_i.sort(descending=True)
            predicted_prob = predicted_prob[:num_topk]
            topk_idxs = topk_idxs[:num_topk]

            # filter out the proposals with low confidence score
            keep_idxs = predicted_prob > self.score_threshold
            predicted_prob = predicted_prob[keep_idxs]
            topk_idxs = topk_idxs[keep_idxs]

            anchor_idxs = topk_idxs // self.num_classes
            classes_idxs = topk_idxs % self.num_classes

            box_reg_i = box_reg_i[anchor_idxs]
            anchors_i = anchors_i[anchor_idxs]
            # predict boxes
            predicted_boxes = self.box2box_transform.apply_deltas(
                box_reg_i, anchors_i.tensor)

            boxes_all.append(predicted_boxes)
            scores_all.append(predicted_prob)
            class_idxs_all.append(classes_idxs)

        boxes_all, scores_all, class_idxs_all = [
            cat(x) for x in [boxes_all, scores_all, class_idxs_all]
        ]
        keep = batched_nms(boxes_all, scores_all, class_idxs_all,
                           self.nms_threshold)
        keep = keep[:self.max_detections_per_image]

        result = Instances(image_size)
        result.pred_boxes = Boxes(boxes_all[keep])
        result.scores = scores_all[keep]
        result.pred_classes = class_idxs_all[keep]
        return result

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images,
                                        self.backbone.size_divisibility)
        return images


class RetinaNetHead(nn.Module):
    """
    The head used in RetinaNet for object classification and box regression.
    It has two subnets for the two tasks, with a common structure but separate parameters.
    """
    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super().__init__()
        # fmt: off
        in_channels = input_shape[0].channels
        num_classes = cfg.MODEL.RETINANET.NUM_CLASSES
        num_convs = cfg.MODEL.RETINANET.NUM_CONVS
        prior_prob = cfg.MODEL.RETINANET.PRIOR_PROB
        num_anchors = cfg.build_anchor_generator(cfg, input_shape).num_cell_anchors
        # fmt: on
        assert (
            len(set(num_anchors)) == 1
        ), "Using different number of anchors between levels is not currently supported!"
        num_anchors = num_anchors[0]

        cls_subnet = []
        bbox_subnet = []
        for _ in range(num_convs):
            cls_subnet.append(
                nn.Conv2d(in_channels,
                          in_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1))
            cls_subnet.append(nn.ReLU())
            bbox_subnet.append(
                nn.Conv2d(in_channels,
                          in_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1))
            bbox_subnet.append(nn.ReLU())

        self.cls_subnet = nn.Sequential(*cls_subnet)
        self.bbox_subnet = nn.Sequential(*bbox_subnet)
        self.cls_score = nn.Conv2d(in_channels,
                                   num_anchors * num_classes,
                                   kernel_size=3,
                                   stride=1,
                                   padding=1)
        self.bbox_pred = nn.Conv2d(in_channels,
                                   num_anchors * 4,
                                   kernel_size=3,
                                   stride=1,
                                   padding=1)

        # Initialization
        for modules in [
                self.cls_subnet, self.bbox_subnet, self.cls_score,
                self.bbox_pred
        ]:
            for layer in modules.modules():
                if isinstance(layer, nn.Conv2d):
                    torch.nn.init.normal_(layer.weight, mean=0, std=0.01)
                    torch.nn.init.constant_(layer.bias, 0)

        # Use prior in model initialization to improve stability
        bias_value = -math.log((1 - prior_prob) / prior_prob)
        torch.nn.init.constant_(self.cls_score.bias, bias_value)

    def forward(self, features):
        """
        Arguments:
            features (list[Tensor]): FPN feature map tensors in high to low resolution.
                Each tensor in the list correspond to different feature levels.

        Returns:
            logits (list[Tensor]): #lvl tensors, each has shape (N, AxK, Hi, Wi).
                The tensor predicts the classification probability
                at each spatial position for each of the A anchors and K object
                classes.
            bbox_reg (list[Tensor]): #lvl tensors, each has shape (N, Ax4, Hi, Wi).
                The tensor predicts 4-vector (dx,dy,dw,dh) box
                regression values for every anchor. These values are the
                relative offset between the anchor and the ground truth box.
        """
        logits = []
        bbox_reg = []
        for feature in features:
            logits.append(self.cls_score(self.cls_subnet(feature)))
            bbox_reg.append(self.bbox_pred(self.bbox_subnet(feature)))
        return logits, bbox_reg
```

#### cvpods/modeling/meta_arch/atss.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import logging

import torch
import torch.nn.functional as F
from torch import nn

from cvpods.layers import ShapeSpec, cat, generalized_batched_nms
from cvpods.modeling.box_regression import Shift2BoxTransform
from cvpods.modeling.losses import iou_loss, sigmoid_focal_loss_jit
from cvpods.modeling.meta_arch.fcos import FCOSHead, permute_all_cls_and_box_to_N_HWA_K_and_concat
from cvpods.modeling.meta_arch.retinanet import permute_to_N_HWA_K
from cvpods.modeling.postprocessing import detector_postprocess
from cvpods.structures import Boxes, ImageList, Instances, pairwise_iou
from cvpods.utils import comm, log_first_n


class ATSS(nn.Module):
    """
    Implement ATSS (https://arxiv.org/abs/1912.02424).
    """
    def __init__(self, cfg):
        super().__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)

        # fmt: off
        self.num_classes = cfg.MODEL.FCOS.NUM_CLASSES
        self.in_features = cfg.MODEL.FCOS.IN_FEATURES
        self.fpn_strides = cfg.MODEL.FCOS.FPN_STRIDES
        # Loss parameters:
        self.focal_loss_alpha = cfg.MODEL.FCOS.FOCAL_LOSS_ALPHA
        self.focal_loss_gamma = cfg.MODEL.FCOS.FOCAL_LOSS_GAMMA
        self.iou_loss_type = cfg.MODEL.FCOS.IOU_LOSS_TYPE
        self.reg_weight = cfg.MODEL.FCOS.REG_WEIGHT
        # Inference parameters:
        self.score_threshold = cfg.MODEL.FCOS.SCORE_THRESH_TEST
        self.topk_candidates = cfg.MODEL.FCOS.TOPK_CANDIDATES_TEST
        self.nms_threshold = cfg.MODEL.FCOS.NMS_THRESH_TEST
        self.nms_type = cfg.MODEL.NMS_TYPE
        self.max_detections_per_image = cfg.TEST.DETECTIONS_PER_IMAGE
        # fmt: on

        self.backbone = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))

        backbone_shape = self.backbone.output_shape()
        feature_shapes = [backbone_shape[f] for f in self.in_features]
        self.head = FCOSHead(cfg, feature_shapes)
        self.shift_generator = cfg.build_shift_generator(cfg, feature_shapes)

        # Matching and loss
        self.shift2box_transform = Shift2BoxTransform(
            weights=cfg.MODEL.FCOS.BBOX_REG_WEIGHTS)
        self.anchor_scale = cfg.MODEL.ATSS.ANCHOR_SCALE
        self.atss_topk = cfg.MODEL.ATSS.TOPK

        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(
            3, 1, 1)
        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(
            3, 1, 1)
        self.normalizer = lambda x: (x - pixel_mean) / pixel_std
        self.to(self.device)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
                Each item in the list contains the inputs for one image.
                For now, each item in the list is a dict that contains:

                * image: Tensor, image in (C, H, W) format.
                * instances: Instances

                Other information that's included in the original dicts, such as:

                * "height", "width" (int): the output resolution of the model, used in inference.
                    See :meth:`postprocess` for details.
        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a tensor storing the loss. Used during training only.
        """
        images = self.preprocess_image(batched_inputs)
        if "instances" in batched_inputs[0]:
            gt_instances = [
                x["instances"].to(self.device) for x in batched_inputs
            ]
        elif "targets" in batched_inputs[0]:
            log_first_n(
                logging.WARN,
                "'targets' in the model inputs is now renamed to 'instances'!",
                n=10)
            gt_instances = [
                x["targets"].to(self.device) for x in batched_inputs
            ]
        else:
            gt_instances = None

        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]
        box_cls, box_delta, box_center = self.head(features)
        shifts = self.shift_generator(features)

        if self.training:
            gt_classes, gt_shifts_reg_deltas, gt_centerness = self.get_ground_truth(
                shifts, gt_instances)
            return self.losses(gt_classes, gt_shifts_reg_deltas, gt_centerness,
                               box_cls, box_delta, box_center)
        else:
            results = self.inference(box_cls, box_delta, box_center, shifts,
                                     images)
            processed_results = []
            for results_per_image, input_per_image, image_size in zip(
                    results, batched_inputs, images.image_sizes):
                height = input_per_image.get("height", image_size[0])
                width = input_per_image.get("width", image_size[1])
                r = detector_postprocess(results_per_image, height, width)
                processed_results.append({"instances": r})
            return processed_results

    def losses(self, gt_classes, gt_shifts_deltas, gt_centerness,
               pred_class_logits, pred_shift_deltas, pred_centerness):
        """
        Args:
            For `gt_classes`, `gt_shifts_deltas` and `gt_centerness` parameters, see
                :meth:`FCOS.get_ground_truth`.
            Their shapes are (N, R) and (N, R, 4), respectively, where R is
            the total number of shifts across levels, i.e. sum(Hi x Wi)
            For `pred_class_logits`, `pred_shift_deltas` and `pred_centerness`, see
                :meth:`FCOSHead.forward`.

        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a scalar tensor
                storing the loss. Used during training only. The dict keys are:
                "loss_cls" and "loss_box_reg"
        """
        pred_class_logits, pred_shift_deltas, pred_centerness = \
            permute_all_cls_and_box_to_N_HWA_K_and_concat(
                pred_class_logits, pred_shift_deltas, pred_centerness,
                self.num_classes
            )  # Shapes: (N x R, K) and (N x R, 4), respectively.

        gt_classes = gt_classes.flatten()
        gt_shifts_deltas = gt_shifts_deltas.view(-1, 4)
        gt_centerness = gt_centerness.view(-1, 1)

        valid_idxs = gt_classes >= 0
        foreground_idxs = (gt_classes >= 0) & (gt_classes != self.num_classes)
        num_foreground = foreground_idxs.sum()

        gt_classes_target = torch.zeros_like(pred_class_logits)
        gt_classes_target[foreground_idxs, gt_classes[foreground_idxs]] = 1

        num_foreground = comm.all_reduce(num_foreground) / float(comm.get_world_size())
        num_foreground_centerness = gt_centerness[foreground_idxs].sum()
        num_targets = comm.all_reduce(num_foreground_centerness)  / float(comm.get_world_size())

        # logits loss
        loss_cls = sigmoid_focal_loss_jit(
            pred_class_logits[valid_idxs],
            gt_classes_target[valid_idxs],
            alpha=self.focal_loss_alpha,
            gamma=self.focal_loss_gamma,
            reduction="sum",
        ) / max(1.0, num_foreground)

        # regression loss
        loss_box_reg = iou_loss(
            pred_shift_deltas[foreground_idxs],
            gt_shifts_deltas[foreground_idxs],
            gt_centerness[foreground_idxs],
            box_mode="ltrb",
            loss_type=self.iou_loss_type,
            reduction="sum",
        ) / max(1.0, num_targets) * self.reg_weight
        # ) / max(1.0, num_foreground) * self.reg_weight

        # centerness loss
        loss_centerness = F.binary_cross_entropy_with_logits(
            pred_centerness[foreground_idxs],
            gt_centerness[foreground_idxs],
            reduction="sum",
        ) / max(1, num_foreground)

        return {
            "loss_cls": loss_cls,
            "loss_box_reg": loss_box_reg,
            "loss_centerness": loss_centerness
        }

    @torch.no_grad()
    def get_ground_truth(self, shifts, targets):
        """
        Args:
            shifts (list[list[Tensor]]): a list of N=#image elements. Each is a
                list of #feature level tensors. The tensors contains shifts of
                this image on the specific feature level.
            targets (list[Instances]): a list of N `Instances`s. The i-th
                `Instances` contains the ground-truth per-instance annotations
                for the i-th input image.  Specify `targets` during training only.

        Returns:
            gt_classes (Tensor):
                An integer tensor of shape (N, R) storing ground-truth
                labels for each shift.
                R is the total number of shifts, i.e. the sum of Hi x Wi for all levels.
                Shifts in the valid boxes are assigned their corresponding label in the
                [0, K-1] range. Shifts in the background are assigned the label "K".
                Shifts in the ignore areas are assigned a label "-1", i.e. ignore.
            gt_shifts_deltas (Tensor):
                Shape (N, R, 4).
                The last dimension represents ground-truth shift2box transform
                targets (dl, dt, dr, db) that map each shift to its matched ground-truth box.
                The values in the tensor are meaningful only when the corresponding
                shift is labeled as foreground.
            gt_centerness (Tensor):
                An float tensor (0, 1) of shape (N, R) whose values in [0, 1]
                storing ground-truth centerness for each shift.

        """
        gt_classes = []
        gt_shifts_deltas = []
        gt_centerness = []

        for shifts_per_image, targets_per_image in zip(shifts, targets):
            shifts_over_all_feature_maps = torch.cat(shifts_per_image, dim=0)

            gt_boxes = targets_per_image.gt_boxes

            is_in_boxes = self.shift2box_transform.get_deltas(
                shifts_over_all_feature_maps, gt_boxes.tensor.unsqueeze(1)
            ).min(dim=-1).values > 0

            gt_positions_iou = []
            candidate_idxs = []
            base = 0
            for stride, shifts_i in zip(self.fpn_strides, shifts_per_image):
                gt_positions_iou.append(pairwise_iou(
                    gt_boxes,
                    Boxes(torch.cat((
                        shifts_i - stride * self.anchor_scale / 2,
                        shifts_i + stride * self.anchor_scale / 2,
                    ), dim=1))
                ))

                distances = (
                    gt_boxes.get_centers().unsqueeze(1) - shifts_i
                ).pow_(2).sum(dim=-1).sqrt_()
                _, topk_idxs = distances.topk(
                    self.atss_topk, dim=1, largest=False)
                candidate_idxs.append(base + topk_idxs)
                base += len(shifts_i)
            gt_positions_iou = torch.cat(gt_positions_iou, dim=1)
            candidate_idxs = torch.cat(candidate_idxs, dim=1)

            candidate_ious = gt_positions_iou.gather(1, candidate_idxs)
            ious_thr = (candidate_ious.mean(dim=1, keepdim=True)
                        + candidate_ious.std(dim=1, keepdim=True))
            is_foreground = torch.zeros_like(
                is_in_boxes).scatter_(1, candidate_idxs, True)
            is_foreground &= gt_positions_iou >= ious_thr

            gt_positions_iou[~is_in_boxes] = -1
            gt_positions_iou[~is_foreground] = -1

            # if there are still more than one objects for a position,
            # we choose the one with maximum iou
            positions_max_iou, gt_matched_idxs = gt_positions_iou.max(dim=0)

            # ground truth box regression
            gt_shifts_reg_deltas_i = self.shift2box_transform.get_deltas(
                shifts_over_all_feature_maps, gt_boxes[gt_matched_idxs].tensor)

            # ground truth classes
            has_gt = len(targets_per_image) > 0
            if has_gt:
                gt_classes_i = targets_per_image.gt_classes[gt_matched_idxs]
                # Shifts with iou -1 are treated as background.
                gt_classes_i[positions_max_iou == -1] = self.num_classes
            else:
                gt_classes_i = torch.zeros_like(
                    gt_matched_idxs) + self.num_classes

            # ground truth centerness
            left_right = gt_shifts_reg_deltas_i[:, [0, 2]]
            top_bottom = gt_shifts_reg_deltas_i[:, [1, 3]]
            gt_centerness_i = torch.sqrt(
                (left_right.min(dim=-1).values / left_right.max(dim=-1).values).clamp_(min=0)
                * (top_bottom.min(dim=-1).values / top_bottom.max(dim=-1).values).clamp_(min=0)
            )

            gt_classes.append(gt_classes_i)
            gt_shifts_deltas.append(gt_shifts_reg_deltas_i)
            gt_centerness.append(gt_centerness_i)

        return torch.stack(gt_classes), torch.stack(
            gt_shifts_deltas), torch.stack(gt_centerness)

    def inference(self, box_cls, box_delta, box_center, shifts, images):
        """
        Arguments:
            box_cls, box_delta, box_center: Same as the output of :meth:`FCOSHead.forward`
            shifts (list[list[Tensor]): a list of #images elements. Each is a
                list of #feature level tensor. The tensor contain shifts of this
                image on the specific feature level.
            images (ImageList): the input images

        Returns:
            results (List[Instances]): a list of #images elements.
        """
        assert len(shifts) == len(images)
        results = []

        box_cls = [permute_to_N_HWA_K(x, self.num_classes) for x in box_cls]
        box_delta = [permute_to_N_HWA_K(x, 4) for x in box_delta]
        box_center = [permute_to_N_HWA_K(x, 1) for x in box_center]
        # list[Tensor], one per level, each has shape (N, Hi x Wi, K or 4)

        for img_idx, shifts_per_image in enumerate(shifts):
            image_size = images.image_sizes[img_idx]
            box_cls_per_image = [
                box_cls_per_level[img_idx] for box_cls_per_level in box_cls
            ]
            box_reg_per_image = [
                box_reg_per_level[img_idx] for box_reg_per_level in box_delta
            ]
            box_ctr_per_image = [
                box_ctr_per_level[img_idx] for box_ctr_per_level in box_center
            ]
            results_per_image = self.inference_single_image(
                box_cls_per_image, box_reg_per_image, box_ctr_per_image,
                shifts_per_image, tuple(image_size))
            results.append(results_per_image)
        return results

    def inference_single_image(self, box_cls, box_delta, box_center, shifts,
                               image_size):
        """
        Single-image inference. Return bounding-box detection results by thresholding
        on scores and applying non-maximum suppression (NMS).

        Arguments:
            box_cls (list[Tensor]): list of #feature levels. Each entry contains
                tensor of size (H x W, K)
            box_delta (list[Tensor]): Same shape as 'box_cls' except that K becomes 4.
            box_center (list[Tensor]): Same shape as 'box_cls' except that K becomes 1.
            shifts (list[Tensor]): list of #feature levels. Each entry contains
                a tensor, which contains all the shifts for that
                image in that feature level.
            image_size (tuple(H, W)): a tuple of the image height and width.

        Returns:
            Same as `inference`, but for only one image.
        """
        boxes_all = []
        scores_all = []
        class_idxs_all = []

        # Iterate over every feature level
        for box_cls_i, box_reg_i, box_ctr_i, shifts_i in zip(
                box_cls, box_delta, box_center, shifts):
            # (HxWxK,)
            box_cls_i = box_cls_i.flatten().sigmoid_()

            # Keep top k top scoring indices only.
            num_topk = min(self.topk_candidates, box_reg_i.size(0))
            # torch.sort is actually faster than .topk (at least on GPUs)
            predicted_prob, topk_idxs = box_cls_i.sort(descending=True)
            predicted_prob = predicted_prob[:num_topk]
            topk_idxs = topk_idxs[:num_topk]

            # filter out the proposals with low confidence score
            keep_idxs = predicted_prob > self.score_threshold
            predicted_prob = predicted_prob[keep_idxs]
            topk_idxs = topk_idxs[keep_idxs]

            shift_idxs = topk_idxs // self.num_classes
            classes_idxs = topk_idxs % self.num_classes

            box_reg_i = box_reg_i[shift_idxs]
            shifts_i = shifts_i[shift_idxs]
            # predict boxes
            predicted_boxes = self.shift2box_transform.apply_deltas(
                box_reg_i, shifts_i)

            box_ctr_i = box_ctr_i.flatten().sigmoid_()[shift_idxs]
            predicted_prob = torch.sqrt(predicted_prob * box_ctr_i)

            boxes_all.append(predicted_boxes)
            scores_all.append(predicted_prob)
            class_idxs_all.append(classes_idxs)

        boxes_all, scores_all, class_idxs_all = [
            cat(x) for x in [boxes_all, scores_all, class_idxs_all]
        ]

        keep = generalized_batched_nms(
            boxes_all, scores_all, class_idxs_all,
            self.nms_threshold, nms_type=self.nms_type
        )
        keep = keep[:self.max_detections_per_image]

        result = Instances(image_size)
        result.pred_boxes = Boxes(boxes_all[keep])
        result.scores = scores_all[keep]
        result.pred_classes = class_idxs_all[keep]
        return result

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [self.normalizer(x) for x in images]
        images = ImageList.from_tensors(images,
                                        self.backbone.size_divisibility)
        return images

    def _inference_for_ms_test(self, batched_inputs):
        """
        function used for multiscale test, will be refactor in the future.
        The same input with `forward` function.
        """
        assert not self.training, "inference mode with training=True"
        assert len(batched_inputs) == 1, "inference image number > 1"
        images = self.preprocess_image(batched_inputs)

        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]
        box_cls, box_delta, box_center = self.head(features)
        shifts = self.shift_generator(features)

        results = self.inference(box_cls, box_delta, box_center, shifts, images)
        for results_per_image, input_per_image, image_size in zip(
                results, batched_inputs, images.image_sizes
        ):
            height = input_per_image.get("height", image_size[0])
            width = input_per_image.get("width", image_size[1])
            processed_results = detector_postprocess(results_per_image, height, width)
        return processed_results
```

#### cvpods/modeling/proposal_generator/rpn.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from typing import Dict, List

import torch
import torch.nn.functional as F
from torch import nn

from cvpods.layers import ShapeSpec
from cvpods.modeling.anchor_generator import DefaultAnchorGenerator

from ..box_regression import Box2BoxTransform
from ..matcher import Matcher
from .rpn_outputs import RPNOutputs, find_top_rpn_proposals


"""
Registry for RPN heads, which take feature maps and perform
objectness classification and bounding box regression for anchors.

The registered object will be called with `obj(cfg, input_shape)`.
The call should return a `nn.Module` object.
"""


class StandardRPNHead(nn.Module):
    """
    RPN classification and regression heads. Uses a 3x3 conv to produce a shared
    hidden state from which one 1x1 conv predicts objectness logits for each anchor
    and a second 1x1 conv predicts bounding-box deltas specifying how to deform
    each anchor into an object proposal.
    """

    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super().__init__()

        # Standard RPN is shared across levels:
        in_channels = [s.channels for s in input_shape]
        assert len(set(in_channels)) == 1, "Each level must have the same channel!"
        in_channels = in_channels[0]

        # RPNHead should take the same input as anchor generator
        # NOTE: it assumes that creating an anchor generator does not have unwanted side effect.
        anchor_generator = DefaultAnchorGenerator(cfg, input_shape)
        num_cell_anchors = anchor_generator.num_cell_anchors
        box_dim = anchor_generator.box_dim
        assert (
            len(set(num_cell_anchors)) == 1
        ), "Each level must have the same number of cell anchors"
        num_cell_anchors = num_cell_anchors[0]

        # 3x3 conv for the hidden representation
        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)
        # 1x1 conv for predicting objectness logits
        self.objectness_logits = nn.Conv2d(in_channels, num_cell_anchors, kernel_size=1, stride=1)
        # 1x1 conv for predicting box2box transform deltas
        self.anchor_deltas = nn.Conv2d(
            in_channels, num_cell_anchors * box_dim, kernel_size=1, stride=1
        )

        for layer in [self.conv, self.objectness_logits, self.anchor_deltas]:
            nn.init.normal_(layer.weight, std=0.01)
            nn.init.constant_(layer.bias, 0)

    def forward(self, features):
        """
        Args:
            features (list[Tensor]): list of feature maps
        """
        pred_objectness_logits = []
        pred_anchor_deltas = []
        for x in features:
            t = F.relu(self.conv(x))
            pred_objectness_logits.append(self.objectness_logits(t))
            pred_anchor_deltas.append(self.anchor_deltas(t))
        return pred_objectness_logits, pred_anchor_deltas


class RPN(nn.Module):
    """
    Region Proposal Network, introduced by the Faster R-CNN paper.
    """

    def __init__(self, cfg, input_shape: Dict[str, ShapeSpec]):
        super().__init__()

        # fmt: off
        self.min_box_side_len        = cfg.MODEL.PROPOSAL_GENERATOR.MIN_SIZE
        self.in_features             = cfg.MODEL.RPN.IN_FEATURES
        self.nms_thresh              = cfg.MODEL.RPN.NMS_THRESH
        self.nms_type                = cfg.MODEL.RPN.NMS_TYPE
        self.batch_size_per_image    = cfg.MODEL.RPN.BATCH_SIZE_PER_IMAGE
        self.positive_fraction       = cfg.MODEL.RPN.POSITIVE_FRACTION
        self.smooth_l1_beta          = cfg.MODEL.RPN.SMOOTH_L1_BETA
        self.loss_weight             = cfg.MODEL.RPN.LOSS_WEIGHT
        # fmt: on

        # Map from self.training state to train/test settings
        self.pre_nms_topk = {
            True: cfg.MODEL.RPN.PRE_NMS_TOPK_TRAIN,
            False: cfg.MODEL.RPN.PRE_NMS_TOPK_TEST,
        }
        self.post_nms_topk = {
            True: cfg.MODEL.RPN.POST_NMS_TOPK_TRAIN,
            False: cfg.MODEL.RPN.POST_NMS_TOPK_TEST,
        }
        self.boundary_threshold = cfg.MODEL.RPN.BOUNDARY_THRESH

        self.anchor_generator = DefaultAnchorGenerator(
            cfg, [input_shape[f] for f in self.in_features]
        )
        self.box2box_transform = Box2BoxTransform(weights=cfg.MODEL.RPN.BBOX_REG_WEIGHTS)
        self.anchor_matcher = Matcher(
            cfg.MODEL.RPN.IOU_THRESHOLDS, cfg.MODEL.RPN.IOU_LABELS, allow_low_quality_matches=True
        )
        self.rpn_head = StandardRPNHead(cfg, [input_shape[f] for f in self.in_features])

    def forward(self, images, features, gt_instances=None):
        """
        Args:
            images (ImageList): input images of length `N`
            features (dict[str: Tensor]): input data as a mapping from feature
                map name to tensor. Axis 0 represents the number of images `N` in
                the input data; axes 1-3 are channels, height, and width, which may
                vary between feature maps (e.g., if a feature pyramid is used).
            gt_instances (list[Instances], optional): a length `N` list of `Instances`s.
                Each `Instances` stores ground-truth instances for the corresponding image.

        Returns:
            proposals: list[Instances]: contains fields "proposal_boxes", "objectness_logits"
            loss: dict[Tensor] or None
        """
        gt_boxes = [x.gt_boxes for x in gt_instances] if gt_instances is not None else None
        del gt_instances
        features = [features[f] for f in self.in_features]
        pred_objectness_logits, pred_anchor_deltas = self.rpn_head(features)
        anchors = self.anchor_generator(features)
        # TODO: The anchors only depend on the feature map shape; there's probably
        # an opportunity for some optimizations (e.g., caching anchors).
        outputs = RPNOutputs(
            self.box2box_transform,
            self.anchor_matcher,
            self.batch_size_per_image,
            self.positive_fraction,
            images,
            pred_objectness_logits,
            pred_anchor_deltas,
            anchors,
            self.boundary_threshold,
            gt_boxes,
            self.smooth_l1_beta,
        )

        if self.training:
            losses = {k: v * self.loss_weight for k, v in outputs.losses().items()}
        else:
            losses = {}

        with torch.no_grad():
            # Find the top proposals by applying NMS and removing boxes that
            # are too small. The proposals are treated as fixed for approximate
            # joint training with roi heads. This approach ignores the derivative
            # w.r.t. the proposal boxes’ coordinates that are also network
            # responses, so is approximate.
            proposals = find_top_rpn_proposals(
                outputs.predict_proposals(),
                outputs.predict_objectness_logits(),
                images,
                self.nms_thresh,
                self.nms_type,
                self.pre_nms_topk[self.training],
                self.post_nms_topk[self.training],
                self.min_box_side_len,
                self.training,
            )
            # For RPN-only models, the proposals are the final output and we return them in
            # high-to-low confidence order.
            # For end-to-end models, the RPN proposals are an intermediate state
            # and this sorting is actually not needed. But the cost is negligible.
            inds = [p.objectness_logits.sort(descending=True)[1] for p in proposals]
            proposals = [p[ind] for p, ind in zip(proposals, inds)]

        return proposals, losses
```

#### cvpods/modeling/proposal_generator/rrpn.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import logging
from typing import Dict

import torch

from cvpods.layers import ShapeSpec

from ..box_regression import Box2BoxTransformRotated
from .rpn import RPN
from .rrpn_outputs import RRPNOutputs, find_top_rrpn_proposals

logger = logging.getLogger(__name__)


class RRPN(RPN):
    """
    Rotated RPN subnetwork.
    Please refer to https://arxiv.org/pdf/1703.01086.pdf for the original RRPN paper:
    Ma, J., Shao, W., Ye, H., Wang, L., Wang, H., Zheng, Y., & Xue, X. (2018).
    Arbitrary-oriented scene text detection via rotation proposals.
    IEEE Transactions on Multimedia, 20(11), 3111-3122.
    """

    def __init__(self, cfg, input_shape: Dict[str, ShapeSpec]):
        super().__init__(cfg, input_shape)
        self.box2box_transform = Box2BoxTransformRotated(weights=cfg.MODEL.RPN.BBOX_REG_WEIGHTS)

    def forward(self, images, features, gt_instances=None):
        # same signature as RPN.forward
        gt_boxes = [x.gt_boxes for x in gt_instances] if gt_instances is not None else None
        del gt_instances
        features = [features[f] for f in self.in_features]
        pred_objectness_logits, pred_anchor_deltas = self.rpn_head(features)
        anchors = self.anchor_generator(features)

        outputs = RRPNOutputs(
            self.box2box_transform,
            self.anchor_matcher,
            self.batch_size_per_image,
            self.positive_fraction,
            images,
            pred_objectness_logits,
            pred_anchor_deltas,
            anchors,
            self.boundary_threshold,
            gt_boxes,
            self.smooth_l1_beta,
        )

        if self.training:
            losses = outputs.losses()
        else:
            losses = {}

        with torch.no_grad():
            # Find the top proposals by applying NMS and removing boxes that
            # are too small. The proposals are treated as fixed for approximate
            # joint training with roi heads. This approach ignores the derivative
            # w.r.t. the proposal boxes’ coordinates that are also network
            # responses, so is approximate.
            proposals = find_top_rrpn_proposals(
                outputs.predict_proposals(),
                outputs.predict_objectness_logits(),
                images,
                self.nms_thresh,
                self.pre_nms_topk[self.training],
                self.post_nms_topk[self.training],
                self.min_box_side_len,
                self.training,
            )

        return proposals, losses
```

#### cvpods/modeling/proposal_generator/rrpn_outputs.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import itertools
import logging

import torch

from cvpods.layers import batched_nms_rotated, cat
from cvpods.structures import Instances, RotatedBoxes, pairwise_iou_rotated

from .rpn_outputs import RPNOutputs

logger = logging.getLogger(__name__)

"""
Shape shorthand in this module:

    N: number of images in the minibatch
    L: number of feature maps per image on which RRPN is run
    A: number of cell anchors (must be the same for all feature maps)
    Hi, Wi: height and width of the i-th feature map
    5: size of the box parameterization

Naming convention:

    objectness: refers to the binary classification of an anchor as object vs. not
    object.

    deltas: refers to the 5-d (dx, dy, dw, dh, da) deltas that parameterize the rotated box2box
    transform (see :class:`box_regression.Box2BoxTransformRotated`).

    pred_objectness_logits: predicted objectness scores in [-inf, +inf]; use
        sigmoid(pred_objectness_logits) to estimate P(object).

    gt_objectness_logits: ground-truth binary classification labels for objectness

    pred_anchor_deltas: predicted rotated box2box transform deltas

    gt_anchor_deltas: ground-truth rotated box2box transform deltas
"""


def find_top_rrpn_proposals(
    proposals,
    pred_objectness_logits,
    images,
    nms_thresh,
    pre_nms_topk,
    post_nms_topk,
    min_box_side_len,
    training,  # pylint: disable=W0613
):
    """
    For each feature map, select the `pre_nms_topk` highest scoring proposals,
    apply NMS, clip proposals, and remove small boxes. Return the `post_nms_topk`
    highest scoring proposals among all the feature maps if `training` is True,
    otherwise, returns the highest `post_nms_topk` scoring proposals for each
    feature map.

    Args:
        proposals (list[Tensor]): A list of L tensors. Tensor i has shape (N, Hi*Wi*A, 5).
            All proposal predictions on the feature maps.
        pred_objectness_logits (list[Tensor]): A list of L tensors. Tensor i has shape (N, Hi*Wi*A).
        images (ImageList): Input images as an :class:`ImageList`.
        nms_thresh (float): IoU threshold to use for NMS
        pre_nms_topk (int): number of top k scoring proposals to keep before applying NMS.
            When RRPN is run on multiple feature maps (as in FPN) this number is per
            feature map.
        post_nms_topk (int): number of top k scoring proposals to keep after applying NMS.
            When RRPN is run on multiple feature maps (as in FPN) this number is total,
            over all feature maps.
        min_box_side_len (float): minimum proposal box side length in pixels (absolute units
            wrt input images).
        training (bool): True if proposals are to be used in training, otherwise False.
            This arg exists only to support a legacy bug; look for the "NB: Legacy bug ..."
            comment.

    Returns:
        proposals (list[Instances]): list of N Instances. The i-th Instances
            stores post_nms_topk object proposals for image i.
    """
    image_sizes = images.image_sizes  # in (h, w) order
    num_images = len(image_sizes)
    device = proposals[0].device

    # 1. Select top-k anchor for every level and every image
    topk_scores = []  # #lvl Tensor, each of shape N x topk
    topk_proposals = []
    level_ids = []  # #lvl Tensor, each of shape (topk,)
    batch_idx = torch.arange(num_images, device=device)
    for level_id, proposals_i, logits_i in zip(
        itertools.count(), proposals, pred_objectness_logits
    ):
        Hi_Wi_A = logits_i.shape[1]
        num_proposals_i = min(pre_nms_topk, Hi_Wi_A)

        # sort is faster than topk (https://github.com/pytorch/pytorch/issues/22812)
        # topk_scores_i, topk_idx = logits_i.topk(num_proposals_i, dim=1)
        logits_i, idx = logits_i.sort(descending=True, dim=1)
        topk_scores_i = logits_i[batch_idx, :num_proposals_i]
        topk_idx = idx[batch_idx, :num_proposals_i]

        # each is N x topk
        topk_proposals_i = proposals_i[batch_idx[:, None], topk_idx]  # N x topk x 5

        topk_proposals.append(topk_proposals_i)
        topk_scores.append(topk_scores_i)
        level_ids.append(torch.full((num_proposals_i,), level_id, dtype=torch.int64, device=device))

    # 2. Concat all levels together
    topk_scores = cat(topk_scores, dim=1)
    topk_proposals = cat(topk_proposals, dim=1)
    level_ids = cat(level_ids, dim=0)

    # 3. For each image, run a per-level NMS, and choose topk results.
    results = []
    for n, image_size in enumerate(image_sizes):
        boxes = RotatedBoxes(topk_proposals[n])
        scores_per_img = topk_scores[n]
        boxes.clip(image_size)

        # filter empty boxes
        keep = boxes.nonempty(threshold=min_box_side_len)
        lvl = level_ids
        if keep.sum().item() != len(boxes):
            boxes, scores_per_img, lvl = (boxes[keep], scores_per_img[keep], level_ids[keep])

        keep = batched_nms_rotated(boxes.tensor, scores_per_img, lvl, nms_thresh)
        # In Detectron1, there was different behavior during training vs. testing.
        # (https://github.com/facebookresearch/Detectron/issues/459)
        # During training, topk is over the proposals from *all* images in the training batch.
        # During testing, it is over the proposals for each image separately.
        # As a result, the training behavior becomes batch-dependent,
        # and the configuration "POST_NMS_TOPK_TRAIN" end up relying on the batch size.
        # This bug is addressed in cvpods to make the behavior independent of batch size.
        keep = keep[:post_nms_topk]

        res = Instances(image_size)
        res.proposal_boxes = boxes[keep]
        res.objectness_logits = scores_per_img[keep]
        results.append(res)
    return results


class RRPNOutputs(RPNOutputs):
    def __init__(
        self,
        box2box_transform,
        anchor_matcher,
        batch_size_per_image,
        positive_fraction,
        images,
        pred_objectness_logits,
        pred_anchor_deltas,
        anchors,
        boundary_threshold=0,
        gt_boxes=None,
        smooth_l1_beta=0.0,
    ):
        """
        Args:
            box2box_transform (Box2BoxTransformRotated): :class:`Box2BoxTransformRotated`
                instance for anchor-proposal transformations.
            anchor_matcher (Matcher): :class:`Matcher` instance for matching anchors to
                ground-truth boxes; used to determine training labels.
            batch_size_per_image (int): number of proposals to sample when training
            positive_fraction (float): target fraction of sampled proposals that should be positive
            images (ImageList): :class:`ImageList` instance representing N input images
            pred_objectness_logits (list[Tensor]): A list of L elements.
                Element i is a tensor of shape (N, A, Hi, Wi) representing
                the predicted objectness logits for anchors.
            pred_anchor_deltas (list[Tensor]): A list of L elements. Element i is a tensor of shape
                (N, A*5, Hi, Wi) representing the predicted "deltas" used to transform anchors
                to proposals.
            anchors (list[list[RotatedBoxes]]): A list of N elements. Each element is a list of L
                RotatedBoxes. The RotatedBoxes at (n, l) stores the entire anchor array for
                feature map l in image n (i.e. the cell anchors repeated over all locations in
                feature map (n, l)).
            boundary_threshold (int): if >= 0, then anchors that extend beyond the image
                boundary by more than boundary_thresh are not used in training. Set to a very large
                number or < 0 to disable this behavior. Only needed in training.
            gt_boxes (list[RotatedBoxes], optional): A list of N elements. Element i a RotatedBoxes
                storing the ground-truth ("gt") rotated boxes for image i.
            smooth_l1_beta (float): The transition point between L1 and L2 loss in
                the smooth L1 loss function. When set to 0, the loss becomes L1. When
                set to +inf, the loss becomes constant 0.
        """
        super(RRPNOutputs, self).__init__(
            box2box_transform,
            anchor_matcher,
            batch_size_per_image,
            positive_fraction,
            images,
            pred_objectness_logits,
            pred_anchor_deltas,
            anchors,
            boundary_threshold,
            gt_boxes,
            smooth_l1_beta,
        )

    def _get_ground_truth(self):
        """
        Returns:
            gt_objectness_logits: list of N tensors. Tensor i is a vector whose length is the
                total number of anchors in image i (i.e., len(anchors[i])). Label values are
                in {-1, 0, 1}, with meanings: -1 = ignore; 0 = negative class; 1 = positive class.
            gt_anchor_deltas: list of N tensors. Tensor i has shape (len(anchors[i]), 5).
        """
        gt_objectness_logits = []
        gt_anchor_deltas = []
        # Concatenate anchors from all feature maps into a single RotatedBoxes per image
        anchors = [RotatedBoxes.cat(anchors_i) for anchors_i in self.anchors]
        for image_size_i, anchors_i, gt_boxes_i in zip(self.image_sizes, anchors, self.gt_boxes):
            """
            image_size_i: (h, w) for the i-th image
            anchors_i: anchors for i-th image
            gt_boxes_i: ground-truth boxes for i-th image
            """
            match_quality_matrix = pairwise_iou_rotated(gt_boxes_i, anchors_i)
            matched_idxs, gt_objectness_logits_i = self.anchor_matcher(match_quality_matrix)

            if self.boundary_threshold >= 0:
                # Discard anchors that go out of the boundaries of the image
                # NOTE: This is legacy functionality that is turned off by default in cvpods
                anchors_inside_image = anchors_i.inside_box(image_size_i, self.boundary_threshold)
                gt_objectness_logits_i[~anchors_inside_image] = -1

            if len(gt_boxes_i) == 0:
                # These values won't be used anyway since the anchor is labeled as background
                gt_anchor_deltas_i = torch.zeros_like(anchors_i.tensor)
            else:
                # TODO wasted computation for ignored boxes
                matched_gt_boxes = gt_boxes_i[matched_idxs]
                gt_anchor_deltas_i = self.box2box_transform.get_deltas(
                    anchors_i.tensor, matched_gt_boxes.tensor
                )

            gt_objectness_logits.append(gt_objectness_logits_i)
            gt_anchor_deltas.append(gt_anchor_deltas_i)

        return gt_objectness_logits, gt_anchor_deltas
```

#### cvpods/modeling/proposal_generator/__init__.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from .rpn import RPN
from .rrpn import RRPN
```

#### cvpods/modeling/proposal_generator/rpn_outputs.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import itertools
import logging

import numpy as np

import torch
import torch.nn.functional as F

from cvpods.layers import cat, generalized_batched_nms
from cvpods.modeling.losses import smooth_l1_loss
from cvpods.structures import Boxes, Instances, pairwise_iou
from cvpods.utils import get_event_storage, retry_if_cuda_oom

from ..sampling import subsample_labels

logger = logging.getLogger(__name__)

# TODO: comments for future refactoring of this module
#
# From @rbg:
# This code involves a significant amount of tensor reshaping and permuting. Look for
# ways to simplify this.

"""
Shape shorthand in this module:

    N: number of images in the minibatch
    L: number of feature maps per image on which RPN is run
    A: number of cell anchors (must be the same for all feature maps)
    Hi, Wi: height and width of the i-th feature map
    4: size of the box parameterization

Naming convention:

    objectness: refers to the binary classification of an anchor as object vs. not
    object.

    deltas: refers to the 4-d (dx, dy, dw, dh) deltas that parameterize the box2box
    transform (see :class:`box_regression.Box2BoxTransform`).

    pred_objectness_logits: predicted objectness scores in [-inf, +inf]; use
        sigmoid(pred_objectness_logits) to estimate P(object).

    gt_objectness_logits: ground-truth binary classification labels for objectness

    pred_anchor_deltas: predicted box2box transform deltas

    gt_anchor_deltas: ground-truth box2box transform deltas
"""


def find_top_rpn_proposals(
    proposals,
    pred_objectness_logits,
    images,
    nms_thresh,
    nms_type,
    pre_nms_topk,
    post_nms_topk,
    min_box_side_len,
    training,  # pylint: disable=W0613
):
    """
    For each feature map, select the `pre_nms_topk` highest scoring proposals,
    apply NMS, clip proposals, and remove small boxes. Return the `post_nms_topk`
    highest scoring proposals among all the feature maps if `training` is True,
    otherwise, returns the highest `post_nms_topk` scoring proposals for each
    feature map.

    Args:
        proposals (list[Tensor]): A list of L tensors. Tensor i has shape (N, Hi*Wi*A, 4).
            All proposal predictions on the feature maps.
        pred_objectness_logits (list[Tensor]): A list of L tensors. Tensor i has shape (N, Hi*Wi*A).
        images (ImageList): Input images as an :class:`ImageList`.
        nms_thresh (float): IoU threshold to use for NMS
        pre_nms_topk (int): number of top k scoring proposals to keep before applying NMS.
            When RPN is run on multiple feature maps (as in FPN) this number is per
            feature map.
        post_nms_topk (int): number of top k scoring proposals to keep after applying NMS.
            When RPN is run on multiple feature maps (as in FPN) this number is total,
            over all feature maps.
        min_box_side_len (float): minimum proposal box side length in pixels (absolute units
            wrt input images).
        training (bool): True if proposals are to be used in training, otherwise False.
            This arg exists only to support a legacy bug; look for the "NB: Legacy bug ..."
            comment.

    Returns:
        proposals (list[Instances]): list of N Instances. The i-th Instances
            stores post_nms_topk object proposals for image i.
    """
    image_sizes = images.image_sizes  # in (h, w) order
    num_images = len(image_sizes)
    device = proposals[0].device

    # 1. Select top-k anchor for every level and every image
    topk_scores = []  # #lvl Tensor, each of shape N x topk
    topk_proposals = []
    level_ids = []  # #lvl Tensor, each of shape (topk,)
    batch_idx = torch.arange(num_images, device=device)
    for level_id, proposals_i, logits_i in zip(
        itertools.count(), proposals, pred_objectness_logits
    ):
        Hi_Wi_A = logits_i.shape[1]
        num_proposals_i = min(pre_nms_topk, Hi_Wi_A)

        # sort is faster than topk (https://github.com/pytorch/pytorch/issues/22812)
        # topk_scores_i, topk_idx = logits_i.topk(num_proposals_i, dim=1)
        logits_i, idx = logits_i.sort(descending=True, dim=1)
        topk_scores_i = logits_i[batch_idx, :num_proposals_i]
        topk_idx = idx[batch_idx, :num_proposals_i]

        # each is N x topk
        topk_proposals_i = proposals_i[batch_idx[:, None], topk_idx]  # N x topk x 4

        topk_proposals.append(topk_proposals_i)
        topk_scores.append(topk_scores_i)
        level_ids.append(torch.full((num_proposals_i,), level_id, dtype=torch.int64, device=device))

    # 2. Concat all levels together
    topk_scores = cat(topk_scores, dim=1)
    topk_proposals = cat(topk_proposals, dim=1)
    level_ids = cat(level_ids, dim=0)

    # 3. For each image, run a per-level NMS, and choose topk results.
    results = []
    for n, image_size in enumerate(image_sizes):
        boxes = Boxes(topk_proposals[n])
        scores_per_img = topk_scores[n]
        boxes.clip(image_size)

        # filter empty boxes
        keep = boxes.nonempty(threshold=min_box_side_len)
        lvl = level_ids
        if keep.sum().item() != len(boxes):
            boxes, scores_per_img, lvl = boxes[keep], scores_per_img[keep], level_ids[keep]

        keep = generalized_batched_nms(boxes.tensor, scores_per_img, lvl,
                                       nms_thresh, nms_type=nms_type)

        # In Detectron1, there was different behavior during training vs. testing.
        # (https://github.com/facebookresearch/Detectron/issues/459)
        # During training, topk is over the proposals from *all* images in the training batch.
        # During testing, it is over the proposals for each image separately.
        # As a result, the training behavior becomes batch-dependent,
        # and the configuration "POST_NMS_TOPK_TRAIN" end up relying on the batch size.
        # This bug is addressed in cvpods to make the behavior independent of batch size.
        keep = keep[:post_nms_topk]

        res = Instances(image_size)
        res.proposal_boxes = boxes[keep]
        res.objectness_logits = scores_per_img[keep]
        results.append(res)
    return results


def rpn_losses(
    gt_objectness_logits,
    gt_anchor_deltas,
    pred_objectness_logits,
    pred_anchor_deltas,
    smooth_l1_beta,
):
    """
    Args:
        gt_objectness_logits (Tensor): shape (N,), each element in {-1, 0, 1} representing
            ground-truth objectness labels with: -1 = ignore; 0 = not object; 1 = object.
        gt_anchor_deltas (Tensor): shape (N, box_dim), row i represents ground-truth
            box2box transform targets (dx, dy, dw, dh) or (dx, dy, dw, dh, da) that map anchor i to
            its matched ground-truth box.
        pred_objectness_logits (Tensor): shape (N,), each element is a predicted objectness
            logit.
        pred_anchor_deltas (Tensor): shape (N, box_dim), each row is a predicted box2box
            transform (dx, dy, dw, dh) or (dx, dy, dw, dh, da)
        smooth_l1_beta (float): The transition point between L1 and L2 loss in
            the smooth L1 loss function. When set to 0, the loss becomes L1. When
            set to +inf, the loss becomes constant 0.

    Returns:
        objectness_loss, localization_loss, both unnormalized (summed over samples).
    """
    pos_masks = gt_objectness_logits == 1
    localization_loss = smooth_l1_loss(
        pred_anchor_deltas[pos_masks], gt_anchor_deltas[pos_masks], smooth_l1_beta, reduction="sum"
    )

    valid_masks = gt_objectness_logits >= 0
    objectness_loss = F.binary_cross_entropy_with_logits(
        pred_objectness_logits[valid_masks],
        gt_objectness_logits[valid_masks].to(torch.float32),
        reduction="sum",
    )
    return objectness_loss, localization_loss


class RPNOutputs(object):
    def __init__(
        self,
        box2box_transform,
        anchor_matcher,
        batch_size_per_image,
        positive_fraction,
        images,
        pred_objectness_logits,
        pred_anchor_deltas,
        anchors,
        boundary_threshold=0,
        gt_boxes=None,
        smooth_l1_beta=0.0,
    ):
        """
        Args:
            box2box_transform (Box2BoxTransform): :class:`Box2BoxTransform` instance for
                anchor-proposal transformations.
            anchor_matcher (Matcher): :class:`Matcher` instance for matching anchors to
                ground-truth boxes; used to determine training labels.
            batch_size_per_image (int): number of proposals to sample when training
            positive_fraction (float): target fraction of sampled proposals that should be positive
            images (ImageList): :class:`ImageList` instance representing N input images
            pred_objectness_logits (list[Tensor]): A list of L elements.
                Element i is a tensor of shape (N, A, Hi, Wi) representing
                the predicted objectness logits for anchors.
            pred_anchor_deltas (list[Tensor]): A list of L elements. Element i is a tensor of shape
                (N, A*4, Hi, Wi) representing the predicted "deltas" used to transform anchors
                to proposals.
            anchors (list[list[Boxes]]): A list of N elements. Each element is a list of L
                Boxes. The Boxes at (n, l) stores the entire anchor array for feature map l in image
                n (i.e. the cell anchors repeated over all locations in feature map (n, l)).
            boundary_threshold (int): if >= 0, then anchors that extend beyond the image
                boundary by more than boundary_thresh are not used in training. Set to a very large
                number or < 0 to disable this behavior. Only needed in training.
            gt_boxes (list[Boxes], optional): A list of N elements. Element i a Boxes storing
                the ground-truth ("gt") boxes for image i.
            smooth_l1_beta (float): The transition point between L1 and L2 loss in
                the smooth L1 loss function. When set to 0, the loss becomes L1. When
                set to +inf, the loss becomes constant 0.
        """
        self.box2box_transform = box2box_transform
        self.anchor_matcher = anchor_matcher
        self.batch_size_per_image = batch_size_per_image
        self.positive_fraction = positive_fraction
        self.pred_objectness_logits = pred_objectness_logits
        self.pred_anchor_deltas = pred_anchor_deltas

        self.anchors = anchors
        self.gt_boxes = gt_boxes
        self.num_feature_maps = len(pred_objectness_logits)
        self.num_images = len(images)
        self.image_sizes = images.image_sizes
        self.boundary_threshold = boundary_threshold
        self.smooth_l1_beta = smooth_l1_beta

    def _get_ground_truth(self):
        """
        Returns:
            gt_objectness_logits: list of N tensors. Tensor i is a vector whose length is the
                total number of anchors in image i (i.e., len(anchors[i])). Label values are
                in {-1, 0, 1}, with meanings: -1 = ignore; 0 = negative class; 1 = positive class.
            gt_anchor_deltas: list of N tensors. Tensor i has shape (len(anchors[i]), 4).
        """
        gt_objectness_logits = []
        gt_anchor_deltas = []
        # Concatenate anchors from all feature maps into a single Boxes per image
        anchors = [Boxes.cat(anchors_i) for anchors_i in self.anchors]
        for image_size_i, anchors_i, gt_boxes_i in zip(self.image_sizes, anchors, self.gt_boxes):
            """
            image_size_i: (h, w) for the i-th image
            anchors_i: anchors for i-th image
            gt_boxes_i: ground-truth boxes for i-th image
            """
            match_quality_matrix = retry_if_cuda_oom(pairwise_iou)(gt_boxes_i, anchors_i)
            matched_idxs, gt_objectness_logits_i = retry_if_cuda_oom(self.anchor_matcher)(
                match_quality_matrix
            )
            # Matching is memory-expensive and may result in CPU tensors. But the result is small
            gt_objectness_logits_i = gt_objectness_logits_i.to(device=gt_boxes_i.device)
            del match_quality_matrix

            if self.boundary_threshold >= 0:
                # Discard anchors that go out of the boundaries of the image
                # NOTE: This is legacy functionality that is turned off by default in cvpods
                anchors_inside_image = anchors_i.inside_box(image_size_i, self.boundary_threshold)
                gt_objectness_logits_i[~anchors_inside_image] = -1

            if len(gt_boxes_i) == 0:
                # These values won't be used anyway since the anchor is labeled as background
                gt_anchor_deltas_i = torch.zeros_like(anchors_i.tensor)
            else:
                # TODO wasted computation for ignored boxes
                matched_gt_boxes = gt_boxes_i[matched_idxs]
                gt_anchor_deltas_i = self.box2box_transform.get_deltas(
                    anchors_i.tensor, matched_gt_boxes.tensor
                )

            gt_objectness_logits.append(gt_objectness_logits_i)
            gt_anchor_deltas.append(gt_anchor_deltas_i)

        return gt_objectness_logits, gt_anchor_deltas

    def losses(self):
        """
        Return the losses from a set of RPN predictions and their associated ground-truth.

        Returns:
            dict[loss name -> loss value]: A dict mapping from loss name to loss value.
                Loss names are: `loss_rpn_cls` for objectness classification and
                `loss_rpn_loc` for proposal localization.
        """

        def resample(label):
            """
            Randomly sample a subset of positive and negative examples by overwriting
            the label vector to the ignore value (-1) for all elements that are not
            included in the sample.
            """
            pos_idx, neg_idx = subsample_labels(
                label, self.batch_size_per_image, self.positive_fraction, 0
            )
            # Fill with the ignore label (-1), then set positive and negative labels
            label.fill_(-1)
            label.scatter_(0, pos_idx, 1)
            label.scatter_(0, neg_idx, 0)
            return label

        gt_objectness_logits, gt_anchor_deltas = self._get_ground_truth()
        """
        gt_objectness_logits: list of N tensors. Tensor i is a vector whose length is the
            total number of anchors in image i (i.e., len(anchors[i]))
        gt_anchor_deltas: list of N tensors. Tensor i has shape (len(anchors[i]), B),
            where B is the box dimension
        """
        # Collect all objectness labels and delta targets over feature maps and images
        # The final ordering is L, N, H, W, A from slowest to fastest axis.
        num_anchors_per_map = [np.prod(x.shape[1:]) for x in self.pred_objectness_logits]
        num_anchors_per_image = sum(num_anchors_per_map)

        # Stack to: (N, num_anchors_per_image)
        gt_objectness_logits = torch.stack(
            [resample(label) for label in gt_objectness_logits], dim=0
        )

        # Log the number of positive/negative anchors per-image that's used in training
        num_pos_anchors = (gt_objectness_logits == 1).sum().item()
        num_neg_anchors = (gt_objectness_logits == 0).sum().item()
        storage = get_event_storage()
        storage.put_scalar("rpn/num_pos_anchors", num_pos_anchors / self.num_images)
        storage.put_scalar("rpn/num_neg_anchors", num_neg_anchors / self.num_images)

        assert gt_objectness_logits.shape[1] == num_anchors_per_image
        # Split to tuple of L tensors, each with shape (N, num_anchors_per_map)
        gt_objectness_logits = torch.split(gt_objectness_logits, num_anchors_per_map, dim=1)
        # Concat from all feature maps
        gt_objectness_logits = cat([x.flatten() for x in gt_objectness_logits], dim=0)

        # Stack to: (N, num_anchors_per_image, B)
        gt_anchor_deltas = torch.stack(gt_anchor_deltas, dim=0)
        assert gt_anchor_deltas.shape[1] == num_anchors_per_image
        B = gt_anchor_deltas.shape[2]  # box dimension (4 or 5)

        # Split to tuple of L tensors, each with shape (N, num_anchors_per_image)
        gt_anchor_deltas = torch.split(gt_anchor_deltas, num_anchors_per_map, dim=1)
        # Concat from all feature maps
        gt_anchor_deltas = cat([x.reshape(-1, B) for x in gt_anchor_deltas], dim=0)

        # Collect all objectness logits and delta predictions over feature maps
        # and images to arrive at the same shape as the labels and targets
        # The final ordering is L, N, H, W, A from slowest to fastest axis.
        pred_objectness_logits = cat(
            [
                # Reshape: (N, A, Hi, Wi) -> (N, Hi, Wi, A) -> (N*Hi*Wi*A, )
                x.permute(0, 2, 3, 1).flatten()
                for x in self.pred_objectness_logits
            ],
            dim=0,
        )
        pred_anchor_deltas = cat(
            [
                # Reshape: (N, A*B, Hi, Wi) -> (N, A, B, Hi, Wi) -> (N, Hi, Wi, A, B)
                #          -> (N*Hi*Wi*A, B)
                x.view(x.shape[0], -1, B, x.shape[-2], x.shape[-1])
                .permute(0, 3, 4, 1, 2)
                .reshape(-1, B)
                for x in self.pred_anchor_deltas
            ],
            dim=0,
        )

        objectness_loss, localization_loss = rpn_losses(
            gt_objectness_logits,
            gt_anchor_deltas,
            pred_objectness_logits,
            pred_anchor_deltas,
            self.smooth_l1_beta,
        )
        normalizer = 1.0 / (self.batch_size_per_image * self.num_images)
        loss_cls = objectness_loss * normalizer  # cls: classification loss
        loss_loc = localization_loss * normalizer  # loc: localization loss
        losses = {"loss_rpn_cls": loss_cls, "loss_rpn_loc": loss_loc}

        return losses

    def predict_proposals(self):
        """
        Transform anchors into proposals by applying the predicted anchor deltas.

        Returns:
            proposals (list[Tensor]): A list of L tensors. Tensor i has shape
                (N, Hi*Wi*A, B), where B is box dimension (4 or 5).
        """
        proposals = []
        # Transpose anchors from images-by-feature-maps (N, L) to feature-maps-by-images (L, N)
        anchors = list(zip(*self.anchors))
        # For each feature map
        for anchors_i, pred_anchor_deltas_i in zip(anchors, self.pred_anchor_deltas):
            B = anchors_i[0].tensor.size(1)
            N, _, Hi, Wi = pred_anchor_deltas_i.shape
            # Reshape: (N, A*B, Hi, Wi) -> (N, A, B, Hi, Wi) -> (N, Hi, Wi, A, B) -> (N*Hi*Wi*A, B)
            pred_anchor_deltas_i = (
                pred_anchor_deltas_i.view(N, -1, B, Hi, Wi).permute(0, 3, 4, 1, 2).reshape(-1, B)
            )
            # Concatenate all anchors to shape (N*Hi*Wi*A, B)
            # type(anchors_i[0]) is Boxes (B = 4) or RotatedBoxes (B = 5)
            anchors_i = type(anchors_i[0]).cat(anchors_i)
            proposals_i = self.box2box_transform.apply_deltas(
                pred_anchor_deltas_i, anchors_i.tensor
            )
            # Append feature map proposals with shape (N, Hi*Wi*A, B)
            proposals.append(proposals_i.view(N, -1, B))
        return proposals

    def predict_objectness_logits(self):
        """
        Return objectness logits in the same format as the proposals returned by
        :meth:`predict_proposals`.

        Returns:
            pred_objectness_logits (list[Tensor]): A list of L tensors. Tensor i has shape
                (N, Hi*Wi*A).
        """
        pred_objectness_logits = [
            # Reshape: (N, A, Hi, Wi) -> (N, Hi, Wi, A) -> (N, Hi*Wi*A)
            score.permute(0, 2, 3, 1).reshape(self.num_images, -1)
            for score in self.pred_objectness_logits
        ]
        return pred_objectness_logits
```

#### cvpods/modeling/proposal_generator/proposal_utils.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import math

import torch

from cvpods.structures import Instances


def add_ground_truth_to_proposals(gt_boxes, proposals):
    """
    Call `add_ground_truth_to_proposals_single_image` for all images.

    Args:
        gt_boxes(list[Boxes]): list of N elements. Element i is a Boxes
            representing the gound-truth for image i.
        proposals (list[Instances]): list of N elements. Element i is a Instances
            representing the proposals for image i.

    Returns:
        list[Instances]: list of N Instances. Each is the proposals for the image,
            with field "proposal_boxes" and "objectness_logits".
    """
    assert gt_boxes is not None

    assert len(proposals) == len(gt_boxes)
    if len(proposals) == 0:
        return proposals

    return [
        add_ground_truth_to_proposals_single_image(gt_boxes_i, proposals_i)
        for gt_boxes_i, proposals_i in zip(gt_boxes, proposals)
    ]


def add_ground_truth_to_proposals_single_image(gt_boxes, proposals):
    """
    Augment `proposals` with ground-truth boxes from `gt_boxes`.

    Args:
        Same as `add_ground_truth_to_proposals`, but with gt_boxes and proposals
        per image.

    Returns:
        Same as `add_ground_truth_to_proposals`, but for only one image.
    """
    device = proposals.objectness_logits.device
    # Concatenating gt_boxes with proposals requires them to have the same fields
    # Assign all ground-truth boxes an objectness logit corresponding to P(object) \approx 1.
    gt_logit_value = math.log((1.0 - 1e-10) / (1 - (1.0 - 1e-10)))

    gt_logits = gt_logit_value * torch.ones(len(gt_boxes), device=device)
    gt_proposal = Instances(proposals.image_size)

    gt_proposal.proposal_boxes = gt_boxes
    gt_proposal.objectness_logits = gt_logits
    new_proposals = Instances.cat([proposals, gt_proposal])

    return new_proposals
```

#### cvpods/modeling/roi_heads/mask_head.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import torch
from torch import nn
from torch.nn import functional as F

from cvpods.layers import Conv2d, ConvTranspose2d, ShapeSpec, cat, get_norm
from cvpods.modeling.nn_utils import weight_init
from cvpods.utils import get_event_storage


def mask_rcnn_loss(pred_mask_logits, instances):
    """
    Compute the mask prediction loss defined in the Mask R-CNN paper.

    Args:
        pred_mask_logits (Tensor): A tensor of shape (B, C, Hmask, Wmask) or (B, 1, Hmask, Wmask)
            for class-specific or class-agnostic, where B is the total number of predicted masks
            in all images, C is the number of foreground classes, and Hmask, Wmask are the height
            and width of the mask predictions. The values are logits.
        instances (list[Instances]): A list of N Instances, where N is the number of images
            in the batch. These instances are in 1:1
            correspondence with the pred_mask_logits. The ground-truth labels (class, box, mask,
            ...) associated with each instance are stored in fields.

    Returns:
        mask_loss (Tensor): A scalar tensor containing the loss.
    """
    cls_agnostic_mask = pred_mask_logits.size(1) == 1
    total_num_masks = pred_mask_logits.size(0)
    mask_side_len = pred_mask_logits.size(2)
    assert pred_mask_logits.size(2) == pred_mask_logits.size(3), "Mask prediction must be square!"

    gt_classes = []
    gt_masks = []
    for instances_per_image in instances:
        if len(instances_per_image) == 0:
            continue
        if not cls_agnostic_mask:
            gt_classes_per_image = instances_per_image.gt_classes.to(dtype=torch.int64)
            gt_classes.append(gt_classes_per_image)

        gt_masks_per_image = instances_per_image.gt_masks.crop_and_resize(
            instances_per_image.proposal_boxes.tensor, mask_side_len
        ).to(device=pred_mask_logits.device)
        # A tensor of shape (N, M, M), N=#instances in the image; M=mask_side_len
        gt_masks.append(gt_masks_per_image)

    if len(gt_masks) == 0:
        return pred_mask_logits.sum() * 0

    gt_masks = cat(gt_masks, dim=0)

    if cls_agnostic_mask:
        pred_mask_logits = pred_mask_logits[:, 0]
    else:
        indices = torch.arange(total_num_masks)
        gt_classes = cat(gt_classes, dim=0)
        pred_mask_logits = pred_mask_logits[indices, gt_classes]

    if gt_masks.dtype == torch.bool:
        gt_masks_bool = gt_masks
    else:
        # Here we allow gt_masks to be float as well (depend on the implementation of rasterize())
        gt_masks_bool = gt_masks > 0.5

    # Log the training accuracy (using gt classes and 0.5 threshold)
    mask_incorrect = (pred_mask_logits > 0.0) != gt_masks_bool
    mask_accuracy = 1 - (mask_incorrect.sum().item() / max(mask_incorrect.numel(), 1.0))
    num_positive = gt_masks_bool.sum().item()
    false_positive = (mask_incorrect & ~gt_masks_bool).sum().item() / max(
        gt_masks_bool.numel() - num_positive, 1.0
    )
    false_negative = (mask_incorrect & gt_masks_bool).sum().item() / max(num_positive, 1.0)

    storage = get_event_storage()
    storage.put_scalar("mask_rcnn/accuracy", mask_accuracy)
    storage.put_scalar("mask_rcnn/false_positive", false_positive)
    storage.put_scalar("mask_rcnn/false_negative", false_negative)

    mask_loss = F.binary_cross_entropy_with_logits(
        pred_mask_logits, gt_masks.to(dtype=torch.float32), reduction="mean"
    )
    return mask_loss


def mask_rcnn_inference(pred_mask_logits, pred_instances):
    """
    Convert pred_mask_logits to estimated foreground probability masks while also
    extracting only the masks for the predicted classes in pred_instances. For each
    predicted box, the mask of the same class is attached to the instance by adding a
    new "pred_masks" field to pred_instances.

    Args:
        pred_mask_logits (Tensor): A tensor of shape (B, C, Hmask, Wmask) or (B, 1, Hmask, Wmask)
            for class-specific or class-agnostic, where B is the total number of predicted masks
            in all images, C is the number of foreground classes, and Hmask, Wmask are the height
            and width of the mask predictions. The values are logits.
        pred_instances (list[Instances]): A list of N Instances, where N is the number of images
            in the batch. Each Instances must have field "pred_classes".

    Returns:
        None. pred_instances will contain an extra "pred_masks" field storing a mask of size (Hmask,
            Wmask) for predicted class. Note that the masks are returned as a soft (non-quantized)
            masks the resolution predicted by the network; post-processing steps, such as resizing
            the predicted masks to the original image resolution and/or binarizing them, is left
            to the caller.
    """
    cls_agnostic_mask = pred_mask_logits.size(1) == 1

    if cls_agnostic_mask:
        mask_probs_pred = pred_mask_logits.sigmoid()
    else:
        # Select masks corresponding to the predicted classes
        num_masks = pred_mask_logits.shape[0]
        class_pred = cat([i.pred_classes for i in pred_instances])
        indices = torch.arange(num_masks, device=class_pred.device)
        mask_probs_pred = pred_mask_logits[indices, class_pred][:, None].sigmoid()
    # mask_probs_pred.shape: (B, 1, Hmask, Wmask)

    num_boxes_per_image = [len(i) for i in pred_instances]
    mask_probs_pred = mask_probs_pred.split(num_boxes_per_image, dim=0)

    for prob, instances in zip(mask_probs_pred, pred_instances):
        instances.pred_masks = prob  # (1, Hmask, Wmask)


class MaskRCNNConvUpsampleHead(nn.Module):
    """
    A mask head with several conv layers, plus an upsample layer (with `ConvTranspose2d`).
    """

    def __init__(self, cfg, input_shape: ShapeSpec):
        """
        The following attributes are parsed from config:
            num_conv: the number of conv layers
            conv_dim: the dimension of the conv layers
            norm: normalization for the conv layers
        """
        super(MaskRCNNConvUpsampleHead, self).__init__()

        # fmt: off
        num_classes       = cfg.MODEL.ROI_HEADS.NUM_CLASSES
        conv_dims         = cfg.MODEL.ROI_MASK_HEAD.CONV_DIM
        self.norm         = cfg.MODEL.ROI_MASK_HEAD.NORM
        num_conv          = cfg.MODEL.ROI_MASK_HEAD.NUM_CONV
        input_channels    = input_shape.channels
        cls_agnostic_mask = cfg.MODEL.ROI_MASK_HEAD.CLS_AGNOSTIC_MASK
        # fmt: on

        self.conv_norm_relus = []

        for k in range(num_conv):
            conv = Conv2d(
                input_channels if k == 0 else conv_dims,
                conv_dims,
                kernel_size=3,
                stride=1,
                padding=1,
                bias=not self.norm,
                norm=get_norm(self.norm, conv_dims),
                activation=F.relu,
            )
            self.add_module("mask_fcn{}".format(k + 1), conv)
            self.conv_norm_relus.append(conv)

        self.deconv = ConvTranspose2d(
            conv_dims if num_conv > 0 else input_channels,
            conv_dims,
            kernel_size=2,
            stride=2,
            padding=0,
        )

        num_mask_classes = 1 if cls_agnostic_mask else num_classes
        self.predictor = Conv2d(conv_dims, num_mask_classes, kernel_size=1, stride=1, padding=0)

        for layer in self.conv_norm_relus + [self.deconv]:
            weight_init.c2_msra_fill(layer)
        # use normal distribution initialization for mask prediction layer
        nn.init.normal_(self.predictor.weight, std=0.001)
        if self.predictor.bias is not None:
            nn.init.constant_(self.predictor.bias, 0)

    def forward(self, x):
        for layer in self.conv_norm_relus:
            x = layer(x)
        x = F.relu(self.deconv(x))
        return self.predictor(x)
```

#### cvpods/modeling/roi_heads/fast_rcnn.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import logging

import numpy as np

import torch
from torch import nn
from torch.nn import functional as F

from cvpods.layers import cat, generalized_batched_nms
from cvpods.modeling.losses import smooth_l1_loss
from cvpods.structures import Boxes, Instances
from cvpods.utils import get_event_storage

logger = logging.getLogger(__name__)

"""
Shape shorthand in this module:

    N: number of images in the minibatch
    R: number of ROIs, combined over all images, in the minibatch
    Ri: number of ROIs in image i
    K: number of foreground classes. E.g.,there are 80 foreground classes in COCO.

Naming convention:

    deltas: refers to the 4-d (dx, dy, dw, dh) deltas that parameterize the box2box
    transform (see :class:`box_regression.Box2BoxTransform`).

    pred_class_logits: predicted class scores in [-inf, +inf]; use
        softmax(pred_class_logits) to estimate P(class).

    gt_classes: ground-truth classification labels in [0, K], where [0, K) represent
        foreground object classes and K represents the background class.

    pred_proposal_deltas: predicted box2box transform deltas for transforming proposals
        to detection box predictions.

    gt_proposal_deltas: ground-truth box2box transform deltas
"""


def fast_rcnn_inference(boxes, scores, image_shapes, score_thresh, nms_thresh, nms_type,
                        topk_per_image):
    """
    Call `fast_rcnn_inference_single_image` for all images.

    Args:
        boxes (list[Tensor]): A list of Tensors of predicted class-specific or class-agnostic
            boxes for each image. Element i has shape (Ri, K * 4) if doing
            class-specific regression, or (Ri, 4) if doing class-agnostic
            regression, where Ri is the number of predicted objects for image i.
            This is compatible with the output of :meth:`FastRCNNOutputs.predict_boxes`.
        scores (list[Tensor]): A list of Tensors of predicted class scores for each image.
            Element i has shape (Ri, K + 1), where Ri is the number of predicted objects
            for image i. Compatible with the output of :meth:`FastRCNNOutputs.predict_probs`.
        image_shapes (list[tuple]): A list of (width, height) tuples for each image in the batch.
        score_thresh (float): Only return detections with a confidence score exceeding this
            threshold.
        nms_thresh (float):  The threshold to use for box non-maximum suppression. Value in [0, 1].
        topk_per_image (int): The number of top scoring detections to return. Set < 0 to return
            all detections.

    Returns:
        instances: (list[Instances]): A list of N instances, one for each image in the batch,
            that stores the topk most confidence detections.
        kept_indices: (list[Tensor]): A list of 1D tensor of length of N, each element indicates
            the corresponding boxes/scores index in [0, Ri) from the input, for image i.
    """
    result_per_image = [
        fast_rcnn_inference_single_image(
            boxes_per_image, scores_per_image, image_shape, score_thresh, nms_thresh, nms_type,
            topk_per_image
        )
        for scores_per_image, boxes_per_image, image_shape in zip(scores, boxes, image_shapes)
    ]
    return tuple(list(x) for x in zip(*result_per_image))


def fast_rcnn_inference_single_image(
    boxes, scores, image_shape, score_thresh, nms_thresh, nms_type, topk_per_image
):
    """
    Single-image inference. Return bounding-box detection results by thresholding
    on scores and applying non-maximum suppression (NMS).

    Args:
        Same as `fast_rcnn_inference`, but with boxes, scores, and image shapes
        per image.

    Returns:
        Same as `fast_rcnn_inference`, but for only one image.
    """
    scores = scores[:, :-1]
    num_bbox_reg_classes = boxes.shape[1] // 4
    # Convert to Boxes to use the `clip` function ...
    boxes = Boxes(boxes.reshape(-1, 4))
    boxes.clip(image_shape)
    boxes = boxes.tensor.view(-1, num_bbox_reg_classes, 4)  # R x C x 4

    # Filter results based on detection scores
    filter_mask = scores > score_thresh  # R x K
    # R' x 2. First column contains indices of the R predictions;
    # Second column contains indices of classes.
    filter_inds = filter_mask.nonzero(as_tuple=False)
    if num_bbox_reg_classes == 1:
        boxes = boxes[filter_inds[:, 0], 0]
    else:
        boxes = boxes[filter_mask]
    scores = scores[filter_mask]

    # Apply per-class NMS
    keep = generalized_batched_nms(boxes, scores, filter_inds[:, 1],
                                   nms_thresh, nms_type=nms_type)

    if topk_per_image >= 0:
        keep = keep[:topk_per_image]
    boxes, scores, filter_inds = boxes[keep], scores[keep], filter_inds[keep]

    result = Instances(image_shape)
    result.pred_boxes = Boxes(boxes)
    result.scores = scores
    result.pred_classes = filter_inds[:, 1]
    return result, filter_inds[:, 0]


class FastRCNNOutputs(object):
    """
    A class that stores information about outputs of a Fast R-CNN head.
    """

    def __init__(
        self, box2box_transform, pred_class_logits, pred_proposal_deltas, proposals, smooth_l1_beta
    ):
        """
        Args:
            box2box_transform (Box2BoxTransform/Box2BoxTransformRotated):
                box2box transform instance for proposal-to-detection transformations.
            pred_class_logits (Tensor): A tensor of shape (R, K + 1) storing the predicted class
                logits for all R predicted object instances.
                Each row corresponds to a predicted object instance.
            pred_proposal_deltas (Tensor): A tensor of shape (R, K * B) or (R, B) for
                class-specific or class-agnostic regression. It stores the predicted deltas that
                transform proposals into final box detections.
                B is the box dimension (4 or 5).
                When B is 4, each row is [dx, dy, dw, dh (, ....)].
                When B is 5, each row is [dx, dy, dw, dh, da (, ....)].
            proposals (list[Instances]): A list of N Instances, where Instances i stores the
                proposals for image i, in the field "proposal_boxes".
                When training, each Instances must have ground-truth labels
                stored in the field "gt_classes" and "gt_boxes".
            smooth_l1_beta (float): The transition point between L1 and L2 loss in
                the smooth L1 loss function. When set to 0, the loss becomes L1. When
                set to +inf, the loss becomes constant 0.
        """
        self.box2box_transform = box2box_transform
        self.num_preds_per_image = [len(p) for p in proposals]
        self.pred_class_logits = pred_class_logits
        self.pred_proposal_deltas = pred_proposal_deltas
        self.smooth_l1_beta = smooth_l1_beta

        if len(proposals):
            box_type = type(proposals[0].proposal_boxes)
            # cat(..., dim=0) concatenates over all images in the batch
            self.proposals = box_type.cat([p.proposal_boxes for p in proposals])
            assert (
                not self.proposals.tensor.requires_grad
            ), "Proposals should not require gradients!"
            self.image_shapes = [x.image_size for x in proposals]

            # The following fields should exist only when training.
            if proposals[0].has("gt_boxes"):
                self.gt_boxes = box_type.cat([p.gt_boxes for p in proposals])
                assert proposals[0].has("gt_classes")
                self.gt_classes = cat([p.gt_classes for p in proposals], dim=0)
        else:
            self.proposals = Boxes(torch.zeros(0, 4, device=self.pred_proposal_deltas.device))
        self._no_instances = len(proposals) == 0  # no instances found

    def _log_accuracy(self):
        """
        Log the accuracy metrics to EventStorage.
        """
        num_instances = self.gt_classes.numel()
        pred_classes = self.pred_class_logits.argmax(dim=1)
        bg_class_ind = self.pred_class_logits.shape[1] - 1

        fg_inds = (self.gt_classes >= 0) & (self.gt_classes < bg_class_ind)
        num_fg = fg_inds.nonzero(as_tuple=False).numel()
        fg_gt_classes = self.gt_classes[fg_inds]
        fg_pred_classes = pred_classes[fg_inds]

        num_false_negative = (fg_pred_classes == bg_class_ind).nonzero(as_tuple=False).numel()
        num_accurate = (pred_classes == self.gt_classes).nonzero(as_tuple=False).numel()
        fg_num_accurate = (fg_pred_classes == fg_gt_classes).nonzero(as_tuple=False).numel()

        storage = get_event_storage()
        storage.put_scalar("fast_rcnn/cls_accuracy", num_accurate / num_instances)
        if num_fg > 0:
            storage.put_scalar("fast_rcnn/fg_cls_accuracy", fg_num_accurate / num_fg)
            storage.put_scalar("fast_rcnn/false_negative", num_false_negative / num_fg)

    def softmax_cross_entropy_loss(self):
        """
        Compute the softmax cross entropy loss for box classification.

        Returns:
            scalar Tensor
        """
        if self._no_instances:
            return 0.0 * self.pred_class_logits.sum()
        else:
            self._log_accuracy()
            return F.cross_entropy(self.pred_class_logits, self.gt_classes, reduction="mean")

    def smooth_l1_loss(self):
        """
        Compute the smooth L1 loss for box regression.

        Returns:
            scalar Tensor
        """
        if self._no_instances:
            return 0.0 * self.pred_proposal_deltas.sum()

        gt_proposal_deltas = self.box2box_transform.get_deltas(
            self.proposals.tensor, self.gt_boxes.tensor
        )
        box_dim = gt_proposal_deltas.size(1)  # 4 or 5
        cls_agnostic_bbox_reg = self.pred_proposal_deltas.size(1) == box_dim
        device = self.pred_proposal_deltas.device

        bg_class_ind = self.pred_class_logits.shape[1] - 1

        # Box delta loss is only computed between the prediction for the gt class k
        # (if 0 <= k < bg_class_ind) and the target; there is no loss defined on predictions
        # for non-gt classes and background.
        # Empty fg_inds produces a valid loss of zero as long as the size_average
        # arg to smooth_l1_loss is False (otherwise it uses torch.mean internally
        # and would produce a nan loss).
        fg_inds = torch.nonzero((self.gt_classes >= 0) & (self.gt_classes < bg_class_ind),
                                as_tuple=False).squeeze(1)
        if cls_agnostic_bbox_reg:
            # pred_proposal_deltas only corresponds to foreground class for agnostic
            gt_class_cols = torch.arange(box_dim, device=device)
        else:
            fg_gt_classes = self.gt_classes[fg_inds]
            # pred_proposal_deltas for class k are located in columns [b * k : b * k + b],
            # where b is the dimension of box representation (4 or 5)
            # Note that compared to Detectron1,
            # we do not perform bounding box regression for background classes.
            gt_class_cols = box_dim * fg_gt_classes[:, None] + torch.arange(box_dim, device=device)

        loss_box_reg = smooth_l1_loss(
            self.pred_proposal_deltas[fg_inds[:, None], gt_class_cols],
            gt_proposal_deltas[fg_inds],
            self.smooth_l1_beta,
            reduction="sum",
        )
        # The loss is normalized using the total number of regions (R), not the number
        # of foreground regions even though the box regression loss is only defined on
        # foreground regions. Why? Because doing so gives equal training influence to
        # each foreground example. To see how, consider two different minibatches:
        #  (1) Contains a single foreground region
        #  (2) Contains 100 foreground regions
        # If we normalize by the number of foreground regions, the single example in
        # minibatch (1) will be given 100 times as much influence as each foreground
        # example in minibatch (2). Normalizing by the total number of regions, R,
        # means that the single example in minibatch (1) and each of the 100 examples
        # in minibatch (2) are given equal influence.
        loss_box_reg = loss_box_reg / self.gt_classes.numel()
        return loss_box_reg

    def losses(self):
        """
        Compute the default losses for box head in Fast(er) R-CNN,
        with softmax cross entropy loss and smooth L1 loss.

        Returns:
            A dict of losses (scalar tensors) containing keys "loss_cls" and "loss_box_reg".
        """
        return {
            "loss_cls": self.softmax_cross_entropy_loss(),
            "loss_box_reg": self.smooth_l1_loss(),
        }

    def _predict_boxes(self):
        """
        Returns:
            Tensor: A Tensors of predicted class-specific or class-agnostic boxes
                for all images in a batch. Element i has shape (Ri, K * B) or (Ri, B), where Ri is
                the number of predicted objects for image i and B is the box dimension (4 or 5)
        """
        num_pred = len(self.proposals)
        B = self.proposals.tensor.shape[1]
        K = self.pred_proposal_deltas.shape[1] // B
        boxes = self.box2box_transform.apply_deltas(
            self.pred_proposal_deltas.view(num_pred * K, B),
            self.proposals.tensor.unsqueeze(1).expand(num_pred, K, B).reshape(-1, B),
        )
        return boxes.view(num_pred, K * B)

    def predict_boxes(self):
        """
        Returns:
            list[Tensor]: A list of Tensors of predicted class-specific or class-agnostic boxes
                for each image. Element i has shape (Ri, K * B) or (Ri, B), where Ri is
                the number of predicted objects for image i and B is the box dimension (4 or 5)
        """
        return self._predict_boxes().split(self.num_preds_per_image, dim=0)

    def predict_boxes_for_gt_classes(self):
        """
        Returns:
            list[Tensor]: A list of Tensors of predicted boxes for GT classes in case of
                class-specific box head. Element i of the list has shape (Ri, B), where Ri is
                the number of predicted objects for image i and B is the box dimension (4 or 5)
        """
        predicted_boxes = self._predict_boxes()
        B = self.proposals.tensor.shape[1]
        # If the box head is class-agnostic, then the method is equivalent to `predicted_boxes`.
        if predicted_boxes.shape[1] > B:
            num_pred = len(self.proposals)
            num_classes = predicted_boxes.shape[1] // B
            # Some proposals are ignored or have a background class. Their gt_classes
            # cannot be used as index.
            gt_classes = torch.clamp(self.gt_classes, 0, num_classes - 1)
            predicted_boxes = predicted_boxes.view(num_pred, num_classes, B)[
                torch.arange(num_pred, dtype=torch.long, device=predicted_boxes.device), gt_classes
            ]
        return predicted_boxes.split(self.num_preds_per_image, dim=0)

    def predict_probs(self):
        """
        Returns:
            list[Tensor]: A list of Tensors of predicted class probabilities for each image.
                Element i has shape (Ri, K + 1), where Ri is the number of predicted objects
                for image i.
        """
        probs = F.softmax(self.pred_class_logits, dim=-1)
        return probs.split(self.num_preds_per_image, dim=0)

    def inference(self, score_thresh, nms_thresh, nms_type, topk_per_image):
        """
        Args:
            score_thresh (float): same as fast_rcnn_inference.
            nms_thresh (float): same as fast_rcnn_inference.
            topk_per_image (int): same as fast_rcnn_inference.
        Returns:
            list[Instances]: same as fast_rcnn_inference.
            list[Tensor]: same as fast_rcnn_inference.
        """
        boxes = self.predict_boxes()
        scores = self.predict_probs()
        image_shapes = self.image_shapes

        return fast_rcnn_inference(
            boxes, scores, image_shapes, score_thresh, nms_thresh, nms_type, topk_per_image
        )


class FastRCNNOutputLayers(nn.Module):
    """
    Two linear layers for predicting Fast R-CNN outputs:
      (1) proposal-to-detection box regression deltas
      (2) classification scores
    """

    def __init__(self, input_size, num_classes, cls_agnostic_bbox_reg, box_dim=4):
        """
        Args:
            input_size (int): channels, or (channels, height, width)
            num_classes (int): number of foreground classes
            cls_agnostic_bbox_reg (bool): whether to use class agnostic for bbox regression
            box_dim (int): the dimension of bounding boxes.
                Example box dimensions: 4 for regular XYXY boxes and 5 for rotated XYWHA boxes
        """
        super(FastRCNNOutputLayers, self).__init__()

        if not isinstance(input_size, int):
            input_size = np.prod(input_size)

        # The prediction layer for num_classes foreground classes and one background class
        # (hence + 1)
        self.cls_score = nn.Linear(input_size, num_classes + 1)
        num_bbox_reg_classes = 1 if cls_agnostic_bbox_reg else num_classes
        self.bbox_pred = nn.Linear(input_size, num_bbox_reg_classes * box_dim)

        nn.init.normal_(self.cls_score.weight, std=0.01)
        nn.init.normal_(self.bbox_pred.weight, std=0.001)
        for layer in [self.cls_score, self.bbox_pred]:
            nn.init.constant_(layer.bias, 0)

    def forward(self, x):
        if x.dim() > 2:
            x = torch.flatten(x, start_dim=1)
        scores = self.cls_score(x)
        proposal_deltas = self.bbox_pred(x)
        return scores, proposal_deltas
```

#### cvpods/modeling/roi_heads/box_head.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import numpy as np

import torch
from torch import nn
from torch.nn import functional as F

from cvpods.layers import Conv2d, ShapeSpec, get_norm
from cvpods.modeling.nn_utils import weight_init


"""
Registry for box heads, which make box predictions from per-region features.

The registered object will be called with `obj(cfg, input_shape)`.
"""


class FastRCNNConvFCHead(nn.Module):
    """
    A head with several 3x3 conv layers (each followed by norm & relu) and
    several fc layers (each followed by relu).
    """

    def __init__(self, cfg, input_shape: ShapeSpec):
        """
        The following attributes are parsed from config:
            num_conv, num_fc: the number of conv/fc layers
            conv_dim/fc_dim: the dimension of the conv/fc layers
            norm: normalization for the conv layers
        """
        super().__init__()

        # fmt: off
        num_conv   = cfg.MODEL.ROI_BOX_HEAD.NUM_CONV
        conv_dim   = cfg.MODEL.ROI_BOX_HEAD.CONV_DIM
        num_fc     = cfg.MODEL.ROI_BOX_HEAD.NUM_FC
        fc_dim     = cfg.MODEL.ROI_BOX_HEAD.FC_DIM
        norm       = cfg.MODEL.ROI_BOX_HEAD.NORM
        # fmt: on
        assert num_conv + num_fc > 0

        self._output_size = (input_shape.channels, input_shape.height, input_shape.width)

        self.conv_norm_relus = []
        for k in range(num_conv):
            conv = Conv2d(
                self._output_size[0],
                conv_dim,
                kernel_size=3,
                padding=1,
                bias=not norm,
                norm=get_norm(norm, conv_dim),
                activation=F.relu,
            )
            self.add_module("conv{}".format(k + 1), conv)
            self.conv_norm_relus.append(conv)
            self._output_size = (conv_dim, self._output_size[1], self._output_size[2])

        self.fcs = []
        for k in range(num_fc):
            fc = nn.Linear(np.prod(self._output_size), fc_dim)
            self.add_module("fc{}".format(k + 1), fc)
            self.fcs.append(fc)
            self._output_size = fc_dim

        for layer in self.conv_norm_relus:
            weight_init.c2_msra_fill(layer)
        for layer in self.fcs:
            weight_init.c2_xavier_fill(layer)

    def forward(self, x):
        for layer in self.conv_norm_relus:
            x = layer(x)
        if len(self.fcs):
            if x.dim() > 2:
                x = torch.flatten(x, start_dim=1)
            for layer in self.fcs:
                x = F.relu(layer(x))
        return x

    @property
    def output_size(self):
        return self._output_size
```

#### cvpods/modeling/roi_heads/keypoint_head.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import torch
from torch import nn
from torch.nn import functional as F

from cvpods.layers import Conv2d, ConvTranspose2d, ShapeSpec, cat, interpolate
from cvpods.structures import heatmaps_to_keypoints
from cvpods.utils import get_event_storage

_TOTAL_SKIPPED = 0


def keypoint_rcnn_loss(pred_keypoint_logits, instances, normalizer):
    """
    Arguments:
        pred_keypoint_logits (Tensor): A tensor of shape (N, K, S, S) where N is the total number
            of instances in the batch, K is the number of keypoints, and S is the side length
            of the keypoint heatmap. The values are spatial logits.
        instances (list[Instances]): A list of M Instances, where M is the batch size.
            These instances are predictions from the model
            that are in 1:1 correspondence with pred_keypoint_logits.
            Each Instances should contain a `gt_keypoints` field containing a `structures.Keypoint`
            instance.
        normalizer (float): Normalize the loss by this amount.
            If not specified, we normalize by the number of visible keypoints in the minibatch.

    Returns a scalar tensor containing the loss.
    """
    heatmaps = []
    valid = []

    keypoint_side_len = pred_keypoint_logits.shape[2]
    for instances_per_image in instances:
        if len(instances_per_image) == 0:
            continue
        keypoints = instances_per_image.gt_keypoints
        heatmaps_per_image, valid_per_image = keypoints.to_heatmap(
            instances_per_image.proposal_boxes.tensor, keypoint_side_len
        )
        heatmaps.append(heatmaps_per_image.view(-1))
        valid.append(valid_per_image.view(-1))

    if len(heatmaps):
        keypoint_targets = cat(heatmaps, dim=0)
        valid = cat(valid, dim=0).to(dtype=torch.uint8)
        valid = torch.nonzero(valid, as_tuple=False).squeeze(1)

    # torch.mean (in binary_cross_entropy_with_logits) doesn't
    # accept empty tensors, so handle it separately
    if len(heatmaps) == 0 or valid.numel() == 0:
        global _TOTAL_SKIPPED
        _TOTAL_SKIPPED += 1
        storage = get_event_storage()
        storage.put_scalar("kpts_num_skipped_batches", _TOTAL_SKIPPED, smoothing_hint=False)
        return pred_keypoint_logits.sum() * 0

    N, K, H, W = pred_keypoint_logits.shape
    pred_keypoint_logits = pred_keypoint_logits.view(N * K, H * W)

    keypoint_loss = F.cross_entropy(
        pred_keypoint_logits[valid], keypoint_targets[valid], reduction="sum"
    )

    # If a normalizer isn't specified, normalize by the number of visible keypoints in the minibatch
    if normalizer is None:
        normalizer = valid.numel()
    keypoint_loss /= normalizer

    return keypoint_loss


def keypoint_rcnn_inference(pred_keypoint_logits, pred_instances):
    """
    Post process each predicted keypoint heatmap in `pred_keypoint_logits` into (x, y, score)
        and add it to the `pred_instances` as a `pred_keypoints` field.

    Args:
        pred_keypoint_logits (Tensor): A tensor of shape (R, K, S, S) where R is the total number
           of instances in the batch, K is the number of keypoints, and S is the side length of
           the keypoint heatmap. The values are spatial logits.
        pred_instances (list[Instances]): A list of N Instances, where N is the number of images.

    Returns:
        None. Each element in pred_instances will contain an extra "pred_keypoints" field.
            The field is a tensor of shape (#instance, K, 3) where the last
            dimension corresponds to (x, y, score).
            The scores are larger than 0.
    """
    # flatten all bboxes from all images together (list[Boxes] -> Rx4 tensor)
    bboxes_flat = cat([b.pred_boxes.tensor for b in pred_instances], dim=0)

    keypoint_results = heatmaps_to_keypoints(pred_keypoint_logits.detach(), bboxes_flat.detach())
    num_instances_per_image = [len(i) for i in pred_instances]
    keypoint_results = keypoint_results[:, :, [0, 1, 3]].split(num_instances_per_image, dim=0)

    for keypoint_results_per_image, instances_per_image in zip(keypoint_results, pred_instances):
        # keypoint_results_per_image is (num instances)x(num keypoints)x(x, y, score)
        instances_per_image.pred_keypoints = keypoint_results_per_image


class KRCNNConvDeconvUpsampleHead(nn.Module):
    """
    A standard keypoint head containing a series of 3x3 convs, followed by
    a transpose convolution and bilinear interpolation for upsampling.
    """

    def __init__(self, cfg, input_shape: ShapeSpec):
        """
        The following attributes are parsed from config:
            conv_dims: an iterable of output channel counts for each conv in the head
                         e.g. (512, 512, 512) for three convs outputting 512 channels.
            num_keypoints: number of keypoint heatmaps to predicts, determines the number of
                           channels in the final output.
        """
        super(KRCNNConvDeconvUpsampleHead, self).__init__()

        # fmt: off
        # default up_scale to 2 (this can eventually be moved to config)
        up_scale      = 2
        conv_dims     = cfg.MODEL.ROI_KEYPOINT_HEAD.CONV_DIMS
        num_keypoints = cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS
        in_channels   = input_shape.channels
        # fmt: on

        self.blocks = []
        for idx, layer_channels in enumerate(conv_dims, 1):
            module = Conv2d(in_channels, layer_channels, 3, stride=1, padding=1)
            self.add_module("conv_fcn{}".format(idx), module)
            self.blocks.append(module)
            in_channels = layer_channels

        deconv_kernel = 4
        self.score_lowres = ConvTranspose2d(
            in_channels, num_keypoints, deconv_kernel, stride=2, padding=deconv_kernel // 2 - 1
        )
        self.up_scale = up_scale

        for name, param in self.named_parameters():
            if "bias" in name:
                nn.init.constant_(param, 0)
            elif "weight" in name:
                # Caffe2 implementation uses MSRAFill, which in fact
                # corresponds to kaiming_normal_ in PyTorch
                nn.init.kaiming_normal_(param, mode="fan_out", nonlinearity="relu")

    def forward(self, x):
        for layer in self.blocks:
            x = F.relu(layer(x))
        x = self.score_lowres(x)
        x = interpolate(x, scale_factor=self.up_scale, mode="bilinear", align_corners=False)
        return x
```

#### cvpods/modeling/roi_heads/__init__.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from .roi_heads import Res5ROIHeads, ROIHeads, StandardROIHeads, select_foreground_proposals
from .rotated_fast_rcnn import RROIHeads

from . import cascade_rcnn  # isort:skip
```

#### cvpods/modeling/roi_heads/rotated_fast_rcnn.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import logging
from typing import Dict

import numpy as np

import torch

from cvpods.layers import ShapeSpec, batched_nms_rotated
from cvpods.structures import Instances, RotatedBoxes, pairwise_iou_rotated
from cvpods.utils import get_event_storage

from ..box_regression import Box2BoxTransformRotated
from ..poolers import ROIPooler
from ..proposal_generator.proposal_utils import add_ground_truth_to_proposals
from .fast_rcnn import FastRCNNOutputLayers, FastRCNNOutputs
from .roi_heads import StandardROIHeads

logger = logging.getLogger(__name__)

"""
Shape shorthand in this module:

    N: number of images in the minibatch
    R: number of ROIs, combined over all images, in the minibatch
    Ri: number of ROIs in image i
    K: number of foreground classes. E.g.,there are 80 foreground classes in COCO.

Naming convention:

    deltas: refers to the 5-d (dx, dy, dw, dh, da) deltas that parameterize the box2box
    transform (see :class:`box_regression.Box2BoxTransformRotated`).

    pred_class_logits: predicted class scores in [-inf, +inf]; use
        softmax(pred_class_logits) to estimate P(class).

    gt_classes: ground-truth classification labels in [0, K], where [0, K) represent
        foreground object classes and K represents the background class.

    pred_proposal_deltas: predicted rotated box2box transform deltas for transforming proposals
        to detection box predictions.

    gt_proposal_deltas: ground-truth rotated box2box transform deltas
"""


def fast_rcnn_inference_rotated(
    boxes, scores, image_shapes, score_thresh, nms_thresh, topk_per_image
):
    """
    Call `fast_rcnn_inference_single_image_rotated` for all images.

    Args:
        boxes (list[Tensor]): A list of Tensors of predicted class-specific or class-agnostic
            boxes for each image. Element i has shape (Ri, K * 5) if doing
            class-specific regression, or (Ri, 5) if doing class-agnostic
            regression, where Ri is the number of predicted objects for image i.
            This is compatible with the output of :meth:`FastRCNNOutputs.predict_boxes`.
        scores (list[Tensor]): A list of Tensors of predicted class scores for each image.
            Element i has shape (Ri, K + 1), where Ri is the number of predicted objects
            for image i. Compatible with the output of :meth:`FastRCNNOutputs.predict_probs`.
        image_shapes (list[tuple]): A list of (width, height) tuples for each image in the batch.
        score_thresh (float): Only return detections with a confidence score exceeding this
            threshold.
        nms_thresh (float):  The threshold to use for box non-maximum suppression. Value in [0, 1].
        topk_per_image (int): The number of top scoring detections to return. Set < 0 to return
            all detections.

    Returns:
        instances: (list[Instances]): A list of N instances, one for each image in the batch,
            that stores the topk most confidence detections.
        kept_indices: (list[Tensor]): A list of 1D tensor of length of N, each element indicates
            the corresponding boxes/scores index in [0, Ri) from the input, for image i.
    """
    result_per_image = [
        fast_rcnn_inference_single_image_rotated(
            boxes_per_image, scores_per_image, image_shape, score_thresh, nms_thresh, topk_per_image
        )
        for scores_per_image, boxes_per_image, image_shape in zip(scores, boxes, image_shapes)
    ]
    return tuple(list(x) for x in zip(*result_per_image))


def fast_rcnn_inference_single_image_rotated(
    boxes, scores, image_shape, score_thresh, nms_thresh, topk_per_image
):
    """
    Single-image inference. Return rotated bounding-box detection results by thresholding
    on scores and applying rotated non-maximum suppression (Rotated NMS).

    Args:
        Same as `fast_rcnn_inference_rotated`, but with rotated boxes, scores, and image shapes
        per image.

    Returns:
        Same as `fast_rcnn_inference_rotated`, but for only one image.
    """
    B = 5  # box dimension
    scores = scores[:, :-1]
    num_bbox_reg_classes = boxes.shape[1] // B
    # Convert to Boxes to use the `clip` function ...
    boxes = RotatedBoxes(boxes.reshape(-1, B))
    boxes.clip(image_shape)
    boxes = boxes.tensor.view(-1, num_bbox_reg_classes, B)  # R x C x B
    # Filter results based on detection scores
    filter_mask = scores > score_thresh  # R x K
    # R' x 2. First column contains indices of the R predictions;
    # Second column contains indices of classes.
    filter_inds = filter_mask.nonzero(as_tuple=False)
    if num_bbox_reg_classes == 1:
        boxes = boxes[filter_inds[:, 0], 0]
    else:
        boxes = boxes[filter_mask]
    scores = scores[filter_mask]

    # Apply per-class Rotated NMS
    keep = batched_nms_rotated(boxes, scores, filter_inds[:, 1], nms_thresh)
    if topk_per_image >= 0:
        keep = keep[:topk_per_image]
    boxes, scores, filter_inds = boxes[keep], scores[keep], filter_inds[keep]

    result = Instances(image_shape)
    result.pred_boxes = RotatedBoxes(boxes)
    result.scores = scores
    result.pred_classes = filter_inds[:, 1]

    return result, filter_inds[:, 0]


class RotatedFastRCNNOutputs(FastRCNNOutputs):
    """
    A class that stores information about outputs of a Fast R-CNN head with RotatedBoxes.
    """

    def inference(self, score_thresh, nms_thresh, topk_per_image):
        """
        Args:
            score_thresh (float): same as `fast_rcnn_inference_rotated`.
            nms_thresh (float): same as `fast_rcnn_inference_rotated`.
            topk_per_image (int): same as `fast_rcnn_inference_rotated`.
        Returns:
            list[Instances]: same as `fast_rcnn_inference_rotated`.
            list[Tensor]: same as `fast_rcnn_inference_rotated`.
        """
        boxes = self.predict_boxes()
        scores = self.predict_probs()
        image_shapes = self.image_shapes

        return fast_rcnn_inference_rotated(
            boxes, scores, image_shapes, score_thresh, nms_thresh, topk_per_image
        )


class RROIHeads(StandardROIHeads):
    """
    This class is used by Rotated RPN (RRPN).
    For now, it just supports box head but not mask or keypoints.
    """

    def __init__(self, cfg, input_shape: Dict[str, ShapeSpec]):
        super().__init__(cfg, input_shape)
        self.box2box_transform = Box2BoxTransformRotated(
            weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS
        )
        assert (
            not self.mask_on and not self.keypoint_on
        ), "Mask/Keypoints not supported in Rotated ROIHeads."

    def _init_box_head(self, cfg):
        # fmt: off
        pooler_resolution = cfg.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION
        pooler_scales     = tuple(1.0 / self.feature_strides[k] for k in self.in_features)
        sampling_ratio    = cfg.MODEL.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO
        pooler_type       = cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE
        # fmt: on

        # If StandardROIHeads is applied on multiple feature maps (as in FPN),
        # then we share the same predictors and therefore the channel counts must be the same
        in_channels = [self.feature_channels[f] for f in self.in_features]
        # Check all channel counts are equal
        assert len(set(in_channels)) == 1, in_channels
        in_channels = in_channels[0]

        assert pooler_type in ["ROIAlignRotated"]

        self.box_pooler = ROIPooler(
            output_size=pooler_resolution,
            scales=pooler_scales,
            sampling_ratio=sampling_ratio,
            pooler_type=pooler_type,
        )
        self.box_head = cfg.build_box_head(
            cfg, ShapeSpec(channels=in_channels, height=pooler_resolution, width=pooler_resolution)
        )

        self.box_predictor = FastRCNNOutputLayers(
            input_size=self.box_head.output_size,
            num_classes=self.num_classes,
            cls_agnostic_bbox_reg=self.cls_agnostic_bbox_reg,
            box_dim=5,
        )

    @torch.no_grad()
    def label_and_sample_proposals(self, proposals, targets):
        """
        Prepare some proposals to be used to train the RROI heads.
        It performs box matching between `proposals` and `targets`, and assigns
        training labels to the proposals.
        It returns `self.batch_size_per_image` random samples from proposals and groundtruth boxes,
        with a fraction of positives that is no larger than `self.positive_sample_fraction.

        Args:
            See :meth:`StandardROIHeads.forward`

        Returns:
            list[Instances]: length `N` list of `Instances`s containing the proposals
                sampled for training. Each `Instances` has the following fields:
                - proposal_boxes: the rotated proposal boxes
                - gt_boxes: the ground-truth rotated boxes that the proposal is assigned to
                  (this is only meaningful if the proposal has a label > 0; if label = 0
                   then the ground-truth box is random)
                - gt_classes: the ground-truth classification lable for each proposal
        """
        gt_boxes = [x.gt_boxes for x in targets]
        if self.proposal_append_gt:
            proposals = add_ground_truth_to_proposals(gt_boxes, proposals)

        proposals_with_gt = []

        num_fg_samples = []
        num_bg_samples = []
        for proposals_per_image, targets_per_image in zip(proposals, targets):
            has_gt = len(targets_per_image) > 0
            match_quality_matrix = pairwise_iou_rotated(
                targets_per_image.gt_boxes, proposals_per_image.proposal_boxes
            )
            matched_idxs, matched_labels = self.proposal_matcher(match_quality_matrix)
            sampled_idxs, gt_classes = self._sample_proposals(
                matched_idxs, matched_labels, targets_per_image.gt_classes
            )

            proposals_per_image = proposals_per_image[sampled_idxs]
            proposals_per_image.gt_classes = gt_classes

            if has_gt:
                sampled_targets = matched_idxs[sampled_idxs]
                proposals_per_image.gt_boxes = targets_per_image.gt_boxes[sampled_targets]
            else:
                gt_boxes = RotatedBoxes(
                    targets_per_image.gt_boxes.tensor.new_zeros((len(sampled_idxs), 5))
                )
                proposals_per_image.gt_boxes = gt_boxes

            num_bg_samples.append((gt_classes == self.num_classes).sum().item())
            num_fg_samples.append(gt_classes.numel() - num_bg_samples[-1])
            proposals_with_gt.append(proposals_per_image)

        # Log the number of fg/bg samples that are selected for training ROI heads
        storage = get_event_storage()
        storage.put_scalar("roi_head/num_fg_samples", np.mean(num_fg_samples))
        storage.put_scalar("roi_head/num_bg_samples", np.mean(num_bg_samples))

        return proposals_with_gt

    def _forward_box(self, features, proposals):
        """
        Forward logic of the box prediction branch.

        Args:
            features (list[Tensor]): #level input features for box prediction
            proposals (list[Instances]): the per-image object proposals with
                their matching ground truth.
                Each has fields "proposal_boxes", and "objectness_logits",
                "gt_classes", "gt_boxes".

        Returns:
            In training, a dict of losses.
            In inference, a list of `Instances`, the predicted instances.
        """
        box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
        box_features = self.box_head(box_features)
        pred_class_logits, pred_proposal_deltas = self.box_predictor(box_features)
        del box_features

        outputs = RotatedFastRCNNOutputs(
            self.box2box_transform,
            pred_class_logits,
            pred_proposal_deltas,
            proposals,
            self.smooth_l1_beta,
        )
        if self.training:
            return outputs.losses()
        else:
            pred_instances, _ = outputs.inference(
                self.test_score_thresh, self.test_nms_thresh, self.test_detections_per_img
            )
            return pred_instances
```

#### cvpods/modeling/roi_heads/cascade_rcnn.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import torch
from torch import nn

from cvpods.layers import ShapeSpec
from cvpods.modeling.nn_utils.scale_grad import _ScaleGradient
from cvpods.structures import Boxes, Instances, pairwise_iou
from cvpods.utils import get_event_storage

from ..box_regression import Box2BoxTransform
from ..matcher import Matcher
from ..poolers import ROIPooler
from .fast_rcnn import FastRCNNOutputLayers, FastRCNNOutputs, fast_rcnn_inference
from .roi_heads import StandardROIHeads


class CascadeROIHeads(StandardROIHeads):
    def _init_box_head(self, cfg):
        # fmt: off
        pooler_resolution        = cfg.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION
        pooler_scales            = tuple(1.0 / self.feature_strides[k] for k in self.in_features)
        sampling_ratio           = cfg.MODEL.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO
        pooler_type              = cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE
        cascade_bbox_reg_weights = cfg.MODEL.ROI_BOX_CASCADE_HEAD.BBOX_REG_WEIGHTS
        cascade_ious             = cfg.MODEL.ROI_BOX_CASCADE_HEAD.IOUS
        self.num_cascade_stages  = len(cascade_ious)
        assert len(cascade_bbox_reg_weights) == self.num_cascade_stages
        assert cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG,  \
            "CascadeROIHeads only support class-agnostic regression now!"
        assert cascade_ious[0] == cfg.MODEL.ROI_HEADS.IOU_THRESHOLDS[0]
        # fmt: on

        in_channels = [self.feature_channels[f] for f in self.in_features]
        # Check all channel counts are equal
        assert len(set(in_channels)) == 1, in_channels
        in_channels = in_channels[0]

        self.box_pooler = ROIPooler(
            output_size=pooler_resolution,
            scales=pooler_scales,
            sampling_ratio=sampling_ratio,
            pooler_type=pooler_type,
        )
        pooled_shape = ShapeSpec(
            channels=in_channels, width=pooler_resolution, height=pooler_resolution
        )

        self.box_head = nn.ModuleList()
        self.box_predictor = nn.ModuleList()
        self.box2box_transform = []
        self.proposal_matchers = []
        for k in range(self.num_cascade_stages):
            box_head = cfg.build_box_head(cfg, pooled_shape)
            self.box_head.append(box_head)
            self.box_predictor.append(
                FastRCNNOutputLayers(
                    box_head.output_size, self.num_classes, cls_agnostic_bbox_reg=True
                )
            )
            self.box2box_transform.append(Box2BoxTransform(weights=cascade_bbox_reg_weights[k]))

            if k == 0:
                # The first matching is done by the matcher of ROIHeads (self.proposal_matcher).
                self.proposal_matchers.append(None)
            else:
                self.proposal_matchers.append(
                    Matcher([cascade_ious[k]], [0, 1], allow_low_quality_matches=False)
                )

    def forward(self, images, features, proposals, targets=None):
        del images
        if self.training:
            proposals = self.label_and_sample_proposals(proposals, targets)

        features_list = [features[f] for f in self.in_features]

        if self.training:
            # Need targets to box head
            losses = self._forward_box(features_list, proposals, targets)
            losses.update(self._forward_mask(features_list, proposals))
            losses.update(self._forward_keypoint(features_list, proposals))
            return proposals, losses
        else:
            pred_instances = self._forward_box(features_list, proposals)
            pred_instances = self.forward_with_given_boxes(features, pred_instances)
            return pred_instances, {}

    def _forward_box(self, features, proposals, targets=None):
        head_outputs = []
        image_sizes = [x.image_size for x in proposals]
        for k in range(self.num_cascade_stages):
            if k > 0:
                # The output boxes of the previous stage are the input proposals of the next stage
                proposals = self._create_proposals_from_boxes(
                    head_outputs[-1].predict_boxes(), image_sizes
                )
                if self.training:
                    proposals = self._match_and_label_boxes(proposals, k, targets)
            head_outputs.append(self._run_stage(features, proposals, k))

        if self.training:
            losses = {}
            storage = get_event_storage()
            for stage, output in enumerate(head_outputs):
                with storage.name_scope("stage{}".format(stage)):
                    stage_losses = output.losses()
                losses.update({k + "_stage{}".format(stage): v for k, v in stage_losses.items()})
            return losses
        else:
            # Each is a list[Tensor] of length #image. Each tensor is Ri x (K+1)
            scores_per_stage = [h.predict_probs() for h in head_outputs]

            # Average the scores across heads
            scores = [
                sum(list(scores_per_image)) * (1.0 / self.num_cascade_stages)
                for scores_per_image in zip(*scores_per_stage)
            ]
            # Use the boxes of the last head
            boxes = head_outputs[-1].predict_boxes()
            pred_instances, _ = fast_rcnn_inference(
                boxes,
                scores,
                image_sizes,
                self.test_score_thresh,
                self.test_nms_thresh,
                self.test_detections_per_img,
            )
            return pred_instances

    @torch.no_grad()
    def _match_and_label_boxes(self, proposals, stage, targets):
        """
        Match proposals with groundtruth using the matcher at the given stage.
        Label the proposals as foreground or background based on the match.

        Args:
            proposals (list[Instances]): One Instances for each image, with
                the field "proposal_boxes".
            stage (int): the current stage
            targets (list[Instances]): the ground truth instances

        Returns:
            list[Instances]: the same proposals, but with fields "gt_classes" and "gt_boxes"
        """
        num_fg_samples, num_bg_samples = [], []
        for proposals_per_image, targets_per_image in zip(proposals, targets):
            match_quality_matrix = pairwise_iou(
                targets_per_image.gt_boxes, proposals_per_image.proposal_boxes
            )
            # proposal_labels are 0 or 1
            matched_idxs, proposal_labels = self.proposal_matchers[stage](match_quality_matrix)
            if len(targets_per_image) > 0:
                gt_classes = targets_per_image.gt_classes[matched_idxs]
                # Label unmatched proposals (0 label from matcher) as background (label=num_classes)
                gt_classes[proposal_labels == 0] = self.num_classes
                gt_boxes = targets_per_image.gt_boxes[matched_idxs]
            else:
                gt_classes = torch.zeros_like(matched_idxs) + self.num_classes
                gt_boxes = Boxes(
                    targets_per_image.gt_boxes.tensor.new_zeros((len(proposals_per_image), 4))
                )
            proposals_per_image.gt_classes = gt_classes
            proposals_per_image.gt_boxes = gt_boxes

            num_fg_samples.append((proposal_labels == 1).sum().item())
            num_bg_samples.append(proposal_labels.numel() - num_fg_samples[-1])

        # Log the number of fg/bg samples in each stage
        storage = get_event_storage()
        storage.put_scalar(
            "stage{}/roi_head/num_fg_samples".format(stage),
            sum(num_fg_samples) / len(num_fg_samples),
        )
        storage.put_scalar(
            "stage{}/roi_head/num_bg_samples".format(stage),
            sum(num_bg_samples) / len(num_bg_samples),
        )
        return proposals

    def _run_stage(self, features, proposals, stage):
        """
        Args:
            features (list[Tensor]): #lvl input features to ROIHeads
            proposals (list[Instances]): #image Instances, with the field "proposal_boxes"
            stage (int): the current stage

        Returns:
            FastRCNNOutputs: the output of this stage
        """
        box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
        # The original implementation averages the losses among heads,
        # but scale up the parameter gradients of the heads.
        # This is equivalent to adding the losses among heads,
        # but scale down the gradients on features.
        box_features = _ScaleGradient.apply(box_features, 1.0 / self.num_cascade_stages)
        box_features = self.box_head[stage](box_features)
        pred_class_logits, pred_proposal_deltas = self.box_predictor[stage](box_features)
        del box_features

        outputs = FastRCNNOutputs(
            self.box2box_transform[stage],
            pred_class_logits,
            pred_proposal_deltas,
            proposals,
            self.smooth_l1_beta,
        )
        return outputs

    def _create_proposals_from_boxes(self, boxes, image_sizes):
        """
        Args:
            boxes (list[Tensor]): per-image predicted boxes, each of shape Ri x 4
            image_sizes (list[tuple]): list of image shapes in (h, w)

        Returns:
            list[Instances]: per-image proposals with the given boxes.
        """
        # Just like RPN, the proposals should not have gradients
        boxes = [Boxes(b.detach()) for b in boxes]
        proposals = []
        for boxes_per_image, image_size in zip(boxes, image_sizes):
            boxes_per_image.clip(image_size)
            if self.training:
                # do not filter empty boxes at inference time,
                # because the scores from each stage need to be aligned and added later
                boxes_per_image = boxes_per_image[boxes_per_image.nonempty()]
            prop = Instances(image_size)
            prop.proposal_boxes = boxes_per_image
            proposals.append(prop)
        return proposals
```

#### cvpods/modeling/roi_heads/roi_heads.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import logging
from typing import Dict, List, Optional, Tuple, Union

import numpy as np

import torch
from torch import nn

from cvpods.layers import ShapeSpec
from cvpods.structures import Boxes, ImageList, Instances, pairwise_iou
from cvpods.utils import get_event_storage

from ..backbone.resnet import BottleneckBlock, make_stage
from ..box_regression import Box2BoxTransform
from ..matcher import Matcher
from ..poolers import ROIPooler
from ..proposal_generator.proposal_utils import add_ground_truth_to_proposals
from ..sampling import subsample_labels
from .fast_rcnn import FastRCNNOutputLayers, FastRCNNOutputs
from .keypoint_head import keypoint_rcnn_inference, keypoint_rcnn_loss
from .mask_head import mask_rcnn_inference, mask_rcnn_loss

logger = logging.getLogger(__name__)


def select_foreground_proposals(proposals, bg_label):
    """
    Given a list of N Instances (for N images), each containing a `gt_classes` field,
    return a list of Instances that contain only instances with `gt_classes != -1 &&
    gt_classes != bg_label`.

    Args:
        proposals (list[Instances]): A list of N Instances, where N is the number of
            images in the batch.
        bg_label: label index of background class.

    Returns:
        list[Instances]: N Instances, each contains only the selected foreground instances.
        list[Tensor]: N boolean vector, correspond to the selection mask of
            each Instance object. True for selected instances.
    """
    assert isinstance(proposals, (list, tuple))
    assert isinstance(proposals[0], Instances)
    assert proposals[0].has("gt_classes")
    fg_proposals = []
    fg_selection_masks = []
    for proposals_per_image in proposals:
        gt_classes = proposals_per_image.gt_classes
        fg_selection_mask = (gt_classes != -1) & (gt_classes != bg_label)
        fg_idxs = fg_selection_mask.nonzero(as_tuple=False).squeeze(1)
        fg_proposals.append(proposals_per_image[fg_idxs])
        fg_selection_masks.append(fg_selection_mask)
    return fg_proposals, fg_selection_masks


def select_proposals_with_visible_keypoints(proposals: List[Instances]) -> List[Instances]:
    """
    Args:
        proposals (list[Instances]): a list of N Instances, where N is the
            number of images.

    Returns:
        proposals: only contains proposals with at least one visible keypoint.

    Note that this is still slightly different from Detectron.
    In Detectron, proposals for training keypoint head are re-sampled from
    all the proposals with IOU>threshold & >=1 visible keypoint.

    Here, the proposals are first sampled from all proposals with
    IOU>threshold, then proposals with no visible keypoint are filtered out.
    This strategy seems to make no difference on Detectron and is easier to implement.
    """
    ret = []
    all_num_fg = []
    for proposals_per_image in proposals:
        # If empty/unannotated image (hard negatives), skip filtering for train
        if len(proposals_per_image) == 0:
            ret.append(proposals_per_image)
            continue
        gt_keypoints = proposals_per_image.gt_keypoints.tensor
        # #fg x K x 3
        vis_mask = gt_keypoints[:, :, 2] >= 1
        xs, ys = gt_keypoints[:, :, 0], gt_keypoints[:, :, 1]
        proposal_boxes = proposals_per_image.proposal_boxes.tensor.unsqueeze(dim=1)  # #fg x 1 x 4
        kp_in_box = (
            (xs >= proposal_boxes[:, :, 0])
            & (xs <= proposal_boxes[:, :, 2])
            & (ys >= proposal_boxes[:, :, 1])
            & (ys <= proposal_boxes[:, :, 3])
        )
        selection = (kp_in_box & vis_mask).any(dim=1)
        selection_idxs = torch.nonzero(selection, as_tuple=False).squeeze(1)
        all_num_fg.append(selection_idxs.numel())
        ret.append(proposals_per_image[selection_idxs])

    storage = get_event_storage()
    storage.put_scalar("keypoint_head/num_fg_samples", np.mean(all_num_fg))
    return ret


class ROIHeads(torch.nn.Module):
    """
    ROIHeads perform all per-region computation in an R-CNN.

    It contains logic of cropping the regions, extract per-region features,
    and make per-region predictions.

    It can have many variants, implemented as subclasses of this class.
    """

    def __init__(self, cfg, input_shape: Dict[str, ShapeSpec]):
        super(ROIHeads, self).__init__()

        # fmt: off
        self.batch_size_per_image     = cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE
        self.positive_sample_fraction = cfg.MODEL.ROI_HEADS.POSITIVE_FRACTION
        self.test_score_thresh        = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST
        self.test_nms_thresh          = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST
        self.test_nms_type            = cfg.MODEL.NMS_TYPE
        self.test_detections_per_img  = cfg.TEST.DETECTIONS_PER_IMAGE
        self.in_features              = cfg.MODEL.ROI_HEADS.IN_FEATURES
        self.num_classes              = cfg.MODEL.ROI_HEADS.NUM_CLASSES
        self.proposal_append_gt       = cfg.MODEL.ROI_HEADS.PROPOSAL_APPEND_GT
        self.feature_strides          = {k: v.stride for k, v in input_shape.items()}
        self.feature_channels         = {k: v.channels for k, v in input_shape.items()}
        self.cls_agnostic_bbox_reg    = cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG
        self.smooth_l1_beta           = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA
        # fmt: on

        # Matcher to assign box proposals to gt boxes
        self.proposal_matcher = Matcher(
            cfg.MODEL.ROI_HEADS.IOU_THRESHOLDS,
            cfg.MODEL.ROI_HEADS.IOU_LABELS,
            allow_low_quality_matches=False,
        )

        # Box2BoxTransform for bounding box regression
        self.box2box_transform = Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)

    def _sample_proposals(
        self, matched_idxs: torch.Tensor, matched_labels: torch.Tensor, gt_classes: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Based on the matching between N proposals and M groundtruth,
        sample the proposals and set their classification labels.

        Args:
            matched_idxs (Tensor): a vector of length N, each is the best-matched
                gt index in [0, M) for each proposal.
            matched_labels (Tensor): a vector of length N, the matcher's label
                (one of cfg.MODEL.ROI_HEADS.IOU_LABELS) for each proposal.
            gt_classes (Tensor): a vector of length M.

        Returns:
            Tensor: a vector of indices of sampled proposals. Each is in [0, N).
            Tensor: a vector of the same length, the classification label for
                each sampled proposal. Each sample is labeled as either a category in
                [0, num_classes) or the background (num_classes).
        """
        has_gt = gt_classes.numel() > 0
        # Get the corresponding GT for each proposal
        if has_gt:
            gt_classes = gt_classes[matched_idxs]
            # Label unmatched proposals (0 label from matcher) as background (label=num_classes)
            gt_classes[matched_labels == 0] = self.num_classes
            # Label ignore proposals (-1 label)
            gt_classes[matched_labels == -1] = -1
        else:
            gt_classes = torch.zeros_like(matched_idxs) + self.num_classes

        sampled_fg_idxs, sampled_bg_idxs = subsample_labels(
            gt_classes, self.batch_size_per_image, self.positive_sample_fraction, self.num_classes
        )

        sampled_idxs = torch.cat([sampled_fg_idxs, sampled_bg_idxs], dim=0)
        return sampled_idxs, gt_classes[sampled_idxs]

    @torch.no_grad()
    def label_and_sample_proposals(
        self, proposals: List[Instances], targets: List[Instances]
    ) -> List[Instances]:
        """
        Prepare some proposals to be used to train the ROI heads.
        It performs box matching between `proposals` and `targets`, and assigns
        training labels to the proposals.
        It returns ``self.batch_size_per_image`` random samples from proposals and groundtruth
        boxes, with a fraction of positives that is no larger than
        ``self.positive_sample_fraction``.

        Args:
            See :meth:`ROIHeads.forward`

        Returns:
            list[Instances]:
                length `N` list of `Instances`s containing the proposals
                sampled for training. Each `Instances` has the following fields:

                - proposal_boxes: the proposal boxes
                - gt_boxes: the ground-truth box that the proposal is assigned to
                  (this is only meaningful if the proposal has a label > 0; if label = 0
                  then the ground-truth box is random)

                Other fields such as "gt_classes", "gt_masks", that's included in `targets`.
        """
        gt_boxes = [x.gt_boxes for x in targets]
        # Augment proposals with ground-truth boxes.
        # In the case of learned proposals (e.g., RPN), when training starts
        # the proposals will be low quality due to random initialization.
        # It's possible that none of these initial
        # proposals have high enough overlap with the gt objects to be used
        # as positive examples for the second stage components (box head,
        # cls head, mask head). Adding the gt boxes to the set of proposals
        # ensures that the second stage components will have some positive
        # examples from the start of training. For RPN, this augmentation improves
        # convergence and empirically improves box AP on COCO by about 0.5
        # points (under one tested configuration).
        if self.proposal_append_gt:
            proposals = add_ground_truth_to_proposals(gt_boxes, proposals)

        proposals_with_gt = []

        num_fg_samples = []
        num_bg_samples = []
        for proposals_per_image, targets_per_image in zip(proposals, targets):
            has_gt = len(targets_per_image) > 0
            match_quality_matrix = pairwise_iou(
                targets_per_image.gt_boxes, proposals_per_image.proposal_boxes
            )
            matched_idxs, matched_labels = self.proposal_matcher(match_quality_matrix)
            sampled_idxs, gt_classes = self._sample_proposals(
                matched_idxs, matched_labels, targets_per_image.gt_classes
            )

            # Set target attributes of the sampled proposals:
            proposals_per_image = proposals_per_image[sampled_idxs]
            proposals_per_image.gt_classes = gt_classes

            # We index all the attributes of targets that start with "gt_"
            # and have not been added to proposals yet (="gt_classes").
            if has_gt:
                sampled_targets = matched_idxs[sampled_idxs]
                # NOTE: here the indexing waste some compute, because heads
                # like masks, keypoints, etc, will filter the proposals again,
                # (by foreground/background, or number of keypoints in the image, etc)
                # so we essentially index the data twice.
                for (trg_name, trg_value) in targets_per_image.get_fields().items():
                    if trg_name.startswith("gt_") and not proposals_per_image.has(trg_name):
                        proposals_per_image.set(trg_name, trg_value[sampled_targets])
            else:
                gt_boxes = Boxes(
                    targets_per_image.gt_boxes.tensor.new_zeros((len(sampled_idxs), 4))
                )
                proposals_per_image.gt_boxes = gt_boxes

            num_bg_samples.append((gt_classes == self.num_classes).sum().item())
            num_fg_samples.append(gt_classes.numel() - num_bg_samples[-1])
            proposals_with_gt.append(proposals_per_image)

        # Log the number of fg/bg samples that are selected for training ROI heads
        storage = get_event_storage()
        storage.put_scalar("roi_head/num_fg_samples", np.mean(num_fg_samples))
        storage.put_scalar("roi_head/num_bg_samples", np.mean(num_bg_samples))

        return proposals_with_gt

    def forward(
        self,
        images: ImageList,
        features: Dict[str, torch.Tensor],
        proposals: List[Instances],
        targets: Optional[List[Instances]] = None,
    ) -> Tuple[List[Instances], Dict[str, torch.Tensor]]:
        """
        Args:
            images (ImageList):
            features (dict[str: Tensor]): input data as a mapping from feature
                map name to tensor. Axis 0 represents the number of images `N` in
                the input data; axes 1-3 are channels, height, and width, which may
                vary between feature maps (e.g., if a feature pyramid is used).
            proposals (list[Instances]): length `N` list of `Instances`. The i-th
                `Instances` contains object proposals for the i-th input image,
                with fields "proposal_boxes" and "objectness_logits".
            targets (list[Instances], optional): length `N` list of `Instances`. The i-th
                `Instances` contains the ground-truth per-instance annotations
                for the i-th input image.  Specify `targets` during training only.
                It may have the following fields:

                - gt_boxes: the bounding box of each instance.
                - gt_classes: the label for each instance with a category ranging in [0, #class].
                - gt_masks: PolygonMasks or BitMasks, the ground-truth masks of each instance.
                - gt_keypoints: NxKx3, the groud-truth keypoints for each instance.

        Returns:
            results (list[Instances]): length `N` list of `Instances` containing the
            detected instances. Returned during inference only; may be [] during training.
            losses (dict[str->Tensor]):
            mapping from a named loss to a tensor storing the loss. Used during training only.
        """
        raise NotImplementedError()


class Res5ROIHeads(ROIHeads):
    """
    The ROIHeads in a typical "C4" R-CNN model, where
    the box and mask head share the cropping and
    the per-region feature computation by a Res5 block.
    """

    def __init__(self, cfg, input_shape):
        super().__init__(cfg, input_shape)

        assert len(self.in_features) == 1

        # fmt: off
        pooler_resolution = cfg.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION
        pooler_type       = cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE
        pooler_scales     = (1.0 / self.feature_strides[self.in_features[0]], )
        sampling_ratio    = cfg.MODEL.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO
        self.mask_on      = cfg.MODEL.MASK_ON
        # fmt: on
        assert not cfg.MODEL.KEYPOINT_ON

        self.pooler = ROIPooler(
            output_size=pooler_resolution,
            scales=pooler_scales,
            sampling_ratio=sampling_ratio,
            pooler_type=pooler_type,
        )

        self.res5, out_channels = self._build_res5_block(cfg)
        self.box_predictor = FastRCNNOutputLayers(
            out_channels, self.num_classes, self.cls_agnostic_bbox_reg
        )

        if self.mask_on:
            self.mask_head = cfg.build_mask_head(
                cfg,
                ShapeSpec(channels=out_channels, width=pooler_resolution, height=pooler_resolution),
            )

    def _build_res5_block(self, cfg):
        # fmt: off
        stage_channel_factor = 2 ** 3  # res5 is 8x res2
        num_groups           = cfg.MODEL.RESNETS.NUM_GROUPS
        width_per_group      = cfg.MODEL.RESNETS.WIDTH_PER_GROUP
        bottleneck_channels  = num_groups * width_per_group * stage_channel_factor
        out_channels         = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS * stage_channel_factor
        stride_in_1x1        = cfg.MODEL.RESNETS.STRIDE_IN_1X1
        norm                 = cfg.MODEL.RESNETS.NORM
        activation           = cfg.MODEL.RESNETS.ACTIVATION
        if "DEFORM_ON_PER_STAGE" in dir(cfg.MODEL.RESNETS):
            assert not cfg.MODEL.RESNETS.DEFORM_ON_PER_STAGE[-1], \
                "Deformable conv is not yet supported in res5 head."
        # fmt: on

        blocks = make_stage(
            BottleneckBlock,
            3,
            first_stride=2,
            in_channels=out_channels // 2,
            bottleneck_channels=bottleneck_channels,
            out_channels=out_channels,
            num_groups=num_groups,
            norm=norm,
            activation=activation,
            stride_in_1x1=stride_in_1x1,
        )
        return nn.Sequential(*blocks), out_channels

    def _shared_roi_transform(self, features, boxes):
        x = self.pooler(features, boxes)
        return self.res5(x)

    def forward(self, images, features, proposals, targets=None):
        """
        See :class:`ROIHeads.forward`.
        """
        del images

        if self.training:
            assert targets
            proposals = self.label_and_sample_proposals(proposals, targets)
        del targets

        proposal_boxes = [x.proposal_boxes for x in proposals]
        box_features = self._shared_roi_transform(
            [features[f] for f in self.in_features], proposal_boxes
        )
        feature_pooled = box_features.mean(dim=[2, 3])  # pooled to 1x1
        pred_class_logits, pred_proposal_deltas = self.box_predictor(feature_pooled)
        del feature_pooled

        outputs = FastRCNNOutputs(
            self.box2box_transform,
            pred_class_logits,
            pred_proposal_deltas,
            proposals,
            self.smooth_l1_beta,
        )

        if self.training:
            del features
            losses = outputs.losses()
            if self.mask_on:
                proposals, fg_selection_masks = select_foreground_proposals(
                    proposals, self.num_classes
                )
                # Since the ROI feature transform is shared between boxes and masks,
                # we don't need to recompute features. The mask loss is only defined
                # on foreground proposals, so we need to select out the foreground
                # features.
                mask_features = box_features[torch.cat(fg_selection_masks, dim=0)]
                del box_features
                mask_logits = self.mask_head(mask_features)
                losses["loss_mask"] = mask_rcnn_loss(mask_logits, proposals)
            return [], losses
        else:
            pred_instances, _ = outputs.inference(
                self.test_score_thresh, self.test_nms_thresh, self.test_nms_type,
                self.test_detections_per_img
            )
            pred_instances = self.forward_with_given_boxes(features, pred_instances)
            return pred_instances, {}

    def forward_with_given_boxes(self, features, instances):
        """
        Use the given boxes in `instances` to produce other (non-box) per-ROI outputs.

        Args:
            features: same as in `forward()`
            instances (list[Instances]): instances to predict other outputs. Expect the keys
                "pred_boxes" and "pred_classes" to exist.

        Returns:
            instances (Instances):
                the same `Instances` object, with extra
                fields such as `pred_masks` or `pred_keypoints`.
        """
        assert not self.training
        assert instances[0].has("pred_boxes") and instances[0].has("pred_classes")

        if self.mask_on:
            features = [features[f] for f in self.in_features]
            x = self._shared_roi_transform(features, [x.pred_boxes for x in instances])
            mask_logits = self.mask_head(x)
            mask_rcnn_inference(mask_logits, instances)
        return instances


class StandardROIHeads(ROIHeads):
    """
    It's "standard" in a sense that there is no ROI transform sharing
    or feature sharing between tasks.
    The cropped rois go to separate branches (boxes and masks) directly.
    This way, it is easier to make separate abstractions for different branches.

    This class is used by most models, such as FPN and C5.
    To implement more models, you can subclass it and implement a different
    :meth:`forward()` or a head.
    """

    def __init__(self, cfg, input_shape):
        super(StandardROIHeads, self).__init__(cfg, input_shape)
        self._init_box_head(cfg)
        self._init_mask_head(cfg)
        self._init_keypoint_head(cfg)

    def _init_box_head(self, cfg):
        # fmt: off
        pooler_resolution        = cfg.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION
        pooler_scales            = tuple(1.0 / self.feature_strides[k] for k in self.in_features)
        sampling_ratio           = cfg.MODEL.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO
        pooler_type              = cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE
        self.train_on_pred_boxes = cfg.MODEL.ROI_BOX_HEAD.TRAIN_ON_PRED_BOXES
        # fmt: on

        # If StandardROIHeads is applied on multiple feature maps (as in FPN),
        # then we share the same predictors and therefore the channel counts must be the same
        in_channels = [self.feature_channels[f] for f in self.in_features]
        # Check all channel counts are equal
        assert len(set(in_channels)) == 1, in_channels
        in_channels = in_channels[0]

        self.box_pooler = ROIPooler(
            output_size=pooler_resolution,
            scales=pooler_scales,
            sampling_ratio=sampling_ratio,
            pooler_type=pooler_type,
        )
        # Here we split "box head" and "box predictor", which is mainly due to historical reasons.
        # They are used together so the "box predictor" layers should be part of the "box head".
        # New subclasses of ROIHeads do not need "box predictor"s.
        self.box_head = cfg.build_box_head(
            cfg, ShapeSpec(channels=in_channels, height=pooler_resolution, width=pooler_resolution)
        )
        self.box_predictor = FastRCNNOutputLayers(
            self.box_head.output_size, self.num_classes, self.cls_agnostic_bbox_reg
        )

    def _init_mask_head(self, cfg):
        # fmt: off
        self.mask_on           = cfg.MODEL.MASK_ON
        if not self.mask_on:
            return
        pooler_resolution = cfg.MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION
        pooler_scales     = tuple(1.0 / self.feature_strides[k] for k in self.in_features)
        sampling_ratio    = cfg.MODEL.ROI_MASK_HEAD.POOLER_SAMPLING_RATIO
        pooler_type       = cfg.MODEL.ROI_MASK_HEAD.POOLER_TYPE
        # fmt: on

        in_channels = [self.feature_channels[f] for f in self.in_features][0]

        self.mask_pooler = ROIPooler(
            output_size=pooler_resolution,
            scales=pooler_scales,
            sampling_ratio=sampling_ratio,
            pooler_type=pooler_type,
        )
        self.mask_head = cfg.build_mask_head(
            cfg,
            ShapeSpec(
                channels=in_channels,
                width=pooler_resolution,
                height=pooler_resolution
            )
        )

    def _init_keypoint_head(self, cfg):
        # fmt: off
        self.keypoint_on                         = cfg.MODEL.KEYPOINT_ON
        if not self.keypoint_on:
            return
        pooler_resolution                        = cfg.MODEL.ROI_KEYPOINT_HEAD.POOLER_RESOLUTION
        pooler_scales                            = tuple(1.0 / self.feature_strides[k] for k in self.in_features)  # noqa
        sampling_ratio                           = cfg.MODEL.ROI_KEYPOINT_HEAD.POOLER_SAMPLING_RATIO
        pooler_type                              = cfg.MODEL.ROI_KEYPOINT_HEAD.POOLER_TYPE
        self.normalize_loss_by_visible_keypoints = cfg.MODEL.ROI_KEYPOINT_HEAD.NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS  # noqa
        self.keypoint_loss_weight                = cfg.MODEL.ROI_KEYPOINT_HEAD.LOSS_WEIGHT
        # fmt: on

        in_channels = [self.feature_channels[f] for f in self.in_features][0]

        self.keypoint_pooler = ROIPooler(
            output_size=pooler_resolution,
            scales=pooler_scales,
            sampling_ratio=sampling_ratio,
            pooler_type=pooler_type,
        )
        self.keypoint_head = cfg.build_keypoint_head(
            cfg, ShapeSpec(channels=in_channels, width=pooler_resolution, height=pooler_resolution)
        )

    def forward(
        self,
        images: ImageList,
        features: Dict[str, torch.Tensor],
        proposals: List[Instances],
        targets: Optional[List[Instances]] = None,
    ) -> Tuple[List[Instances], Dict[str, torch.Tensor]]:
        """
        See :class:`ROIHeads.forward`.
        """
        del images
        if self.training:
            assert targets
            proposals = self.label_and_sample_proposals(proposals, targets)
        del targets

        features_list = [features[f] for f in self.in_features]

        if self.training:
            losses = self._forward_box(features_list, proposals)
            # Usually the original proposals used by the box head are used by the mask, keypoint
            # heads. But when `self.train_on_pred_boxes is True`, proposals will contain boxes
            # predicted by the box head.
            losses.update(self._forward_mask(features_list, proposals))
            losses.update(self._forward_keypoint(features_list, proposals))
            return proposals, losses
        else:
            pred_instances = self._forward_box(features_list, proposals)
            # During inference cascaded prediction is used: the mask and keypoints heads are only
            # applied to the top scoring box detections.
            pred_instances = self.forward_with_given_boxes(features, pred_instances)
            return pred_instances, {}

    def forward_with_given_boxes(
        self, features: Dict[str, torch.Tensor], instances: List[Instances]
    ) -> List[Instances]:
        """
        Use the given boxes in `instances` to produce other (non-box) per-ROI outputs.

        This is useful for downstream tasks where a box is known, but need to obtain
        other attributes (outputs of other heads).
        Test-time augmentation also uses this.

        Args:
            features: same as in `forward()`
            instances (list[Instances]): instances to predict other outputs. Expect the keys
                pred_boxes" and "pred_classes" to exist.

        Returns:
            instances (list[Instances]):
                the same `Instances` objects, with extra
                fields such as `pred_masks` or `pred_keypoints`.
        """
        assert not self.training
        assert instances[0].has("pred_boxes") and instances[0].has("pred_classes")
        features_list = [features[f] for f in self.in_features]

        instances = self._forward_mask(features_list, instances)
        instances = self._forward_keypoint(features_list, instances)
        return instances

    def _forward_box(
        self, features: List[torch.Tensor], proposals: List[Instances]
    ) -> Union[Dict[str, torch.Tensor], List[Instances]]:
        """
        Forward logic of the box prediction branch. If `self.train_on_pred_boxes is True`,
            the function puts predicted boxes in the `proposal_boxes` field of `proposals` argument.

        Args:
            features (list[Tensor]): #level input features for box prediction
            proposals (list[Instances]): the per-image object proposals with
                their matching ground truth.
                Each has fields "proposal_boxes", and "objectness_logits",
                "gt_classes", "gt_boxes".

        Returns:
            In training, a dict of losses.
            In inference, a list of `Instances`, the predicted instances.
        """
        box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
        box_features = self.box_head(box_features)
        pred_class_logits, pred_proposal_deltas = self.box_predictor(box_features)
        del box_features

        outputs = FastRCNNOutputs(
            self.box2box_transform,
            pred_class_logits,
            pred_proposal_deltas,
            proposals,
            self.smooth_l1_beta,
        )
        if self.training:
            if self.train_on_pred_boxes:
                with torch.no_grad():
                    pred_boxes = outputs.predict_boxes_for_gt_classes()
                    for proposals_per_image, pred_boxes_per_image in zip(proposals, pred_boxes):
                        proposals_per_image.proposal_boxes = Boxes(pred_boxes_per_image)
            return outputs.losses()
        else:
            pred_instances, _ = outputs.inference(
                self.test_score_thresh, self.test_nms_thresh, self.test_nms_type,
                self.test_detections_per_img
            )
            return pred_instances

    def _forward_mask(
        self, features: List[torch.Tensor], instances: List[Instances]
    ) -> Union[Dict[str, torch.Tensor], List[Instances]]:
        """
        Forward logic of the mask prediction branch.

        Args:
            features (list[Tensor]): #level input features for mask prediction
            instances (list[Instances]): the per-image instances to train/predict masks.
                In training, they can be the proposals.
                In inference, they can be the predicted boxes.

        Returns:
            In training, a dict of losses.
            In inference, update `instances` with new fields "pred_masks" and return it.
        """
        if not self.mask_on:
            return {} if self.training else instances

        if self.training:
            # The loss is only defined on positive proposals.
            proposals, _ = select_foreground_proposals(instances, self.num_classes)
            proposal_boxes = [x.proposal_boxes for x in proposals]
            mask_features = self.mask_pooler(features, proposal_boxes)
            mask_logits = self.mask_head(mask_features)
            return {"loss_mask": mask_rcnn_loss(mask_logits, proposals)}
        else:
            pred_boxes = [x.pred_boxes for x in instances]
            mask_features = self.mask_pooler(features, pred_boxes)
            mask_logits = self.mask_head(mask_features)
            mask_rcnn_inference(mask_logits, instances)
            return instances

    def _forward_keypoint(
        self, features: List[torch.Tensor], instances: List[Instances]
    ) -> Union[Dict[str, torch.Tensor], List[Instances]]:
        """
        Forward logic of the keypoint prediction branch.

        Args:
            features (list[Tensor]): #level input features for keypoint prediction
            instances (list[Instances]): the per-image instances to train/predict keypoints.
                In training, they can be the proposals.
                In inference, they can be the predicted boxes.

        Returns:
            In training, a dict of losses.
            In inference, update `instances` with new fields "pred_keypoints" and return it.
        """
        if not self.keypoint_on:
            return {} if self.training else instances

        num_images = len(instances)

        if self.training:
            # The loss is defined on positive proposals with at >=1 visible keypoints.
            proposals, _ = select_foreground_proposals(instances, self.num_classes)
            proposals = select_proposals_with_visible_keypoints(proposals)
            proposal_boxes = [x.proposal_boxes for x in proposals]

            keypoint_features = self.keypoint_pooler(features, proposal_boxes)
            keypoint_logits = self.keypoint_head(keypoint_features)

            normalizer = (
                num_images
                * self.batch_size_per_image
                * self.positive_sample_fraction
                * keypoint_logits.shape[1]
            )
            loss = keypoint_rcnn_loss(
                keypoint_logits,
                proposals,
                normalizer=None if self.normalize_loss_by_visible_keypoints else normalizer,
            )
            return {"loss_keypoint": loss * self.keypoint_loss_weight}
        else:
            pred_boxes = [x.pred_boxes for x in instances]
            keypoint_features = self.keypoint_pooler(features, pred_boxes)
            keypoint_logits = self.keypoint_head(keypoint_features)
            keypoint_rcnn_inference(keypoint_logits, instances)
            return instances
```

#### cvpods/modeling/backbone/bifpn.py

```python
import math

import torch
import torch.nn as nn
import torch.nn.functional as F

from cvpods.layers import Conv2dSamePadding as Conv2d
from cvpods.layers import MaxPool2dSamePadding as MaxPool2d
from cvpods.layers import MemoryEfficientSwish, SeparableConvBlock, Swish, get_norm
from cvpods.modeling.backbone import Backbone

from .efficientnet import build_efficientnet_backbone
from .fpn import _assert_strides_are_log2_contiguous


class BiFPNLayer(nn.Module):
    """
    This module implements one layer of BiFPN, and BiFPN can be obtained
    by stacking this module multiple times.
    See: https://arxiv.org/pdf/1911.09070.pdf for more details.
    """

    def __init__(self, input_size, in_channels_list, out_channels,
                 fuse_type="fast", norm="BN", memory_efficient=True):
        """
        input_size (int): the input image size.
        in_channels_list (list): the number of input tensor channels per level.
        out_channels (int): the number of output tensor channels.
        fuse_type (str): now only support three weighted fusion approaches:

            * fast:    Output = sum(Input_i * w_i / sum(w_j))
            * sotfmax: Output = sum(Input_i * e ^ w_i / sum(e ^ w_j))
            * sum:     Output = sum(Input_i) / len(Input_i)

        norm (str): the normalization to use.
        memory_efficient (bool): use `MemoryEfficientSwish` or `Swish` as activation function.
        """
        super(BiFPNLayer, self).__init__()
        assert fuse_type in ("fast", "softmax", "sum"), f"Unknown fuse method: {fuse_type}." \
            " Please select in [fast, sotfmax, sum]."

        self.input_size = input_size
        self.in_channels_list = in_channels_list
        self.fuse_type = fuse_type
        self.levels = len(in_channels_list)
        self.nodes_input_offsets = [
            [3, 4],
            [2, 5],
            [1, 6],
            [0, 7],
            [1, 7, 8],
            [2, 6, 9],
            [3, 5, 10],
            [4, 11],
        ]
        self.nodes_strides = [
            2 ** x
            for x in [6, 5, 4, 3, 4, 5, 6, 7]
        ]

        # Change input feature map to have target number of channels.
        self.resample_convs = nn.ModuleList()
        for node_i_input_offsets in self.nodes_input_offsets:
            resample_convs_i = nn.ModuleList()
            for input_offset in node_i_input_offsets:
                if self.in_channels_list[input_offset] != out_channels:
                    resample_conv = Conv2d(
                        self.in_channels_list[input_offset],
                        out_channels,
                        kernel_size=1,
                        stride=1,
                        padding=0,
                        norm=get_norm(norm, out_channels),
                        activation=None,
                    )
                else:
                    resample_conv = nn.Identity()
                self.in_channels_list.append(out_channels)
                resample_convs_i.append(resample_conv)
            self.resample_convs.append(resample_convs_i)

        # fpn combine weights
        self.edge_weights = nn.ParameterList()
        for node_i_input_offsets in self.nodes_input_offsets:
            # combine weight
            if fuse_type == "fast" or fuse_type == "softmax":
                weights_i = nn.Parameter(
                    torch.ones(len(node_i_input_offsets), dtype=torch.float32),
                    requires_grad=True,
                )
            elif fuse_type == "sum":
                weights_i = nn.Parameter(
                    torch.ones(len(node_i_input_offsets), dtype=torch.float32),
                    requires_grad=False,
                )
            else:
                raise ValueError("Unknown fuse method: {}".format(self.fuse_type))
            self.edge_weights.append(weights_i)

        # Convs for combine edge features
        self.combine_convs = nn.ModuleList()
        for node_i_input_offsets in self.nodes_input_offsets:
            combine_conv = SeparableConvBlock(
                out_channels,
                out_channels,
                kernel_size=3,
                padding="SAME",
                norm=get_norm(norm, out_channels),
                activation=None,
            )
            self.combine_convs.append(combine_conv)

        self.act = MemoryEfficientSwish() if memory_efficient else Swish()
        self.down_sampling = MaxPool2d(kernel_size=3, stride=2, padding="SAME")
        self.up_sampling = nn.Upsample(scale_factor=2, mode='nearest')

    def forward(self, inputs):
        assert len(inputs) == self.levels
        # Build top-down and bottom-up path
        self.nodes_features = inputs
        for node_idx, (node_i_input_offsets, node_i_stride) in enumerate(
                zip(self.nodes_input_offsets, self.nodes_strides)):
            # edge weights
            if self.fuse_type == "fast":
                weights_i = F.relu(self.edge_weights[node_idx])
            elif self.fuse_type == "softmax":
                weights_i = self.edge_weights[node_idx].softmax(dim=0)
            elif self.fuse_type == "sum":
                weights_i = self.edge_weights[node_idx]

            target_width = self.input_size / node_i_stride
            edge_features = []
            for offset_idx, offset in enumerate(node_i_input_offsets):
                edge_feature = self.nodes_features[offset]
                resample_conv = self.resample_convs[node_idx][offset_idx]
                # 1x1 conv for change feature map channels if necessary
                edge_feature = resample_conv(edge_feature)
                width = edge_feature.size(-1)
                if width > target_width:
                    # Downsampling for change feature map size
                    assert width / target_width == 2.0
                    edge_feature = self.down_sampling(edge_feature)
                elif width < target_width:
                    # Upsampling for change feature map size
                    assert target_width / width == 2.0
                    edge_feature = self.up_sampling(edge_feature)
                edge_feature = edge_feature * (weights_i[offset_idx] / (weights_i.sum() + 1e-4))
                edge_features.append(edge_feature)
            node_i_feature = sum(edge_features)
            node_i_feature = self.act(node_i_feature)
            node_i_feature = self.combine_convs[node_idx](node_i_feature)
            self.nodes_features.append(node_i_feature)

        # The number of node in one bifpn layer is 13
        assert len(self.nodes_features) == 13
        # The bifpn layer output is the last 5 nodes
        return self.nodes_features[-5:]


class BiFPN(Backbone):
    """
    This module implements the BIFPN module in EfficientDet.
    See: https://arxiv.org/pdf/1911.09070.pdf for more details.
    """

    def __init__(self, input_size, bottom_up, in_features, out_channels, num_bifpn_layers,
                 fuse_type="weighted_sum", top_block=None, norm="BN", bn_momentum=0.01, bn_eps=1e-3,
                 memory_efficient=True):
        """
        input_size (int): the input image size.
        bottom_up (Backbone): module representing the bottom up subnetwork.
            Must be a subclass of :class:`Backbone`. The multi-scale feature
            maps generated by the bottom up network, and listed in `in_features`,
            are used to generate FPN levels.
        in_features (list[str]): names of the input feature maps coming
            from the backbone to which FPN is attached. For example, if the
            backbone produces ["res2", "res3", "res4"], any *contiguous* sublist
            of these may be used; order must be from high to low resolution.
        out_channels (int): the number of channels in the output feature maps.
        num_bifpn_layers (str): the number of bifpn layer.
        fuse_type (str): weighted feature fuse type. see: `BiFPNLayer`
        top_block (nn.Module or None): if provided, an extra operation will
            be performed on the output of the last (smallest resolution)
            FPN output, and the result will extend the result list. The top_block
            further downsamples the feature map. It must have an attribute
            "num_levels", meaning the number of extra FPN levels added by
            this block, and "in_feature", which is a string representing
            its input feature (e.g., p5).
        norm (str): the normalization to use.
        bn_momentum (float): the `momentum` parameter of the norm module.
        bn_eps (float): the `eps` parameter of the norm module.
        """
        super(BiFPN, self).__init__()
        assert isinstance(bottom_up, Backbone)

        self.bottom_up = bottom_up
        self.top_block = top_block
        self.in_features = in_features
        self.bn_momentum = bn_momentum
        self.bn_eps = bn_eps

        # Feature map strides and channels from the bottom up network
        input_shapes = bottom_up.output_shape()
        in_strides = [input_shapes[f].stride for f in in_features]
        in_channels = [input_shapes[f].channels for f in in_features]

        _assert_strides_are_log2_contiguous(in_strides)

        # Return feature names are "p<stage>", like ["p2", "p3", ..., "p6"]
        self._out_feature_strides = {"p{}".format(int(math.log2(s))): s for s in in_strides}

        # top block output feature maps.
        if self.top_block is not None:
            s = int(math.log2(in_strides[-1]))
            for i in range(self.top_block.num_levels):
                self._out_feature_strides[f"p{s + i + 1}"] = 2 ** (s + i + 1)

        self._out_features = list(self._out_feature_strides.keys())
        self._out_feature_channels = {k: out_channels for k in self._out_features}

        # build bifpn layers
        self.bifpn_layers = nn.ModuleList()
        for idx in range(num_bifpn_layers):
            if idx == 0:
                bifpn_layer_in_channels = in_channels + [out_channels] * self.top_block.num_levels
            else:
                bifpn_layer_in_channels = [out_channels] * len(self._out_features)
            bifpn_layer = BiFPNLayer(input_size, bifpn_layer_in_channels,
                                     out_channels, fuse_type, norm, memory_efficient)
            self.bifpn_layers.append(bifpn_layer)

        self._size_divisibility = in_strides[-1]
        self._init_weights()

    def _init_weights(self):
        """
        Weight initialization as per Tensorflow official implementations.
        See: https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/ops/init_ops.py
             #L437
        """
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                fan_in = m.kernel_size[0] * m.kernel_size[1] * m.in_channels
                stddev = math.sqrt(1. / max(1., fan_in))
                m.weight.data.normal_(0, stddev)
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                if self.bn_momentum is not None and self.bn_eps is not None:
                    m.momentum = self.bn_momentum
                    m.eps = self.bn_eps
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def forward(self, x):
        bottom_up_features = self.bottom_up(x)
        results = [bottom_up_features[f] for f in self.in_features]

        # top block
        if self.top_block is not None:
            top_block_in_feature = bottom_up_features[self.top_block.in_feature]
            results.extend(self.top_block(top_block_in_feature))

        # build top-down and bottom-up path with stack
        for bifpn_layer in self.bifpn_layers:
            results = bifpn_layer(results)
        assert len(self._out_features) == len(results)
        return dict(zip(self._out_features, results))


class BiFPNP6P7(nn.Module):
    """
    This module is used in BiFPN to generate extra layers,
    P6 and P7 from EfficientNet "stage8" feature.
    """

    def __init__(self, in_channels, out_channels, norm="BN"):
        """
        Args:
            in_channels (int): the number of input tensor channels.
            out_channels (int): the number of output tensor channels.
            norm (str): the normalization to use.
        """
        super().__init__()
        self.num_levels = 2
        self.in_feature = "stage8"
        self.p6_conv = Conv2d(
            in_channels,
            out_channels,
            kernel_size=1,
            stride=1,
            padding=0,
            norm=get_norm(norm, out_channels),
            activation=None
        )
        self.down_sampling = MaxPool2d(kernel_size=3, stride=2, padding="SAME")

    def forward(self, x):
        x = self.p6_conv(x)
        p6 = self.down_sampling(x)
        p7 = self.down_sampling(p6)
        return [p6, p7]


def build_efficientnet_bifpn_backbone(cfg, input_shape):
    """
    Args:
        cfg: a cvpods `Config` instance.

    Returns:
        bifpn (Backbone): backbone module, must be a subclass of
            :class:`Backbone`.
    """
    in_features = cfg.MODEL.BIFPN.IN_FEATURES
    norm = cfg.MODEL.BIFPN.NORM
    bn_momentum = cfg.MODEL.BIFPN.BN_MOMENTUM
    bn_eps = cfg.MODEL.BIFPN.BN_EPS
    memory_efficient = cfg.MODEL.BIFPN.MEMORY_EFFICIENT_SWISH
    input_size = cfg.MODEL.BIFPN.INPUT_SIZE
    out_channels = cfg.MODEL.BIFPN.OUT_CHANNELS
    bifpn_layers = cfg.MODEL.BIFPN.NUM_LAYERS
    fuse_type = cfg.MODEL.BIFPN.FUSE_TYPE

    # when norm is "" or None, set norm = "BN"
    if not norm:
        norm = "BN"

    bottom_up = build_efficientnet_backbone(cfg, input_shape)
    top_block = BiFPNP6P7(
        bottom_up.output_shape()[in_features[-1]].channels,
        out_channels, norm)
    bifpn = BiFPN(input_size, bottom_up, in_features, out_channels,
                  bifpn_layers, fuse_type, top_block, norm, bn_momentum,
                  bn_eps, memory_efficient)
    return bifpn
```

#### cvpods/modeling/backbone/fpn.py

```python
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.
import math

import torch.nn.functional as F
from torch import nn

from cvpods.layers import Conv2d, ShapeSpec, get_norm
from cvpods.modeling.nn_utils import weight_init

from .backbone import Backbone
from .mobilenet import build_mobilenetv2_backbone
from .resnet import build_resnet_backbone
from .shufflenet import build_shufflenetv2_backbone

__all__ = [
    "_assert_strides_are_log2_contiguous",
    "build_mobilenetv2_fpn_backbone",
    "build_retinanet_mobilenetv2_fpn_backbone",
    "build_retinanet_mobilenetv2_fpn_p5_backbone",
    "build_resnet_fpn_backbone",
    "build_retinanet_resnet_fpn_backbone",
    "FPN",
    "LastLevelP6P7"
]


class FPN(Backbone):
    """
    This module implements Feature Pyramid Network.
    It creates pyramid features built on top of some input feature maps.
    """

    def __init__(
        self, bottom_up, in_features, out_channels, norm="", top_block=None, fuse_type="sum"
    ):
        """
        Args:
            bottom_up (Backbone): module representing the bottom up subnetwork.
                Must be a subclass of :class:`Backbone`. The multi-scale feature
                maps generated by the bottom up network, and listed in `in_features`,
                are used to generate FPN levels.
            in_features (list[str]): names of the input feature maps coming
                from the backbone to which FPN is attached. For example, if the
                backbone produces ["res2", "res3", "res4"], any *contiguous* sublist
                of these may be used; order must be from high to low resolution.
            out_channels (int): number of channels in the output feature maps.
            norm (str): the normalization to use.
            top_block (nn.Module or None): if provided, an extra operation will
                be performed on the output of the last (smallest resolution)
                FPN output, and the result will extend the result list. The top_block
                further downsamples the feature map. It must have an attribute
                "num_levels", meaning the number of extra FPN levels added by
                this block, and "in_feature", which is a string representing
                its input feature (e.g., p5).
            fuse_type (str): types for fusing the top down features and the lateral
                ones. It can be "sum" (default), which sums up element-wise; or "avg",
                which takes the element-wise mean of the two.
        """
        super(FPN, self).__init__()
        assert isinstance(bottom_up, Backbone)

        # Feature map strides and channels from the bottom up network (e.g. ResNet)
        input_shapes = bottom_up.output_shape()
        in_strides = [input_shapes[f].stride for f in in_features]
        in_channels = [input_shapes[f].channels for f in in_features]

        _assert_strides_are_log2_contiguous(in_strides)
        lateral_convs = []
        output_convs = []

        use_bias = norm == ""
        for idx, in_channels in enumerate(in_channels):
            lateral_norm = get_norm(norm, out_channels)
            output_norm = get_norm(norm, out_channels)

            lateral_conv = Conv2d(
                in_channels, out_channels, kernel_size=1, bias=use_bias, norm=lateral_norm
            )
            output_conv = Conv2d(
                out_channels,
                out_channels,
                kernel_size=3,
                stride=1,
                padding=1,
                bias=use_bias,
                norm=output_norm,
            )
            weight_init.c2_xavier_fill(lateral_conv)
            weight_init.c2_xavier_fill(output_conv)
            stage = int(math.log2(in_strides[idx]))
            self.add_module("fpn_lateral{}".format(stage), lateral_conv)
            self.add_module("fpn_output{}".format(stage), output_conv)

            lateral_convs.append(lateral_conv)
            output_convs.append(output_conv)
        # Place convs into top-down order (from low to high resolution)
        # to make the top-down computation in forward clearer.
        self.lateral_convs = lateral_convs[::-1]
        self.output_convs = output_convs[::-1]
        self.top_block = top_block
        self.in_features = in_features
        self.bottom_up = bottom_up
        # Return feature names are "p<stage>", like ["p2", "p3", ..., "p6"]
        self._out_feature_strides = {"p{}".format(int(math.log2(s))): s for s in in_strides}
        # top block output feature maps.
        if self.top_block is not None:
            for s in range(stage, stage + self.top_block.num_levels):
                self._out_feature_strides["p{}".format(s + 1)] = 2 ** (s + 1)

        self._out_features = list(self._out_feature_strides.keys())
        self._out_feature_channels = {k: out_channels for k in self._out_features}
        self._size_divisibility = in_strides[-1]
        assert fuse_type in {"avg", "sum"}
        self._fuse_type = fuse_type

    @property
    def size_divisibility(self):
        return self._size_divisibility

    def forward(self, x):
        """
        Args:
            input (dict[str->Tensor]): mapping feature map name (e.g., "res5") to
                feature map tensor for each feature level in high to low resolution order.

        Returns:
            dict[str->Tensor]:
                mapping from feature map name to FPN feature map tensor
                in high to low resolution order. Returned feature names follow the FPN
                paper convention: "p<stage>", where stage has stride = 2 ** stage e.g.,
                ["p2", "p3", ..., "p6"].
        """
        # Reverse feature maps into top-down order (from low to high resolution)
        bottom_up_features = self.bottom_up(x)
        x = [bottom_up_features[f] for f in self.in_features[::-1]]
        results = []
        prev_features = self.lateral_convs[0](x[0])
        results.append(self.output_convs[0](prev_features))
        for features, lateral_conv, output_conv in zip(
            x[1:], self.lateral_convs[1:], self.output_convs[1:]
        ):
            top_down_features = F.interpolate(prev_features, scale_factor=2, mode="nearest")
            lateral_features = lateral_conv(features)
            prev_features = lateral_features + top_down_features
            if self._fuse_type == "avg":
                prev_features /= 2
            results.insert(0, output_conv(prev_features))

        if self.top_block is not None:
            if self.top_block.in_feature in bottom_up_features:
                top_block_in_feature = bottom_up_features[self.top_block.in_feature]
            else:
                top_block_in_feature = results[self._out_features.index(self.top_block.in_feature)]
            results.extend(self.top_block(top_block_in_feature))
        assert len(self._out_features) == len(results)
        return dict(zip(self._out_features, results))

    def output_shape(self):
        return {
            name: ShapeSpec(
                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]
            )
            for name in self._out_features
        }


def _assert_strides_are_log2_contiguous(strides):
    """
    Assert that each stride is 2x times its preceding stride, i.e. "contiguous in log2".
    """
    for i, stride in enumerate(strides[1:], 1):
        assert stride == 2 * strides[i - 1], "Strides {} {} are not log2 contiguous".format(
            stride, strides[i - 1]
        )


class LastLevelMaxPool(nn.Module):
    """
    This module is used in the original FPN to generate a downsampled
    P6 feature from P5.
    """

    def __init__(self):
        super().__init__()
        self.num_levels = 1
        self.in_feature = "p5"

    def forward(self, x):
        return [F.max_pool2d(x, kernel_size=1, stride=2, padding=0)]


class LastLevelP6P7(nn.Module):
    """
    This module is used in Retinanet and follow-up network to generate extra layers
    P6 and P7 from C5/P5 feature.
    """

    def __init__(self, in_channels, out_channels, in_feature="res5"):
        """
        Args:
            in_feature: input feature name, e.g. "res5" stands for C5 features,
                "p5" stands for P5 feature.
        """
        super().__init__()
        self.num_levels = 2
        self.in_feature = in_feature
        self.p6 = nn.Conv2d(in_channels, out_channels, 3, 2, 1)
        self.p7 = nn.Conv2d(out_channels, out_channels, 3, 2, 1)
        for module in [self.p6, self.p7]:
            weight_init.c2_xavier_fill(module)

    def forward(self, x):
        p6 = self.p6(x)
        p7 = self.p7(F.relu(p6))
        return [p6, p7]


def build_resnet_fpn_backbone(cfg, input_shape: ShapeSpec):
    """
    Args:
        cfg: a cvpods config dict

    Returns:
        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.
    """
    bottom_up = build_resnet_backbone(cfg, input_shape)
    in_features = cfg.MODEL.FPN.IN_FEATURES
    out_channels = cfg.MODEL.FPN.OUT_CHANNELS
    backbone = FPN(
        bottom_up=bottom_up,
        in_features=in_features,
        out_channels=out_channels,
        norm=cfg.MODEL.FPN.NORM,
        top_block=LastLevelMaxPool(),
        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,
    )
    return backbone


def build_mobilenetv2_fpn_backbone(cfg, input_shape: ShapeSpec):
    """
    Args:
        cfg: a cvpods config dict
        input_shape: cvpods.layers.ShapeSpec

    Returns:
        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.
    """
    bottom_up = build_mobilenetv2_backbone(cfg, input_shape)
    in_features = cfg.MODEL.FPN.IN_FEATURES
    out_channels = cfg.MODEL.FPN.OUT_CHANNELS
    backbone = FPN(
        bottom_up=bottom_up,
        in_features=in_features,
        out_channels=out_channels,
        norm=cfg.MODEL.FPN.NORM,
        top_block=LastLevelMaxPool(),
        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,
    )
    return backbone


def build_retinanet_resnet_fpn_backbone(cfg, input_shape: ShapeSpec):
    """
    Args:
        cfg: a cvpods config dict

    Returns:
        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.
    """
    bottom_up = build_resnet_backbone(cfg, input_shape)
    in_features = cfg.MODEL.FPN.IN_FEATURES
    out_channels = cfg.MODEL.FPN.OUT_CHANNELS

    block_in_feature = cfg.MODEL.FPN.BLOCK_IN_FEATURES
    if block_in_feature == "p5":
        in_channels_p6p7 = out_channels
    elif block_in_feature == "res5":
        in_channels_p6p7 = bottom_up.output_shape()[block_in_feature].channels
    else:
        raise ValueError(block_in_feature)

    backbone = FPN(
        bottom_up=bottom_up,
        in_features=in_features,
        out_channels=out_channels,
        norm=cfg.MODEL.FPN.NORM,
        top_block=LastLevelP6P7(in_channels_p6p7, out_channels, in_feature=block_in_feature),
        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,
    )
    return backbone


def build_shufflenetv2_fpn_backbone(cfg, input_shape: ShapeSpec):
    """
    Args:
        cfg: a cvpods config dict

    Returns:
        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.
    """
    bottom_up = build_shufflenetv2_backbone(cfg, input_shape)
    in_features = cfg.MODEL.FPN.IN_FEATURES
    out_channels = cfg.MODEL.FPN.OUT_CHANNELS
    backbone = FPN(
        bottom_up=bottom_up,
        in_features=in_features,
        out_channels=out_channels,
        norm=cfg.MODEL.FPN.NORM,
        top_block=LastLevelMaxPool(),
        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,
    )
    return backbone


def build_retinanet_resnet_fpn_p5_backbone(cfg, input_shape: ShapeSpec):
    """
    Will be deprecated in the future.

    Args:
        cfg: a cvpods config dict

    Returns:
        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.
    """
    return build_retinanet_resnet_fpn_backbone(cfg, input_shape)


def build_retinanet_mobilenetv2_fpn_backbone(cfg, input_shape: ShapeSpec):
    """
    Args:
        cfg: a cvpods config dict

    Returns:
        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.
    """
    bottom_up = build_mobilenetv2_backbone(cfg, input_shape)
    in_features = cfg.MODEL.FPN.IN_FEATURES
    out_channels = cfg.MODEL.FPN.OUT_CHANNELS

    block_in_feature = cfg.MODEL.FPN.BLOCK_IN_FEATURES
    if block_in_feature == "p5":
        in_channels_p6p7 = out_channels
    elif block_in_feature == "mobile5-last" or block_in_feature == "mobile5":
        in_channels_p6p7 = bottom_up.output_shape()[block_in_feature].channels
    else:
        raise ValueError(block_in_feature)

    backbone = FPN(
        bottom_up=bottom_up,
        in_features=in_features,
        out_channels=out_channels,
        norm=cfg.MODEL.FPN.NORM,
        top_block=LastLevelP6P7(in_channels_p6p7, out_channels, in_feature=block_in_feature),
        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,
    )
    return backbone


def build_retinanet_mobilenetv2_fpn_p5_backbone(cfg, input_shape: ShapeSpec):
    """
    Will be deprecated in the future.

    Args:
        cfg: a cvpods config dict

    Returns:
        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.
    """
    return build_retinanet_mobilenetv2_fpn_backbone(cfg, input_shape)
```

#### cvpods/modeling/backbone/vgg.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.

import torch.nn as nn

from cvpods.layers import get_norm
from cvpods.modeling.backbone import Backbone


class VGG(Backbone):
    """
    This module implements VGG.
    See: https://arxiv.org/pdf/1409.1556.pdf for more details.
    """

    def __init__(self, stage_args, num_classes=None, out_features=None, fc_to_conv=False):
        """
        Args:
            stage_args (list[dict[str->int or str]]): the list contains the configuration dict
                corresponding to each stage of the vgg network.
                Each item in the list is a dict that contains:

                * num_blocks: int, the number of conv layer in the stage.
                * in_channels: int, the number of input tensor channels in the stage.
                * out_channels: int, the number of output tensor channels in the stage.
                * norm: str or callable, the normalization to use.
                * pool_args: tuple, contains the pool parameters of the stage,
                        which are kernel_size, stride, pading, ceil_mode.

            num_classes (None or int): if None, will not perform classification.
            out_features (list[str]): name of the layers whose outputs should
                be returned in forward. Can be anything in "Conv1_2", "Conv2_2",
                "Conv3_3" or "Conv4_3"...
                If None, will return the output of the last layer.
            fc_to_conv (bool): if True, change FC6, FC7 to conv layer, this is very useful in SSD.
        """
        super(VGG, self).__init__()
        self.num_classes = num_classes
        self._out_features = list()
        self._out_feature_strides = dict()
        self._out_feature_channels = dict()
        self.stages_and_names = list()

        self.layers = list()
        self.feature_idx_to_name = dict()
        for stage_idx, stage_karg in enumerate(stage_args, 1):
            name = "Conv{}_{}".format(stage_idx, stage_karg["num_blocks"])
            stage = self._make_layers(**stage_karg)
            self.layers.extend(stage)

            self.feature_idx_to_name[len(self.layers) - 2] = name
            self._out_feature_strides[name] = 2 ** (stage_idx - 1)
            self._out_feature_channels[name] = stage_karg["out_channels"]

        if fc_to_conv:
            conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)
            conv7 = nn.Conv2d(1024, 1024, kernel_size=1)
            self.layers += [conv6,
                            nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]

            self.feature_idx_to_name[len(self.layers) - 1] = "Conv7"
            self._out_feature_strides["Conv7"] = 2 ** (5 - 1)
            self._out_feature_channels["Conv7"] = 1024

        self.features = nn.ModuleList(self.layers)

        # for classification
        if self.num_classes is not None:
            if not fc_to_conv:
                self.classifier = nn.Sequential(
                    nn.Linear(512 * 7 * 7, 4096),
                    nn.ReLU(True),
                    nn.Dropout(),
                    nn.Linear(4096, 4096),
                    nn.ReLU(True),
                    nn.Dropout(),
                    nn.Linear(4096, num_classes),
                )
            else:
                self.classifier = nn.Sequential(
                    nn.Linear(1024 * 19 * 19, num_classes),
                )
            name = "linear"

        if out_features is None:
            out_features = [name]
        self._out_features = out_features
        assert len(self._out_features)

    def forward(self, x):
        """
        Args:
            x (Tensor): the input tensor.

        Returns:
            dict[str->Tensor]:
                mapping from feature map name to VGG feature map tensor
                in high to low resolution order. Noted that only the feature name
                in parameter `out_features` are returned.
        """
        outputs = dict()
        for idx, layer in enumerate(self.features):
            x = layer(x)
            feature_name = self.feature_idx_to_name.get(idx, None)
            if feature_name is not None and feature_name in self._out_features:
                outputs[feature_name] = x

        if self.num_classes is not None:
            x = x.view(x.size(0), -1)
            x = self.classifier(x)
            if "linear" in self._out_features:
                outputs["linear"] = x

        return outputs

    def _make_layers(self, num_blocks, **kwargs):
        """
        Create a vgg-net stage by creating many blocks(conv layers).

        Args:
            num_blocks (int): the number of conv layer in the stage.
            kwargs: other arguments, see: method:`__init__`.

        Returns:
            list[nn.Module]: a list of block module.
        """
        blocks = list()
        for _ in range(num_blocks):
            conv2d = nn.Conv2d(
                kwargs["in_channels"], kwargs["out_channels"], kernel_size=3, padding=1)
            if kwargs["norm"]:
                blocks += [conv2d, get_norm(kwargs["norm"],
                                            kwargs["out_channels"]), nn.ReLU(inplace=True)]
            else:
                blocks += [conv2d, nn.ReLU(inplace=True)]
            kwargs["in_channels"] = kwargs["out_channels"]
        pool = nn.MaxPool2d(kernel_size=kwargs["pool_args"][0],
                            stride=kwargs["pool_args"][1],
                            padding=kwargs["pool_args"][2],
                            ceil_mode=kwargs["pool_args"][3])
        blocks.append(pool)
        return blocks


def build_ssd_vgg_backbone(cfg, input_shape):
    """
    Create a VGG instance from config.

    Returns:
        VGG: a :class:`VGG` instance.
    """
    in_channels = input_shape.channels
    vgg_arch = cfg.MODEL.VGG.ARCH
    norm = cfg.MODEL.VGG.NORM
    num_classes = cfg.MODEL.VGG.NUM_CLASSES
    out_features = cfg.MODEL.VGG.OUT_FEATURES
    pool_args = cfg.MODEL.VGG.POOL_ARGS
    fc_to_conv = cfg.MODEL.VGG.FC_TO_CONV

    stage_args = []
    num_blocks_per_stage = {
        'A': (1, 1, 2, 2, 2),
        'B': (2, 2, 2, 2, 2),
        'D': (2, 2, 3, 3, 3),
        'E': (2, 2, 4, 4, 4)
    }[vgg_arch]

    for idx, num_blocks in enumerate(num_blocks_per_stage):
        out_channels = 64 * 2 ** idx if idx < 4 else 512
        stage_kargs = {
            "num_blocks": num_blocks,
            "in_channels": in_channels,
            "out_channels": out_channels,
            "norm": norm,
            # default: kernel_size=2, stride=2, pading=0, ceil_mode=False
            "pool_args": pool_args.get("pool{}".format(idx + 1), (2, 2, 0, False)),
        }
        in_channels = out_channels
        stage_args.append(stage_kargs)

    model = VGG(stage_args, num_classes, out_features, fc_to_conv)

    return model
```

#### cvpods/modeling/backbone/backbone.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

from abc import ABCMeta, abstractmethod

import torch.nn as nn

from cvpods.layers import ShapeSpec

__all__ = ["Backbone"]


class Backbone(nn.Module, metaclass=ABCMeta):
    """
    Abstract base class for network backbones.
    """
    def __init__(self):
        """
        The `__init__` method of any subclass can specify its own set of arguments.
        """
        super().__init__()

    @abstractmethod
    def forward(self):
        """
        Subclasses must override this method, but adhere to the same return type.

        Returns:
            dict[str->Tensor]: mapping from feature name (e.g., "res2") to tensor
        """
        pass

    @property
    def size_divisibility(self):
        """
        Some backbones require the input height and width to be divisible by a
        specific integer. This is typically true for encoder / decoder type networks
        with lateral connection (e.g., FPN) for which feature maps need to match
        dimension in the "bottom up" and "top down" paths. Set to 0 if no specific
        input size divisibility is required.
        """
        return 0

    def output_shape(self):
        """
        Returns:
            dict[str->ShapeSpec]
        """
        # this is a backward-compatible default
        return {
            name: ShapeSpec(channels=self._out_feature_channels[name],
                            stride=self._out_feature_strides[name])
            for name in self._out_features
        }
```

#### cvpods/modeling/backbone/splat.py

```python
"""Split-Attention"""

import torch
import torch.nn.functional as F
from torch.nn import Module, ReLU
from torch.nn.modules.utils import _pair

from cvpods.layers import Conv2d, get_norm

__all__ = ['SplAtConv2d']


class RFConv2d(object):
    def __init__(self, *args, **kwargs):
        raise NotImplementedError


class DropBlock2D(RFConv2d):
    pass


class SplAtConv2d(Module):
    """Split-Attention Conv2d
    """

    def __init__(
        self,
        in_channels,
        channels,
        kernel_size,
        stride=(1, 1),
        padding=(0, 0),
        dilation=(1, 1),
        groups=1,
        bias=True,
        radix=2,
        reduction_factor=4,
        rectify=False,
        rectify_avg=False,
        norm=None,
        dropblock_prob=0.0,
        **kwargs
    ):
        super().__init__()
        padding = _pair(padding)
        self.rectify = rectify and (padding[0] > 0 or padding[1] > 0)
        self.rectify_avg = rectify_avg
        inter_channels = max(in_channels * radix // reduction_factor, 32)
        self.radix = radix
        self.cardinality = groups
        self.channels = channels
        self.dropblock_prob = dropblock_prob
        if self.rectify:
            self.conv = RFConv2d(
                in_channels,
                channels * radix,
                kernel_size,
                stride,
                padding,
                dilation,
                groups=groups * radix,
                bias=bias,
                average_mode=rectify_avg,
                **kwargs
            )
        else:
            self.conv = Conv2d(
                in_channels,
                channels * radix,
                kernel_size,
                stride,
                padding,
                dilation,
                groups=groups * radix,
                bias=bias,
                **kwargs
            )
        self.use_bn = norm is not None
        self.bn0 = get_norm(norm, channels * radix)
        self.relu = ReLU(inplace=True)
        self.fc1 = Conv2d(channels, inter_channels, 1, groups=self.cardinality)
        self.bn1 = get_norm(norm, inter_channels)
        self.fc2 = Conv2d(inter_channels, channels * radix, 1, groups=self.cardinality)
        if dropblock_prob > 0.0:
            self.dropblock = DropBlock2D(dropblock_prob, 3)

    def forward(self, x):
        x = self.conv(x)
        if self.use_bn:
            x = self.bn0(x)
        if self.dropblock_prob > 0.0:
            x = self.dropblock(x)
        x = self.relu(x)

        batch, channel = x.shape[:2]
        if self.radix > 1:
            splited = torch.split(x, channel // self.radix, dim=1)
            gap = sum(splited)
        else:
            gap = x
        gap = F.adaptive_avg_pool2d(gap, 1)
        gap = self.fc1(gap)

        if self.use_bn:
            gap = self.bn1(gap)
        gap = self.relu(gap)

        atten = self.fc2(gap).view((batch, self.radix, self.channels))
        if self.radix > 1:
            atten = F.softmax(atten, dim=1).view(batch, -1, 1, 1)
        else:
            atten = F.sigmoid(atten, dim=1).view(batch, -1, 1, 1)

        if self.radix > 1:
            atten = torch.split(atten, channel // self.radix, dim=1)
            out = sum([att * split for (att, split) in zip(atten, splited)])
        else:
            out = atten * x
        return out.contiguous()
```

#### cvpods/modeling/backbone/__init__.py

```python
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

from .backbone import Backbone
from .bifpn import BiFPN, build_efficientnet_bifpn_backbone
from .darknet import Darknet, build_darknet_backbone
from .dynamic_arch import DynamicNetwork, build_dynamic_backbone
from .efficientnet import EfficientNet, build_efficientnet_backbone
from .fpn import (
    FPN,
    build_retinanet_mobilenetv2_fpn_p5_backbone,
    build_retinanet_resnet_fpn_p5_backbone
)
from .mobilenet import InvertedResBlock, MobileNetV2, MobileStem, build_mobilenetv2_backbone
from .resnet import ResNet, ResNetBlockBase, build_resnet_backbone, make_stage
# TODO can expose more resnet blocks after careful consideration
from .shufflenet import ShuffleNetV2, ShuffleV2Block, build_shufflenetv2_backbone
from .snet import SNet, build_snet_backbone
from .transformer import Transformer
```

#### cvpods/modeling/backbone/snet.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.

import numpy as np

import torch.nn as nn

from cvpods.layers import Conv2d, get_norm

from .shufflenet import ShuffleNetV2, ShuffleV2Block


class SNet(ShuffleNetV2):
    def __init__(
        self, in_channels, channels, num_classes=None, dropout=False, out_features=None,
        norm="BN"
    ):
        """
        See: https://arxiv.org/pdf/1903.11752.pdf

        Args:
            num_blocks (int): the number of blocks in this stage.
            in_channels (int): the input channel number.
            channels (int): output channel numbers for stem and every stages.
            num_classes (None or int): if None, will not perform classification.
            dropout (bool): whether to use dropout.
            out_features (list[str]): name of the layers whose outputs should
                be returned in forward. Can be anything in "stem", "linear", or "snet3" ...
                If None, will return the output of the last layer.
            norm (str or callable): a callable that takes the number of
                channels and return a `nn.Module`, or a pre-defined string
                (See cvpods.layer.get_norm for more details).
        """
        super(ShuffleNetV2, self).__init__()
        self.stage_out_channels = channels
        self.num_classes = num_classes

        # ---------------- Stem ---------------------- #
        input_channels = self.stage_out_channels[0]
        self.stem = nn.Sequential(*[
            Conv2d(
                in_channels, input_channels, kernel_size=3,
                stride=2, padding=1, bias=False,
                norm=get_norm(norm, input_channels),
                activation=nn.ReLU(inplace=True),
            ),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
        ])

        # TODO: use a stem class and property stride
        current_stride = 4
        self._out_feature_strides = {"stem": current_stride}
        self._out_feature_channels = {"stem": input_channels}

        # ---------------- Stages --------------------- #
        self.stage_num_blocks = [4, 8, 4]
        self.stages_and_names = []
        for i in range(len(self.stage_num_blocks)):
            num_blocks = self.stage_num_blocks[i]
            output_channels = self.stage_out_channels[i + 1]
            name = "snet" + str(i + 3)
            block_list = make_stage(num_blocks, input_channels, output_channels, norm)
            current_stride = current_stride * np.prod([block.stride for block in block_list])
            stages = nn.Sequential(*block_list)

            self._out_feature_strides[name] = current_stride
            self._out_feature_channels[name] = output_channels
            self.add_module(name, stages)
            self.stages_and_names.append((stages, name))
            input_channels = output_channels

        if len(self.stage_out_channels) == len(self.stage_num_blocks) + 2:
            name = "snet" + str(len(self.stage_num_blocks) + 2) + "-last"
            last_output_channels = self.stage_out_channels[-1]
            last_conv = Conv2d(
                output_channels, last_output_channels,
                kernel_size=1, bias=False,
                norm=get_norm(norm, last_output_channels),
                activation=nn.ReLU(inplace=True)
            )
            self._out_feature_strides[name] = current_stride
            self._out_feature_channels[name] = last_output_channels
            self.add_module(name, last_conv)
            self.stages_and_names.append((last_conv, name))
        # ---------------- Classifer ------------------- #
        if num_classes is not None:
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.dropout = dropout
            if dropout:
                self.dropout = nn.Dropout(0.2)
            self.classifier = nn.Linear(self.stage_out_channels[-1], num_classes, bias=False)
            name = "linear"

        self._out_features = [name] if out_features is None else out_features

        self._initialize_weights()


def make_stage(num_blocks, input_channels, output_channels, norm):
    """
    Create a snet stage by creating many blocks.

    Args:
        num_blocks (int): the number of blocks in this stage.
        input_channels (int): the input channel number.
        output_channels (int): the output channel number.
        norm (str or callable): a callable that takes the number of
            channels and return a `nn.Module`, or a pre-defined string
            (See cvpods.layer.get_norm for more details).

    Returns:
        list[nn.Module]: a list of block module.
    """
    blocks = []
    blocks.append(ShuffleV2Block(
        input_channels, output_channels, mid_channels=output_channels // 2,
        kernel_size=5, stride=2, norm=norm)
    )
    input_channels = output_channels
    for i in range(num_blocks - 1):
        blocks.append(ShuffleV2Block(
            input_channels // 2, output_channels, mid_channels=output_channels // 2,
            kernel_size=5, stride=1, norm=norm)
        )

    return blocks


def build_snet_backbone(cfg, input_shape):
    """
    Create a SNet instance from config.

    Returns:
        SNet: a :class:`SNet` instance.
    """
    channel_mapper = {
        49: [24, 60, 120, 240, 512],
        146: [24, 132, 264, 528],
        535: [48, 248, 496, 992],
    }
    model_depth = cfg.MODEL.SNET.DEPTH
    output_feautres = cfg.MODEL.SNET.OUT_FEATURES
    num_classes = cfg.MODEL.SNET.NUM_CLASSES
    norm = cfg.MODEL.SNET.NORM

    assert model_depth in channel_mapper, "Depth {} not supported.".format(model_depth)
    channels = channel_mapper[model_depth]

    model = SNet(
        input_shape.channels,
        channels,
        num_classes=num_classes,
        dropout=model_depth == 535,
        out_features=output_feautres,
        norm=norm,
    )
    model.freeze(cfg.MODEL.BACKBONE.FREEZE_AT)
    return model
```

#### cvpods/modeling/backbone/efficientnet.py

```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import math
import re
from copy import deepcopy
from easydict import EasyDict as edict

import torch
from torch import nn
from torch.nn import functional as F

from cvpods.layers import Conv2dSamePadding as Conv2d
from cvpods.layers import MemoryEfficientSwish, Swish, get_norm
from cvpods.modeling.backbone import Backbone


def round_filters(channels, global_params, skip=False):
    """
    Calculate and round number of channels based on depth multiplier.

    Args:
        channels (int): base number of channels.
        global_params (EasyDict): global args, see: class: `EfficientNet`.
        skip (bool): if True, do nothing and return the base number of channels.

    Returns:
        int: the number of channels calculated based on the depth multiplier.
    """
    multiplier = global_params.width_coefficient
    if skip or not multiplier:
        return channels
    divisor = global_params.depth_divisor
    min_depth = global_params.min_depth
    channels *= multiplier
    min_depth = min_depth or divisor
    new_channels = max(min_depth, int(channels + divisor / 2) // divisor * divisor)
    # prevent rounding by more than 10%
    if new_channels < 0.9 * channels:
        new_channels += divisor
    return int(new_channels)


def round_repeats(repeats, global_params):
    """
    Round number of repeats based on depth multiplier.

    Args:
        repeats (int): the number of `MBConvBlock` int the stage, see: class: `EfficientNet`.
        global_params (EasyDict): global args, see: class: `EfficientNet`.

    Returns:
        int: the number calculated based on the depth coefficient.
    """
    multiplier = global_params.depth_coefficient
    if not multiplier:
        return repeats
    return int(math.ceil(multiplier * repeats))


def drop_connect(inputs, p, training):
    """
    Drop connect.

    Args:
        inputs (Tensor): input tensor.
        p (float): between 0 to 1, drop connect rate.
        training (bool): whether it is training phase.
            if False, will skip drop connect op.

    Returns:
        output (Tensor): the result after drop connect.
    """
    if not training:
        return inputs
    batch_size = inputs.shape[0]
    keep_prob = 1 - p
    random_tensor = keep_prob
    random_tensor += torch.rand([batch_size, 1, 1, 1],
                                dtype=inputs.dtype, device=inputs.device)
    binary_tensor = torch.floor(random_tensor)
    output = inputs / keep_prob * binary_tensor
    return output


class MBConvBlock(nn.Module):
    """
    Mobile Inverted Residual Bottleneck Block.
    """

    def __init__(self, block_args, global_params):
        """
        Args:
            block_args (EasyDict): block args, see: class: `EfficientNet`.
            global_params (EasyDict): global args, see: class: `EfficientNet`.
        """
        super().__init__()
        self._block_args = block_args
        self.has_se = (block_args.se_ratio is not None) and (
            0 < block_args.se_ratio <= 1)
        self.id_skip = block_args.id_skip

        # Expansion phase
        # number of input channels
        inp = block_args.in_channels
        # number of output channels
        oup = block_args.in_channels * block_args.expand_ratio
        if block_args.expand_ratio != 1:
            self._expand_conv = Conv2d(
                in_channels=inp, out_channels=oup, kernel_size=1, padding=0, bias=False)
            self._bn0 = get_norm(global_params.norm, out_channels=oup)

        # Depthwise convolution phase
        k = block_args.kernel_size
        s = block_args.stride
        self._depthwise_conv = Conv2d(
            in_channels=oup, out_channels=oup, groups=oup,
            kernel_size=k, stride=s, padding="SAME", bias=False)
        self._bn1 = get_norm(global_params.norm, out_channels=oup)

        # Squeeze and Excitation layer, if desired
        if self.has_se:
            num_squeezed_channels = max(
                1, int(block_args.in_channels * block_args.se_ratio))
            self._se_reduce = Conv2d(
                in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1, padding=0)
            self._se_expand = Conv2d(
                in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1, padding=0)

        # Output phase
        final_oup = block_args.out_channels
        self._project_conv = Conv2d(
            in_channels=oup, out_channels=final_oup, kernel_size=1, padding=0, bias=False)
        self._bn2 = get_norm(global_params.norm, final_oup)
        self._swish = MemoryEfficientSwish()

    def forward(self, inputs, drop_connect_rate=None):
        """
        Args:
            inputs (Tensor): the input tensor.
            drop_connect_rate (float): float, between 0 to 1, drop connect rate.

        Returns:
            x (Tensor): Output of block.
        """
        # Expansion and Depthwise Convolution
        x = inputs
        if self._block_args.expand_ratio != 1:
            x = self._swish(self._bn0(self._expand_conv(inputs)))
        x = self._swish(self._bn1(self._depthwise_conv(x)))

        # Squeeze and Excitation
        if self.has_se:
            x_squeezed = F.adaptive_avg_pool2d(x, 1)
            x_squeezed = self._se_expand(self._swish(self._se_reduce(x_squeezed)))
            x = torch.sigmoid(x_squeezed) * x

        x = self._bn2(self._project_conv(x))

        # Skip connection and drop connect
        in_channels = self._block_args.in_channels
        out_channels = self._block_args.out_channels
        if self.id_skip and self._block_args.stride == 1 and in_channels == out_channels:
            if drop_connect_rate:
                x = drop_connect(x, p=drop_connect_rate, training=self.training)
            # skip connection
            x = x + inputs
        return x

    def set_swish(self, memory_efficient=True):
        """
        Sets swish function as memory efficient or standard.
        """
        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()


class EfficientNet(Backbone):
    """
    This module implements EfficientNet.
    See: https://arxiv.org/pdf/1905.11946.pdf for more details.
    """

    def __init__(self, in_channels=3, blocks_args=None, global_params=None,
                 out_features=None):
        """
        Args:
            in_channels (int): Number of input image channels.
            blocks_args (list[EasyDict]): a list of EasyDict to construct blocks.
                Each item in the list contains:

                * num_repeat: int, the number of `MBConvBlock` in the stage.
                * in_channels: int, the number of input tensor channels in the stage.
                * out_channels: int, the number of output tensor channels in the stage.
                * kernel_size: int, the kernel size of conv layer in the stage.
                * stride: int or list or tuple, the stride of conv layer in the stage.
                * expand_ratio: int, the channel expansion ratio at expansion phase
                    in `MBConvBlock`.
                * id_skip: bool, if `True`, apply skip connection in `MBConvBlock`
                    when stride is equal to 1 and the input and output channels are equal.
                * se_ratio: float, Squeeze layer channel reduction ratio in SE module,
                    between 0 and 1.

            global_params (namedtuple): a EasyDict contains global params shared between blocks.
                Which contains:

                * norm: str, the normalization to use.
                * bn_momentum: float, the `momentum` parameter of the norm module.
                * bn_eps: float, the `eps` parameter of the norm module.
                * dropout_rate: dropout rate.
                * num_classes: None or int: if None, will not perform classification.
                * width_coefficient: float, coefficient of width.
                * depth_coefficient: float, coefficient of depth.
                * depth_divisor: int, when calculating and rounding the number of channels
                    of each stage according to the depth coefficient, the number of channels
                    must be an integer multiple of "depth_divisor".
                * min_depth: int, the lower bound of the number of channels in each stage.
                * drop_connect_rate: float, between 0 to 1, drop connect rate.
                * image_size: int, input image size.

            out_features (list[str]): name of the layers whose outputs should be returned
                in forward. Can be anything in "stage1", "stage2", ..., "stage8" or "linear".
                If None, will return the output of the last layer.
        """
        super().__init__()
        assert isinstance(blocks_args, list), "blocks_args should be a list"
        assert len(blocks_args) > 0, "block_args must be greater than 0"
        self._size_divisibility = 0
        self._global_params = global_params
        self._blocks_args = blocks_args
        self._out_features = list()
        self._out_feature_strides = dict()
        self._out_feature_channels = dict()
        self.num_classes = global_params.num_classes

        # Stem
        # number of output channels
        out_channels = round_filters(32, global_params, skip=global_params.fix_head_stem)
        self._conv_stem = Conv2d(
            in_channels, out_channels, kernel_size=3, stride=2, padding="SAME", bias=False)
        self._bn0 = get_norm(global_params.norm, out_channels=out_channels)

        # Build blocks
        self._blocks = nn.ModuleList([])
        curr_stride = 2
        curr_block_idx = 0
        self.block_idx_to_name = dict()
        for stage_idx, block_args in enumerate(blocks_args):
            # Update block input and output filters based on depth multiplier.
            block_args.update(
                in_channels=round_filters(block_args.in_channels, global_params),
                out_channels=round_filters(block_args.out_channels, global_params),
                num_repeat=round_repeats(block_args.num_repeat, global_params)
            )

            name = "stage{}".format(stage_idx + 2)
            curr_stride *= block_args.stride
            self._out_feature_strides[name] = curr_stride
            self._out_feature_channels[name] = block_args.out_channels
            curr_block_idx += block_args.num_repeat
            self.block_idx_to_name[curr_block_idx - 1] = name

            # The first block needs to take care of stride and
            # filter size increase.
            self._blocks.append(MBConvBlock(block_args, global_params))
            if block_args.num_repeat > 1:
                next_block_args = deepcopy(block_args)
                next_block_args.update(in_channels=block_args.out_channels, stride=1)
            for _ in range(block_args.num_repeat - 1):
                self._blocks.append(MBConvBlock(next_block_args, global_params))

        # Head
        if self.num_classes is not None:
            in_channels = block_args.out_channels  # output of final block
            out_channels = round_filters(1280, global_params, skip=global_params.fix_head_stem)
            self._conv_head = Conv2d(
                in_channels, out_channels, kernel_size=1, padding=0, bias=False)
            self._bn1 = get_norm(global_params.norm, out_channels=out_channels)

            # Final linear layers
            self._avg_pooling = nn.AdaptiveAvgPool2d(1)
            self._dropout = nn.Dropout(global_params.dropout_rate)
            self._fc = nn.Linear(out_channels, global_params.num_classes)
            name = "linear"

        self._swish = MemoryEfficientSwish()

        if out_features is None:
            out_features = [name]
        self._out_features = out_features
        assert len(self._out_features)

        # init bn params
        bn_mom = global_params.bn_momentum
        bn_eps = global_params.bn_eps
        if bn_mom is not None and bn_eps is not None:
            for m in self.modules():
                if isinstance(m, nn.BatchNorm2d):
                    m.momentum = bn_mom
                    m.eps = bn_eps

    def set_swish(self, memory_efficient=True):
        """
        Sets swish function as memory efficient (for training) or standard.
        """
        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()
        for block in self._blocks:
            block.set_swish(memory_efficient)

    def forward(self, inputs):
        """
        Args:
            inputs (Tensor): The input tensor.

        Returns:
            outputs (dict[str->Tensor]):
                mapping from feature map name to feature map tensor in high to low resolution order,
                shape like (N, C, Hi, Wi).
                Noted that only the feature name in parameter `out_features` are returned.
        """
        outputs = dict()
        bs = inputs.size(0)

        # Stem
        x = self._swish(self._bn0(self._conv_stem(inputs)))

        # Blocks
        for idx, block in enumerate(self._blocks):
            drop_connect_rate = self._global_params.drop_connect_rate
            if drop_connect_rate:
                drop_connect_rate *= float(idx) / len(self._blocks)
            x = block(x, drop_connect_rate=drop_connect_rate)

            name = self.block_idx_to_name.get(idx, None)
            if name is not None and name in self._out_features:
                outputs[name] = x

        if self.num_classes is not None:
            # Head
            x = self._swish(self._bn1(self._conv_head(x)))
            # Pooling and final linear layer
            x = self._avg_pooling(x)
            x = x.view(bs, -1)
            x = self._dropout(x)
            x = self._fc(x)
            if "linear" in self._out_features:
                outputs["linear"] = x
        return outputs

    @property
    def size_divisibility(self):
        return self._size_divisibility

    @size_divisibility.setter
    def size_divisibility(self, size_divisibility):
        self._size_divisibility = size_divisibility


def check_model_name_is_valid(model_name):
    """
    Validates model name.
    Model name must be one of efficientnet-b0 ~ b7.
    """
    valid_models = ['efficientnet-b' + str(i) for i in range(9)]
    if model_name not in valid_models:
        raise ValueError('model_name should be one of: {}.'.format(', '.join(valid_models)))


def build_efficientnet_backbone(cfg, input_shape):
    """
    Create a EfficientNet instance from config.

    Returns:
        EfficientNet: a :class:`EfficientNet` instance.
    """
    in_channels = input_shape.channels
    model_name = cfg.MODEL.EFFICIENTNET.MODEL_NAME
    norm = cfg.MODEL.EFFICIENTNET.NORM
    bn_momentum = cfg.MODEL.EFFICIENTNET.BN_MOMENTUM
    bn_eps = cfg.MODEL.EFFICIENTNET.BN_EPS
    drop_connect_rate = cfg.MODEL.EFFICIENTNET.DROP_CONNECT_RATE
    depth_divisor = cfg.MODEL.EFFICIENTNET.DEPTH_DIVISOR
    min_depth = cfg.MODEL.EFFICIENTNET.MIN_DEPTH
    num_classes = cfg.MODEL.EFFICIENTNET.NUM_CLASSES
    fix_head_stem = cfg.MODEL.EFFICIENTNET.FIX_HEAD_STEAM
    memory_efficient = cfg.MODEL.EFFICIENTNET.MEMORY_EFFICIENT_SWISH
    out_features = cfg.MODEL.EFFICIENTNET.OUT_FEATURES

    # when norm is "" or None, set norm = "BN"
    if not norm:
        norm = "BN"

    check_model_name_is_valid(model_name)

    width_coefficient, depth_coefficient, image_size, dropout_rate = {
        # (width_coefficient, depth_coefficient, resolution, dropout_rate)
        'efficientnet-b0': (1.0, 1.0, 224, 0.2),
        'efficientnet-b1': (1.0, 1.1, 240, 0.2),
        'efficientnet-b2': (1.1, 1.2, 260, 0.3),
        'efficientnet-b3': (1.2, 1.4, 300, 0.3),
        'efficientnet-b4': (1.4, 1.8, 380, 0.4),
        'efficientnet-b5': (1.6, 2.2, 456, 0.4),
        'efficientnet-b6': (1.8, 2.6, 528, 0.5),
        'efficientnet-b7': (2.0, 3.1, 600, 0.5),
        'efficientnet-b8': (2.2, 3.6, 672, 0.5),
        'efficientnet-l2': (4.3, 5.3, 800, 0.5),
    }[model_name]

    global_params = edict(
        bn_momentum=bn_momentum,
        bn_eps=bn_eps,
        norm=norm,
        dropout_rate=dropout_rate,
        drop_connect_rate=drop_connect_rate,
        num_classes=num_classes,
        width_coefficient=width_coefficient,
        depth_coefficient=depth_coefficient,
        depth_divisor=depth_divisor,
        min_depth=min_depth,
        image_size=image_size,
        fix_head_stem=fix_head_stem,
    )

    block_args_str = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

    blocks_args = []
    for block_args_str_i in block_args_str:
        ops = block_args_str_i.split("_")
        options = {}
        for op in ops:
            splits = re.split(r'(\d.*)', op)
            if len(splits) >= 2:
                key, value = splits[:2]
                options[key] = value

        blocks_args_i = edict(
            num_repeat=int(options['r']),
            in_channels=int(options['i']),
            out_channels=int(options['o']),
            kernel_size=int(options['k']),
            stride=int(options['s'][0]),
            expand_ratio=int(options['e']),
            id_skip=('noskip' not in block_args_str_i),
            se_ratio=float(options['se']) if 'se' in options else None
        )
        blocks_args.append(blocks_args_i)
    model = EfficientNet(in_channels, blocks_args, global_params, out_features)
    model.set_swish(memory_efficient)
    return model
```

#### cvpods/modeling/backbone/darknet.py

```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from collections import OrderedDict

import numpy as np

import torch
from torch import nn
from torch.nn import Module
from torch.nn.modules.batchnorm import _BatchNorm

from cvpods.utils import PathManager

__all__ = ['Darknet', 'ResLayer']


def parse_darknet_conv_weights(module, weights, ptr):
    """
    Utility function to parse official darknet weights into torch.
    """
    conv_layer = module[0]
    try:
        batch_normalize = isinstance(module[1], _BatchNorm)
    except Exception:
        batch_normalize = False
    if batch_normalize:
        # Load BN bias, weights, running mean and running variance
        bn_layer = module[1]
        num_b = bn_layer.bias.numel()  # Number of biases
        # Bias
        bn_b = torch.from_numpy(
            weights[ptr:ptr + num_b]).view_as(bn_layer.bias)
        bn_layer.bias.data.copy_(bn_b)
        ptr += num_b
        # Weight
        bn_w = torch.from_numpy(
            weights[ptr:ptr + num_b]).view_as(bn_layer.weight)
        bn_layer.weight.data.copy_(bn_w)
        ptr += num_b
        # Running Mean
        bn_rm = torch.from_numpy(
            weights[ptr:ptr + num_b]).view_as(bn_layer.running_mean)
        bn_layer.running_mean.data.copy_(bn_rm)
        ptr += num_b
        # Running Var
        bn_rv = torch.from_numpy(
            weights[ptr:ptr + num_b]).view_as(bn_layer.running_var)
        bn_layer.running_var.data.copy_(bn_rv)
        ptr += num_b
    else:
        # Load conv. bias
        num_b = conv_layer.bias.numel()
        conv_b = torch.from_numpy(
            weights[ptr:ptr + num_b]).view_as(conv_layer.bias)
        conv_layer.bias.data.copy_(conv_b)
        ptr += num_b
    # Load conv. weights
    num_w = conv_layer.weight.numel()
    conv_w = torch.from_numpy(
        weights[ptr:ptr + num_w]).view_as(conv_layer.weight)
    conv_layer.weight.data.copy_(conv_w)
    ptr += num_w

    return ptr


def conv_bn_lrelu(ni: int, nf: int, ks: int = 3, stride: int = 1) -> nn.Sequential:
    "Create a seuence Conv2d->BatchNorm2d->LeakyReLu layer."
    return nn.Sequential(
        OrderedDict([
            ("conv", nn.Conv2d(ni, nf, kernel_size=ks, bias=False, stride=stride, padding=ks // 2)),
            ("bn", nn.BatchNorm2d(nf)),
            ("relu", nn.LeakyReLU(negative_slope=0.1, inplace=True)),
        ]))


class Flatten(Module):
    "Flatten `x` to a single dimension, often used at the end of a model. `full` for rank-1 tensor"

    def __init__(self, full: bool = False):
        super(Flatten, self).__init__()
        self.full = full

    def forward(self, x):
        return x.view(-1) if self.full else x.view(x.size(0), -1)


class ResLayer(Module):
    "Resnet style layer with `ni` inputs."

    def __init__(self, ni: int):
        super(ResLayer, self).__init__()
        self.layer1 = conv_bn_lrelu(ni, ni // 2, ks=1)
        self.layer2 = conv_bn_lrelu(ni // 2, ni, ks=3)

    def forward(self, x):
        out = self.layer2(self.layer1(x))
        return x + out


class Darknet(Module):
    "https://github.com/pjreddie/darknet"
    depth2blocks = {
        21: [1, 1, 2, 2, 1],
        53: [1, 2, 8, 8, 4],
    }

    def make_group_layer(self, ch_in: int, num_blocks: int, stride: int = 1):
        "starts with conv layer - `ch_in` channels in - then has `num_blocks` `ResLayer`"
        return [conv_bn_lrelu(ch_in, ch_in * 2, stride=stride)] \
            + [(ResLayer(ch_in * 2)) for i in range(num_blocks)]

    def __init__(self, depth, ch_in=3, nf=32, out_features=None, num_classes=None):
        """
        depth (int): depth of darknet used in model, usually use [21, 53] for this param
        ch_in (int): input channels, for example, ch_in of RGB image is 3
        nf (int): number of filters output in stem.
        out_features (List[str]): desired output layer name.
        num_classes (int): For ImageNet, num_classes is 1000. If None, no linear layer will be
            added.
        """
        super(Darknet, self).__init__()
        self.stem = conv_bn_lrelu(ch_in, nf, ks=3, stride=1)
        self.num_classes = num_classes

        current_stride = 1
        self._out_feature_strides = {"stem": current_stride}
        self._out_feature_channels = {"stem": nf}

        "create darknet with `nf` and `num_blocks` layers"
        self.stages_and_names = []
        num_blocks = Darknet.depth2blocks[depth]
        # out_idx = [0]
        # for nb in num_blocks:
        #     out_idx.append(out_idx[-1] + 1 + nb)
        # out_idx.pop(0)
        self._output_shape = []

        for i, nb in enumerate(num_blocks):
            stage = nn.Sequential(
                *self.make_group_layer(nf, nb, stride=2))
            name = 'dark' + str(i + 1)
            self.add_module(name, stage)
            self.stages_and_names.append((stage, name))
            self._out_feature_strides[name] = current_stride
            current_stride *= 2
            nf *= 2
            self._out_feature_channels[name] = nf
            self._output_shape.append(nf)
        if num_classes is not None:
            name = "linear"
            self.add_module(name, nn.Sequential([
                nn.AdaptiveAvgPool2d(1),
                Flatten(),
                nn.Linear(nf, num_classes)]))

        if out_features is None:
            out_features = [name]
        self._out_features = out_features
        assert(len(self._out_features))
        children = [x[0] for x in self.named_children()]
        for out_feature in self._out_features:
            assert out_feature in children, "Available children: {}".format(
                ", ".join(children))

    def forward(self, x):
        outputs = {}
        x = self.stem(x)
        if "stem" in self._out_features:
            outputs["stem"] = x
        for stage, name in self.stages_and_names:
            x = stage(x)
            if name in self._out_features:
                outputs[name] = x
        if self.num_classes is not None:
            x = self.linear(x)
            if "linear" in self._out_features:
                outputs["linear"] = x

        return outputs

    @property
    def output_shape(self):
        return self._output_shape

    def load_darknet_weights(self, weights):
        # Parses and loads the weights stored in 'weights'

        # Read weights file
        with open(weights, 'rb') as f:
            # Read Header https://github.com/AlexeyAB/darknet/issues/2914#issuecomment-496675346
            # (int32) version info: major, minor, revision
            self.version = np.fromfile(f, dtype=np.int32, count=3)
            # (int64) number of images seen during training
            self.seen = np.fromfile(f, dtype=np.int64, count=1)

            weights = np.fromfile(f, dtype=np.float32)  # the rest are weights

        ptr = 0
        for i, (mdef, module) in enumerate(self.named_children()):
            if mdef == "stem":
                ptr = parse_darknet_conv_weights(module, weights, ptr)
            elif mdef.startswith("dark"):
                for j, (sub_mdef, sub_module) in enumerate(module.named_children()):
                    if isinstance(sub_module, nn.Sequential):
                        ptr = parse_darknet_conv_weights(
                            sub_module, weights, ptr)
                    elif isinstance(sub_module, ResLayer):
                        for sub_sub_mdef, sub_sub_module in sub_module.named_children():
                            if isinstance(sub_sub_module, nn.Sequential):
                                ptr = parse_darknet_conv_weights(
                                    sub_sub_module, weights, ptr)


def build_darknet_backbone(cfg, input_shape):
    depth = cfg.MODEL.DARKNET.DEPTH
    stem_channels = cfg.MODEL.DARKNET.STEM_OUT_CHANNELS
    output_features = cfg.MODEL.DARKNET.OUT_FEATURES

    model = Darknet(depth, input_shape.channels,
                    stem_channels, output_features)
    filename = cfg.MODEL.DARKNET.WEIGHTS
    if filename.startswith("s3://"):
        with PathManager.open(filename, "rb") as f:
            state_dict = torch.load(f, map_location='cpu')
    model.load_state_dict(state_dict)

    return model


if __name__ == "__main__":
    model = Darknet(53, 32)
    print(model)
```

#### cvpods/modeling/backbone/mobilenet.py

```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved
from copy import deepcopy

import numpy as np

import torch.nn as nn

from cvpods.layers import Conv2d, FrozenBatchNorm2d, ShapeSpec, get_activation, get_norm
from cvpods.modeling.backbone import Backbone

__all__ = [
    "InvertedResBlock",
    "MobileStem",
    "MobileNetV2",
    "build_mobilenetv2_backbone",
]


class MobileStem(nn.Module):
    def __init__(self, input_channels, output_channels, norm, activation):
        """
        Args:
            input_channels (int): the input channel number.
            output_channels (int): the output channel number.
            norm (str or callable): a callable that takes the number of
                channels and return a `nn.Module`, or a pre-defined string
                (one of {"FrozenBN", "BN", "GN"}).
            activation (str): a pre-defined string
                (See cvpods.layer.get_activation for more details).
        """
        super().__init__()
        self.input_channels = input_channels
        self.output_channels = output_channels
        self.stride = 2

        self.conv = Conv2d(input_channels, output_channels, 3, stride=2, padding=1, bias=False,
                           norm=get_norm(norm, output_channels),
                           activation=get_activation(activation))

    def forward(self, x):
        return self.conv(x)

    def freeze(self):
        for p in self.parameters():
            p.requires_grad = False
        FrozenBatchNorm2d.convert_frozen_batchnorm(self)
        return self


class InvertedResBlock(nn.Module):
    def __init__(self, input_channels, output_channels, stride, expand_ratio,
                 norm, activation, use_shortcut=True):
        """
        Args:
            input_channels (int): the input channel number.
            output_channels (int): the output channel number.
            stride (int): the stride of the current block.
            expand_ratio(int): the channel expansion ratio for `mid_channels` in InvertedResBlock.
            norm (str or callable): a callable that takes the number of
                channels and return a `nn.Module`, or a pre-defined string
                (See cvpods.layer.get_norm for more details).
            activation (str): a pre-defined string
                (See cvpods.layer.get_activation for more details).
            use_shortcut (bool): whether to use the residual path.
        """
        super(InvertedResBlock, self).__init__()
        self.stride = stride
        assert stride in [1, 2]

        mid_channels = int(round(input_channels * expand_ratio))
        self.use_shortcut = use_shortcut

        if self.use_shortcut:
            assert stride == 1
            assert input_channels == output_channels

        conv_kwargs = {
            "norm": get_norm(norm, mid_channels),
            "activation": get_activation(activation)
        }

        layers = []
        if expand_ratio > 1:
            layers.append(
                Conv2d(input_channels, mid_channels, 1, bias=False,  # Pixel-wise non-linear
                       **deepcopy(conv_kwargs))
            )

        layers += [
            Conv2d(mid_channels, mid_channels, 3, padding=1, bias=False,  # Depth-wise 3x3
                   stride=stride, groups=mid_channels, **deepcopy(conv_kwargs)),
            Conv2d(mid_channels, output_channels, 1, bias=False,  # Pixel-wise linear
                   norm=get_norm(norm, output_channels))
        ]
        self.conv = nn.Sequential(*layers)

    def forward(self, x):
        if self.use_shortcut:
            return x + self.conv(x)
        else:
            return self.conv(x)


class MobileNetV2(Backbone):
    def __init__(
        self,
        stem,
        inverted_residual_setting,
        norm,
        activation,
        num_classes=None,
        out_features=None,
    ):
        """
        See: https://arxiv.org/pdf/1801.04381.pdf

        Args:
            stem (nn.Module): a stem module
            inverted_residual_setting(list of list): Network structure.
                (See https://arxiv.org/pdf/1801.04381.pdf Table 2)
            norm (str or callable): a callable that takes the number of
                channels and return a `nn.Module`, or a pre-defined string
                (See cvpods.layer.get_norm for more details).
            activation (str): a pre-defined string
                (See cvpods.layer.get_activation for more details).
            num_classes (None or int): if None, will not perform classification.
            out_features (list[str]): name of the layers whose outputs should
                be returned in forward. Can be anything in "stem", "linear", or "MobileNetV23" ...
                If None, will return the output of the last layer.
        """
        super(MobileNetV2, self).__init__()

        self.num_classes = num_classes

        # only check the first element, assuming user knows t,c,n,s are required
        if len(inverted_residual_setting[0]) != 4:
            raise ValueError("inverted_residual_setting should be a "
                             "4-element list, got {}".format(inverted_residual_setting))

        self.stem = stem
        self.last_channel = 1280

        input_channels = stem.output_channels

        current_stride = stem.stride
        self._out_feature_strides = {"stem": current_stride}
        self._out_feature_channels = {"stem": input_channels}

        # ---------------- Stages --------------------- #
        ext = 0
        self.stages_and_names = []
        for i, (t, c, n, s) in enumerate(inverted_residual_setting):
            # t: expand ratio
            # c: output channels
            # n: block number
            # s: stride
            # See https://arxiv.org/pdf/1801.04381.pdf Table 2 for more details
            if s == 1 and i > 0:
                ext += 1
            else:
                ext = 0

            current_stride *= s
            assert int(np.log2(current_stride)) == np.log2(current_stride)

            name = "mobile" + str(int(np.log2(current_stride)))
            if ext != 0:
                name += "-{}".format(ext + 1)

            stage = nn.Sequential(*make_stage(n, input_channels, c, s, t, norm, activation))

            self._out_feature_strides[name] = current_stride
            self._out_feature_channels[name] = c

            self.add_module(name, stage)
            self.stages_and_names.append((stage, name))
            input_channels = c

        name = "mobile" + str(int(np.log2(current_stride))) + "-last"
        stage = Conv2d(input_channels, self.last_channel, kernel_size=1, bias=False,
                       norm=get_norm("BN", self.last_channel),
                       activation=get_activation(activation))
        self.stages_and_names.append((stage, name))
        self.add_module(name, stage)

        self._out_feature_strides[name] = current_stride
        self._out_feature_channels[name] = self.last_channel

        # ---------------- Classifer ------------------- #
        if num_classes is not None:
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.dropout = nn.Dropout(0.2)
            self.classifier = nn.Linear(self.last_channel, num_classes)
            name = "linear"

        self._out_features = [name] if out_features is None else out_features

        self._initialize_weights()

    def forward(self, x):
        outputs = {}
        x = self.stem(x)
        if "stem" in self._out_features:
            outputs["stem"] = x
        for stages, name in self.stages_and_names:
            x = stages(x)
            if name in self._out_features:
                outputs[name] = x

        if self.num_classes is not None:
            x = self.avgpool(x)
            x = self.dropout(x)
            x = x.reshape(-1, self.last_channel)
            x = self.classifier(x)
            if "linear" in self._out_features:
                outputs["linear"] = x
        return outputs

    def output_shape(self):
        return {
            name: ShapeSpec(
                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]
            ) if name != 'linear' else
            ShapeSpec(
                channels=self.num_classes, height=1
            )
            for name in self._out_features
        }

    def freeze(self, freeze_at):
        if freeze_at >= 1:
            self.stem.freeze()
        for i, (stage, _) in enumerate(self.stages_and_names):
            if (i + 2) > freeze_at:
                break
            for p in stage.parameters():
                p.requires_grad = False
            FrozenBatchNorm2d.convert_frozen_batchnorm(stage)

    def _initialize_weights(self):
        for name, m in self.named_modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0001)
                nn.init.constant_(m.running_mean, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0001)
                nn.init.constant_(m.running_mean, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)


def make_stage(num_blocks, input_channels, output_channels, stride, expand_ratio, norm, activation):
    """
    Create a mobilenetv2 stage by creating many blocks.

    Args:
        num_blocks (int): the number of blocks in this stage.
        input_channels (int): the input channel number.
        output_channels (int): the output channel number.
        stride (int): the stride of the first block. The other blocks will have stride=1.
            A `stride` argument will be passed to the block constructor.
        expand_ratio(int): the channel expansion ratio for `mid_channels` in InvertedResBlock.
        norm (str or callable): a callable that takes the number of
            channels and return a `nn.Module`, or a pre-defined string
            (See cvpods.layer.get_norm for more details).
        activation (str): a pre-defined string
            (See cvpods.layer.get_activation for more details).

    Returns:
        list[nn.Module]: a list of block module.
    """
    blocks = []
    blocks.append(
        InvertedResBlock(input_channels, output_channels, stride=stride, expand_ratio=expand_ratio,
                         norm=norm, activation=activation, use_shortcut=False)
    )
    for i in range(num_blocks - 1):
        blocks.append(
            InvertedResBlock(output_channels, output_channels, stride=1, expand_ratio=expand_ratio,
                             norm=norm, activation=activation)
        )

    return blocks


def build_mobilenetv2_backbone(cfg, input_shape):
    """
    Create a MobileNetV2 instance from config.

    Returns:
        MobileNetV2: a :class:`MobileNetV2` instance.
    """
    stem = MobileStem(
        input_shape.channels,
        cfg.MODEL.MOBILENET.STEM_OUT_CHANNELS,
        cfg.MODEL.MOBILENET.NORM,
        cfg.MODEL.MOBILENET.ACTIVATION
    )

    model = MobileNetV2(
        stem,
        cfg.MODEL.MOBILENET.INVERTED_RESIDUAL_SETTING,
        cfg.MODEL.MOBILENET.NORM,
        cfg.MODEL.MOBILENET.ACTIVATION,
        cfg.MODEL.MOBILENET.NUM_CLASSES,
        cfg.MODEL.MOBILENET.OUT_FEATURES,
    )

    model.freeze(cfg.MODEL.BACKBONE.FREEZE_AT)
    return model
```

#### cvpods/modeling/backbone/shufflenet.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.

import numpy as np

import torch
import torch.nn as nn

from cvpods.layers import Conv2d, FrozenBatchNorm2d, ShapeSpec, get_norm
from cvpods.modeling.backbone import Backbone


class ShuffleV2Block(nn.Module):

    def __init__(
        self, input_channels, output_channels, mid_channels,
        kernel_size, stride, bias=False, norm="BN"
    ):
        """
        Args:
            input_channels (int): the input channel number.
            output_channels (int): the output channel number.
            mid_channels (int): the middle channel number.
            kernel_size (int): the kernel size in conv filters.
            stride (int): the stride of the current block.
            bias (bool): whether to have bias in conv.
            norm (str or callable): a callable that takes the number of
                channels and return a `nn.Module`, or a pre-defined string
                (See cvpods.layer.get_norm for more details).
        """
        super(ShuffleV2Block, self).__init__()
        assert stride in [1, 2]
        self.stride = stride
        padding = kernel_size // 2

        delta_channels = output_channels - input_channels
        branch_main = [
            # point-wise conv
            Conv2d(
                input_channels, mid_channels, kernel_size=1, bias=bias,
                norm=get_norm(norm, mid_channels),
                activation=nn.ReLU(inplace=True),
            ),
            # depth-wise conv
            Conv2d(
                mid_channels, mid_channels, kernel_size, stride,
                padding, groups=mid_channels, bias=bias,
                norm=get_norm(norm, mid_channels),
            ),
            # point-wise conv
            Conv2d(
                mid_channels, delta_channels, kernel_size=1, bias=bias,
                norm=get_norm(norm, delta_channels),
                activation=nn.ReLU(inplace=True),
            )
        ]
        self.branch_main = nn.Sequential(*branch_main)

        self.branch_proj = None

        if stride == 2:
            branch_proj = [
                # depth-wise conv
                Conv2d(
                    input_channels, input_channels, kernel_size, stride,
                    padding, groups=input_channels, bias=bias,
                    norm=get_norm(norm, input_channels)
                ),
                # point-wise conv
                Conv2d(
                    input_channels, input_channels, kernel_size=1, bias=bias,
                    norm=get_norm(norm, input_channels),
                    activation=nn.ReLU(inplace=True)
                )
            ]
            self.branch_proj = nn.Sequential(*branch_proj)

    def forward(self, x):
        if self.branch_proj is None:
            x_proj, x = self.channel_shuffle(x)
        else:
            x_proj = self.branch_proj(x)

        x = self.branch_main(x)
        return torch.cat([x_proj, x], dim=1)

    def channel_shuffle(self, x):
        N, C, H, W = x.shape
        assert C % 2 == 0, "number of channels must be divided by 2, got {}".format(C)
        # (N, C, H, W) -> (N, C/2, 2, H, W) -> (2, N, C/2, H, W)
        x = x.view(N, C // 2, 2, H, W).permute(2, 0, 1, 3, 4).contiguous()
        return x[0], x[1]

    def freeze(self):
        for p in self.parameters():
            p.requires_grad = False
        FrozenBatchNorm2d.convert_frozen_batchnorm(self)
        return self


class ShuffleNetV2(Backbone):

    def __init__(
        self, in_channels, channels, num_classes=None, dropout=False, out_features=None,
        norm="BN"
    ):
        """
        See: https://arxiv.org/pdf/1807.11164.pdf

        Args:
            num_blocks (int): the number of blocks in this stage.
            in_channels (int): the input channel number.
            channels (int): output channel numbers for stem and every stages.
            num_classes (None or int): if None, will not perform classification.
            dropout (bool): whether to use dropout.
            out_features (list[str]): name of the layers whose outputs should
                be returned in forward. Can be anything in "stem", "linear", or "shuffle3" ...
                If None, will return the output of the last layer.
            norm (str or callable): a callable that takes the number of
                channels and return a `nn.Module`, or a pre-defined string
                (See cvpods.layer.get_norm for more details).
        """
        super(ShuffleNetV2, self).__init__()

        self.stage_out_channels = channels
        self.num_classes = num_classes

        # ---------------- Stem ---------------------- #
        input_channels = self.stage_out_channels[0]
        self.stem = nn.Sequential(*[
            Conv2d(
                in_channels, input_channels, kernel_size=3,
                stride=2, padding=1, bias=False,
                norm=get_norm(norm, input_channels),
                activation=nn.ReLU(inplace=True),
            ),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
        ])

        # TODO: use a stem class and property stride
        current_stride = 4
        self._out_feature_strides = {"stem": current_stride}
        self._out_feature_channels = {"stem": input_channels}

        # ---------------- Stages --------------------- #
        self.stage_num_blocks = [4, 8, 4]
        self.stages_and_names = []
        for i in range(len(self.stage_num_blocks)):
            num_blocks = self.stage_num_blocks[i]
            output_channels = self.stage_out_channels[i + 1]
            name = "shuffle" + str(i + 3)
            block_list = make_stage(num_blocks, input_channels, output_channels, norm)
            current_stride = current_stride * np.prod([block.stride for block in block_list])
            stages = nn.Sequential(*block_list)

            self._out_feature_strides[name] = current_stride
            self._out_feature_channels[name] = output_channels
            self.add_module(name, stages)
            self.stages_and_names.append((stages, name))
            input_channels = output_channels

        name = "shuffle" + str(len(self.stage_num_blocks) + 2) + "-last"
        last_output_channels = self.stage_out_channels[-1]
        last_conv = Conv2d(
            output_channels, last_output_channels,
            kernel_size=1, bias=False,
            norm=get_norm(norm, last_output_channels),
            activation=nn.ReLU(inplace=True)
        )
        self._out_feature_strides[name] = current_stride
        self._out_feature_channels[name] = last_output_channels
        self.add_module(name, last_conv)
        self.stages_and_names.append((last_conv, name))

        # ---------------- Classifer ------------------- #
        if num_classes is not None:
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.dropout = dropout
            if dropout:
                self.dropout = nn.Dropout(0.2)
            self.classifier = nn.Linear(self.stage_out_channels[-1], num_classes, bias=False)
            name = "linear"

        self._out_features = [name] if out_features is None else out_features

        self._initialize_weights()

    def forward(self, x):
        outputs = {}
        x = self.stem(x)
        if "stem" in self._out_features:
            outputs["stem"] = x
        for stages, name in self.stages_and_names:
            x = stages(x)
            if name in self._out_features:
                outputs[name] = x

        if self.num_classes is not None:
            x = self.avgpool(x)
            if self.dropout:
                x = self.dropout(x)
            x = x.reshape(-1, self.stage_out_channels[-1])
            x = self.classifier(x)
            if "linear" in self._out_features:
                outputs["linear"] = x
        return outputs

    def output_shape(self):
        """
        Returns:
            dict[str->ShapeSpec]
        """
        return {
            name: ShapeSpec(
                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]
            )
            for name in self._out_features
        }

    def freeze(self, freeze_at):
        """
        Args:
            freeze_at (int): freeze the stem and the first `freeze_at - 1` stages.
        """
        if freeze_at >= 1:
            for p in self.stem.parameters():
                p.requires_grad = False
            FrozenBatchNorm2d.convert_frozen_batchnorm(self.stem)

        for i in range(freeze_at - 1):
            FrozenBatchNorm2d.convert_frozen_batchnorm(self.stages_and_names[i][0])

    def _initialize_weights(self):
        for name, m in self.named_modules():
            if isinstance(m, nn.Conv2d):
                if 'first' in name:
                    nn.init.normal_(m.weight, 0, 0.01)
                else:
                    nn.init.normal_(m.weight, 0, 1.0 / m.weight.shape[1])
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0001)
                nn.init.constant_(m.running_mean, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0001)
                nn.init.constant_(m.running_mean, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)


def make_stage(num_blocks, input_channels, output_channels, norm):
    """
    Create a shufflenetv2 stage by creating many blocks.

    Args:
        num_blocks (int): the number of blocks in this stage.
        input_channels (int): the input channel number.
        output_channels (int): the output channel number.
        norm (str or callable): a callable that takes the number of
            channels and return a `nn.Module`, or a pre-defined string
            (See cvpods.layer.get_norm for more details).

    Returns:
        list[nn.Module]: a list of block module.
    """
    blocks = []
    blocks.append(ShuffleV2Block(
        input_channels, output_channels, mid_channels=output_channels // 2,
        kernel_size=3, stride=2, norm=norm)
    )
    input_channels = output_channels
    for i in range(num_blocks - 1):
        blocks.append(ShuffleV2Block(
            input_channels // 2, output_channels, mid_channels=output_channels // 2,
            kernel_size=3, stride=1, norm=norm)
        )

    return blocks


def build_shufflenetv2_backbone(cfg, input_shape):
    """
    Create a ShuffleNetV2 instance from config.

    Returns:
        ShuffleNetV2: a :class:`ShuffleNetV2` instance.
    """
    channel_mapper = {
        "0.5x": [24, 48, 96, 192, 1024],
        "1.0x": [24, 116, 232, 464, 1024],
        "1.5x": [24, 176, 352, 704, 1024],
        "2.0x": [24, 244, 488, 976, 2048],
    }
    model_size = cfg.MODEL.SHUFFLENET.MODEL_SIZE
    output_feautres = cfg.MODEL.SHUFFLENET.OUT_FEATURES
    num_classes = cfg.MODEL.SHUFFLENET.NUM_CLASSES
    norm = cfg.MODEL.SHUFFLENET.NORM

    assert model_size in channel_mapper, "Model size {} not supported.".format(model_size)
    channels = channel_mapper[model_size]

    model = ShuffleNetV2(
        input_shape.channels,
        channels,
        num_classes=num_classes,
        dropout=model_size == "2.0",
        out_features=output_feautres,
        norm=norm,
    )
    model.freeze(cfg.MODEL.BACKBONE.FREEZE_AT)
    return model
```

#### cvpods/modeling/backbone/resnet.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import logging

import numpy as np

import torch
from torch import nn

from cvpods.layers import (
    Conv2d,
    DeformConv,
    FrozenBatchNorm2d,
    ModulatedDeformConv,
    ShapeSpec,
    get_activation,
    get_norm
)
from cvpods.modeling.nn_utils import weight_init

from .backbone import Backbone

__all__ = [
    "ResNetBlockBase",
    "BottleneckBlock",
    "AVDBottleneckBlock",
    "DeformBottleneckBlock",
    "BasicStem",
    "ResNet",
    "make_stage",
    "build_resnet_backbone",
]


class ResNetBlockBase(nn.Module):
    def __init__(self, in_channels, out_channels, stride):
        """
        The `__init__` method of any subclass should also contain these arguments.

        Args:
            in_channels (int):
            out_channels (int):
            stride (int):
        """
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride

    def freeze(self):
        for p in self.parameters():
            p.requires_grad = False
        FrozenBatchNorm2d.convert_frozen_batchnorm(self)
        return self


class BasicBlock(ResNetBlockBase):
    def __init__(self, in_channels, out_channels, *, stride=1, norm="BN", activation=None,
                 **kwargs):
        """
        The standard block type for ResNet18 and ResNet34.
        Args:
            in_channels (int): Number of input channels.
            out_channels (int): Number of output channels.
            stride (int): Stride for the first conv.
            norm (str or callable): A callable that takes the number of
                channels and returns a `nn.Module`, or a pre-defined string
                (one of {"FrozenBN", "BN", "GN"}).
        """
        super().__init__(in_channels, out_channels, stride)

        if in_channels != out_channels:
            self.shortcut = Conv2d(
                in_channels,
                out_channels,
                kernel_size=1,
                stride=stride,
                bias=False,
                norm=get_norm(norm, out_channels),
            )
        else:
            self.shortcut = None

        self.activation = get_activation(activation)

        self.conv1 = Conv2d(
            in_channels,
            out_channels,
            kernel_size=3,
            stride=stride,
            padding=1,
            bias=False,
            norm=get_norm(norm, out_channels),
        )

        self.conv2 = Conv2d(
            out_channels,
            out_channels,
            kernel_size=3,
            stride=1,
            padding=1,
            bias=False,
            norm=get_norm(norm, out_channels),
        )

        for layer in [self.conv1, self.conv2, self.shortcut]:
            if layer is not None:  # shortcut can be None
                weight_init.c2_msra_fill(layer)

    def forward(self, x):
        out = self.conv1(x)
        out = self.activation(out)
        out = self.conv2(out)

        if self.shortcut is not None:
            shortcut = self.shortcut(x)
        else:
            shortcut = x

        out += shortcut
        out = self.activation(out)
        return out


class BottleneckBlock(ResNetBlockBase):
    def __init__(
        self,
        in_channels,
        out_channels,
        *,
        bottleneck_channels,
        stride=1,
        num_groups=1,
        norm="BN",
        activation=None,
        stride_in_1x1=False,
        dilation=1,
    ):
        """
        Args:
            norm (str or callable): a callable that takes the number of
                channels and return a `nn.Module`, or a pre-defined string
                (one of {"FrozenBN", "BN", "GN"}).
            stride_in_1x1 (bool): when stride==2, whether to put stride in the
                first 1x1 convolution or the bottleneck 3x3 convolution.
        """
        super().__init__(in_channels, out_channels, stride)

        if in_channels != out_channels:
            self.shortcut = Conv2d(
                in_channels,
                out_channels,
                kernel_size=1,
                stride=stride,
                bias=False,
                norm=get_norm(norm, out_channels),
            )
        else:
            self.shortcut = None

        # The original MSRA ResNet models have stride in the first 1x1 conv
        # The subsequent fb.torch.resnet and Caffe2 ResNe[X]t implementations have
        # stride in the 3x3 conv
        stride_1x1, stride_3x3 = (stride, 1) if stride_in_1x1 else (1, stride)

        self.activation = get_activation(activation)

        self.conv1 = Conv2d(
            in_channels,
            bottleneck_channels,
            kernel_size=1,
            stride=stride_1x1,
            bias=False,
            norm=get_norm(norm, bottleneck_channels),
        )

        self.conv2 = Conv2d(
            bottleneck_channels,
            bottleneck_channels,
            kernel_size=3,
            stride=stride_3x3,
            padding=1 * dilation,
            bias=False,
            groups=num_groups,
            dilation=dilation,
            norm=get_norm(norm, bottleneck_channels),
        )

        self.conv3 = Conv2d(
            bottleneck_channels,
            out_channels,
            kernel_size=1,
            bias=False,
            norm=get_norm(norm, out_channels),
        )

        for layer in [self.conv1, self.conv2, self.conv3, self.shortcut]:
            if layer is not None:  # shortcut can be None
                weight_init.c2_msra_fill(layer)

        # Zero-initialize the last normalization in each residual branch,
        # so that at the beginning, the residual branch starts with zeros,
        # and each residual block behaves like an identity.
        # See Sec 5.1 in "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour":
        # "For BN layers, the learnable scaling coefficient γ is initialized
        # to be 1, except for each residual block's last BN
        # where γ is initialized to be 0."

        # nn.init.constant_(self.conv3.norm.weight, 0)
        # TODO this somehow hurts performance when training GN models from scratch.
        # Add it as an option when we need to use this code to train a backbone.

    def forward(self, x):
        out = self.conv1(x)
        out = self.activation(out)

        out = self.conv2(out)
        out = self.activation(out)

        out = self.conv3(out)

        if self.shortcut is not None:
            shortcut = self.shortcut(x)
        else:
            shortcut = x

        out += shortcut
        out = self.activation(out)

        return out


class AVDBottleneckBlock(BottleneckBlock):
    def __init__(
        self,
        in_channels,
        out_channels,
        *,
        bottleneck_channels,
        stride=1,
        num_groups=1,
        norm="BN",
        activation=None,
        stride_in_1x1=False,
        dilation=1,
        avd=False,
        avg_down=False,
        radix=1,
        bottleneck_width=64,
    ):
        super().__init__(
            in_channels=in_channels,
            out_channels=out_channels,
            bottleneck_channels=bottleneck_channels,
            stride=stride,
            num_groups=num_groups,
            norm=norm,
            activation=activation,
            stride_in_1x1=stride_in_1x1,
            dilation=dilation
        )

        self.avd = avd and (stride > 1)
        self.avg_down = avg_down
        self.radix = radix

        cardinality = num_groups
        group_width = int(bottleneck_channels * (bottleneck_width / 64.)) * cardinality

        # The original MSRA ResNet models have stride in the first 1x1 conv
        # The subsequent fb.torch.resnet and Caffe2 ResNe[X]t implementations have
        # stride in the 3x3 conv
        stride_1x1, stride_3x3 = (stride, 1) if stride_in_1x1 else (1, stride)

        if in_channels != out_channels and self.avg_down:
            assert self.shortcut is not None

            self.shortcut_avgpool = nn.AvgPool2d(kernel_size=stride, stride=stride,
                                                 ceil_mode=True, count_include_pad=False)
            self.shortcut = Conv2d(
                in_channels,
                out_channels,
                kernel_size=1,
                stride=1,
                bias=False,
                norm=get_norm(norm, out_channels),
            )

        if self.radix > 1:
            from .splat import SplAtConv2d
            self.conv2 = SplAtConv2d(
                group_width,
                group_width,
                kernel_size=3,
                stride=1 if self.avd else stride_3x3,
                padding=dilation, dilation=dilation,
                groups=cardinality,
                bias=False,
                radix=self.radix,
                norm=norm,
            )
        else:
            assert hasattr(self, "conv2")

        if self.avd:
            self.avd_layer = nn.AvgPool2d(3, stride, padding=1)

        if self.radix > 1:
            for layer in [self.conv1, self.conv3, self.shortcut]:
                if layer is not None:  # shortcut can be None
                    weight_init.c2_msra_fill(layer)
        else:
            for layer in [self.conv1, self.conv2, self.conv3, self.shortcut]:
                if layer is not None:  # shortcut can be None
                    weight_init.c2_msra_fill(layer)

    def forward(self, x):
        out = self.conv1(x)
        out = self.activation(out)

        if self.radix > 1:
            out = self.conv2(out)
        else:
            out = self.conv2(out)
            out = self.activation(out)

        if self.avd:
            out = self.avd_layer(out)

        out = self.conv3(out)

        if self.shortcut is not None:
            if self.avg_down:
                x = self.shortcut_avgpool(x)
            shortcut = self.shortcut(x)
        else:
            shortcut = x

        out += shortcut
        out = self.activation(out)

        return out


class DeformBottleneckBlock(ResNetBlockBase):
    def __init__(
        self,
        in_channels,
        out_channels,
        *,
        bottleneck_channels,
        stride=1,
        num_groups=1,
        norm="BN",
        activation=None,
        stride_in_1x1=False,
        dilation=1,
        deform_modulated=False,
        deform_num_groups=1,
    ):
        """
        Similar to :class:`BottleneckBlock`, but with deformable conv in the 3x3 convolution.
        """
        super().__init__(in_channels, out_channels, stride)
        self.deform_modulated = deform_modulated

        if in_channels != out_channels:
            self.shortcut = Conv2d(
                in_channels,
                out_channels,
                kernel_size=1,
                stride=stride,
                bias=False,
                norm=get_norm(norm, out_channels),
            )
        else:
            self.shortcut = None

        stride_1x1, stride_3x3 = (stride, 1) if stride_in_1x1 else (1, stride)

        self.activation = get_activation(activation)

        self.conv1 = Conv2d(
            in_channels,
            bottleneck_channels,
            kernel_size=1,
            stride=stride_1x1,
            bias=False,
            norm=get_norm(norm, bottleneck_channels),
        )

        if deform_modulated:
            deform_conv_op = ModulatedDeformConv
            # offset channels are 2 or 3 (if with modulated) * kernel_size * kernel_size
            offset_channels = 27
        else:
            deform_conv_op = DeformConv
            offset_channels = 18

        self.conv2_offset = Conv2d(
            bottleneck_channels,
            offset_channels * deform_num_groups,
            kernel_size=3,
            stride=stride_3x3,
            padding=1 * dilation,
            dilation=dilation,
        )
        self.conv2 = deform_conv_op(
            bottleneck_channels,
            bottleneck_channels,
            kernel_size=3,
            stride=stride_3x3,
            padding=1 * dilation,
            bias=False,
            groups=num_groups,
            dilation=dilation,
            deformable_groups=deform_num_groups,
            norm=get_norm(norm, bottleneck_channels),
        )

        self.conv3 = Conv2d(
            bottleneck_channels,
            out_channels,
            kernel_size=1,
            bias=False,
            norm=get_norm(norm, out_channels),
        )

        for layer in [self.conv1, self.conv2, self.conv3, self.shortcut]:
            if layer is not None:  # shortcut can be None
                weight_init.c2_msra_fill(layer)

        nn.init.constant_(self.conv2_offset.weight, 0)
        nn.init.constant_(self.conv2_offset.bias, 0)

    def forward(self, x):
        out = self.conv1(x)
        out = self.activation(out)

        if self.deform_modulated:
            offset_mask = self.conv2_offset(out)
            offset_x, offset_y, mask = torch.chunk(offset_mask, 3, dim=1)
            offset = torch.cat((offset_x, offset_y), dim=1)
            mask = mask.sigmoid()
            out = self.conv2(out, offset, mask)
        else:
            offset = self.conv2_offset(out)
            out = self.conv2(out, offset)
        out = self.activation(out)

        out = self.conv3(out)

        if self.shortcut is not None:
            shortcut = self.shortcut(x)
        else:
            shortcut = x

        out += shortcut
        out = self.activation(out)
        return out


def make_stage(block_class, num_blocks, first_stride, **kwargs):
    """
    Create a resnet stage by creating many blocks.

    Args:
        block_class (class): a subclass of ResNetBlockBase
        num_blocks (int):
        first_stride (int): the stride of the first block. The other blocks will have stride=1.
            A `stride` argument will be passed to the block constructor.
        kwargs: other arguments passed to the block constructor.

    Returns:
        list[nn.Module]: a list of block module.
    """
    blocks = []
    for i in range(num_blocks):
        blocks.append(block_class(stride=first_stride if i == 0 else 1, **kwargs))
        kwargs["in_channels"] = kwargs["out_channels"]
    return blocks


class BasicStem(nn.Module):
    def __init__(self, in_channels=3, out_channels=64, norm="BN", activation=None,
                 deep_stem=False, stem_width=32):
        """
        Args:
            norm (str or callable): a callable that takes the number of
                channels and return a `nn.Module`, or a pre-defined string
                (one of {"FrozenBN", "BN", "GN"}).
        """
        super().__init__()
        self.deep_stem = deep_stem

        if self.deep_stem:
            self.conv1_1 = Conv2d(
                3,
                stem_width,
                kernel_size=3,
                stride=2,
                padding=1,
                bias=False,
                norm=get_norm(norm, stem_width),
            )
            self.conv1_2 = Conv2d(
                stem_width,
                stem_width,
                kernel_size=3,
                stride=1,
                padding=1,
                bias=False,
                norm=get_norm(norm, stem_width),
            )
            self.conv1_3 = Conv2d(
                stem_width,
                stem_width * 2,
                kernel_size=3,
                stride=1,
                padding=1,
                bias=False,
                norm=get_norm(norm, stem_width * 2),
            )
            for layer in [self.conv1_1, self.conv1_2, self.conv1_3]:
                if layer is not None:
                    weight_init.c2_msra_fill(layer)
        else:
            self.conv1 = Conv2d(
                in_channels,
                out_channels,
                kernel_size=7,
                stride=2,
                padding=3,
                bias=False,
                norm=get_norm(norm, out_channels),
            )
            weight_init.c2_msra_fill(self.conv1)

        self.activation = get_activation(activation)
        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

    def forward(self, x):
        if self.deep_stem:
            x = self.conv1_1(x)
            x = self.activation(x)
            x = self.conv1_2(x)
            x = self.activation(x)
            x = self.conv1_3(x)
            x = self.activation(x)
        else:
            x = self.conv1(x)
            x = self.activation(x)
        x = self.max_pool(x)
        return x

    @property
    def out_channels(self):
        if self.deep_stem:
            return self.conv1_3.out_channels
        else:
            return self.conv1.out_channels

    @property
    def stride(self):
        return 4  # = stride 2 conv -> stride 2 max pool


class ResNet(Backbone):
    def __init__(self, stem, stages, num_classes=None, out_features=None, zero_init_residual=False):
        """
        Args:
            stem (nn.Module): a stem module
            stages (list[list[ResNetBlock]]): several (typically 4) stages,
                each contains multiple :class:`ResNetBlockBase`.
            num_classes (None or int): if None, will not perform classification.
            out_features (list[str]): name of the layers whose outputs should
                be returned in forward. Can be anything in "stem", "linear", or "res2" ...
                If None, will return the output of the last layer.
        """
        super(ResNet, self).__init__()
        self.stem = stem
        self.num_classes = num_classes

        current_stride = self.stem.stride
        self._out_feature_strides = {"stem": current_stride}
        self._out_feature_channels = {"stem": self.stem.out_channels}

        self.stages_and_names = []
        for i, blocks in enumerate(stages):
            for block in blocks:
                assert isinstance(block, ResNetBlockBase), block
                curr_channels = block.out_channels
            stage = nn.Sequential(*blocks)
            name = "res" + str(i + 2)
            self.add_module(name, stage)
            self.stages_and_names.append((stage, name))
            self._out_feature_strides[name] = current_stride = int(
                current_stride * np.prod([k.stride for k in blocks])
            )
            self._out_feature_channels[name] = blocks[-1].out_channels

        if num_classes is not None:
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.linear = nn.Linear(curr_channels, num_classes)

            # Sec 5.1 in "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour":
            # "The 1000-way fully-connected layer is initialized by
            # drawing weights from a zero-mean Gaussian with standard deviation of 0.01."
            nn.init.normal_(self.linear.weight, std=0.01)
            name = "linear"

        if out_features is None:
            out_features = [name]
        self._out_features = out_features
        assert len(self._out_features)
        children = [x[0] for x in self.named_children()]
        for out_feature in self._out_features:
            assert out_feature in children, "Available children: {}".format(", ".join(children))

        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, BottleneckBlock):
                    nn.init.constant_(m.conv3.norm.weight, 0)
                elif isinstance(m, BasicBlock):
                    nn.init.constant_(m.conv2.norm.weight, 0)

    def forward(self, x):
        outputs = {}
        x = self.stem(x)
        if "stem" in self._out_features:
            outputs["stem"] = x
        for stage, name in self.stages_and_names:
            x = stage(x)
            if name in self._out_features:
                outputs[name] = x
        if self.num_classes is not None:
            x = self.avgpool(x)
            x = torch.flatten(x, 1)
            x = self.linear(x)
            if "linear" in self._out_features:
                outputs["linear"] = x
        return outputs

    def output_shape(self):
        return {
            name: ShapeSpec(
                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]
            )
            for name in self._out_features
        }


def build_resnet_backbone(cfg, input_shape):
    """
    Create a ResNet instance from config.

    Returns:
        ResNet: a :class:`ResNet` instance.
    """
    depth = cfg.MODEL.RESNETS.DEPTH
    stem_width = {18: 32, 34: 32, 50: 32, 101: 64, 152: 64, 200: 64, 269: 64}[depth]
    deep_stem = cfg.MODEL.RESNETS.DEEP_STEM

    if not deep_stem:
        assert getattr(cfg.MODEL.RESNETS, "RADIX", 1) <= 1, \
            "cfg.MODEL.RESNETS.RADIX: {} > 1".format(cfg.MODEL.RESNETS.RADIX)

    # need registration of new blocks/stems?
    norm = cfg.MODEL.RESNETS.NORM
    activation = cfg.MODEL.RESNETS.ACTIVATION
    stem = BasicStem(
        in_channels=input_shape.channels,
        out_channels=cfg.MODEL.RESNETS.STEM_OUT_CHANNELS,
        norm=norm,
        activation=activation,
        deep_stem=deep_stem,
        stem_width=stem_width,
    )
    freeze_at = cfg.MODEL.BACKBONE.FREEZE_AT

    if freeze_at >= 1:
        for p in stem.parameters():
            p.requires_grad = False
        stem = FrozenBatchNorm2d.convert_frozen_batchnorm(stem)

    # fmt: off
    out_features = cfg.MODEL.RESNETS.OUT_FEATURES
    num_groups = cfg.MODEL.RESNETS.NUM_GROUPS
    width_per_group = cfg.MODEL.RESNETS.WIDTH_PER_GROUP
    bottleneck_channels = num_groups * width_per_group
    in_channels = cfg.MODEL.RESNETS.STEM_OUT_CHANNELS
    out_channels = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS
    stride_in_1x1 = cfg.MODEL.RESNETS.STRIDE_IN_1X1
    res5_dilation = cfg.MODEL.RESNETS.RES5_DILATION
    num_classes = cfg.MODEL.RESNETS.NUM_CLASSES
    zero_init_residual = cfg.MODEL.RESNETS.ZERO_INIT_RESIDUAL
    # fmt: on
    assert res5_dilation in {1, 2}, "res5_dilation cannot be {}.".format(res5_dilation)

    num_blocks_per_stage = {
        18: [2, 2, 2, 2],
        34: [3, 4, 6, 3],
        50: [3, 4, 6, 3],
        101: [3, 4, 23, 3],
        152: [3, 8, 36, 3],
        200: [3, 24, 36, 3],
        269: [3, 30, 48, 8],
    }[depth]

    # Avoid creating variables without gradients
    # which consume extra memory and may cause allreduce to fail
    out_stage_idx = [
        {"res2": 2, "res3": 3, "res4": 4, "res5": 5, "linear": 5}[f] for f in out_features]
    max_stage_idx = max(out_stage_idx)
    # Apply Deformable Convolution in stages
    # Specify if apply deform_conv on Res2, Res3, Res4, Res5
    deform_on_per_stage = getattr(cfg.MODEL.RESNETS,
                                  "DEFORM_ON_PER_STAGE",
                                  [False] * (max_stage_idx - 1))

    if depth in [18, 34]:
        assert out_channels == 64, "Must set MODEL.RESNETS.RES2_OUT_CHANNELS = 64 for R18/R34"
        assert not any(
            deform_on_per_stage
        ), "MODEL.RESNETS.DEFORM_ON_PER_STAGE unsupported for R18/R34"
        assert res5_dilation == 1, "Must set MODEL.RESNETS.RES5_DILATION = 1 for R18/R34"
        assert num_groups == 1, "Must set MODEL.RESNETS.NUM_GROUPS = 1 for R18/R34"

    stages = []

    logger = logging.getLogger(__name__)
    # See cvpods/configs/base_detection_config.py for details
    if not stride_in_1x1 and "torchvision" not in cfg.MODEL.WEIGHTS:
        logger.warning(
            "Using pretrain weight not from torchvision with MODEL.RESNETS.STRIDE_IN_1X1 == False"
        )
    elif stride_in_1x1 and "torchvision" in cfg.MODEL.WEIGHTS and cfg.MODEL.WEIGHTS:
        logger.warning(
            "Using pretrain weight from torchvision with MODEL.RESNETS.STRIDE_IN_1X1 == True"
        )

    in_channels = 2 * stem_width if deep_stem else in_channels
    for idx, stage_idx in enumerate(range(2, max_stage_idx + 1)):
        dilation = res5_dilation if stage_idx == 5 else 1
        first_stride = 1 if idx == 0 or (stage_idx == 5 and dilation == 2) else 2
        stage_kargs = {
            "num_blocks": num_blocks_per_stage[idx],
            "first_stride": first_stride,
            "in_channels": in_channels,
            "out_channels": out_channels,
            "norm": norm,
            "activation": activation,
        }
        # Use BasicBlock for R18 and R34.
        if depth in [18, 34]:
            stage_kargs["block_class"] = BasicBlock
        else:
            stage_kargs["bottleneck_channels"] = bottleneck_channels
            stage_kargs["stride_in_1x1"] = stride_in_1x1
            stage_kargs["dilation"] = dilation
            stage_kargs["num_groups"] = num_groups
            if deform_on_per_stage[idx]:
                stage_kargs["block_class"] = DeformBottleneckBlock
                # Use True to use modulated deform_conv (DeformableV2);
                # Use False for DeformableV1.
                stage_kargs["deform_modulated"] = cfg.MODEL.RESNETS.DEFORM_MODULATED
                # Number of groups in deformable conv.
                stage_kargs["deform_num_groups"] = cfg.MODEL.RESNETS.DEFORM_NUM_GROUPS
            elif hasattr(cfg.MODEL.RESNETS, "RADIX"):
                # Radix in ResNeSt
                radix = cfg.MODEL.RESNETS.RADIX
                # Apply avg after conv2 in the BottleBlock
                # When AVD=True, the STRIDE_IN_1X1 should be False
                avd = cfg.MODEL.RESNETS.AVD or (radix > 1)
                # Apply avg_down to the downsampling layer for residual path
                avg_down = cfg.MODEL.RESNETS.AVG_DOWN or (radix > 1)
                # Bottleneck_width in ResNeSt
                bottleneck_width = cfg.MODEL.RESNETS.BOTTLENECK_WIDTH

                stage_kargs["block_class"] = AVDBottleneckBlock
                stage_kargs["avd"] = avd
                stage_kargs["avg_down"] = avg_down
                stage_kargs["radix"] = radix
                stage_kargs["bottleneck_width"] = bottleneck_width
            else:
                stage_kargs["block_class"] = BottleneckBlock
        blocks = make_stage(**stage_kargs)
        in_channels = out_channels
        out_channels *= 2
        bottleneck_channels *= 2

        if freeze_at >= stage_idx:
            for block in blocks:
                block.freeze()
        stages.append(blocks)

    return ResNet(stem,
                  stages,
                  num_classes=num_classes,
                  out_features=out_features,
                  zero_init_residual=zero_init_residual)
```

#### cvpods/modeling/backbone/transformer.py

```python
"""
DETR Transformer class.
Copy-paste from torch.nn.Transformer with modifications:
    * positional encodings are passed in MHattention
    * extra LN at the end of encoder is removed
    * decoder returns a stack of activations from all decoding layers
"""
import copy
from typing import Optional

import torch
import torch.nn.functional as F
from torch import Tensor, nn


class Transformer(nn.Module):
    """
    Transformer structure for DETR
    """
    def __init__(self, cfg):
        super(Transformer, self).__init__()

        d_model = cfg.MODEL.DETR.TRANSFORMER.D_MODEL
        nhead = cfg.MODEL.DETR.TRANSFORMER.N_HEAD
        num_encoder_layers = cfg.MODEL.DETR.TRANSFORMER.NUM_ENC_LAYERS
        num_decoder_layers = cfg.MODEL.DETR.TRANSFORMER.NUM_DEC_LAYERS
        dim_feedforward = cfg.MODEL.DETR.TRANSFORMER.DIM_FFN
        dropout = cfg.MODEL.DETR.TRANSFORMER.DROPOUT_RATE
        activation = cfg.MODEL.DETR.TRANSFORMER.ACTIVATION
        normalize_before = cfg.MODEL.DETR.TRANSFORMER.PRE_NORM
        return_intermediate_dec = cfg.MODEL.DETR.TRANSFORMER.RETURN_INTERMEDIATE_DEC

        encoder_layer = TransformerEncoderLayer(
            d_model, nhead, dim_feedforward, dropout, activation, normalize_before
        )
        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None
        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)

        decoder_layer = TransformerDecoderLayer(
            d_model, nhead, dim_feedforward, dropout, activation, normalize_before
        )
        decoder_norm = nn.LayerNorm(d_model)
        self.decoder = TransformerDecoder(
            decoder_layer,
            num_decoder_layers,
            decoder_norm,
            return_intermediate=return_intermediate_dec,
        )

        self._reset_parameters()

        self.d_model = d_model
        self.nhead = nhead

    def _reset_parameters(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def forward(self, src, mask, query_embed, pos_embed):
        # flatten NxCxHxW to HWxNxC
        bs, c, h, w = src.shape
        src = src.flatten(2).permute(2, 0, 1)
        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)
        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)
        mask = mask.flatten(1)

        tgt = torch.zeros_like(query_embed)
        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)
        hs = self.decoder(
            tgt, memory, memory_key_padding_mask=mask, pos=pos_embed, query_pos=query_embed
        )
        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)


class TransformerEncoder(nn.Module):
    def __init__(self, encoder_layer, num_layers, norm=None):
        super().__init__()
        self.layers = _get_clones(encoder_layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm

    def forward(
        self,
        src,
        mask: Optional[Tensor] = None,
        src_key_padding_mask: Optional[Tensor] = None,
        pos: Optional[Tensor] = None,
    ):
        output = src

        for layer in self.layers:
            output = layer(
                output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, pos=pos
            )

        if self.norm is not None:
            output = self.norm(output)

        return output


class TransformerDecoder(nn.Module):
    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):
        super().__init__()
        self.layers = _get_clones(decoder_layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm
        self.return_intermediate = return_intermediate

    def forward(
        self,
        tgt,
        memory,
        tgt_mask: Optional[Tensor] = None,
        memory_mask: Optional[Tensor] = None,
        tgt_key_padding_mask: Optional[Tensor] = None,
        memory_key_padding_mask: Optional[Tensor] = None,
        pos: Optional[Tensor] = None,
        query_pos: Optional[Tensor] = None,
    ):
        output = tgt

        intermediate = []

        for layer in self.layers:
            output = layer(
                output,
                memory,
                tgt_mask=tgt_mask,
                memory_mask=memory_mask,
                tgt_key_padding_mask=tgt_key_padding_mask,
                memory_key_padding_mask=memory_key_padding_mask,
                pos=pos,
                query_pos=query_pos,
            )
            if self.return_intermediate:
                intermediate.append(self.norm(output))

        if self.norm is not None:
            output = self.norm(output)
            if self.return_intermediate:
                intermediate.pop()
                intermediate.append(output)

        if self.return_intermediate:
            return torch.stack(intermediate)

        return output


class TransformerEncoderLayer(nn.Module):
    def __init__(
        self,
        d_model,
        nhead,
        dim_feedforward=2048,
        dropout=0.1,
        activation="relu",
        normalize_before=False,
    ):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        # Implementation of Feedforward model
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

        self.activation = _get_activation_fn(activation)
        self.normalize_before = normalize_before

    def with_pos_embed(self, tensor, pos: Optional[Tensor]):
        return tensor if pos is None else tensor + pos

    def forward_post(
        self,
        src,
        src_mask: Optional[Tensor] = None,
        src_key_padding_mask: Optional[Tensor] = None,
        pos: Optional[Tensor] = None,
    ):
        q = k = self.with_pos_embed(src, pos)
        src2 = self.self_attn(
            q, k, value=src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask
        )[0]
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = src + self.dropout2(src2)
        src = self.norm2(src)
        return src

    def forward_pre(
        self,
        src,
        src_mask: Optional[Tensor] = None,
        src_key_padding_mask: Optional[Tensor] = None,
        pos: Optional[Tensor] = None,
    ):
        src2 = self.norm1(src)
        q = k = self.with_pos_embed(src2, pos)
        src2 = self.self_attn(
            q, k, value=src2, attn_mask=src_mask, key_padding_mask=src_key_padding_mask
        )[0]
        src = src + self.dropout1(src2)
        src2 = self.norm2(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))
        src = src + self.dropout2(src2)
        return src

    def forward(
        self,
        src,
        src_mask: Optional[Tensor] = None,
        src_key_padding_mask: Optional[Tensor] = None,
        pos: Optional[Tensor] = None,
    ):
        if self.normalize_before:
            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)
        return self.forward_post(src, src_mask, src_key_padding_mask, pos)


class TransformerDecoderLayer(nn.Module):
    def __init__(
        self,
        d_model,
        nhead,
        dim_feedforward=2048,
        dropout=0.1,
        activation="relu",
        normalize_before=False,
    ):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        # Implementation of Feedforward model
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

        self.activation = _get_activation_fn(activation)
        self.normalize_before = normalize_before

    def with_pos_embed(self, tensor, pos: Optional[Tensor]):
        return tensor if pos is None else tensor + pos

    def forward_post(
        self,
        tgt,
        memory,
        tgt_mask: Optional[Tensor] = None,
        memory_mask: Optional[Tensor] = None,
        tgt_key_padding_mask: Optional[Tensor] = None,
        memory_key_padding_mask: Optional[Tensor] = None,
        pos: Optional[Tensor] = None,
        query_pos: Optional[Tensor] = None,
    ):
        q = k = self.with_pos_embed(tgt, query_pos)
        tgt2 = self.self_attn(
            q, k, value=tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask
        )[0]
        tgt = tgt + self.dropout1(tgt2)
        tgt = self.norm1(tgt)
        tgt2 = self.multihead_attn(
            query=self.with_pos_embed(tgt, query_pos),
            key=self.with_pos_embed(memory, pos),
            value=memory,
            attn_mask=memory_mask,
            key_padding_mask=memory_key_padding_mask,
        )[0]
        tgt = tgt + self.dropout2(tgt2)
        tgt = self.norm2(tgt)
        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
        tgt = tgt + self.dropout3(tgt2)
        tgt = self.norm3(tgt)
        return tgt

    def forward_pre(
        self,
        tgt,
        memory,
        tgt_mask: Optional[Tensor] = None,
        memory_mask: Optional[Tensor] = None,
        tgt_key_padding_mask: Optional[Tensor] = None,
        memory_key_padding_mask: Optional[Tensor] = None,
        pos: Optional[Tensor] = None,
        query_pos: Optional[Tensor] = None,
    ):
        tgt2 = self.norm1(tgt)
        q = k = self.with_pos_embed(tgt2, query_pos)
        tgt2 = self.self_attn(
            q, k, value=tgt2, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask
        )[0]
        tgt = tgt + self.dropout1(tgt2)
        tgt2 = self.norm2(tgt)
        tgt2 = self.multihead_attn(
            query=self.with_pos_embed(tgt2, query_pos),
            key=self.with_pos_embed(memory, pos),
            value=memory,
            attn_mask=memory_mask,
            key_padding_mask=memory_key_padding_mask,
        )[0]
        tgt = tgt + self.dropout2(tgt2)
        tgt2 = self.norm3(tgt)
        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))
        tgt = tgt + self.dropout3(tgt2)
        return tgt

    def forward(
        self,
        tgt,
        memory,
        tgt_mask: Optional[Tensor] = None,
        memory_mask: Optional[Tensor] = None,
        tgt_key_padding_mask: Optional[Tensor] = None,
        memory_key_padding_mask: Optional[Tensor] = None,
        pos: Optional[Tensor] = None,
        query_pos: Optional[Tensor] = None,
    ):
        if self.normalize_before:
            return self.forward_pre(
                tgt,
                memory,
                tgt_mask,
                memory_mask,
                tgt_key_padding_mask,
                memory_key_padding_mask,
                pos,
                query_pos,
            )
        return self.forward_post(
            tgt,
            memory,
            tgt_mask,
            memory_mask,
            tgt_key_padding_mask,
            memory_key_padding_mask,
            pos,
            query_pos,
        )


def _get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])


def _get_activation_fn(activation):
    """
    Return an activation function given a string
    # TODO: move to layers
    """
    if activation == "relu":
        return F.relu
    if activation == "gelu":
        return F.gelu
    if activation == "glu":
        return F.glu
    raise RuntimeError(f"activation should be relu/gelu, not {activation}.")
```

##### cvpods/modeling/backbone/dynamic_arch/cal_op_flops.py

```python
# Count Operation MFLOPs when fix batch to 1
# @author: yanwei.li


def count_Conv_flop(
    in_h, in_w, in_channel, out_channel,
    kernel_size, is_bias=False, stride=1, groups=1
):
    out_h = in_h // stride
    out_w = in_w // stride
    bias_ops = 1 if is_bias else 0
    kernel_ops = kernel_size[0] * kernel_size[1] * (in_channel // groups)
    delta_ops = (kernel_ops + bias_ops) * out_channel * out_h * out_w
    return delta_ops / 1e6


def count_Linear_flop(in_num, out_num, is_bias):
    weight_ops = in_num * out_num
    bias_ops = out_num if is_bias else 0
    delta_ops = weight_ops + bias_ops
    return delta_ops / 1e6


def count_BN_flop(in_h, in_w, in_channel, is_affine):
    multi_affine = 2 if is_affine else 1
    delta_ops = multi_affine * in_h * in_w * in_channel
    return delta_ops / 1e6


def count_ReLU_flop(in_h, in_w, in_channel):
    delta_ops = in_h * in_w * in_channel
    return delta_ops / 1e6


def count_Pool2d_flop(in_h, in_w, out_channel, kernel_size, stride):
    out_h = in_h // stride
    out_w = in_w // stride
    kernel_ops = kernel_size[0] * kernel_size[1]
    delta_ops = kernel_ops * out_w * out_h * out_channel
    return delta_ops / 1e6


def count_ConvBNReLU_flop(
    in_h, in_w, in_channel, out_channel,
    kernel_size, is_bias=False, stride=1,
    groups=1, is_affine=True
):
    flops = 0.0
    flops += count_Conv_flop(
        in_h, in_w, in_channel, out_channel,
        kernel_size, is_bias, stride, groups
    )
    in_h = in_h // stride
    in_w = in_w // stride
    flops += count_BN_flop(in_h, in_w, out_channel, is_affine)
    flops += count_ReLU_flop(in_h, in_w, out_channel)
    return flops
```

##### cvpods/modeling/backbone/dynamic_arch/op_with_flops.py

```python
# pylint: disable=W0613

# Routinng candidates for dynamic networks with calculated FLOPs,
# modified Search Space in DARTS to have different input and output channels.
# @author: yanwei.li
import torch
import torch.nn as nn

from cvpods.layers import Conv2d, get_norm
from cvpods.modeling.backbone.dynamic_arch import cal_op_flops as flops
from cvpods.modeling.nn_utils import weight_init

OPS = {
    'none': lambda C_in, C_out, stride, norm_layer, affine, input_size:
        Zero(stride=stride),
    'avg_pool_3x3': lambda C_in, C_out, stride, norm_layer, affine, input_size:
        AvgPool2d(
            C_in, C_out, 3, stride=stride, padding=1, input_size=input_size
        ),
    'max_pool_3x3': lambda C_in, C_out, stride, norm_layer, affine, input_size:
        MaxPool2d(
            C_in, C_out, 3, stride=stride, padding=1, input_size=input_size
        ),
    'skip_connect': lambda C_in, C_out, stride, norm_layer, affine, input_size:
        Identity(C_in, C_out, norm_layer=norm_layer, affine=affine, input_size=input_size)
        if stride == 1 else
        FactorizedReduce(C_in, C_out, norm_layer=norm_layer, affine=affine, input_size=input_size),
    'sep_conv_3x3': lambda C_in, C_out, stride, norm_layer, affine, input_size:
        SepConv(
            C_in, C_out, 3, stride, 1, norm_layer=norm_layer,
            affine=affine, input_size=input_size
        ),
    'sep_conv_5x5': lambda C_in, C_out, stride, norm_layer, affine, input_size:
        SepConv(
            C_in, C_out, 5, stride, 2, norm_layer=norm_layer,
            affine=affine, input_size=input_size
        ),
    'sep_conv_7x7': lambda C_in, C_out, stride, norm_layer, affine, input_size:
        SepConv(
            C_in, C_out, 7, stride, 3, norm_layer=norm_layer,
            affine=affine, input_size=input_size
        ),
    'sep_conv_heavy_3x3': lambda C_in, C_out, stride, norm_layer, affine, input_size:
        SepConvHeavy(
            C_in, C_out, 3, stride, 1, norm_layer=norm_layer,
            affine=affine, input_size=input_size
        ),
    'sep_conv_heavy_5x5': lambda C_in, C_out, stride, norm_layer, affine, input_size:
        SepConvHeavy(
            C_in, C_out, 5, stride, 2, norm_layer=norm_layer,
            affine=affine, input_size=input_size
        ),
    'dil_conv_3x3': lambda C_in, C_out, stride, norm_layer, affine, input_size:
        DilConv(
            C_in, C_out, 3, stride, 2, 2, norm_layer=norm_layer,
            affine=affine, input_size=input_size
        ),
    'dil_conv_5x5': lambda C_in, C_out, stride, norm_layer, affine, input_size:
        DilConv(
            C_in, C_out, 5, stride, 4, 2, norm_layer=norm_layer,
            affine=affine, input_size=input_size
        ),
    'conv_3x3_basic': lambda C_in, C_out, stride, norm_layer, affine, input_size:
        BasicResBlock(
            C_in, C_out, 3, stride, 1, norm_layer=norm_layer,
            affine=affine, input_size=input_size
        ),
    'Bottleneck_3x3': lambda C_in, C_out, stride, norm_layer, affine, input_size:
        Bottleneck(
            C_in, C_out, 3, stride, 1, norm_layer=norm_layer,
            affine=affine, input_size=input_size
        ),
    'Bottleneck_5x5': lambda C_in, C_out, stride, norm_layer, affine, input_size:
        Bottleneck(
            C_in, C_out, 5, stride, 2, norm_layer=norm_layer,
            affine=affine, input_size=input_size
        ),
    'MBConv_3x3': lambda C_in, C_out, stride, norm_layer, affine, input_size:
        MBConv(
            C_in, C_out, 3, stride, 1, norm_layer=norm_layer,
            affine=affine, input_size=input_size
        ),
    'MBConv_5x5': lambda C_in, C_out, stride, norm_layer, affine, input_size:
        MBConv(
            C_in, C_out, 5, stride, 2, norm_layer=norm_layer,
            affine=affine, input_size=input_size
        ),
}


class BasicResBlock(nn.Module):

    def __init__(
        self, C_in, C_out, kernel_size, stride, padding,
        norm_layer, affine=True, input_size=None
    ):
        super(BasicResBlock, self).__init__()
        self.op = Conv2d(
            C_in, C_out, kernel_size, stride=stride, padding=padding,
            bias=False, norm=get_norm(norm_layer, C_out)
        )
        self.flops = self.get_flop(
            [kernel_size, kernel_size], stride, C_in, C_out,
            affine, input_size[0], input_size[1]
        )
        # using Kaiming init
        for m in self.op.modules():
            if isinstance(m, nn.Conv2d):
                weight_init.kaiming_init(m, mode='fan_in')
            elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):
                if m.weight is not None:
                    nn.init.constant_(m.weight, 1)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def get_flop(
        self, kernel_size, stride, in_channel,
        out_channel, affine, in_h, in_w
    ):
        cal_flop = flops.count_Conv_flop(
            in_h, in_w, in_channel, out_channel, kernel_size, False, stride
        )
        in_h, in_w = in_h // stride, in_w // stride
        cal_flop += flops.count_BN_flop(in_h, in_w, out_channel, affine)
        return cal_flop

    def forward(self, x):
        return self.op(x)


class DilConv(nn.Module):

    def __init__(
        self, C_in, C_out, kernel_size, stride, padding,
        dilation, norm_layer, affine=True, input_size=None
    ):
        super(DilConv, self).__init__()
        self.op = nn.Sequential(
            Conv2d(
                C_in, C_in, kernel_size=kernel_size, stride=stride,
                padding=padding, dilation=dilation, groups=C_in, bias=False
            ),
            Conv2d(
                C_in, C_out, kernel_size=1, padding=0, bias=False,
                norm=get_norm(norm_layer, C_out)
            )
        )
        self.flops = self.get_flop(
            [kernel_size, kernel_size], stride, C_in, C_out,
            affine, input_size[0], input_size[1]
        )
        # using Kaiming init
        for m in self.op.modules():
            if isinstance(m, nn.Conv2d):
                weight_init.kaiming_init(m, mode='fan_in')
            elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):
                if m.weight is not None:
                    nn.init.constant_(m.weight, 1)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def get_flop(self, kernel_size, stride, in_channel, out_channel, affine, in_h, in_w):
        cal_flop = flops.count_Conv_flop(
            in_h, in_w, in_channel, in_channel, kernel_size,
            False, stride, groups=in_channel
        )
        in_h, in_w = in_h // stride, in_w // stride
        cal_flop += flops.count_Conv_flop(
            in_h, in_w, in_channel, out_channel,
            kernel_size=[1, 1], is_bias=False
        )
        cal_flop += flops.count_BN_flop(in_h, in_w, out_channel, affine)
        return cal_flop

    def forward(self, x):
        return self.op(x)


class SepConv(nn.Module):
    def __init__(
        self, C_in, C_out, kernel_size, stride, padding,
        norm_layer, affine=True, input_size=None
    ):
        super(SepConv, self).__init__()
        self.op = nn.Sequential(
            # depth wise
            Conv2d(
                C_in, C_in, kernel_size=kernel_size, stride=stride,
                padding=padding, groups=C_in, bias=False
            ),
            # point wise
            Conv2d(
                C_in, C_in, kernel_size=1, padding=0, bias=False,
                norm=get_norm(norm_layer, C_in),
                activation=nn.ReLU()
            ),
            # stack 2 separate depthwise-conv.
            Conv2d(
                C_in, C_in, kernel_size=kernel_size, stride=1,
                padding=padding, groups=C_in, bias=False
            ),
            Conv2d(
                C_in, C_out, kernel_size=1, padding=0, bias=False,
                norm=get_norm(norm_layer, C_out)
            )
        )
        self.flops = self.get_flop(
            [kernel_size, kernel_size], stride, C_in, C_out,
            affine, input_size[0], input_size[1]
        )
        # using Kaiming init
        for m in self.op.modules():
            if isinstance(m, nn.Conv2d):
                weight_init.kaiming_init(m, mode='fan_in')
            elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):
                if m.weight is not None:
                    nn.init.constant_(m.weight, 1)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def get_flop(self, kernel_size, stride, in_channel, out_channel, affine, in_h, in_w):
        cal_flop = flops.count_Conv_flop(
            in_h, in_w, in_channel, in_channel, kernel_size,
            False, stride, groups=in_channel
        )
        in_h, in_w = in_h // stride, in_w // stride
        cal_flop += flops.count_ConvBNReLU_flop(
            in_h, in_w, in_channel, in_channel,
            kernel_size=[1, 1], is_bias=False,
            is_affine=affine
        )
        # stack 2 separate depthwise-conv.
        cal_flop += flops.count_Conv_flop(
            in_h, in_w, in_channel, in_channel,
            kernel_size, False, stride=1, groups=in_channel
        )
        cal_flop += flops.count_Conv_flop(
            in_h, in_w, in_channel, out_channel,
            kernel_size=[1, 1], is_bias=False
        )
        cal_flop += flops.count_BN_flop(in_h, in_w, out_channel, affine)
        return cal_flop

    def forward(self, x):
        return self.op(x)


# Heavy SepConv, stack 3 SepConv
class SepConvHeavy(nn.Module):
    def __init__(
        self, C_in, C_out, kernel_size, stride, padding,
        norm_layer, affine=True, input_size=None
    ):
        super(SepConvHeavy, self).__init__()
        self.op = nn.Sequential(
            # depth wise
            Conv2d(
                C_in, C_in, kernel_size=kernel_size,
                stride=stride, padding=padding,
                groups=C_in, bias=False
            ),
            # point wise
            Conv2d(
                C_in, C_in, kernel_size=1, padding=0,
                bias=False, norm=get_norm(norm_layer, C_in),
                activation=nn.ReLU()
            ),
            # stack 2 separate depthwise-conv.
            Conv2d(
                C_in, C_in, kernel_size=kernel_size, stride=1,
                padding=padding, groups=C_in, bias=False
            ),
            Conv2d(
                C_in, C_in, kernel_size=1, padding=0, bias=False,
                norm=get_norm(norm_layer, C_in),
                activation=nn.ReLU()
            ),
            # stack 3 separate depthwise-conv.
            Conv2d(
                C_in, C_in, kernel_size=kernel_size, stride=1,
                padding=padding, groups=C_in, bias=False
            ),
            Conv2d(
                C_in, C_out, kernel_size=1, padding=0, bias=False,
                norm=get_norm(norm_layer, C_out)
            )
        )
        self.flops = self.get_flop(
            [kernel_size, kernel_size], stride, C_in, C_out,
            affine, input_size[0], input_size[1]
        )
        # using Kaiming init
        for m in self.op.modules():
            if isinstance(m, nn.Conv2d):
                weight_init.kaiming_init(m, mode='fan_in')
            elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):
                if m.weight is not None:
                    nn.init.constant_(m.weight, 1)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def get_flop(self, kernel_size, stride, in_channel, out_channel, affine, in_h, in_w):
        cal_flop = flops.count_Conv_flop(
            in_h, in_w, in_channel, in_channel, kernel_size,
            False, stride, groups=in_channel
        )
        in_h, in_w = in_h // stride, in_w // stride
        cal_flop += flops.count_ConvBNReLU_flop(
            in_h, in_w, in_channel, in_channel,
            kernel_size=[1, 1], is_bias=False, is_affine=affine
        )
        # stack 2 separate depthwise-conv.
        cal_flop += flops.count_Conv_flop(
            in_h, in_w, in_channel, in_channel,
            kernel_size, False, stride=1, groups=in_channel
        )
        cal_flop += flops.count_ConvBNReLU_flop(
            in_h, in_w, in_channel, in_channel,
            kernel_size=[1, 1], is_bias=False, is_affine=affine
        )
        # stack 3 separate depthwise-conv.
        cal_flop += flops.count_Conv_flop(
            in_h, in_w, in_channel, in_channel,
            kernel_size, False, stride=1, groups=in_channel
        )
        cal_flop += flops.count_Conv_flop(
            in_h, in_w, in_channel, out_channel,
            kernel_size=[1, 1], is_bias=False
        )
        cal_flop += flops.count_BN_flop(in_h, in_w, out_channel, affine)
        return cal_flop

    def forward(self, x):
        return self.op(x)


# using Bottleneck from ResNet
class Bottleneck(nn.Module):
    def __init__(
        self, C_in, C_out, kernel_size, stride, padding,
        norm_layer, expansion=4, affine=True, input_size=None
    ):
        super(Bottleneck, self).__init__()
        self.hidden_dim = C_in // expansion
        self.op = nn.Sequential(
            Conv2d(
                C_in, self.hidden_dim, kernel_size=1, padding=0, bias=False,
                norm=get_norm(norm_layer, self.hidden_dim),
                activation=nn.ReLU()
            ),
            Conv2d(
                self.hidden_dim, self.hidden_dim, kernel_size=kernel_size,
                stride=stride, padding=padding, bias=False,
                norm=get_norm(norm_layer, self.hidden_dim),
                activation=nn.ReLU()
            ),
            Conv2d(
                self.hidden_dim, C_out, kernel_size=1, padding=0, bias=False,
                norm=get_norm(norm_layer, C_out)
            )
        )
        self.flops = self.get_flop(
            [kernel_size, kernel_size], stride, C_in,
            C_out, affine, input_size[0], input_size[1]
        )
        # using Kaiming init
        for m in self.op.modules():
            if isinstance(m, nn.Conv2d):
                weight_init.kaiming_init(m, mode='fan_in')
            elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):
                if m.weight is not None:
                    nn.init.constant_(m.weight, 1)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def get_flop(self, kernel_size, stride, in_channel, out_channel, affine, in_h, in_w):
        cal_flop = flops.count_ConvBNReLU_flop(
            in_h, in_w, in_channel, self.hidden_dim,
            [1, 1], False, is_affine=affine
        )
        cal_flop += flops.count_ConvBNReLU_flop(
            in_h, in_w, self.hidden_dim, self.hidden_dim,
            kernel_size, False, stride=stride, is_affine=affine
        )
        in_h, in_w = in_h // stride, in_w // stride
        cal_flop += flops.count_Conv_flop(
            in_h, in_w, self.hidden_dim, out_channel,
            kernel_size=[1, 1], is_bias=False
        )
        cal_flop += flops.count_BN_flop(in_h, in_w, out_channel, affine)
        return cal_flop

    def forward(self, x):
        return self.op(x)


# using MBConv from MobileNet V2
class MBConv(nn.Module):
    def __init__(
        self, C_in, C_out, kernel_size, stride,
        padding, norm_layer, expansion=4,
        affine=True, input_size=None
    ):
        super(MBConv, self).__init__()
        self.hidden_dim = expansion * C_in
        self.op = nn.Sequential(
            # pw
            Conv2d(
                C_in, self.hidden_dim, 1, 1, 0, bias=False,
                norm=get_norm(norm_layer, self.hidden_dim),
                activation=nn.ReLU()
            ),
            # dw
            Conv2d(
                self.hidden_dim, self.hidden_dim, kernel_size,
                stride, padding, groups=self.hidden_dim, bias=False,
                norm=get_norm(norm_layer, self.hidden_dim),
                activation=nn.ReLU()
            ),
            # pw-linear without ReLU!
            Conv2d(
                self.hidden_dim, C_out, 1, 1, 0, bias=False,
                norm=get_norm(norm_layer, C_out)
            )
        )
        self.flops = self.get_flop(
            [kernel_size, kernel_size], stride, C_in, C_out,
            affine, input_size[0], input_size[1]
        )
        # using Kaiming init
        for m in self.op.modules():
            if isinstance(m, nn.Conv2d):
                weight_init.kaiming_init(m, mode='fan_in')
            elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):
                if m.weight is not None:
                    nn.init.constant_(m.weight, 1)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def get_flop(
        self, kernel_size, stride, in_channel,
        out_channel, affine, in_h, in_w
    ):
        cal_flop = flops.count_ConvBNReLU_flop(
            in_h, in_w, in_channel, self.hidden_dim,
            kernel_size=[1, 1], is_bias=False, is_affine=affine
        )
        cal_flop += flops.count_Conv_flop(
            in_h, in_w, self.hidden_dim, self.hidden_dim, kernel_size,
            False, stride, groups=self.hidden_dim, is_affine=affine
        )
        in_h, in_w = in_h // stride, in_w // stride
        # pw-linear without ReLU!
        cal_flop += flops.count_Conv_flop(
            in_h, in_w, self.hidden_dim, out_channel,
            kernel_size=[1, 1], is_bias=False
        )
        cal_flop += flops.count_BN_flop(in_h, in_w, out_channel, affine)
        return cal_flop

    def forward(self, x):
        return self.op(x)


class Identity(nn.Module):
    def __init__(
        self, C_in, C_out, norm_layer, affine=True, input_size=None
    ):
        super(Identity, self).__init__()
        if C_in == C_out:
            self.change = False
            self.flops = 0.0
        else:
            self.change = True
            self.op = Conv2d(
                C_in, C_out, kernel_size=1, padding=0, bias=False,
                norm=get_norm(norm_layer, C_out)
            )
            self.flops = self.get_flop(
                [1, 1], 1, C_in, C_out, affine, input_size[0], input_size[1]
            )
            # using Kaiming init
            for m in self.op.modules():
                if isinstance(m, nn.Conv2d):
                    weight_init.kaiming_init(m, mode='fan_in')
                elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):
                    if m.weight is not None:
                        nn.init.constant_(m.weight, 1)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)

    def forward(self, x):
        if not self.change:
            return x
        else:
            return self.op(x)

    def get_flop(self, kernel_size, stride, in_channel, out_channel, affine, in_h, in_w):
        cal_flop = flops.count_Conv_flop(
            in_h, in_w, in_channel, out_channel,
            kernel_size=kernel_size, is_bias=False
        )
        cal_flop += flops.count_BN_flop(in_h, in_w, out_channel, affine)
        return cal_flop


class Zero(nn.Module):
    def __init__(self, stride):
        super(Zero, self).__init__()
        self.stride = stride
        self.flops = 0.0

    def forward(self, x):
        if self.stride == 1:
            return x.mul(0.)
        else:
            return x[:, :, ::self.stride, ::self.stride].mul(0.)


class FactorizedReduce(nn.Module):
    def __init__(self, C_in, C_out, norm_layer, affine=True, input_size=None):
        super(FactorizedReduce, self).__init__()
        assert C_out % 2 == 0
        self.conv_1 = Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)
        self.conv_2 = Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)
        self.bn = norm_layer(C_out, affine=affine)

        self.flops = self.get_flop(
            [1, 1], 2, C_in, C_out, affine, input_size[0], input_size[1]
        )
        # using Kaiming init
        for layer in [self.conv_1, self.conv_2]:
            for m in layer.modules():
                if isinstance(m, nn.Conv2d):
                    weight_init.kaiming_init(m, mode='fan_in')
                elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):
                    if m.weight is not None:
                        nn.init.constant_(m.weight, 1)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)

    def get_flop(
        self, kernel_size, stride, in_channel,
        out_channel, affine, in_h, in_w
    ):
        cal_flop = flops.count_Conv_flop(
            in_h, in_w, in_channel, out_channel // 2,
            kernel_size, False, stride=stride
        )
        cal_flop += flops.count_Conv_flop(
            in_h, in_w, in_channel, out_channel // 2,
            kernel_size, False, stride=stride
        )
        in_h, in_w = in_h // stride, in_w // stride
        cal_flop += flops.count_BN_flop(in_h, in_w, out_channel, affine)
        return cal_flop

    def forward(self, x):
        out = torch.cat([self.conv_1(x), self.conv_2(x[:, :, 1:, 1:])], dim=1)
        out = self.bn(out)
        return out


class AvgPool2d(nn.Module):
    def __init__(self, C_in, C_out, kernel_size, stride, padding, input_size):
        super(AvgPool2d, self).__init__()
        self.avg_pool = nn.AvgPool2d(
            kernel_size, stride=stride, padding=padding, count_include_pad=False
        )
        self.flops = self.get_flop(
            [kernel_size, kernel_size], stride,
            C_out, input_size[0], input_size[1]
        )

    def get_flop(self, kernel_size, stride, out_channel, in_h, in_w):
        cal_flop = flops.count_Pool2d_flop(
            in_h, in_w, out_channel, kernel_size, stride
        )
        return cal_flop

    def forward(self, x):
        return self.avg_pool(x)


class MaxPool2d(nn.Module):
    def __init__(self, C_in, C_out, kernel_size, stride, padding, input_size):
        super(MaxPool2d, self).__init__()
        self.max_pool = nn.MaxPool2d(kernel_size, stride=stride, padding=padding)
        self.flops = self.get_flop(
            [kernel_size, kernel_size], stride, C_out,
            input_size[0], input_size[1]
        )

    def get_flop(self, kernel_size, stride, out_channel, in_h, in_w):
        cal_flop = flops.count_Pool2d_flop(
            in_h, in_w, out_channel, kernel_size, stride
        )
        return cal_flop

    def forward(self, x):
        return self.max_pool(x)
```

##### cvpods/modeling/backbone/dynamic_arch/__init__.py

```python
# -*- coding: utf-8 -*-
# build for dynamic networks
# @Author: yanwei.li

from .dynamic_backbone import DynamicNetwork, build_dynamic_backbone
```

##### cvpods/modeling/backbone/dynamic_arch/dynamic_backbone.py

```python
# encoding: utf-8
# network file -> build backbone for Dynamic Network
# @author: yanwei.li
import numpy as np

import torch
import torch.nn as nn

from cvpods.layers import Conv2d, ShapeSpec, get_norm
from cvpods.modeling.backbone import Backbone
from cvpods.modeling.backbone.dynamic_arch import cal_op_flops
from cvpods.modeling.nn_utils import weight_init

from .dynamic_cell import Cell

__all__ = ["DynamicStem", "DynamicNetwork", "build_dynamic_backbone"]


class DynamicStem(nn.Module):
    def __init__(
        self, in_channels=3, mid_channels=64, out_channels=64,
        input_res=None, sept_stem=True, norm="BN", affine=True
    ):
        """
        Build basic STEM for Dynamic Network.
        Args:
            norm (str or callable): a callable that takes the number of
                channels and return a `nn.Module`, or a pre-defined string
                (one of {"FrozenBN", "BN", "GN"}).
        """
        super().__init__()

        self.real_flops = 0.0
        # start with 3 stem layers down-sampling by 4.
        self.stem_1 = Conv2d(
            in_channels, mid_channels, kernel_size=3, stride=2,
            bias=False, norm=get_norm(norm, mid_channels),
            activation=nn.ReLU()
        )
        self.real_flops += cal_op_flops.count_ConvBNReLU_flop(
            input_res[0], input_res[1], 3, mid_channels,
            [3, 3], stride=2, is_affine=affine
        )
        # stem 2
        input_res = input_res // 2
        if not sept_stem:
            self.stem_2 = Conv2d(
                mid_channels, mid_channels, kernel_size=3,
                stride=1, padding=1, bias=False,
                norm=get_norm(norm, mid_channels),
                activation=nn.ReLU()
            )
            self.real_flops += cal_op_flops.count_ConvBNReLU_flop(
                input_res[0], input_res[1], mid_channels,
                mid_channels, [3, 3], is_affine=affine
            )
        else:
            self.stem_2 = nn.Sequential(
                Conv2d(
                    mid_channels, mid_channels, kernel_size=3, stride=1,
                    padding=1, groups=mid_channels, bias=False
                ),
                Conv2d(
                    mid_channels, mid_channels, kernel_size=1,
                    stride=1, padding=0, bias=False,
                    norm=get_norm(norm, mid_channels),
                    activation=nn.ReLU()
                )
            )
            self.real_flops += (
                cal_op_flops.count_Conv_flop(
                    input_res[0], input_res[1], mid_channels,
                    mid_channels, [3, 3], groups=mid_channels
                ) + cal_op_flops.count_ConvBNReLU_flop(
                    input_res[0], input_res[1], mid_channels,
                    mid_channels, [1, 1], is_affine=affine
                )
            )
        # stem 3
        if not sept_stem:
            self.stem_3 = Conv2d(
                mid_channels, out_channels, kernel_size=3,
                stride=2, padding=1, bias=False,
                norm=get_norm(norm, out_channels),
                activation=nn.ReLU()
            )
            self.real_flops += cal_op_flops.count_ConvBNReLU_flop(
                input_res[0], input_res[1], mid_channels, out_channels,
                [3, 3], stride=2, is_affine=affine
            )
        else:
            self.stem_3 = nn.Sequential(
                Conv2d(
                    mid_channels, mid_channels, kernel_size=3, stride=2,
                    padding=1, groups=mid_channels, bias=False
                ),
                Conv2d(
                    mid_channels, out_channels, kernel_size=1, padding=0,
                    bias=False, norm=get_norm(norm, out_channels),
                    activation=nn.ReLU()
                )
            )
            self.real_flops += (
                cal_op_flops.count_Conv_flop(
                    input_res[0], input_res[1], mid_channels,
                    mid_channels, [3, 3], stride=2, groups=mid_channels
                ) + cal_op_flops.count_ConvBNReLU_flop(
                    input_res[0] // 2, input_res[1] // 2, mid_channels,
                    out_channels, [1, 1], is_affine=affine
                )
            )
        self.out_res = input_res // 2
        self.out_cha = out_channels
        # using Kaiming init
        for layer in [self.stem_1, self.stem_2, self.stem_3]:
            for name, m in layer.named_modules():
                if isinstance(m, nn.Conv2d):
                    weight_init.kaiming_init(m, mode='fan_in')
                elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):
                    if m.weight is not None:
                        nn.init.constant_(m.weight, 1)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.stem_1(x)
        x = self.stem_2(x)
        x = self.stem_3(x)
        return x

    @property
    def out_channels(self):
        return self.out_cha

    @property
    def stride(self):
        return 4

    @property
    def out_resolution(self):
        return self.out_res

    @property
    def flops(self):
        return self.real_flops


class DynamicNetwork(Backbone):
    """
    This module implements Dynamic Routing Network.
    It creates dense connected network on top of some input feature maps.
    """
    def __init__(
        self, init_channel, input_shape, cell_num_list, layer_num,
        ext_layer=None, norm="", cal_flops=True, cell_type='',  # pylint: disable=W0613
        max_stride=32, sep_stem=True, using_gate=False,
        small_gate=False, gate_bias=1.5, drop_prob=0.0,
    ):
        super(DynamicNetwork, self).__init__()
        # set affine in BatchNorm
        if 'Sync' in norm:
            self.affine = True
        else:
            self.affine = False
        # set scheduled drop path
        self.drop_prob = drop_prob
        if self.drop_prob > 0.0001:
            self.drop_path = True
        else:
            self.drop_path = False
        self.cal_flops = cal_flops
        self._size_divisibility = max_stride
        input_res = np.array(input_shape[1:3])

        self.stem = DynamicStem(
            3, out_channels=init_channel, input_res=input_res,
            sept_stem=sep_stem, norm=norm, affine=self.affine
        )
        self.stem_flops = self.stem.flops
        self._out_feature_strides = {"stem": self.stem.stride}
        self._out_feature_channels = {"stem": self.stem.out_channels}
        self._out_feature_resolution = {"stem": self.stem.out_resolution}
        assert self.stem.out_channels == init_channel
        self.all_cell_list = nn.ModuleList()
        self.all_cell_type_list = []
        self.cell_num_list = cell_num_list[:layer_num]
        self._out_features = []
        # using the initial layer
        input_res = input_res // self.stem.stride
        in_channel = out_channel = init_channel
        self.init_layer = Cell(
            C_in=in_channel, C_out=out_channel, norm=norm, allow_up=False,
            allow_down=True, input_size=input_res, cell_type=cell_type,
            cal_flops=False, using_gate=using_gate, small_gate=small_gate,
            gate_bias=gate_bias, affine=self.affine
        )

        # add cells in each layer
        for layer_index in range(len(self.cell_num_list)):
            layer_cell_list = nn.ModuleList()
            layer_cell_type = []
            for cell_index in range(self.cell_num_list[layer_index]):
                # channel multi, when stride:4 -> channel:C, stride:8 -> channel:2C ...
                channel_multi = pow(2, cell_index)
                in_channel_cell = in_channel * channel_multi
                # add res and dim switch to each cell
                allow_up = True
                allow_down = True
                # add res up and dim down by 2
                if cell_index == 0 or layer_index == layer_num - 1:
                    allow_up = False
                # dim down and resolution up by 2
                if cell_index == 3 or layer_index == layer_num - 1:
                    allow_down = False
                res_size = input_res // channel_multi
                layer_cell_list.append(
                    Cell(
                        C_in=in_channel_cell, C_out=in_channel_cell, norm=norm,
                        allow_up=allow_up, allow_down=allow_down,
                        input_size=res_size, cell_type=cell_type,
                        cal_flops=cal_flops, using_gate=using_gate,
                        small_gate=small_gate, gate_bias=gate_bias,
                        affine=self.affine
                    )
                )
                # allow dim change in each aggregation
                dim_up, dim_down, dim_keep = False, False, True
                # dim up and resolution down by 2
                if cell_index > 0:
                    dim_up = True
                # dim down and resolution up by 2
                if (cell_index < self.cell_num_list[layer_index] - 1) and layer_index > 2:
                    dim_down = True
                elif (cell_index < self.cell_num_list[layer_index] - 2) and layer_index <= 2:
                    dim_down = True
                # dim keep unchanged
                if layer_index <= 2 and cell_index == self.cell_num_list[layer_index] - 1:
                    dim_keep = False
                # allowed cell operations
                layer_cell_type.append([dim_up, dim_keep, dim_down])
                if layer_index == len(self.cell_num_list) - 1:
                    name = 'layer_' + str(cell_index)
                    self._out_feature_strides[name] = channel_multi * self.stem.stride
                    self._out_feature_channels[name] = in_channel_cell
                    self._out_feature_resolution[name] = res_size
                    self._out_features.append(name)
            self.all_cell_list.append(layer_cell_list)
            self.all_cell_type_list.append(layer_cell_type)

    @property
    def size_divisibility(self):
        return self._size_divisibility

    def forward(self, x, step_rate=0.0):
        h_l1 = self.stem(x)
        # the initial layer
        h_l1_list, h_beta_list, trans_flops, trans_flops_real = self.init_layer(h_l1=h_l1)
        prev_beta_list, prev_out_list = [h_beta_list], [h_l1_list]  # noqa: F841
        prev_trans_flops, prev_trans_flops_real = [trans_flops], [trans_flops_real]
        # build forward outputs
        cell_flops_list, cell_flops_real_list = [], []
        for layer_index in range(len(self.cell_num_list)):
            layer_input, layer_output = [], []
            layer_trans_flops, layer_trans_flops_real = [], []
            flops_in_expt_list, flops_in_real_list = [], []
            layer_rate = (layer_index + 1) / float(len(self.cell_num_list))
            # aggregate cell input
            for cell_index in range(len(self.all_cell_type_list[layer_index])):
                cell_input, trans_flops_input, trans_flops_real_input = [], [], []
                if self.all_cell_type_list[layer_index][cell_index][0]:
                    cell_input.append(prev_out_list[cell_index - 1][2][0])
                    trans_flops_input.append(prev_trans_flops[cell_index - 1][2][0])
                    trans_flops_real_input.append(prev_trans_flops_real[cell_index - 1][2][0])
                if self.all_cell_type_list[layer_index][cell_index][1]:
                    cell_input.append(prev_out_list[cell_index][1][0])
                    trans_flops_input.append(prev_trans_flops[cell_index][1][0])
                    trans_flops_real_input.append(prev_trans_flops_real[cell_index][1][0])
                if self.all_cell_type_list[layer_index][cell_index][2]:
                    cell_input.append(prev_out_list[cell_index + 1][0][0])
                    trans_flops_input.append(prev_trans_flops[cell_index + 1][0][0])
                    trans_flops_real_input.append(prev_trans_flops_real[cell_index + 1][0][0])

                h_l1 = sum(cell_input)
                # calculate input for gate
                layer_input.append(h_l1)
                # calculate FLOPs input
                flops_in_expt = sum(_flops for _flops in trans_flops_input)
                flop_in_real = sum(_flops for _flops in trans_flops_real_input)
                flops_in_expt_list.append(flops_in_expt)
                flops_in_real_list.append(flop_in_real)

            # calculate each cell
            for _cell_index in range(len(self.all_cell_type_list[layer_index])):
                if self.cal_flops:
                    cell_output, gate_weights_beta, cell_flops, \
                        cell_flops_real, trans_flops, trans_flops_real = \
                        self.all_cell_list[layer_index][_cell_index](
                            h_l1=layer_input[_cell_index],
                            flops_in_expt=flops_in_expt_list[_cell_index],
                            flops_in_real=flops_in_real_list[_cell_index],
                            is_drop_path=self.drop_path, drop_prob=self.drop_prob,
                            layer_rate=layer_rate, step_rate=step_rate
                        )
                    # calculate real flops
                    cell_flops_list.append(cell_flops)
                    cell_flops_real_list.append(cell_flops_real)
                else:
                    cell_output, gate_weights_beta, trans_flops, trans_flops_real = \
                        self.all_cell_list[layer_index][_cell_index](
                            h_l1=layer_input[_cell_index],
                            flops_in_expt=flops_in_expt_list[_cell_index],
                            flops_in_real=flops_in_real_list[_cell_index],
                            is_drop_path=self.drop_path, drop_prob=self.drop_prob,
                            layer_rate=layer_rate, step_rate=step_rate
                        )

                layer_output.append(cell_output)
                # update trans flops output
                layer_trans_flops.append(trans_flops)
                layer_trans_flops_real.append(trans_flops_real)
            # update layer output
            prev_out_list = layer_output
            prev_trans_flops = layer_trans_flops
            prev_trans_flops_real = layer_trans_flops_real

        final_out_list = [prev_out_list[_i][1][0] for _i in range(len(prev_out_list))]
        final_out_dict = dict(zip(self._out_features, final_out_list))
        if self.cal_flops:
            all_cell_flops = torch.mean(sum(cell_flops_list))
            all_flops_real = torch.mean(sum(cell_flops_real_list)) + self.stem_flops
        else:
            all_cell_flops, all_flops_real = None, None
        return final_out_dict, all_cell_flops, all_flops_real

    def output_shape(self):
        return {
            name: ShapeSpec(
                channels=self._out_feature_channels[name],
                height=self._out_feature_resolution[name][0],
                width=self._out_feature_resolution[name][0],
                stride=self._out_feature_strides[name]
            )
            for name in self._out_features
        }


def build_dynamic_backbone(cfg, input_shape: ShapeSpec):
    """
    Create a Dynamic Backbone from config.
    Args:
        cfg: a config dict
    Returns:
        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.
    """
    if input_shape is None:
        input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))
    backbone = DynamicNetwork(
        init_channel=cfg.MODEL.BACKBONE.INIT_CHANNEL,
        input_shape=input_shape,
        cell_num_list=cfg.MODEL.BACKBONE.CELL_NUM_LIST,
        layer_num=cfg.MODEL.BACKBONE.LAYER_NUM,
        norm=cfg.MODEL.BACKBONE.NORM,
        cal_flops=cfg.MODEL.CAL_FLOPS,
        cell_type=cfg.MODEL.BACKBONE.CELL_TYPE,
        max_stride=cfg.MODEL.BACKBONE.MAX_STRIDE,
        sep_stem=cfg.MODEL.BACKBONE.SEPT_STEM,
        using_gate=cfg.MODEL.GATE.GATE_ON,
        small_gate=cfg.MODEL.GATE.SMALL_GATE,
        gate_bias=cfg.MODEL.GATE.GATE_INIT_BIAS,
        drop_prob=cfg.MODEL.BACKBONE.DROP_PROB
    )

    return backbone
```

##### cvpods/modeling/backbone/dynamic_arch/dynamic_cell.py

```python
# encoding: utf-8
# network file -> build Cell for Dynamic Backbone
# @author: yanwei.li
import torch
import torch.nn as nn
import torch.nn.functional as F

from cvpods.layers import Conv2d, get_norm
from cvpods.modeling.backbone.dynamic_arch import cal_op_flops
from cvpods.modeling.nn_utils import weight_init

from .op_with_flops import OPS, Identity

__all__ = ["Mixed_OP", "Cell"]


# soft gate for path choice
def soft_gate(x, x_t=None, momentum=0.1, is_update=False):
    if is_update:
        # using momentum for weight update
        y = (1 - momentum) * x.data + momentum * x_t
        tanh_value = torch.tanh(y)
        return F.relu(tanh_value), y.data
    else:
        tanh_value = torch.tanh(x)
        return F.relu(tanh_value)


# Scheduled Drop Path
def drop_path(x, drop_prob, layer_rate, step_rate):
    """
    :param x: input feature
    :param drop_prob: drop path prob
    :param layer_rate: current_layer/total_layer
    :param step_rate: current_step/total_step
    :return: output feature
    """
    if drop_prob > 0.:
        keep_prob = 1. - drop_prob
        keep_prob = 1. - layer_rate * (1. - keep_prob)
        keep_prob = 1. - step_rate * (1. - keep_prob)
        mask = torch.cuda.FloatTensor(x.size(0), 1, 1, 1).bernoulli_(keep_prob)
        x.div_(keep_prob)
        x.mul_(mask)
    return x


class Mixed_OP(nn.Module):
    """
    Sum up operations according to their weights.
    """
    def __init__(
        self, inplanes, outplanes, stride, cell_type,
        norm='', affine=True, input_size=None
    ):
        super(Mixed_OP, self).__init__()
        self._ops = nn.ModuleList()
        self.op_flops = []
        for key in cell_type:
            op = OPS[key](
                inplanes, outplanes, stride, norm_layer=norm,
                affine=affine, input_size=input_size
            )
            self._ops.append(op)
            self.op_flops.append(op.flops)
        self.real_flops = sum(op_flop for op_flop in self.op_flops)

    def forward(self, x, is_drop_path=False, drop_prob=0.0, layer_rate=0.0, step_rate=0.0):
        if is_drop_path:
            y = []
            for op in self._ops:
                if not isinstance(op, Identity):
                    y.append(drop_path(op(x), drop_prob, layer_rate, step_rate))
                else:
                    y.append(op(x))
            return sum(y)
        else:
            # using sum up rather than random choose one branch.
            return sum(op(x) for op in self._ops)

    @property
    def flops(self):
        return self.real_flops.squeeze()


class Cell(nn.Module):
    def __init__(  # noqa:C901
        self, C_in, C_out, norm, allow_up, allow_down, input_size,
        cell_type, cal_flops=True, using_gate=False,
        small_gate=False, gate_bias=1.5, affine=True
    ):
        super(Cell, self).__init__()
        self.channel_in = C_in
        self.channel_out = C_out
        self.allow_up = allow_up
        self.allow_down = allow_down
        self.cal_flops = cal_flops
        self.using_gate = using_gate
        self.small_gate = small_gate

        self.cell_ops = Mixed_OP(
            inplanes=self.channel_in, outplanes=self.channel_out,
            stride=1, cell_type=cell_type, norm=norm,
            affine=affine, input_size=input_size
        )
        self.cell_flops = self.cell_ops.flops
        # resolution keep
        self.res_keep = nn.ReLU()
        self.res_keep_flops = cal_op_flops.count_ReLU_flop(
            input_size[0], input_size[1], self.channel_out
        )
        # resolution up and dim down
        if self.allow_up:
            self.res_up = nn.Sequential(
                nn.ReLU(),
                Conv2d(
                    self.channel_out, self.channel_out // 2, kernel_size=1,
                    stride=1, padding=0, bias=False,
                    norm=get_norm(norm, self.channel_out // 2),
                    activation=nn.ReLU()
                )
            )
            # calculate Flops
            self.res_up_flops = cal_op_flops.count_ReLU_flop(
                input_size[0], input_size[1], self.channel_out
            ) + cal_op_flops.count_ConvBNReLU_flop(
                input_size[0], input_size[1], self.channel_out,
                self.channel_out // 2, [1, 1], is_affine=affine
            )
            # using Kaiming init
            for m in self.res_up.modules():
                if isinstance(m, nn.Conv2d):
                    weight_init.kaiming_init(m, mode='fan_in')
                elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):
                    if m.weight is not None:
                        nn.init.constant_(m.weight, 1)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
        # resolution down and dim up
        if self.allow_down:
            self.res_down = nn.Sequential(
                nn.ReLU(),
                Conv2d(
                    self.channel_out, 2 * self.channel_out,
                    kernel_size=1, stride=2, padding=0, bias=False,
                    norm=get_norm(norm, 2 * self.channel_out),
                    activation=nn.ReLU()
                )
            )
            # calculate Flops
            self.res_down_flops = cal_op_flops.count_ReLU_flop(
                input_size[0], input_size[1], self.channel_out
            ) + cal_op_flops.count_ConvBNReLU_flop(
                input_size[0], input_size[1], self.channel_out,
                2 * self.channel_out, [1, 1], stride=2, is_affine=affine
            )
            # using Kaiming init
            for m in self.res_down.modules():
                if isinstance(m, nn.Conv2d):
                    weight_init.kaiming_init(m, mode='fan_in')
                elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):
                    if m.weight is not None:
                        nn.init.constant_(m.weight, 1)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
        if self.allow_up and self.allow_down:
            self.gate_num = 3
        elif self.allow_up or self.allow_down:
            self.gate_num = 2
        else:
            self.gate_num = 1
        if self.using_gate:
            self.gate_conv_beta = nn.Sequential(
                Conv2d(
                    self.channel_in, self.channel_in // 2, kernel_size=1,
                    stride=1, padding=0, bias=False,
                    norm=get_norm(norm, self.channel_in // 2),
                    activation=nn.ReLU()
                ),
                nn.AdaptiveAvgPool2d((1, 1)),
                Conv2d(
                    self.channel_in // 2, self.gate_num, kernel_size=1,
                    stride=1, padding=0, bias=True
                )
            )
            if self.small_gate:
                input_size = input_size // 4
            self.gate_flops = cal_op_flops.count_ConvBNReLU_flop(
                input_size[0], input_size[1], self.channel_in,
                self.channel_in // 2, [1, 1], is_affine=affine
            ) + cal_op_flops.count_Pool2d_flop(
                input_size[0], input_size[1], self.channel_in // 2, [1, 1], 1
            ) + cal_op_flops.count_Conv_flop(
                1, 1, self.channel_in // 2, self.gate_num, [1, 1]
            )
            # using Kaiming init and predefined bias for gate
            for m in self.gate_conv_beta.modules():
                if isinstance(m, nn.Conv2d):
                    weight_init.kaiming_init(m, mode='fan_in', bias=gate_bias)
                elif isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):
                    if m.weight is not None:
                        nn.init.constant_(m.weight, 1)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
        else:
            self.register_buffer(
                'gate_weights_beta', torch.ones(1, self.gate_num, 1, 1).cuda()
            )
            self.gate_flops = 0.0

    def forward(
        self, h_l1, flops_in_expt=None, flops_in_real=None,
        is_drop_path=False, drop_prob=0.0,
        layer_rate=0.0, step_rate=0.0
    ):
        """
        :param h_l1: # the former hidden layer output
        :return: current hidden cell result h_l
        """
        drop_cell = False
        # drop the cell if input type is float
        if not isinstance(h_l1, float):
            # calculate soft conditional gate
            if self.using_gate:
                if self.small_gate:
                    h_l1_gate = F.interpolate(
                        input=h_l1, scale_factor=0.25,
                        mode='bilinear', align_corners=False
                    )
                else:
                    h_l1_gate = h_l1
                gate_feat_beta = self.gate_conv_beta(h_l1_gate)
                gate_weights_beta = soft_gate(gate_feat_beta)
            else:
                gate_weights_beta = self.gate_weights_beta
        else:
            drop_cell = True
        # use for inference
        if not self.training:
            if not drop_cell:
                drop_cell = gate_weights_beta.sum() < 0.0001
            if drop_cell:
                result_list = [[0.0], [h_l1], [0.0]]
                weights_list_beta = [[0.0], [0.0], [0.0]]
                trans_flops_expt = [[0.0], [0.0], [0.0]]
                trans_flops_real = [[0.0], [0.0], [0.0]]
                if self.cal_flops:
                    h_l_flops = flops_in_expt
                    h_l_flops_real = flops_in_real + self.gate_flops
                    return (
                        result_list, weights_list_beta, h_l_flops,
                        h_l_flops_real, trans_flops_expt, trans_flops_real
                    )
                else:
                    return (
                        result_list, weights_list_beta,
                        trans_flops_expt, trans_flops_real
                    )

        h_l = self.cell_ops(h_l1, is_drop_path, drop_prob, layer_rate, step_rate)

        # resolution and dimension change
        # resolution: [up, keep, down]
        h_l_keep = self.res_keep(h_l)
        gate_weights_beta_keep = gate_weights_beta[:, 0].unsqueeze(-1)
        # using residual connection if drop cell
        gate_mask = (gate_weights_beta.sum(dim=1, keepdim=True) < 0.0001).float()
        result_list = [[], [gate_mask * h_l1 + gate_weights_beta_keep * h_l_keep], []]
        weights_list_beta = [[], [gate_mask * 1.0 + gate_weights_beta_keep], []]
        # calculate flops for keep res
        gate_mask_keep = (gate_weights_beta_keep > 0.0001).float()
        trans_flops_real = [[], [gate_mask_keep * self.res_keep_flops], []]
        # calculate trans flops
        trans_flops_expt = [[], [self.res_keep_flops * gate_weights_beta_keep], []]

        if self.allow_up:
            h_l_up = self.res_up(h_l)
            h_l_up = F.interpolate(
                input=h_l_up, scale_factor=2, mode='bilinear', align_corners=False
            )
            gate_weights_beta_up = gate_weights_beta[:, 1].unsqueeze(-1)
            result_list[0].append(h_l_up * gate_weights_beta_up)
            weights_list_beta[0].append(gate_weights_beta_up)
            trans_flops_expt[0].append(self.res_up_flops * gate_weights_beta_up)
            # calculate flops for up res
            gate_mask_up = (gate_weights_beta_up > 0.0001).float()
            trans_flops_real[0].append(gate_mask_up * self.res_up_flops)

        if self.allow_down:
            h_l_down = self.res_down(h_l)
            gate_weights_beta_down = gate_weights_beta[:, -1].unsqueeze(-1)
            result_list[2].append(h_l_down * gate_weights_beta_down)
            weights_list_beta[2].append(gate_weights_beta_down)
            trans_flops_expt[2].append(self.res_down_flops * gate_weights_beta_down)
            # calculate flops for down res
            gate_mask_down = (gate_weights_beta_down > 0.0001).float()
            trans_flops_real[2].append(gate_mask_down * self.res_down_flops)

        if self.cal_flops:
            cell_flops = gate_weights_beta.max(dim=1, keepdim=True)[0] * self.cell_flops
            cell_flops_real = (
                gate_weights_beta.sum(dim=1, keepdim=True) > 0.0001
            ).float() * self.cell_flops
            h_l_flops = cell_flops + flops_in_expt
            h_l_flops_real = cell_flops_real + flops_in_real + self.gate_flops
            return (
                result_list, weights_list_beta, h_l_flops,
                h_l_flops_real, trans_flops_expt, trans_flops_real
            )
        else:
            return result_list, weights_list_beta, trans_flops_expt, trans_flops_real
```

### cvpods/analyser/module_profiler.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved
# pylint: disable=W0613

import functools
from collections import OrderedDict, defaultdict, namedtuple
from tabulate import tabulate

import numpy as np

import torch.autograd.profiler as tprofiler

Trace = namedtuple("Trace", ["path", "module"])
Measure = namedtuple("Measure", ["self_cpu_total", "cpu_total", "cuda_total", "hits"])
ModuleInfo = namedtuple(
    "ModuleInfo", ["type", "self_cpu_total", "cpu_total", "cuda_total", "hits"]
)


def walk_modules(module, name=None, path=()):
    """Generator. Walks through a PyTorch Module and outputs Trace tuples"""
    if not name:
        name = module.__class__.__name__
    named_children = list(module.named_children())
    path = path + (name,)
    if len(named_children) == 0:
        yield Trace(".".join(path), module)
    # recursively walk into all submodules
    for name, child_module in named_children:
        yield from walk_modules(child_module, name=name, path=path)


class Profile(object):
    """
    Layer by layer profiling of Pytorch models, using the Pytorch autograd profiler.
    """

    def __init__(self, module, enabled=True, use_cuda=False, paths=None, with_mapping=True):
        """
        Args:
            model:
            enabled:
            use_cuda:
            paths:
            with_mapping:
        """
        self._module = module
        self.enabled = enabled
        self.use_cuda = use_cuda
        self.paths = paths
        self.with_mapping = with_mapping

        self.entered = False
        self.exited = False
        self.traces = ()
        self.trace_profile_events = defaultdict(list)

    def __enter__(self):
        if not self.enabled:
            return self
        if self.entered:
            raise RuntimeError("torchprof profiler is not reentrant")
        self.entered = True
        self._forwards = {}  # store the original forward functions
        self.traces = tuple(map(self._hook_trace, walk_modules(self._module)))
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if not self.enabled:
            return
        tuple(map(self._remove_hook_trace, self.traces))
        del self._forwards  # remove unnecessary forwards
        self.exited = True

    def __str__(self):
        if self.exited:
            return repr(traces_to_display(
                self.traces, self.trace_profile_events,
                self.use_cuda, paths=self.paths
            ))
        return "<unfinished torchprof.profile>"

    def __call__(self, *args, **kwargs):
        return self._module(*args, **kwargs)

    def _hook_trace(self, trace):
        path, module = trace
        if (self.paths is not None and path in self.paths) or (self.paths is None):
            _forward = module.forward
            self._forwards[path] = _forward

            @functools.wraps(_forward)
            def wrap_forward(*args, **kwargs):
                with tprofiler.profile(use_cuda=self.use_cuda) as prof:
                    res = _forward(*args, **kwargs)
                event_list = prof.function_events
                event_list.populate_cpu_children()
                # each profile call should be contained in its own list
                self.trace_profile_events[path].append(event_list)
                return res

            module.forward = wrap_forward
        return trace

    def _remove_hook_trace(self, trace):
        path, module = trace
        if (self.paths is not None and path in self.paths) or (self.paths is None):
            module.forward = self._forwards[path]


class InfoTable():

    def __init__(self, headers, data, average=False):
        assert len(headers) == len(data), "headers and data are not matched"
        self.headers = headers
        self.info = {key: value for key, value in zip(headers, data)}
        if average:
            self.average()

    def insert(self, header, data, position=-1):

        def swap(a, b):
            a, b = b, a

        self.info[header] = data
        if header in self.headers:
            index = self.headers.index(header)
            swap(self.headers[index], self.headers[position])
        else:
            self.headers.insert(position, header)

    def sorted_by(self, keyname=None, descending=True):
        if keyname is None:
            return self
        assert keyname in self.info
        sort_index = np.argsort(self.info[keyname], axis=0).reshape(-1)
        if descending:
            sort_index = sort_index[::-1]
        for header in self.headers:
            self.info[header] = self.info[header][sort_index]

        return self

    def filter(self, filter_list=None):
        self.headers = [header for header in self.headers if header not in filter_list]

    def average(self):
        hits = self.info["hits"]
        for i in range(len(self.headers)):
            header = self.headers[i]
            if header.endswith("time"):
                self.info[header + "_avg"] = self.info[header] / hits
                self.headers[i] += "_avg"
                del self.info[header]

    def __repr__(self):
        formatter = np.vectorize(tprofiler.format_time)
        data = np.concatenate(
            [formatter(self.info[k]) if "time" in k else self.info[k] for k in self.headers],
            axis=1,
        )
        table = tabulate(data, headers=self.headers, tablefmt="fancy_grid")
        return table


def genertate_info_tree(traces, trace_events, level="module"):
    """
    """
    assert level in ["module", "operator", "mixed"]
    tree = OrderedDict()

    for trace in traces:
        path, module = trace
        # unwrap all of the events, in case model is called multiple times
        events = [te for tevents in trace_events[path] for te in tevents]
        if level == "module":
            tree[path] = ModuleInfo(
                repr(module),
                sum([e.self_cpu_time_total for e in events]),
                sum([e.cpu_time_total for e in events]),
                sum([e.cuda_time_total for e in events]),
                len(trace_events[path])
            )
        elif level == "operator" or level == "mixed":
            for op in set(event.name for event in events):
                op_events = [e for e in events if e.name == op]
                measure = Measure(
                    sum([e.self_cpu_time_total for e in op_events]),
                    sum([e.cpu_time_total for e in op_events]),
                    sum([e.cuda_time_total for e in op_events]),
                    len(op_events),
                )
                if level == "mixed":
                    tree[path + "." + op] = measure
                else:
                    # operator mode
                    if op not in tree:
                        tree[op] = measure
                    else:
                        tree[op] = Measure(*(a + b for a, b in zip(tree[op], measure)))

    return tree


def traces_to_display(
    traces, trace_events, with_cuda, paths=None, sorted_by="self_cpu_time", average=True
):
    """Construct human readable output of the profiler traces and events.
    """
    headers = ["name", "self_cpu_time", "cpu_time", "cuda_time", "hits"]
    data_type = ["float32", "float32", "float32", "int32"]
    # tree = genertate_infotable(traces, trace_events, "operator")
    tree = genertate_info_tree(traces, trace_events, "module")
    # tree = genertate_infotable(traces, trace_events, "mixed")
    format_lines = [
        (
            name,
            # info.type,
            info.self_cpu_total,
            info.cpu_total,
            info.cuda_total,
            info.hits,
        ) for name, info in tree.items()
    ]
    data = np.array(format_lines)
    data = np.hsplit(data, len(headers))
    data[1:] = [x.astype(dtype) for x, dtype in zip(data[1:], data_type)]
    table = InfoTable(headers, data)
    # table.average()
    table.sorted_by("cpu_time").filter(["cuda_time"])
    return table
```

#### cvpods/analyser/tide/functions.py

```python
import os
import sys

import numpy as np


def mean(arr: list):
    if len(arr) == 0:
        return 0
    return sum(arr) / len(arr)


def find_first(arr: np.array) -> int:
    """ Finds the index of the first instance of true in a vector or None if not found. """
    if len(arr) == 0:
        return None
    idx = arr.argmax()

    # Numpy argmax will return 0 if no True is found
    if idx == 0 and not arr[0]:
        return None

    return idx


def isiterable(x):
    try:
        iter(x)
        return True
    except Exception:
        return False


def recursive_sum(x):
    if isinstance(x, dict):
        return sum([recursive_sum(v) for v in x.values()])
    elif isiterable(x):
        return sum([recursive_sum(v) for v in x])
    else:
        return x


def apply_messy(x: list, func):
    return [([func(y) for y in e] if isiterable(e) else func(e)) for e in x]


def apply_messy2(x: list, y: list, func):
    return [
        [func(i, j) for i, j in zip(a, b)] if isiterable(a) else func(a, b)
        for a, b in zip(x, y)
    ]


def multi_len(x):
    try:
        return len(x)
    except TypeError:
        return 1


def unzip(x):
    return map(list, zip(*x))


def points(bbox):
    bbox = [int(x) for x in bbox]
    return (bbox[0], bbox[1]), (bbox[0] + bbox[2], bbox[1] + bbox[3])


def nonepack(t):
    if t is None:
        return None, None
    else:
        return t


class HiddenPrints:
    """ From https://stackoverflow.com/questions/8391411/suppress-calls-to-print-python """

    def __enter__(self):
        self._original_stdout = sys.stdout
        sys.stdout = open(os.devnull, "w")

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout.close()
        sys.stdout = self._original_stdout


def toRLE(mask: object, w: int, h: int):
    """
    Borrowed from Pycocotools:
    Convert annotation which can be polygons, uncompressed RLE to RLE.
    :return: binary mask (numpy 2D array)
    """
    import pycocotools.mask as maskUtils

    if type(mask) == list:
        # polygon -- a single object might consist of multiple parts
        # we merge all parts into one mask rle code
        rles = maskUtils.frPyObjects(mask, h, w)
        return maskUtils.merge(rles)
    elif type(mask["counts"]) == list:
        # uncompressed RLE
        return maskUtils.frPyObjects(mask, h, w)
    else:
        return mask


def polyToBox(poly: list):
    """ Converts a polygon in COCO lists of lists format to a bounding box in [x, y, w, h]. """

    xmin = 1e10
    xmax = -1e10
    ymin = 1e10
    ymax = -1e10

    for poly_comp in poly:
        for i in range(len(poly_comp) // 2):
            x = poly_comp[2 * i + 0]
            y = poly_comp[2 * i + 1]

            xmin = min(x, xmin)
            xmax = max(x, xmax)
            ymin = min(y, ymin)
            ymax = max(y, ymax)

    return [xmin, ymin, (xmax - xmin), (ymax - ymin)]
```

#### cvpods/analyser/tide/plotting.py

```python
import os
from collections import OrderedDict
import pandas as pd
import seaborn as sns

import cv2
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np

from .datasets import get_tide_path
from .errors.main_errors import (
    BackgroundError,
    BoxError,
    ClassError,
    DuplicateError,
    FalseNegativeError,
    FalsePositiveError,
    MissedError,
    OtherError
)


def print_table(rows: list, title: str = None):
    # Get all rows to have the same number of columns
    max_cols = max([len(row) for row in rows])
    for row in rows:
        while len(row) < max_cols:
            row.append("")

    # Compute the text width of each column
    col_widths = [
        max([len(rows[i][col_idx]) for i in range(len(rows))])
        for col_idx in range(len(rows[0]))
    ]

    divider = "--" + ("---".join(["-" * w for w in col_widths])) + "-"
    thick_divider = divider.replace("-", "=")

    if title:
        left_pad = (len(divider) - len(title)) // 2
        print(("{:>%ds}" % (left_pad + len(title))).format(title))

    print(thick_divider)
    for row in rows:
        # Print each row while padding to each column's text width
        print(
            "  "
            + "   ".join(
                [
                    ("{:>%ds}" % col_widths[col_idx]).format(row[col_idx])
                    for col_idx in range(len(row))
                ]
            )
            + "  "
        )
        if row == rows[0]:
            print(divider)
    print(thick_divider)


class Plotter:
    """ Sets up a seaborn environment and holds the functions for plotting our figures. """

    def __init__(self, quality: float = 1):
        # Set mpl DPI in case we want to output to the screen / notebook
        mpl.rcParams["figure.dpi"] = 150

        # Seaborn color palette
        sns.set_palette("muted", 10)
        current_palette = sns.color_palette()

        # Seaborn style
        sns.set(style="whitegrid")

        self.colors_main = OrderedDict(
            {
                ClassError.short_name: current_palette[9],
                BoxError.short_name: current_palette[8],
                OtherError.short_name: current_palette[2],
                DuplicateError.short_name: current_palette[6],
                BackgroundError.short_name: current_palette[4],
                MissedError.short_name: current_palette[3],
            }
        )

        self.colors_special = OrderedDict(
            {
                FalsePositiveError.short_name: current_palette[0],
                FalseNegativeError.short_name: current_palette[1],
            }
        )

        self.tide_path = get_tide_path()

        # For the purposes of comparing across models, we fix the scales on our bar plots.
        # Feel free to change these after initializing if you want to change the scale.
        self.MAX_MAIN_DELTA_AP = 10
        self.MAX_SPECIAL_DELTA_AP = 25

        self.quality = quality

    def _prepare_tmp_dir(self):
        tmp_dir = os.path.join(self.tide_path, "_tmp")

        if not os.path.exists(tmp_dir):
            os.makedirs(tmp_dir)

        for _f in os.listdir(tmp_dir):
            os.remove(os.path.join(tmp_dir, _f))

        return tmp_dir

    def make_summary_plot(
        self,
        out_dir: str,
        errors: dict,
        model_name: str,
        rec_type: str,
        hbar_names: bool = False,
    ):
        """
        Make a summary plot of the errors for a model, and save it to the figs folder.

        :param out_dir:    The output directory for the summary image. MUST EXIST.
        :param errors:     Dictionary of both main and special errors.
        :param model_name: Name of the model for which to generate the plot.
        :param rec_type:   Recognition type, either TIDE.BOX or TIDE.MASK
        :param hbar_names: Whether or not to include labels for the horizontal bars.
        """

        tmp_dir = self._prepare_tmp_dir()

        high_dpi = int(500 * self.quality)
        low_dpi = int(300 * self.quality)

        # get the data frame
        error_dfs = {
            errtype: pd.DataFrame(
                data={
                    "Error Type": list(errors[errtype][model_name].keys()),
                    "Delta mAP": list(errors[errtype][model_name].values()),
                }
            )
            for errtype in ["main", "special"]
        }

        # pie plot for error type breakdown
        error_types = list(errors["main"][model_name].keys()) + list(
            errors["special"][model_name].keys()
        )
        error_sum = sum([e for e in errors["main"][model_name].values()])
        error_sizes = [e / error_sum for e in errors["main"][model_name].values()] + [
            0,
            0,
        ]
        fig, ax = plt.subplots(1, 1, figsize=(11, 11), dpi=high_dpi)
        patches, outer_text, inner_text = ax.pie(
            error_sizes,
            colors=self.colors_main.values(),
            labels=error_types,
            autopct="%1.1f%%",
            startangle=90,
        )
        for text in outer_text + inner_text:
            text.set_text("")
        for i in range(len(self.colors_main)):
            if error_sizes[i] > 0.05:
                inner_text[i].set_text(list(self.colors_main.keys())[i])
            inner_text[i].set_fontsize(48)
            inner_text[i].set_fontweight("bold")
        ax.axis("equal")
        plt.title(model_name, fontdict={"fontsize": 60, "fontweight": "bold"})
        pie_path = os.path.join(tmp_dir, "{}_{}_pie.png".format(model_name, rec_type))
        plt.savefig(pie_path, bbox_inches="tight", dpi=low_dpi)
        plt.close()

        # horizontal bar plot for main error types
        fig, ax = plt.subplots(1, 1, figsize=(6, 5), dpi=high_dpi)
        sns.barplot(
            data=error_dfs["main"],
            x="Delta mAP",
            y="Error Type",
            ax=ax,
            palette=self.colors_main.values(),
        )
        ax.set_xlim(0, self.MAX_MAIN_DELTA_AP)
        ax.set_xlabel("")
        ax.set_ylabel("")
        if not hbar_names:
            ax.set_yticklabels([""] * 6)
        plt.setp(ax.get_xticklabels(), fontsize=28)
        plt.setp(ax.get_yticklabels(), fontsize=36)
        ax.grid(False)
        sns.despine(left=True, bottom=True, right=True)
        hbar_path = os.path.join(tmp_dir, "{}_{}_hbar.png".format(model_name, rec_type))
        plt.savefig(hbar_path, bbox_inches="tight", dpi=low_dpi)
        plt.close()

        # vertical bar plot for special error types
        fig, ax = plt.subplots(1, 1, figsize=(2, 5), dpi=high_dpi)
        sns.barplot(
            data=error_dfs["special"],
            x="Error Type",
            y="Delta mAP",
            ax=ax,
            palette=self.colors_special.values(),
        )
        ax.set_ylim(0, self.MAX_SPECIAL_DELTA_AP)
        ax.set_xlabel("")
        ax.set_ylabel("")
        ax.set_xticklabels(["FP", "FN"])
        plt.setp(ax.get_xticklabels(), fontsize=36)
        plt.setp(ax.get_yticklabels(), fontsize=28)
        ax.grid(False)
        sns.despine(left=True, bottom=True, right=True)
        vbar_path = os.path.join(tmp_dir, "{}_{}_vbar.png".format(model_name, rec_type))
        plt.savefig(vbar_path, bbox_inches="tight", dpi=low_dpi)
        plt.close()

        # get each subplot image
        pie_im = cv2.imread(pie_path)
        hbar_im = cv2.imread(hbar_path)
        vbar_im = cv2.imread(vbar_path)

        # pad the hbar image vertically
        hbar_im = np.concatenate(
            [
                np.zeros((vbar_im.shape[0] - hbar_im.shape[0], hbar_im.shape[1], 3))
                + 255,
                hbar_im,
            ],
            axis=0,
        )
        summary_im = np.concatenate([hbar_im, vbar_im], axis=1)

        lpad, rpad = (
            int(np.ceil((pie_im.shape[1] - summary_im.shape[1]) / 2)),
            int(np.floor((pie_im.shape[1] - summary_im.shape[1]) / 2)),
        )
        summary_im = np.concatenate(
            [
                np.zeros((summary_im.shape[0], lpad, 3)) + 255,
                summary_im,
                np.zeros((summary_im.shape[0], rpad, 3)) + 255,
            ],
            axis=1,
        )
        summary_im = np.concatenate([pie_im, summary_im], axis=0)

        if out_dir is None:
            fig = plt.figure()
            ax = plt.axes([0, 0, 1, 1])
            ax.set_axis_off()
            fig.add_axes(ax)
            ax.imshow((summary_im / 255)[:, :, (2, 1, 0)])
            plt.show()
            plt.close()
        else:
            cv2.imwrite(
                os.path.join(out_dir, "{}_{}_summary.png".format(model_name, rec_type)),
                summary_im,
            )
```

#### cvpods/analyser/tide/datasets.py

```python
import json
import os
import shutil
import urllib.request
import zipfile
from collections import defaultdict
from pathlib import Path
from appdirs import user_data_dir

from . import functions as f
from .data import Data


def default_name(path: str) -> str:
    return os.path.splitext(os.path.basename(path))[0]


def get_tide_path():
    if "TIDE_PATH" in os.environ:
        tide_path = os.environ["TIDE_PATH"]
    else:
        tide_path = user_data_dir("tidecv", appauthor=False)

    if not os.path.exists(tide_path):
        os.makedirs(tide_path)

    return tide_path


def download_annotations(name: str, url: str, force_download: bool = False) -> str:
    tide_path = get_tide_path()
    candidate_path = os.path.join(tide_path, name)
    finished_file_path = os.path.join(candidate_path, "_finished")
    zip_file_path = os.path.join(candidate_path, "_tmp.zip")

    # Check if the file has already been downloaded
    # If there isn't a file called _finished,
    # that means we didn't finish downloading last time, so try again
    already_downloaded = os.path.exists(candidate_path) and os.path.exists(
        finished_file_path
    )

    if not force_download and already_downloaded:
        return candidate_path
    else:
        print("{} annotations not found. Downloading...".format(name))

        if os.path.exists(candidate_path):
            shutil.rmtree(candidate_path)
        os.makedirs(candidate_path)

        urllib.request.urlretrieve(url, zip_file_path)
        with zipfile.ZipFile(zip_file_path, "r") as zip_ref:
            zip_ref.extractall(candidate_path)

        os.remove(zip_file_path)
        open(
            finished_file_path, "a"
        ).close()  # Make an empty _finished file to mark that we were successful

        print('Successfully downloaded {} to "{}"'.format(name, candidate_path))
        return candidate_path


def COCO(
    path: str = None,
    name: str = None,
    year: int = 2017,
    ann_set: str = "val",
    force_download: bool = False,
) -> Data:
    """
    Loads ground truth from a COCO-style annotation file.

    If path is not specified, this will download the COCO annotations
    for the year and ann_set specified.
    Valid years are 2014, 2017 and valid ann_sets are 'val' and 'train'.
    """
    if path is None:
        path = download_annotations(
            "COCO{}".format(year),
            "http://images.cocodataset.org/annotations/annotations_trainval{}.zip".format(
                year
            ),
            force_download,
        )

        path = os.path.join(
            path, "annotations", "instances_{}{}.json".format(ann_set, year)
        )

    if name is None:
        name = default_name(path)

    with open(path, "r") as json_file:
        cocojson = json.load(json_file)

    images = cocojson["images"]
    anns = cocojson["annotations"]
    cats = cocojson["categories"] if "categories" in cocojson else None

    # Add everything from the coco json into our data structure
    data = Data(name, max_dets=100)

    image_lookup = {}

    for idx, image in enumerate(images):
        image_lookup[image["id"]] = image
        data.add_image(image["id"], image["file_name"])

    if cats is not None:
        for cat in cats:
            data.add_class(cat["id"], cat["name"])

    for ann in anns:
        image = ann["image_id"]
        _class = ann["category_id"]
        box = ann["bbox"]
        mask = f.toRLE(
            ann["segmentation"],
            image_lookup[image]["width"],
            image_lookup[image]["height"],
        )

        if ann["iscrowd"]:
            data.add_ignore_region(image, _class, box, mask)
        else:
            data.add_ground_truth(image, _class, box, mask)

    return data


def COCOResult(path: str, name: str = None) -> Data:
    """ Loads predictions from a COCO-style results file. """
    if name is None:
        name = default_name(path)

    with open(path, "r") as json_file:
        dets = json.load(json_file)

    data = Data(name)

    for det in dets:
        image = det["image_id"]
        _cls = det["category_id"]
        score = det["score"]
        box = det["bbox"] if "bbox" in det else None
        mask = det["segmentation"] if "segmentation" in det else None

        data.add_detection(image, _cls, score, box, mask)

    return data


def LVIS(
    path: str = None,
    name: str = None,
    version_str: str = "v1",
    force_download: bool = False,
) -> Data:
    """
    Load an LVIS-style dataset.
    The version string is used for downloading the dataset
    and should be one of the versions of LVIS (e.g., v0.5, v1).

    Note that LVIS evaulation is special, but we can emulate it by adding ignore regions.
    The detector isn't punished for predicted class that LVIS annotators haven't guarenteed are in
    the image (i.e., the sum of GT annotated classes in the image and those marked explicitly not
    in the image.) In order to emulate this behavior, add ignore region labels for every class not
    found to be in the image. This is not that inefficient because ignore regions are separate out
    during mAP calculation and error processing, so adding a bunch of them doesn't hurt.

    The LVIS AP numbers are slightly lower than what the LVIS API
    reports because of these workarounds.
    """
    if path is None:
        path = download_annotations(
            "LVIS{}".format(version_str),
            "https://s3-us-west-2.amazonaws.com/dl.fbaipublicfiles.com/LVIS/lvis_{}_val.json.zip".format(  # noqa
                version_str
            ),
            force_download,
        )

        path = os.path.join("lvis_{}_val.json".format(version_str))

    if name is None:
        name = default_name(path)

    with open(path, "r") as json_file:
        lvisjson = json.load(json_file)

    images = lvisjson["images"]
    anns = lvisjson["annotations"]
    cats = lvisjson["categories"] if "categories" in lvisjson else None

    data = Data(name, max_dets=300)
    image_lookup = {}
    classes_in_img = defaultdict(lambda: set())

    for image in images:
        image_lookup[image["id"]] = image
        data.add_image(
            image["id"], image["coco_url"]
        )  # LVIS has no image names, only coco urls

        # Negative categories are guarenteed by the annotators to not be in the image.
        # Thus we should care about them during evaluation.
        for cat_id in image["neg_category_ids"]:
            classes_in_img[image["id"]].add(cat_id)

    if cats is not None:
        for cat in cats:
            data.add_class(cat["id"], cat["synset"])

    for ann in anns:
        image = ann["image_id"]
        _class = ann["category_id"]
        box = ann["bbox"]
        mask = f.toRLE(
            ann["segmentation"],
            image_lookup[image]["width"],
            image_lookup[image]["height"],
        )

        data.add_ground_truth(image, _class, box, mask)

        # There's an annotation for this class, so we should consider the class for evaluation.
        classes_in_img[image].add(_class)

    all_classes = set(data.classes.keys())

    # LVIS doesn't penalize the detector for detecting classes
    # that the annotators haven't guarenteed to be in/out of
    # the image. Here we simulate that property by adding ignore regions for all such classes.
    for image in images:
        ignored_classes = all_classes.difference(classes_in_img[image["id"]])

        # LVIS doesn't penalize the detector for mistakes made on classes
        # explicitly marked as not exhaustively annoted
        # We can emulate this by adding ignore regions for every category listed,
        # so add them to the ignored classes.
        ignored_classes.update(set(image["not_exhaustive_category_ids"]))

        for _cls in ignored_classes:
            data.add_ignore_region(image["id"], _cls)

    return data


def LVISResult(path: str, name: str = None) -> Data:
    """
    Loads predictions from a LVIS-style results file.
    Note that this is the same as a COCO-style results file.
    """
    return COCOResult(path, name)


def Pascal(
    path: str = None,
    name: str = None,
    year: int = 2007,
    ann_set: str = "val",
    force_download: bool = False,
) -> Data:
    """
    Loads the Pascal VOC 2007 or 2012 data from a COCO json.

    Valid years are 2007 and 2012, and valid ann_sets are 'train' and 'val'.
    """
    if path is None:
        path = download_annotations(
            "Pascal",
            "https://s3.amazonaws.com/images.cocodataset.org/external/external_PASCAL_VOC.zip",
            force_download,
        )

        path = os.path.join(
            path, "PASCAL_VOC", "pascal_{}{}.json".format(ann_set, year)
        )

    return COCO(path, name)


def Cityscapes(path: str, name: str = None):
    """
    Loads the fine cityscapes annotations as instance segmentation masks,
    and also generates bounding boxes for them.

    Note that we can't automatically download Cityscapes
    because it requires registration and an agreement to the ToS.
    You can get cityscapes here: https://www.cityscapes-dataset.com/

    Path should be to gtFine/<ann_set>. E.g., <path_to_cityscapes>/gtFine/val.
    """
    if name is None:
        name = default_name(path)
    data = Data(name)

    instance_classes = {
        "person": 24,
        "rider": 25,
        "car": 26,
        "truck": 27,
        "train": 31,
        "motorcycle": 32,
        "bicycle": 33,
        "bus": 28,
        "caravan": 29,
        "trailer": 30,
    }

    ignored_classes = set([29, 30])

    for class_name, class_id in instance_classes.items():
        data.add_class(class_id, class_name)

    for ann in Path(path).glob("*/*.json"):
        with open(ann) as json_file:
            ann_json = json.load(json_file)

        # Note: a string for an image ID is okay
        image_id = os.path.basename(ann).replace("_gtFine_polygons.json", "")
        objs = ann_json["objects"]

        data.add_image(
            image_id, image_id
        )  # The id in this case is just the name of the image

        # Caravan and Trailer should be ignored from all evaluation
        for _cls in ignored_classes:
            data.add_ignore_region(image_id, _cls)

        for obj in objs:
            class_label = obj["label"]
            is_crowd = False

            # Cityscapes labelers can label objects without defined boundaries as 'group'.
            # In COCO-land this would be a crowd annotation.
            # So in this case, let's make it an ignore region.
            if class_label.endswith("group"):
                is_crowd = True
                class_label = class_label[:-5]  # Remove the group at the end

            # We are only considering instance classes
            if class_label not in instance_classes:
                continue

            class_id = instance_classes[class_label]

            # If the class is not used in evaluation, don't include it
            if class_id in ignored_classes:
                continue

            # Converts a list of points to a list of lists of ints,
            # where every 2 ints represents a point
            poly = [sum(obj["polygon"], [])]
            box = f.polyToBox(poly)

            if is_crowd:
                data.add_ignore_region(image_id, class_id, box, poly)
            else:
                data.add_ground_truth(image_id, class_id, box, poly)

    return data
```

#### cvpods/analyser/tide/__init__.py

```python
__version__ = '1.0.0'

from . import datasets
from .errors.qualifiers import *
from .quantify import *
```

#### cvpods/analyser/tide/ap.py

```python
from collections import defaultdict

import numpy as np
from pycocotools import mask as mask_utils

from .data import Data


class APDataObject:
    """
    Stores all the information necessary to calculate the AP for one IoU and one class.
    Note: I type annotated this because why not.
    """

    def __init__(self):
        self.data_points = {}
        self.false_negatives = set()
        self.num_gt_positives = 0
        self.curve = None

    def apply_qualifier(self, kept_preds: set, kept_gts: set) -> object:
        """ Makes a new data object where we remove the ids in the pred and gt lists. """
        obj = APDataObject()
        num_gt_removed = 0

        for pred_id in self.data_points:
            score, is_true, info = self.data_points[pred_id]

            # If the data point we kept was a true positive, there's a corresponding ground truth
            # If so, we should only add that positive
            # if the corresponding ground truth has been kept
            if is_true and info["matched_with"] not in kept_gts:
                num_gt_removed += 1
                continue

            if pred_id in kept_preds:
                obj.data_points[pred_id] = self.data_points[pred_id]

        # Propogate the gt
        obj.false_negatives = self.false_negatives.intersection(kept_gts)
        num_gt_removed += len(self.false_negatives) - len(obj.false_negatives)

        obj.num_gt_positives = self.num_gt_positives - num_gt_removed
        return obj

    def push(self, id: int, score: float, is_true: bool, info: dict = {}):
        self.data_points[id] = (score, is_true, info)

    def push_false_negative(self, id: int):
        self.false_negatives.add(id)

    def add_gt_positives(self, num_positives: int):
        """ Call this once per image. """
        self.num_gt_positives += num_positives

    def is_empty(self) -> bool:
        return len(self.data_points) == 0 and self.num_gt_positives == 0

    def get_pr_curve(self) -> tuple:
        if self.curve is None:
            self.get_ap()
        return self.curve

    def get_ap(self) -> float:
        """ Warning: result not cached. """

        if self.num_gt_positives == 0:
            return 0

        # Sort descending by score
        data_points = list(self.data_points.values())
        data_points.sort(key=lambda x: -x[0])

        precisions = []
        recalls = []
        num_true = 0
        num_false = 0

        # Compute the precision-recall curve. The x axis is recalls and the y axis precisions.
        for datum in data_points:
            # datum[1] is whether the detection a true or false positive
            if datum[1]:
                num_true += 1
            else:
                num_false += 1

            precision = num_true / (num_true + num_false)
            recall = num_true / self.num_gt_positives

            precisions.append(precision)
            recalls.append(recall)

        # Smooth the curve by computing [max(precisions[i:]) for i in range(len(precisions))]
        # Basically, remove any temporary dips from the curve.
        # At least that's what I think, idk. COCOEval did it so I do too.
        for i in range(len(precisions) - 1, 0, -1):
            if precisions[i] > precisions[i - 1]:
                precisions[i - 1] = precisions[i]

        # Compute the integral of precision(recall) d_recall from recall=0->1
        # using fixed-length riemann summation with 101 bars.
        resolution = 100  # Standard COCO Resoluton
        y_range = [0] * (
            resolution + 1
        )  # idx 0 is recall == 0.0 and idx 100 is recall == 1.00
        x_range = np.array([x / resolution for x in range(resolution + 1)])
        recalls = np.array(recalls)

        # I realize this is weird, but all it does is find the
        # nearest precision(x) for a given x in x_range.
        # Basically, if the closest recall we have to 0.01 is 0.009
        # this sets precision(0.01) = precision(0.009).
        # I approximate the integral this way, because that's how COCOEval does it.
        indices = np.searchsorted(recalls, x_range, side="left")
        for bar_idx, precision_idx in enumerate(indices):
            if precision_idx < len(precisions):
                y_range[bar_idx] = precisions[precision_idx]

        self.curve = (x_range, y_range)

        # Finally compute the riemann sum to get our integral.
        # avg([precision(x) for x in 0:0.01:1])
        return sum(y_range) / len(y_range) * 100


class ClassedAPDataObject:
    """ Stores an APDataObject for each class in the dataset. """

    def __init__(self):
        self.objs = defaultdict(lambda: APDataObject())

    def apply_qualifier(self, pred_dict: dict, gt_dict: dict) -> object:
        ret = ClassedAPDataObject()

        for _class, obj in self.objs.items():
            pred_list = pred_dict[_class] if _class in pred_dict else set()
            gt_list = gt_dict[_class] if _class in gt_dict else set()

            ret.objs[_class] = obj.apply_qualifier(pred_list, gt_list)

        return ret

    def push(self, class_: int, id: int, score: float, is_true: bool, info: dict = {}):
        self.objs[class_].push(id, score, is_true, info)

    def push_false_negative(self, class_: int, id: int):
        self.objs[class_].push_false_negative(id)

    def add_gt_positives(self, class_: int, num_positives: int):
        self.objs[class_].add_gt_positives(num_positives)

    def get_mAP(self) -> float:
        aps = [x.get_ap() for x in self.objs.values() if not x.is_empty()]
        return sum(aps) / len(aps)

    def get_gt_positives(self) -> dict:
        return {k: v.num_gt_positives for k, v in self.objs.items()}

    def get_pr_curve(self, cat_id: int = None) -> tuple:
        if cat_id is None:
            # Average out the curves when using all categories
            curves = [x.get_pr_curve() for x in list(self.objs.values())]
            x_range = curves[0][0]
            y_range = [0] * len(curves[0][1])

            for x, y in curves:
                for i in range(len(y)):
                    y_range[i] += y[i]

            for i in range(len(y_range)):
                y_range[i] /= len(curves)
        else:
            x_range, y_range = self.objs[cat_id].get_pr_curve()

        return x_range, y_range


# Note: Unused.
class APEval:
    """
    A class that computes the AP of some dataset.
    Note that TIDE doesn't use this internally.
    This is here so you can get a look at how the AP calculation process works.
    """

    def __init__(self):
        self.iou_thresholds = [x / 100 for x in range(50, 100, 5)]
        self.ap_data = defaultdict(lambda: defaultdict(lambda: APDataObject()))

    def _eval_image(self, preds: list, gt: list, type_str: str = "box"):
        data_type = "segmentation" if type_str == "mask" else "bbox"
        preds_data = [x[data_type] for x in preds]

        # Split gt and crowd annotations
        gt_new = []
        gt_crowd = []

        for x in gt:
            if x["iscrowd"]:
                gt_crowd.append(x)
            else:
                gt_new.append(x)

        gt = gt_new

        # Some setup
        num_pred = len(preds)
        num_gt = len(gt)
        num_crowd = len(gt_crowd)

        iou_cache = mask_utils.iou(
            preds_data, [x[data_type] for x in gt], [False] * num_gt
        )

        if num_crowd > 0:
            crowd_iou_cache = mask_utils.iou(
                preds_data, [x[data_type] for x in gt_crowd], [True] * num_crowd
            )

        # Make sure we're evaluating sorted by score
        indices = list(range(num_pred))
        indices.sort(key=lambda i: -preds[i]["score"])

        classes = [x["category_id"] for x in preds]
        gt_classes = [x["category_id"] for x in gt]
        crowd_classes = [x["category_id"] for x in gt_crowd]

        for _class in set(classes + gt_classes):
            num_gt_for_class = sum([1 for x in gt_classes if x == _class])

            for iouIdx in range(len(self.iou_thresholds)):
                iou_threshold = self.iou_thresholds[iouIdx]

                gt_used = [False] * len(gt_classes)

                ap_obj = self.ap_data[iouIdx][_class]
                ap_obj.add_gt_positives(num_gt_for_class)

                for i in indices:
                    if classes[i] != _class:
                        continue

                    max_iou_found = iou_threshold
                    max_match_idx = -1
                    for j in range(num_gt):
                        if gt_used[j] or gt_classes[j] != _class:
                            continue

                        iou = iou_cache[i][j]

                        if iou > max_iou_found:
                            max_iou_found = iou
                            max_match_idx = j

                    if max_match_idx >= 0:
                        gt_used[max_match_idx] = True
                        ap_obj.push(preds[i]["score"], True)
                    else:
                        # If the detection matches a crowd, we can just ignore it
                        matched_crowd = False

                        if num_crowd > 0:
                            for j in range(len(crowd_classes)):
                                if crowd_classes[j] != _class:
                                    continue

                                iou = crowd_iou_cache[i][j]

                                if iou > iou_threshold:
                                    matched_crowd = True
                                    break

                        # All this crowd code so that we can make sure that our eval code gives the
                        # same result as COCOEval. There aren't even that many crowd annotations to
                        # begin with, but accuracy is of the utmost importance.
                        if not matched_crowd:
                            ap_obj.push(preds[i]["score"], False)

    def evaluate(self, preds: Data, gt: Data, type_str: str = "box"):
        for id in gt.ids:
            x = preds.get(id)
            y = gt.get(id)

            self._eval_image(x, y, type_str)

    def compute_mAP(self):

        num_threshs = len(self.ap_data)
        thresh_APs = []

        for thresh, classes in self.ap_data.items():
            num_classes = len([x for x in classes.values() if not x.is_empty()])
            ap = 0

            if num_classes > 0:
                class_APs = [x.get_ap() for x in classes.values() if not x.is_empty()]
                ap = sum(class_APs) / num_classes

            thresh_APs.append(ap)

        return round(sum(thresh_APs) / num_threshs * 100, 2)
```

#### cvpods/analyser/tide/data.py

```python
from collections import defaultdict


class Data:
    """
    A class to hold ground truth or predictions data in an easy to work with format.
    Note that any time they appear, bounding boxes are [x, y, width, height] and masks
    are either a list of polygons or pycocotools RLEs.

    Also, don't mix ground truth with predictions. Keep them in separate data objects.

    'max_dets' specifies the maximum number of detections the model
    is allowed to output for a given image.
    """

    def __init__(self, name: str, max_dets: int = 100):
        self.name = name
        self.max_dets = max_dets

        self.classes = {}  # Maps class ID to class name
        self.annotations = (
            []
        )  # Maps annotation ids to the corresponding annotation / prediction

        # Maps an image id to an image name and a list of annotation ids
        self.images = defaultdict(lambda: {"name": None, "anns": []})

    def _get_ignored_classes(self, image_id: int) -> set:
        anns = self.get(image_id)

        classes_in_image = set()
        ignored_classes = set()

        for ann in anns:
            if ann["ignore"]:
                if (
                    ann["class"] is not None
                    and ann["bbox"] is None
                    and ann["mask"] is None
                ):
                    ignored_classes.add(ann["class"])
            else:
                classes_in_image.add(ann["class"])

        return ignored_classes.difference(classes_in_image)

    def _make_default_class(self, id: int):
        """ (For internal use) Initializes a class id with a generated name. """

        if id not in self.classes:
            self.classes[id] = "Class " + str(id)

    def _make_default_image(self, id: int):
        if self.images[id]["name"] is None:
            self.images[id]["name"] = "Image " + str(id)

    def _prepare_box(self, box: object):
        return box

    def _prepare_mask(self, mask: object):
        return mask

    def _add(
        self,
        image_id: int,
        class_id: int,
        box: object = None,
        mask: object = None,
        score: float = 1,
        ignore: bool = False,
    ):
        """
        Add a data object to this collection. You should use one of the below functions instead.
        """
        self._make_default_class(class_id)
        self._make_default_image(image_id)
        new_id = len(self.annotations)

        self.annotations.append(
            {
                "_id": new_id,
                "score": score,
                "image": image_id,
                "class": class_id,
                "bbox": self._prepare_box(box),
                "mask": self._prepare_mask(mask),
                "ignore": ignore,
            }
        )

        self.images[image_id]["anns"].append(new_id)

    def add_ground_truth(
        self, image_id: int, class_id: int, box: object = None, mask: object = None
    ):
        """ Add a ground truth. If box or mask is None, this GT will be ignored for that mode. """
        self._add(image_id, class_id, box, mask)

    def add_detection(
        self,
        image_id: int,
        class_id: int,
        score: int,
        box: object = None,
        mask: object = None,
    ):
        """
        Add a predicted detection. If box or mask is None,
        this prediction will be ignored for that mode.
        """
        self._add(image_id, class_id, box, mask, score=score)

    def add_ignore_region(
        self,
        image_id: int,
        class_id: int = None,
        box: object = None,
        mask: object = None,
    ):
        """
        Add a region inside of which background detections should be ignored.
        You can use these to mark a region that has deliberately been left unannotated
        (e.g., if is a huge crowd of people and you
        don't want to annotate every single person in the crowd).

        If class_id is -1, this region will match any class.
        If the box / mask is None, the region will be the entire image.
        """
        self._add(image_id, class_id, box, mask, ignore=True)

    def add_class(self, id: int, name: str):
        """ Register a class name to that class ID. """
        self.classes[id] = name

    def add_image(self, id: int, name: str):
        """ Register an image name/path with an image ID. """
        self.images[id]["name"] = name

    def get(self, image_id: int):
        """ Collects all the annotations / detections for that particular image. """
        return [self.annotations[x] for x in self.images[image_id]["anns"]]
```

#### cvpods/analyser/tide/quantify.py

```python
# pylint: disable=W0613

import os
from collections import OrderedDict, defaultdict

import numpy as np
from pycocotools import mask as mask_utils

from . import functions as f
from . import plotting as P
from .ap import ClassedAPDataObject
from .data import Data
from .errors.main_errors import (
    BackgroundError,
    BoxError,
    ClassError,
    DuplicateError,
    FalseNegativeError,
    FalsePositiveError,
    MissedError,
    OtherError
)
from .errors.qualifiers import Qualifier


class TIDEExample:
    """ Computes all the data needed to evaluate a set of predictions and gt for a single image. """

    def __init__(
        self,
        preds: list,
        gt: list,
        pos_thresh: float,
        mode: str,
        max_dets: int,
        run_errors: bool = True,
    ):
        self.preds = preds
        self.gt = [x for x in gt if not x["ignore"]]
        self.ignore_regions = [x for x in gt if x["ignore"]]

        self.mode = mode
        self.pos_thresh = pos_thresh
        self.max_dets = max_dets
        self.run_errors = run_errors

        self._run()

    def _run(self):
        preds = self.preds
        gt = self.gt
        ignore = self.ignore_regions
        det_type = "bbox" if self.mode == TIDE.BOX else "mask"
        max_dets = self.max_dets

        if len(preds) == 0:
            raise RuntimeError("Example has no predictions!")

        # Sort descending by score
        preds.sort(key=lambda pred: -pred["score"])
        preds = preds[:max_dets]

        # Update internally so TIDERun can update itself if :max_dets takes effect
        self.preds = preds
        detections = [x[det_type] for x in preds]

        # IoU is [len(detections), len(gt)]
        self.gt_iou = mask_utils.iou(
            detections, [x[det_type] for x in gt], [False] * len(gt)
        )

        # Store whether a prediction / gt got used in their data list
        # Note: this is set to None if ignored, keep that in mind
        for idx, pred in enumerate(preds):
            pred["used"] = False
            pred["_idx"] = idx
            pred["iou"] = 0
        for idx, truth in enumerate(gt):
            truth["used"] = False
            truth["usable"] = False
            truth["_idx"] = idx

        pred_cls = np.array([x["class"] for x in preds])
        gt_cls = np.array([x["class"] for x in gt])

        if len(gt) > 0:
            # A[i,j] is true iff the prediction i is of the same class as gt j
            self.gt_cls_matching = pred_cls[:, None] == gt_cls[None, :]
            self.gt_cls_iou = self.gt_iou * self.gt_cls_matching

            # This will be changed in the matching calculation, so make a copy
            iou_buffer = self.gt_cls_iou.copy()

            for pred_idx, pred_elem in enumerate(preds):
                # Find the max iou ground truth for this prediction
                gt_idx = np.argmax(iou_buffer[pred_idx, :])
                iou = iou_buffer[pred_idx, gt_idx]

                pred_elem["iou"] = np.max(self.gt_cls_iou[pred_idx, :])

                if iou >= self.pos_thresh:
                    gt_elem = gt[gt_idx]

                    pred_elem["used"] = True
                    gt_elem["used"] = True
                    pred_elem["matched_with"] = gt_elem["_id"]
                    gt_elem["matched_with"] = pred_elem["_id"]

                    # Make sure this gt can't be used again
                    iou_buffer[:, gt_idx] = 0

        # Ignore regions annotations allow us to ignore predictions that fall within
        if len(ignore) > 0:
            # Because ignore regions have extra parameters,
            # it's more efficient to use a for loop here
            for ignore_region in ignore:
                if ignore_region["mask"] is None and ignore_region["bbox"] is None:
                    # The region should span the whole image
                    ignore_iou = [1] * len(preds)
                else:
                    if ignore_region[det_type] is None:
                        # There is no det_type annotation for this specific region so skip it
                        continue
                    # Otherwise, compute the crowd IoU between the detections and this region
                    ignore_iou = mask_utils.iou(
                        detections, [ignore_region[det_type]], [True]
                    )

                for pred_idx, pred_elem in enumerate(preds):
                    if (
                        not pred_elem["used"]
                        and (ignore_iou[pred_idx] > self.pos_thresh)
                        and (
                            ignore_region["class"] == pred_elem["class"]
                            or ignore_region["class"] == -1
                        )
                    ):
                        # Set the prediction to be ignored
                        pred_elem["used"] = None

        if len(gt) == 0:
            return

        # Some matrices used just for error calculation
        if self.run_errors:
            self.gt_used = np.array([x["used"] is True for x in gt])[None, :]
            self.gt_unused = ~self.gt_used

            self.gt_unused_iou = self.gt_unused * self.gt_iou
            self.gt_unused_cls = self.gt_unused_iou * self.gt_cls_matching
            self.gt_unused_noncls = self.gt_unused_iou * ~self.gt_cls_matching

            self.gt_noncls_iou = self.gt_iou * ~self.gt_cls_matching

            self.gt_used_iou = self.gt_used * self.gt_iou
            self.gt_used_cls = self.gt_used_iou * self.gt_cls_matching


class TIDERun:
    """ Holds the data for a single run of TIDE. """

    # Temporary variables stored in ground truth that we need to clear after a run
    _temp_vars = ["best_score", "best_id", "used", "matched_with", "_idx", "usable"]

    def __init__(
        self,
        gt: Data,
        preds: Data,
        pos_thresh: float,
        bg_thresh: float,
        mode: str,
        max_dets: int,
        run_errors: bool = True,
    ):
        self.gt = gt
        self.preds = preds

        self.errors = []
        self.error_dict = {_type: [] for _type in TIDE._error_types}
        self.ap_data = ClassedAPDataObject()
        self.qualifiers = {}

        # A list of false negatives per class
        self.false_negatives = {_id: [] for _id in self.gt.classes}

        self.pos_thresh = pos_thresh
        self.bg_thresh = bg_thresh
        self.mode = mode
        self.max_dets = max_dets
        self.run_errors = run_errors

        self._run()

    def _run(self):
        """ And awaaay we go """

        for image in self.gt.images:
            x = self.preds.get(image)
            y = self.gt.get(image)

            # These classes are ignored for the whole image and not in the ground truth, so
            # we can safely just remove these detections from the predictions at the start.
            ignored_classes = self.gt._get_ignored_classes(image)
            x = [pred for pred in x if pred["class"] not in ignored_classes]

            self._eval_image(x, y)

        # Store a fixed version of all the errors for testing purposes
        for error in self.errors:
            error.original = f.nonepack(error.unfix())
            error.fixed = f.nonepack(error.fix())
            error.disabled = False

        self.ap = self.ap_data.get_mAP()

        # Now that we've stored the fixed errors, we can clear the gt info
        self._clear()

    def _clear(self):
        """ Clears the ground truth so that it's ready for another run. """
        for gt in self.gt.annotations:
            for var in self._temp_vars:
                if var in gt:
                    del gt[var]

    def _add_error(self, error):
        self.errors.append(error)
        self.error_dict[type(error)].append(error)

    def _eval_image(self, preds: list, gt: list):  # noqa

        for truth in gt:
            if not truth["ignore"]:
                self.ap_data.add_gt_positives(truth["class"], 1)

        if len(preds) == 0:
            # There are no predictions for this image so add all gt as missed
            for truth in gt:
                if not truth["ignore"]:
                    self.ap_data.push_false_negative(truth["class"], truth["_id"])

                    if self.run_errors:
                        self._add_error(MissedError(truth))
                        self.false_negatives[truth["class"]].append(truth)
            return

        ex = TIDEExample(
            preds, gt, self.pos_thresh, self.mode, self.max_dets, self.run_errors
        )
        preds = ex.preds  # In case the number of predictions was restricted to the max

        for pred_idx, pred in enumerate(preds):

            # None means that the prediction was ignored
            if pred["used"] is not None:
                pred["info"] = {"iou": pred["iou"]}
                if pred["used"]:
                    pred["info"]["matched_with"] = pred["matched_with"]
                self.ap_data.push(
                    pred["class"],
                    pred["_id"],
                    pred["score"],
                    pred["used"],
                    pred["info"],
                )

            # ----- ERROR DETECTION ------ #
            # This prediction is a negative, let's find out why
            if self.run_errors and pred["used"] is False:
                # Test for BackgroundError
                if (
                    len(ex.gt) == 0
                ):  # Note this is ex.gt because it doesn't include ignore annotations
                    # There is no ground truth for this image,
                    # so just mark everything as BackgroundError
                    self._add_error(BackgroundError(pred))
                    continue

                # Test for BoxError
                idx = ex.gt_cls_iou[pred_idx, :].argmax()
                if self.bg_thresh <= ex.gt_cls_iou[pred_idx, idx] <= self.pos_thresh:
                    # This detection would have been positive if it had higher IoU with this GT
                    self._add_error(BoxError(pred, ex.gt[idx], ex))
                    continue

                # Test for ClassError
                idx = ex.gt_noncls_iou[pred_idx, :].argmax()
                if ex.gt_noncls_iou[pred_idx, idx] >= self.pos_thresh:
                    # This detection would have been a positive if it was the correct class
                    self._add_error(ClassError(pred, ex.gt[idx], ex))
                    continue

                # Test for DuplicateError
                idx = ex.gt_used_cls[pred_idx, :].argmax()
                if ex.gt_used_cls[pred_idx, idx] >= self.pos_thresh:
                    # The detection would have been marked positive but the GT was already in use
                    suppressor = self.preds.annotations[ex.gt[idx]["matched_with"]]
                    self._add_error(DuplicateError(pred, suppressor))
                    continue

                # Test for BackgroundError
                idx = ex.gt_iou[pred_idx, :].argmax()
                if ex.gt_iou[pred_idx, idx] <= self.bg_thresh:
                    # This should have been marked as background
                    self._add_error(BackgroundError(pred))
                    continue

                # A base case to catch uncaught errors
                self._add_error(OtherError(pred))

        for truth in gt:
            # If the GT wasn't used in matching, meaning it's some kind of false negative
            if not truth["ignore"] and not truth["used"]:
                self.ap_data.push_false_negative(truth["class"], truth["_id"])

                if self.run_errors:
                    self.false_negatives[truth["class"]].append(truth)

                    # The GT was completely missed, no error can correct it
                    if not truth["usable"]:
                        self._add_error(MissedError(truth))

    def fix_errors(
        self,
        condition=lambda x: False,
        transform=None,
        false_neg_dict: dict = None,
        ap_data: ClassedAPDataObject = None,
        disable_errors: bool = False,
    ) -> ClassedAPDataObject:
        """
        Returns a ClassedAPDataObject where all errors
        given the condition returns True are fixed.
        """
        if ap_data is None:
            ap_data = self.ap_data

        gt_pos = ap_data.get_gt_positives()
        new_ap_data = ClassedAPDataObject()

        # Potentially fix every error case
        for error in self.errors:
            if error.disabled:
                continue

            _id = error.get_id()
            _cls, data_point = error.original

            if condition(error):
                _cls, data_point = error.fixed

                if disable_errors:
                    error.disabled = True

                # Specific for MissingError (or anything else that affects #GT)
                if isinstance(data_point, int):
                    gt_pos[_cls] += data_point
                    data_point = None

            if data_point is not None:
                if transform is not None:
                    data_point = transform(*data_point)
                new_ap_data.push(_cls, _id, *data_point)

        # Add back all the correct ones
        for k in gt_pos.keys():
            for _id, (score, correct, info) in ap_data.objs[k].data_points.items():
                if correct:
                    if transform is not None:
                        score, correct, info = transform(score, correct, info)
                    new_ap_data.push(k, _id, score, correct, info)

        # Add the correct amount of GT positives, and also subtract if necessary
        for k, v in gt_pos.items():
            # In case you want to fix all false negatives without affecting precision
            if false_neg_dict is not None and k in false_neg_dict:
                v -= len(false_neg_dict[k])
            new_ap_data.add_gt_positives(k, v)

        return new_ap_data

    def fix_main_errors(
        self,
        progressive: bool = False,
        error_types: list = None,
        qual: Qualifier = None,
    ) -> dict:
        ap_data = self.ap_data
        last_ap = self.ap

        if qual is None:
            qual = Qualifier("", None)

        if error_types is None:
            error_types = TIDE._error_types

        errors = {}

        for error in error_types:
            _ap_data = self.fix_errors(
                qual._make_error_func(error),
                ap_data=ap_data,
                disable_errors=progressive,
            )

            new_ap = _ap_data.get_mAP()
            # If an error is negative that means it's likely due to binning differences, so just
            # Ignore the negative by setting it to 0.
            errors[error] = max(new_ap - last_ap, 0)

            if progressive:
                last_ap = new_ap
                ap_data = _ap_data

        if progressive:
            for error in self.errors:
                error.disabled = False

        return errors

    def fix_special_errors(self, qual=None) -> dict:
        return {
            FalsePositiveError: self.fix_errors(
                transform=FalsePositiveError.fix
            ).get_mAP()
            - self.ap,
            FalseNegativeError: self.fix_errors(
                false_neg_dict=self.false_negatives
            ).get_mAP()
            - self.ap,
        }

    def count_errors(self, error_types: list = None, qual=None):
        counts = {}

        if error_types is None:
            error_types = TIDE._error_types

        for error in error_types:
            if qual is None:
                counts[error] = len(self.error_dict[error])
            else:
                func = qualifiers.make_qualifier(error, qual)  # noqa
                counts[error] = len([x for x in self.errors if func(x)])

        return counts

    def apply_qualifier(self, qualifier: Qualifier) -> ClassedAPDataObject:
        """
        Applies a qualifier lambda to the AP object for this
        runs and stores the result in self.qualifiers.
        """

        pred_keep = defaultdict(lambda: set())
        gt_keep = defaultdict(lambda: set())

        for pred in self.preds.annotations:
            if qualifier.test(pred):
                pred_keep[pred["class"]].add(pred["_id"])

        for gt in self.gt.annotations:
            if not gt["ignore"] and qualifier.test(gt):
                gt_keep[gt["class"]].add(gt["_id"])

        new_ap_data = self.ap_data.apply_qualifier(pred_keep, gt_keep)
        self.qualifiers[qualifier.name] = new_ap_data.get_mAP()
        return new_ap_data


class TIDE:

    # This is just here to define a consistent order of the error types
    _error_types = [
        ClassError,
        BoxError,
        OtherError,
        DuplicateError,
        BackgroundError,
        MissedError,
    ]
    _special_error_types = [FalsePositiveError, FalseNegativeError]

    # Threshold splits for different challenges
    COCO_THRESHOLDS = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]
    VOL_THRESHOLDS = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]

    # The modes of evaluation
    BOX = "bbox"
    MASK = "mask"

    def __init__(
        self,
        pos_threshold: float = 0.5,
        background_threshold: float = 0.1,
        mode: str = BOX,
    ):
        self.pos_thresh = pos_threshold
        self.bg_thresh = background_threshold
        self.mode = mode

        self.pos_thresh_int = int(self.pos_thresh * 100)

        self.runs = {}
        self.run_thresholds = {}
        self.run_main_errors = {}
        self.run_special_errors = {}

        self.qualifiers = OrderedDict()

        self.plotter = P.Plotter()

    def evaluate(
        self,
        gt: Data,
        preds: Data,
        pos_threshold: float = None,
        background_threshold: float = None,
        mode: str = None,
        name: str = None,
        use_for_errors: bool = True,
    ) -> TIDERun:
        pos_thresh = self.pos_thresh if pos_threshold is None else pos_threshold
        bg_thresh = (
            self.bg_thresh if background_threshold is None else background_threshold
        )
        mode = self.mode if mode is None else mode
        name = preds.name if name is None else name

        run = TIDERun(
            gt, preds, pos_thresh, bg_thresh, mode, gt.max_dets, use_for_errors
        )

        if use_for_errors:
            self.runs[name] = run

        return run

    def evaluate_range(
        self,
        gt: Data,
        preds: Data,
        thresholds: list = COCO_THRESHOLDS,
        pos_threshold: float = None,
        background_threshold: float = None,
        mode: str = None,
        name: str = None,
    ) -> dict:

        if pos_threshold is None:
            pos_threshold = self.pos_thresh
        if name is None:
            name = preds.name

        self.run_thresholds[name] = []

        for thresh in thresholds:

            run = self.evaluate(
                gt,
                preds,
                pos_threshold=thresh,
                background_threshold=background_threshold,
                mode=mode,
                name=name,
                use_for_errors=(pos_threshold == thresh),
            )

            self.run_thresholds[name].append(run)

    def add_qualifiers(self, *quals):
        """
        Applies any number of Qualifier objects to evaluations that have been run up to now.
        See qualifiers.py for examples.
        """
        raise NotImplementedError("Qualifiers coming soon.")
        # for q in quals:
        # 	for run_name, run in self.runs.items():
        # 		if run_name in self.run_thresholds:
        # 			# If this was a threshold run, apply the qualifier for every run
        # 			for trun in self.run_thresholds[run_name]:
        # 				trun.apply_qualifier(q)
        # 		else:
        # 			# If this had no threshold, just apply it to the main run
        # 			run.apply_qualifier(q)

        # 	self.qualifiers[q.name] = q

    def summarize(self):
        """
        Summarizes the mAP values and errors for all runs in this TIDE object.
        Results are printed to the console.
        """
        main_errors = self.get_main_errors()
        special_errors = self.get_special_errors()

        for run_name, run in self.runs.items():
            print("-- {} --\n".format(run_name))

            # If we evaluated on all thresholds, print them here
            if run_name in self.run_thresholds:
                thresh_runs = self.run_thresholds[run_name]
                aps = [trun.ap for trun in thresh_runs]

                # Print Overall AP for a threshold run
                ap_title = "{} AP @ [{:d}-{:d}]".format(
                    thresh_runs[0].mode,
                    int(thresh_runs[0].pos_thresh * 100),
                    int(thresh_runs[-1].pos_thresh * 100),
                )
                print("{:s}: {:.2f}".format(ap_title, sum(aps) / len(aps)))

                # Print AP for every threshold on a threshold run
                P.print_table(
                    [
                        ["Thresh"]
                        + [str(int(trun.pos_thresh * 100)) for trun in thresh_runs],
                        ["  AP  "]
                        + ["{:6.2f}".format(trun.ap) for trun in thresh_runs],
                    ],
                    title=ap_title,
                )

                # Print qualifiers for a threshold run
                if len(self.qualifiers) > 0:
                    print()
                    # Can someone ban me from using list comprehension? this is unreadable
                    qAPs = [
                        f.mean(
                            [
                                trun.qualifiers[q]
                                for trun in thresh_runs
                                if q in trun.qualifiers
                            ]
                        )
                        for q in self.qualifiers
                    ]

                    P.print_table(
                        [
                            ["Name"] + list(self.qualifiers.keys()),
                            [" AP "] + ["{:6.2f}".format(qAP) for qAP in qAPs],
                        ],
                        title="Qualifiers {}".format(ap_title),
                    )

            # Otherwise, print just the one run we did
            else:
                # Print Overall AP for a regular run
                ap_title = "{} AP @ {:d}".format(run.mode, int(run.pos_thresh * 100))
                print("{}: {:.2f}".format(ap_title, run.ap))

                # Print qualifiers for a regular run
                if len(self.qualifiers) > 0:
                    print()
                    qAPs = [
                        run.qualifiers[q] if q in run.qualifiers else 0
                        for q in self.qualifiers
                    ]
                    P.print_table(
                        [
                            ["Name"] + list(self.qualifiers.keys()),
                            [" AP "] + ["{:6.2f}".format(qAP) for qAP in qAPs],
                        ],
                        title="Qualifiers {}".format(ap_title),
                    )

            print()
            # Print the main errors
            P.print_table(
                [
                    ["Type"] + [err.short_name for err in TIDE._error_types],
                    [" dAP"]
                    + [
                        "{:6.2f}".format(main_errors[run_name][err.short_name])
                        for err in TIDE._error_types
                    ],
                ],
                title="Main Errors",
            )

            print()
            # Print the special errors
            P.print_table(
                [
                    ["Type"] + [err.short_name for err in TIDE._special_error_types],
                    [" dAP"]
                    + [
                        "{:6.2f}".format(special_errors[run_name][err.short_name])
                        for err in TIDE._special_error_types
                    ],
                ],
                title="Special Error",
            )

            print()

    def plot(self, out_dir: str = None):
        """
        Plots a summary model for each run in this TIDE object.
        Images will be outputted to out_dir, which will be created if it doesn't exist.
        """

        if out_dir is not None:
            if not os.path.exists(out_dir):
                os.makedirs(out_dir)

        errors = self.get_all_errors()

        for run_name, run in self.runs.items():
            self.plotter.make_summary_plot(
                out_dir, errors, run_name, run.mode, hbar_names=True
            )

    def get_main_errors(self):
        errors = {}

        for run_name, run in self.runs.items():
            if run_name in self.run_main_errors:
                errors[run_name] = self.run_main_errors[run_name]
            else:
                errors[run_name] = {
                    error.short_name: value
                    for error, value in run.fix_main_errors().items()
                }

        return errors

    def get_special_errors(self):
        errors = {}

        for run_name, run in self.runs.items():
            if run_name in self.run_special_errors:
                errors[run_name] = self.run_special_errors[run_name]
            else:
                errors[run_name] = {
                    error.short_name: value
                    for error, value in run.fix_special_errors().items()
                }

        return errors

    def get_all_errors(self):
        """
        returns {
                'main'   : { run_name: { error_name: float } },
                'special': { run_name: { error_name: float } },
        }
        """
        return {"main": self.get_main_errors(), "special": self.get_special_errors()}
```

##### cvpods/analyser/tide/errors/error.py

```python
from typing import Union

import cv2

from .. import functions as f


class Error:
    """ A base class for all error types. """

    def fix(self) -> Union[tuple, None]:
        """
        Returns a fixed version of the AP data point for this error or
        None if this error should be suppressed.

        Return type is:
                class:int, (score:float, is_positive:bool, info:dict)
        """
        raise NotImplementedError

    def unfix(self) -> Union[tuple, None]:
        """ Returns the original version of this data point. """

        if hasattr(self, "pred"):
            return self.pred["class"], (self.pred["score"], False, self.pred["info"])
        else:
            return None

    def get_id(self) -> int:
        if hasattr(self, "pred"):
            return self.pred["_id"]
        elif hasattr(self, "gt"):
            return self.gt["_id"]
        else:
            return -1

    def show(
        self,
        dataset,
        out_path: str = None,
        pred_color: tuple = (43, 12, 183),
        gt_color: tuple = (43, 183, 12),
        font=cv2.FONT_HERSHEY_SIMPLEX,
    ):

        pred = self.pred if hasattr(self, "pred") else self.gt
        img = dataset.get_img_with_anns(pred["image_id"])

        if hasattr(self, "gt"):
            img = cv2.rectangle(img, *f.points(self.gt["bbox"]), gt_color, 2)
            img = cv2.putText(
                img,
                dataset.cat_name(self.gt["category_id"]),
                (100, 200),
                font,
                1,
                gt_color,
                2,
                cv2.LINE_AA,
                False,
            )

        if hasattr(self, "pred"):
            img = cv2.rectangle(img, *f.points(pred["bbox"]), pred_color, 2)
            img = cv2.putText(
                img,
                "%s (%.2f)" % (dataset.cat_name(pred["category_id"]), pred["score"]),
                (100, 100),
                font,
                1,
                pred_color,
                2,
                cv2.LINE_AA,
                False,
            )

        if out_path is None:
            cv2.imshow(self.short_name, img)
            cv2.moveWindow(self.short_name, 100, 100)

            cv2.waitKey()
            cv2.destroyAllWindows()
        else:
            cv2.imwrite(out_path, img)

    def get_info(self, dataset):
        info = {}
        info["type"] = self.short_name

        if hasattr(self, "gt"):
            info["gt"] = self.gt
        if hasattr(self, "pred"):
            info["pred"] = self.pred

        img_id = (self.pred if hasattr(self, "pred") else self.gt)["image_id"]
        info["all_gt"] = dataset.get(img_id)
        info["img"] = dataset.get_img(img_id)

        return info


class BestGTMatch:
    """
    Some errors are fixed by changing false positives to true positives.
    The issue with fixing these errors naively is that you might have
    multiple errors attempting to fix the same GT. In that case, we need
    to select which error actually gets fixed, and which others just get
    suppressed (since we can only fix one error per GT).

    To address this, this class finds the prediction with the hiighest
    score and then uses that as the error to fix, while suppressing all
    other errors caused by the same GT.
    """

    def __init__(self, pred, gt):
        self.pred = pred
        self.gt = gt

        if self.gt["used"]:
            self.suppress = True
        else:
            self.suppress = False
            self.gt["usable"] = True

            score = self.pred["score"]

            if "best_score" not in self.gt:
                self.gt["best_score"] = -1

            if self.gt["best_score"] < score:
                self.gt["best_score"] = score
                self.gt["best_id"] = self.pred["_id"]

    def fix(self):
        if self.suppress or self.gt["best_id"] != self.pred["_id"]:
            return None
        else:
            return (self.pred["score"], True, self.pred["info"])
```

##### cvpods/analyser/tide/errors/qualifiers.py

```python
# Defines qualifiers like "Extra small box"


def _area(x):
    return x["bbox"][2] * x["bbox"][3]


def _ar(x):
    return x["bbox"][2] / x["bbox"][3]


class Qualifier:
    """
    Creates a qualifier with the given name.

    test_func should be a callable object (e.g., lambda) that
    takes in as input an annotation object (either a ground truth or prediction)
    and returns whether or not that object qualifies (i.e., a bool).
    """

    def __init__(self, name: str, test_func: object):
        self.test = test_func
        self.name = name

    # This is horrible, but I like it
    def _make_error_func(self, error_type):
        return (
            (
                lambda err: isinstance(err, error_type)
                and (self.test(err.gt) if hasattr(err, "gt") else self.test(err.pred))
            )
            if self.test is not None
            else (lambda err: isinstance(err, error_type))
        )


AREA = [
    Qualifier("Small", lambda x: _area(x) <= 32 ** 2),
    Qualifier("Medium", lambda x: 32 ** 2 < _area(x) <= 96 ** 2),
    Qualifier("Large", lambda x: 96 ** 2 < _area(x)),
]

ASPECT_RATIO = [
    Qualifier("Tall", lambda x: _ar(x) <= 0.75),
    Qualifier("Square", lambda x: 0.75 < _ar(x) <= 1.33),
    Qualifier("Wide", lambda x: 1.33 < _ar(x)),
]
```

##### cvpods/analyser/tide/errors/main_errors.py

```python
# pylint: disable=W0613

from .error import BestGTMatch, Error


class ClassError(Error):

    description = (
        "Error caused when a prediction would have been marked positive "
        + "if it had the correct class."
    )
    short_name = "Cls"

    def __init__(self, pred: dict, gt: dict, ex):
        self.pred = pred
        self.gt = gt

        self.match = BestGTMatch(pred, gt) if not self.gt["used"] else None

    def fix(self):
        if self.match is None:
            return None
        return self.gt["class"], self.match.fix()


class BoxError(Error):

    description = (
        "Error caused when a prediction would have "
        "been marked positive if it was localized better."
    )
    short_name = "Loc"

    def __init__(self, pred: dict, gt: dict, ex):
        self.pred = pred
        self.gt = gt

        self.match = BestGTMatch(pred, gt) if not self.gt["used"] else None

    def fix(self):
        if self.match is None:
            return None
        return self.pred["class"], self.match.fix()


class DuplicateError(Error):

    description = (
        "Error caused when a prediction would have been marked positive "
        + "if the GT wasn't already in use by another detection."
    )
    short_name = "Dupe"

    def __init__(self, pred: dict, suppressor: dict):
        self.pred = pred
        self.suppressor = suppressor

    def fix(self):
        return None


class BackgroundError(Error):

    description = (
        "Error caused when this detection should have been"
        "classified as background (IoU < 0.1)."
    )
    short_name = "Bkg"

    def __init__(self, pred: dict):
        self.pred = pred

    def fix(self):
        return None


class OtherError(Error):

    description = "This detection didn't fall into any of the other error categories."
    short_name = "Both"

    def __init__(self, pred: dict):
        self.pred = pred

    def fix(self):
        return None


class MissedError(Error):

    description = (
        "Represents GT missed by the model."
        "Doesn't include GT corrected elsewhere in the model."
    )
    short_name = "Miss"

    def __init__(self, gt: dict):
        self.gt = gt

    def fix(self):
        return self.gt["class"], -1


# These are special errors so no inheritence


class FalsePositiveError:

    description = (
        "Represents the potential AP gained by having perfect precision"
        + " (e.g., by scoring all false positives as conf=0) without affecting recall."
    )
    short_name = "FalsePos"

    @staticmethod
    def fix(score: float, correct: bool, info: dict) -> tuple:
        if correct:
            return 1, True, info
        else:
            return 0, False, info


class FalseNegativeError:

    description = (
        "Represents the potentially AP gained by having perfect recall"
        + " without affecting precision."
    )
    short_name = "FalseNeg"
```

### cvpods/configs/pointrend_config.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from .rcnn_fpn_config import RCNNFPNConfig

_config_dict = dict(
    MODEL=dict(
        ROI_HEADS=dict(
            # NAME="PointRendROIHeads",
            IN_FEATURES=["p2", "p3", "p4", "p5"],
        ),
        ROI_BOX_HEAD=dict(
            TRAIN_ON_PRED_BOXES=True,
        ),
        ROI_MASK_HEAD=dict(
            # NAME="CoarseMaskHead",
            # Names of the input feature maps to be used by a coarse mask head.
            IN_FEATURES=["p2"],
            FC_DIM=1024,
            NUM_FC=2,
            # The side size of a coarse mask head prediction.
            OUTPUT_SIDE_RESOLUTION=7,
            # True if point head is used.
            POINT_HEAD_ON=True,
        ),
        POINT_HEAD=dict(
            # Names of the input feature maps to be used by a mask point head.
            IN_FEATURES=["p2"],
            NUM_CLASSES=80,
            FC_DIM=256,
            NUM_FC=3,
            # Number of points sampled during training for a mask point head.
            TRAIN_NUM_POINTS=14 * 14,
            # Oversampling parameter for PointRend point sampling during training.
            # Parameter `k` in the original paper.
            OVERSAMPLE_RATIO=3,
            # Importance sampling parameter for PointRend point sampling during training.
            # Parametr `beta` in the original paper.
            IMPORTANCE_SAMPLE_RATIO=0.75,
            # Number of subdivision steps during inference.
            SUBDIVISION_STEPS=5,
            # Maximum number of points selected at each subdivision step (N).
            SUBDIVISION_NUM_POINTS=28 * 28,
            CLS_AGNOSTIC_MASK=False,
            # If True, then coarse prediction features are used as inout for each layer
            # in PointRend's MLP.
            COARSE_PRED_EACH_LAYER=True,
            # COARSE_SEM_SEG_HEAD_NAME="SemSegFPNHead"
        ),
    ),
    INPUT=dict(
        # PointRend for instance segmenation does not work with "polygon" mask_format
        MASK_FORMAT="bitmask",
    ),
    DATALOADER=dict(FILTER_EMPTY_ANNOTATIONS=False,),
)


class PointRendRCNNFPNConfig(RCNNFPNConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = PointRendRCNNFPNConfig()
```

### cvpods/configs/config_helper.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import logging
import re
import time
import warnings
from ast import literal_eval
from colorama import Back, Fore, Style
from easydict import EasyDict


def highlight(keyword, target, color=Fore.BLACK + Back.YELLOW):
    """
    use given color to highlight keyword in target string

    Args:
        keyword(str): highlight string
        target(str): target string
        color(str): string represent the color, use black foreground
        and yellow background as default

    Returns:
        (str) target string with keyword highlighted

    """
    return re.sub(keyword, color + r"\g<0>" + Style.RESET_ALL, target)


def find_key(param_dict: dict, key: str) -> dict:
    """
    find key in dict

    Args:
        param_dict(dict):
        key(str):

    Returns:
        (dict)

    Examples::
        >>> d = dict(abc=2, ab=4, c=4)
        >>> find_key(d, "ab")
        {'abc': 2, 'ab':4}

    """
    find_result = {}
    for k, v in param_dict.items():
        if re.search(key, k):
            find_result[k] = v
        if isinstance(v, dict):
            res = find_key(v, key)
            if res:
                find_result[k] = res
    return find_result


def diff_dict(src, dst):
    """
    find difference between src dict and dst dict

    Args:
        src(dict): src dict
        dst(dict): dst dict

    Returns:
        (dict) dict contains all the difference key

    """
    diff_result = {}
    for k, v in src.items():
        if k not in dst:
            diff_result[k] = v
        elif dst[k] != v:
            if isinstance(v, dict):
                diff_result[k] = diff_dict(v, dst[k])
            else:
                diff_result[k] = v
    return diff_result


def version_update(config):
    """
    Backward compatibility of old config's Augmentation pipelines and Optimizer configs;
    Convert old format into new ones.
    """
    # Old Augmentation config
    input_cfg = config.INPUT
    train_input_pop_list = [
        ("MIN_SIZE_TRAIN", "short_edge_length"),
        ("MIN_SIZE_TRAIN_SAMPLING", "sample_style"),
        ("MAX_SIZE_TRAIN", "max_size")
    ]
    test_input_pop_list = [
        ("MIN_SIZE_TEST", "short_edge_length"),
        ("MAX_SIZE_TEST", "max_size"),
    ]
    train_contains = [k for k in train_input_pop_list if k[0] in input_cfg]
    test_contains = [k for k in test_input_pop_list if k[0] in input_cfg]
    train_transforms = [t[0] for t in input_cfg["AUG"]["TRAIN_PIPELINES"]]
    test_transforms = [t[0] for t in input_cfg["AUG"]["TEST_PIPELINES"]]

    warnings.filterwarnings(
        "default",
        category=DeprecationWarning,
        module=__name__
    )  # only change local warning level
    if train_contains:
        warnings.warn("Old format training config will be deprecated ", DeprecationWarning)
        time.sleep(1)
    for k in train_contains:
        if "ResizeShortestEdge" in train_transforms:
            idx = idx = train_transforms.index("ResizeShortestEdge")
            input_cfg["AUG"]["TRAIN_PIPELINES"][idx][1][k[1]] = input_cfg[k[0]]

    if test_contains:
        warnings.warn("Old format testing config will be deprecated ", DeprecationWarning)
        time.sleep(1)
    for k in test_contains:
        if "ResizeShortestEdge" in test_transforms:
            idx = test_transforms.index("ResizeShortestEdge")
            input_cfg["AUG"]["TEST_PIPELINES"][idx][1][k[1]] = input_cfg[k[0]]

    for elem in train_contains + test_contains:
        config.INPUT.pop(elem[0])

    # Old SOLVER format
    solver_cfg = config.SOLVER
    candidates = list(solver_cfg.keys())
    lr_pop_list = [
        "LR_SCHEDULER_NAME", "MAX_ITER", "STEPS", "WARMUP_FACTOR", "WARMUP_ITERS",
        "WARMUP_METHOD", "GAMMA"
    ]
    optim_pop_list = [
        "BASE_LR", "BIAS_LR_FACTOR", "WEIGHT_DECAY", "WEIGHT_DECAY_NORM", "WEIGHT_DECAY_BIAS",
        "MOMENTUM"
    ]
    if any(item in candidates for item in lr_pop_list + optim_pop_list):
        warnings.warn("Old format solver config will be deprecated ", DeprecationWarning)
        time.sleep(1)
        if "LR_SCHEDULER" in solver_cfg and "OPTIMIZER" in solver_cfg:
            for k in candidates:
                if k in lr_pop_list:
                    solver_cfg["LR_SCHEDULER"][k] = solver_cfg[k]
                elif k in optim_pop_list:
                    solver_cfg["OPTIMIZER"][k] = solver_cfg[k]
        else:
            solver_cfg["LR_SCHEDULER"] = dict(
                NAME=solver_cfg.get("LR_SCHEDULER_NAME", "WarmupMultiStepLR"),
                MAX_ITER=solver_cfg.get("MAX_ITER", 30000),
                STEPS=solver_cfg.get("STEPS", (20000,)),
                WARMUP_FACTOR=solver_cfg.get("WARMUP_FACTOR", 1.0 / 1000),
                WARMUP_ITERS=solver_cfg.get("WARMUP_ITERS", 1000),
                WARMUP_METHOD=solver_cfg.get("WARMUP_METHOD", "linear"),
                GAMMA=solver_cfg.get("GAMMA", 0.1),
            )
            solver_cfg["OPTIMIZER"] = dict(
                NAME="SGD",
                BASE_LR=solver_cfg.get("BASE_LR", 0.001),
                BIAS_LR_FACTOR=solver_cfg.get("BIAS_LR_FACTOR", 1.0),
                WEIGHT_DECAY=solver_cfg.get("WEIGHT_DECAY", 0.0001),
                WEIGHT_DECAY_NORM=solver_cfg.get("WEIGHT_DECAY_NORM", 0.0),
                WEIGHT_DECAY_BIAS=solver_cfg.get("WEIGHT_DECAY_BIAS", 0.0001),
                MOMENTUM=solver_cfg.get("MOMENTUM", 0.9),
            )

        for k in candidates:
            if k in lr_pop_list + optim_pop_list:
                solver_cfg.pop(k)


def _assert_with_logging(cond, msg):
    logger = logging.getLogger(__name__)

    if not cond:
        logger.error(msg)
    assert cond, msg


def _decode_cfg_value(value):
    """
    Decodes a raw config value (e.g., from a yaml config files or command
    line argument) into a Python object.
    If the value is a dict, it will be interpreted as a new config dict.
    If the value is a str, it will be evaluated as literals.
    Otherwise it is returned as-is.

    Args:
        value (dict or str): value to be decoded
    """
    if isinstance(value, str):
        # Try to interpret `value` as a string, number, tuple, list, dict, boolean, or None
        try:
            value = literal_eval(value)
        # The following two excepts allow v to pass through when it represents a
        # string.
        #
        # Longer explanation:
        # The type of v is always a string (before calling literal_eval), but
        # sometimes it *represents* a string and other times a data structure, like
        # a list. In the case that v represents a string, what we got back from the
        # yaml parser is 'foo' *without quotes* (so, not '"foo"'). literal_eval is
        # ok with '"foo"', but will raise a ValueError if given 'foo'. In other
        # cases, like paths (v = 'foo/bar' and not v = '"foo/bar"'), literal_eval
        # will raise a SyntaxError.
        except (ValueError, SyntaxError):
            pass

    if isinstance(value, dict):
        return EasyDict(value)
    else:
        return value


def _cast_cfg_value_type(replacement, original, full_key):
    """
    Checks that `replacement`, which is intended to replace `original` is of
    the right type. The type is correct if it matches exactly or is one of a few
    cases in which the type can be easily coerced.
    """
    logger = logging.getLogger(__name__)
    ori_type = type(original)
    new_type = type(replacement)

    if original is None or replacement is None:
        logger.info("None type, {} to {}".format(ori_type, new_type))
        return replacement

    # The types must match (with some exceptions)
    if new_type == ori_type:
        logger.info(
            "change value of '{}' from {} to {}".format(full_key, original, replacement)
        )
        return replacement

    # try to casts replacement to original type
    try:
        replacement = ori_type(replacement)
        return replacement
    except Exception:
        logger.error(
            "Could not cast '{}' from {} to {} with values ({} vs. {})".format(
                full_key, new_type, ori_type, replacement, original)
        )
        raise ValueError
```

### cvpods/configs/retinanet_config.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from .base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        # Backbone NAME: "build_retinanet_resnet_fpn_backbone"
        RESNETS=dict(OUT_FEATURES=["res3", "res4", "res5"]),
        FPN=dict(
            IN_FEATURES=["res3", "res4", "res5"],
            BLOCK_IN_FEATURES="res5",
        ),
        ANCHOR_GENERATOR=dict(
            SIZES=[
                [x, x * 2 ** (1.0 / 3), x * 2 ** (2.0 / 3)]
                for x in [32, 64, 128, 256, 512]
            ]
        ),
        RETINANET=dict(
            # This is the number of foreground classes.
            NUM_CLASSES=80,
            IN_FEATURES=["p3", "p4", "p5", "p6", "p7"],
            # Convolutions to use in the cls and bbox tower
            # NOTE: this doesn't include the last conv for logits
            NUM_CONVS=4,
            # IoU overlap ratio [bg, fg] for labeling anchors.
            # Anchors with < bg are labeled negative (0)
            # Anchors  with >= bg and < fg are ignored (-1)
            # Anchors with >= fg are labeled positive (1)
            IOU_THRESHOLDS=[0.4, 0.5],
            IOU_LABELS=[0, -1, 1],
            # Prior prob for rare case (i.e. foreground) at the beginning of training.
            # This is used to set the bias for the logits layer of the classifier subnet.
            # This improves training stability in the case of heavy class imbalance.
            PRIOR_PROB=0.01,
            # Inference cls score threshold, only anchors with score > INFERENCE_TH are
            # considered for inference (to improve speed)
            SCORE_THRESH_TEST=0.05,
            TOPK_CANDIDATES_TEST=1000,
            NMS_THRESH_TEST=0.5,
            # Weights on (dx, dy, dw, dh) for normalizing Retinanet anchor regression targets
            BBOX_REG_WEIGHTS=(1.0, 1.0, 1.0, 1.0),
            # Loss parameters
            FOCAL_LOSS_GAMMA=2.0,
            FOCAL_LOSS_ALPHA=0.25,
            SMOOTH_L1_LOSS_BETA=0.1,
        ),
    ),
)


class RetinaNetConfig(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = RetinaNetConfig()
```

### cvpods/configs/base_config.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates.

import collections
import os
import pprint
import re
import six
from colorama import Back, Fore
from easydict import EasyDict
from tabulate import tabulate

from .config_helper import (
    _assert_with_logging,
    _cast_cfg_value_type,
    _decode_cfg_value,
    diff_dict,
    find_key,
    highlight,
    version_update
)

# python 3.8+ compatibility
try:
    collectionsAbc = collections.abc
except ImportError:
    collectionsAbc = collections


_config_dict = dict(
    MODEL=dict(
        DEVICE="cuda",
        # Path (possibly with schema like catalog://, detectron2://, s3://) to a checkpoint file
        # to be loaded to the model. You can find available models in the model zoo.
        WEIGHTS="",
        # Indicate whether convert final checkpoint to use as pretrain weights of preceeding model
        AS_PRETRAIN=False,
        # Values to be used for image normalization (BGR order, since INPUT.FORMAT defaults to BGR)
        # To train on images of different number of channels, just set different mean & std.
        PIXEL_MEAN=[103.530, 116.280, 123.675],
        # When using pre-trained models in Detectron1 or any MSRA models,
        # std has been absorbed into its conv1 weights, so the std needs to be
        # set 1. Otherwise, you can use [57.375, 57.120, 58.395] (ImageNet std)
        PIXEL_STD=[1.0, 1.0, 1.0],
    ),
    INPUT=dict(
        AUG=dict(
            TRAIN_PIPELINES=[
                ("ResizeShortestEdge", dict(
                    short_edge_length=(800,), max_size=1333, sample_style="choice")),
                ("RandomFlip", dict()),
            ],
            TEST_PIPELINES=[
                ("ResizeShortestEdge", dict(
                    short_edge_length=800, max_size=1333, sample_style="choice")),
            ],
        ),
        # Whether the model needs RGB, YUV, HSV etc.
        FORMAT="BGR",
        # Mask R-CNN supports either "polygon" or "bitmask" as ground truth.
        MASK_FORMAT="polygon",
    ),
    DATASETS=dict(
        CUSTOM_TYPE=("ConcatDataset", dict()),
        # List of the dataset names for training.
        # Must be registered in cvpods/data/datasets/paths_route
        TRAIN=(),
        # List of the pre-computed proposal files for training, which must be consistent
        # with datasets listed in DATASETS.TRAIN.
        PROPOSAL_FILES_TRAIN=(),
        # Number of top scoring precomputed proposals to keep for training
        PRECOMPUTED_PROPOSAL_TOPK_TRAIN=2000,
        # List of the dataset names for testing.
        # Must be registered in cvpods/data/datasets/paths_route
        TEST=(),
        # List of the pre-computed proposal files for test, which must be consistent
        # with datasets listed in DATASETS.TEST.
        PROPOSAL_FILES_TEST=(),
        # Number of top scoring precomputed proposals to keep for test
        PRECOMPUTED_PROPOSAL_TOPK_TEST=1000,
    ),
    DATALOADER=dict(
        # Number of data loading threads
        NUM_WORKERS=2,
        # If True, each batch should contain only images for which the aspect ratio
        # is compatible. This groups portrait images together, and landscape images
        # are not batched with portrait images.
        ASPECT_RATIO_GROUPING=True,
        # Default sampler for dataloader
        SAMPLER_TRAIN="DistributedGroupSampler",
        # Repeat threshold for RepeatFactorTrainingSampler
        REPEAT_THRESHOLD=0.0,
        # If True, the dataloader will filter out images that have no associated
        # annotations at train time.
        FILTER_EMPTY_ANNOTATIONS=True,
    ),
    SOLVER=dict(
        # Configs of lr scheduler
        LR_SCHEDULER=dict(
            NAME="WarmupMultiStepLR",
            MAX_ITER=40000,
            MAX_EPOCH=None,
            # STEPS supports both iterations and epochs.
            # If MAX_EPOCH are specified, STEPS will be calculated automatically
            STEPS=(30000,),
            WARMUP_FACTOR=1.0 / 1000,
            WARMUP_ITERS=1000,
            # WARMUP_METHOD in "linear", "constant", "brunin"
            WARMUP_METHOD="linear",
            # Decrease learning rate by GAMMA.
            GAMMA=0.1,
        ),
        OPTIMIZER=dict(
            NAME="D2SGD",
            BASE_LR=0.001,
            # Detectron v1 (and previous detection code) used a 2x higher LR and 0 WD for biases.
            # This is not useful (at least for recent models). You should avoid
            # changing these and they exist only to reproduce Detectron v1 training if desired.
            BIAS_LR_FACTOR=1.0,
            WEIGHT_DECAY=0.0001,
            # The weight decay that's applied to parameters of normalization layers
            WEIGHT_DECAY_NORM=0.0,
            MOMENTUM=0.9,
        ),
        # Gradient clipping
        CLIP_GRADIENTS=dict(
            ENABLED=False,
            # - "value": the absolute values of elements of each gradients are clipped
            # - "norm": the norm of the gradient for each parameter is clipped thus
            #   affecting all elements in the parameter
            CLIP_TYPE="value",
            # Maximum absolute value used for clipping gradients
            CLIP_VALUE=1.0,
            # Floating point number p for L-p norm to be used with the "norm"
            # gradient clipping type; for L-inf, please specify .inf
            NORM_TYPE=2.0,
        ),
        # Save a checkpoint after every this number of iterations
        CHECKPOINT_PERIOD=5000,
        # Number of images per batch across all machines.
        # If we have 16 GPUs and IMS_PER_BATCH = 32,
        # each GPU will see 2 images per batch.
        IMS_PER_BATCH=16,
        IMS_PER_DEVICE=2,
        BATCH_SUBDIVISIONS=1,
    ),
    TEST=dict(
        # For end-to-end tests to verify the expected accuracy.
        # Each item is [task, metric, value, tolerance]
        # e.g.: [['bbox', 'AP', 38.5, 0.2]]
        EXPECTED_RESULTS=[],
        # The period (in terms of steps) to evaluate the model during training.
        # If using positive EVAL_PERIOD, every #EVAL_PERIOD iter will evaluate automaticly.
        # If EVAL_PERIOD = 0, model will be evaluated after training.
        # If using negative EVAL_PERIOD, no evaluation will be applied.
        EVAL_PERIOD=0,
        # The sigmas used to calculate keypoint OKS. See http://cocodataset.org/#keypoints-eval
        # When empty it will use the defaults in COCO.
        # Otherwise it should have the same length as ROI_KEYPOINT_HEAD.NUM_KEYPOINTS.
        KEYPOINT_OKS_SIGMAS=[],
        # Maximum number of detections to return per image during inference (100 is
        # based on the limit established for the COCO dataset).
        DETECTIONS_PER_IMAGE=100,
        AUG=dict(
            ENABLED=False,
            MIN_SIZES=(400, 500, 600, 700, 800, 900, 1000, 1100, 1200),
            MAX_SIZE=4000,
            FLIP=True,
            EXTRA_SIZES=(),
            SCALE_FILTER=False,
            SCALE_RANGES=(),
        ),
        PRECISE_BN=dict(ENABLED=False, NUM_ITER=200),
    ),
    # Trainer is used to specify options related to control the training process
    TRAINER=dict(
        NAME="DefaultRunner",
        WINDOW_SIZE=20,
        FP16=dict(
            ENABLED=False,
            # options: [APEX, PyTorch]
            TYPE="APEX",
            # OPTS: kwargs for each option
            OPTS=dict(
                OPT_LEVEL="O1",
            ),
        ),
    ),
    # Directory where output files are written
    OUTPUT_DIR="./output",
    # Set seed to negative to fully randomize everything.
    # Set seed to positive to use a fixed seed. Note that a fixed seed does not
    # guarantee fully deterministic behavior.
    SEED=-1,
    # Benchmark different cudnn algorithms.
    # If input images have very different sizes, this option will have large overhead
    # for about 10k iterations. It usually hurts total time, but can benefit for certain models.
    # If input images have the same or similar sizes, benchmark is often helpful.
    CUDNN_BENCHMARK=False,
    # The period (in terms of steps) for minibatch visualization at train time.
    # Set to 0 to disable.
    VIS_PERIOD=0,
    # global config is for quick hack purposes.
    # You can set them in command line or config files,
    # Do not commit any configs into it.
    GLOBAL=dict(
        HACK=1.0,
        DUMP_TRAIN=True,
        DUMP_TEST=False,
    ),
)


class ConfigDict(dict):

    def __init__(self, d=None, **kwargs):
        if d is None:
            d = {}
        if kwargs:
            d.update(**kwargs)
        for k, v in d.items():
            setattr(self, k, v)
        # Class attributes
        for k in self.__class__.__dict__.keys():
            if (
                not (k.startswith("__") and k.endswith("__"))
                and k not in self.funcname_not_in_attr()
            ):
                setattr(self, k, getattr(self, k))

    def __setattr__(self, name, value):
        if isinstance(value, (list, tuple)):
            value = [EasyDict(x) if isinstance(x, dict) else x for x in value]
        elif isinstance(value, dict):
            value = EasyDict(value)
        super().__setattr__(name, value)
        super().__setitem__(name, value)

    __setitem__ = __setattr__

    def funcname_not_in_attr(self):
        return [
            "update", "pop", "merge",
            "merge_from_list", "find", "diff",
            "inner_dict", "funcname_not_in_attr"
        ]

    def update(self, e=None, **f):
        d = e or dict()
        d.update(f)
        for k in d:
            setattr(self, k, d[k])

    def pop(self, k, d=None):
        delattr(self, k)
        return super().pop(k, d)

    def merge(self, config=None, **kwargs):
        """
        merge all key and values of config as BaseConfig's attributes.
        Note that kwargs will override values in config if they have the same keys

        Args:
            config (dict): custom config dict
        """
        def update_helper(d, u):
            for k, v in six.iteritems(u):
                dv = d.get(k, EasyDict())
                if not isinstance(dv, collectionsAbc.Mapping):
                    d[k] = v
                elif isinstance(v, collectionsAbc.Mapping):
                    d[k] = update_helper(dv, v)
                else:
                    d[k] = v
            return d

        if config is not None:
            update_helper(self, config)
        if kwargs:
            update_helper(self, kwargs)

    def merge_from_list(self, cfg_list):
        """
        Merge config (keys, values) in a list (e.g., from command line) into
        this config dict.

        Args:
            cfg_list (list): cfg_list must be divided exactly.
            For example, `cfg_list = ['FOO.BAR', 0.5]`.
        """
        _assert_with_logging(
            len(cfg_list) % 2 == 0,
            "Override list has odd length: {}; it must be a list of pairs".format(
                cfg_list
            ),
        )
        for full_key, v in zip(cfg_list[0::2], cfg_list[1::2]):
            key_list = full_key.split(".")
            d = self
            for subkey in key_list[:-1]:
                _assert_with_logging(subkey in d, "Non-existent key: {}".format(full_key))
                d = d[subkey]
            subkey = key_list[-1]
            _assert_with_logging(subkey in d, "Non-existent key: {}".format(full_key))
            value = _decode_cfg_value(v)
            value = _cast_cfg_value_type(value, d[subkey], full_key)
            d[subkey] = value

    def diff(self, cfg) -> dict:
        """
        diff given config with current config object

        Args:
            cfg(ConfigDict): given config, could be any subclass of ConfigDict

        Returns:
            ConfigDict: contains all diff pair
        """
        assert isinstance(cfg, ConfigDict), "config is not a subclass of ConfigDict"
        diff_result = {}
        conf_keys = cfg.keys()
        for param in self.keys():
            if param not in conf_keys:
                diff_result[param] = getattr(self, param)
            else:
                self_val, conf_val = getattr(self, param), getattr(cfg, param)
                if self_val != conf_val:
                    if isinstance(self_val, dict) and isinstance(conf_val, dict):
                        diff_result[param] = diff_dict(self_val, conf_val)
                    else:
                        diff_result[param] = self_val
        return ConfigDict(diff_result)

    def find(self, key: str, show=True, color=Fore.BLACK + Back.YELLOW) -> dict:
        """
        find a given key and its value in config

        Args:
            key (str): the string you want to find
            show (bool): if show is True, print find result; or return the find result
            color (str): color of `key`, default color is black(foreground) yellow(background)

        Returns:
            dict: if  show is False, return dict that contains all find result

        Example::

            >>> from config import config        # suppose you are in your training dir
            >>> config.find("weights")
        """
        key = key.upper()
        find_result = {}
        for param, param_value in self.items():
            param_value = getattr(self, param)
            if re.search(key, param):
                find_result[param] = param_value
            elif isinstance(param_value, dict):
                find_res = find_key(param_value, key)
                if find_res:
                    find_result[param] = find_res
        if not show:
            return find_result
        else:
            pformat_str = repr(ConfigDict(find_result))
            print(highlight(key, pformat_str, color))

    def __repr__(self):
        param_list = [(k, pprint.pformat(v)) for k, v in self.items()]
        table_header = ["config params", "values"]
        return tabulate(param_list, headers=table_header, tablefmt="fancy_grid")


class BaseConfig(ConfigDict):

    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)

    def _register_configuration(self, config):
        self.merge(config)
        version_update(self)

    def link_log(self, link_name="log"):
        """
        create a softlink to output dir.

        Args:
            link_name(str): name of softlink
        """
        if os.path.islink(link_name) and os.readlink(link_name) != self.OUTPUT_DIR:
            os.system("rm " + link_name)
        if not os.path.exists(link_name):
            cmd = "ln -s {} {}".format(self.OUTPUT_DIR, link_name)
            os.system(cmd)

    def funcname_not_in_attr(self):
        namelist = super().funcname_not_in_attr()
        namelist.extend(["link_log", "_register_configuration"])
        return namelist


config = BaseConfig()
```

### cvpods/configs/dynamic_routing_config.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from .base_config import BaseConfig

_config_dict = dict(
    MODEL=dict(
        LOAD_PROPOSALS=False,
        MASK_ON=False,
        KEYPOINT_ON=False,
        BACKBONE=dict(FREEZE_AT=0,),
        RESNETS=dict(
            OUT_FEATURES=["res2", "res3", "res4", "res5"],
            NORM="nnSyncBN",
            NUM_GROUPS=1,
            WIDTH_PER_GROUP=64,
            STRIDE_IN_1X1=True,
            RES5_DILATION=1,
            RES2_OUT_CHANNELS=256,
            STEM_OUT_CHANNELS=64,
            DEFORM_ON_PER_STAGE=[False, False, False, False],
            DEFORM_MODULATED=False,
            DEFORM_NUM_GROUPS=1,
        ),
        FPN=dict(
            IN_FEATURES=[],
            OUT_CHANNELS=256,
            NORM="",
            FUSE_TYPE="sum",
        ),
        SEM_SEG_HEAD=dict(
            # NAME="SemSegFPNHead",
            IN_FEATURES=[],
            IGNORE_VALUE=255,
            NUM_CLASSES=(),
            CONVS_DIM=256,
            COMMON_STRIDE=(),
            NORM="GN",
            LOSS_WEIGHT=1.0,
        ),
    ),
    DATALOADER=dict(FILTER_EMPTY_ANNOTATIONS=False,),
    SOLVER=dict(
        LR_SCHEDULER=dict(
            NAME="PolyLR",
            POLY_POWER=0.9,
            MAX_ITER=40000,
            WARMUP_ITERS=1000,
            WARMUP_FACTOR=0.001,
            WARMUP_METHOD="linear",
        ),
        OPTIMIZER=dict(BASE_LR=0.01, ),
        IMS_PER_BATCH=16,
        CHECKPOINT_PERIOD=5000,
    ),
    TEST=dict(PRECISE_BN=dict(ENABLED=True), ),
)


class SemanticSegmentationConfig(BaseConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = SemanticSegmentationConfig()
```

### cvpods/configs/panoptic_seg_config.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from .rcnn_fpn_config import RCNNFPNConfig

_config_dict = dict(
    MODEL=dict(
        SEM_SEG_HEAD=dict(
            # NAME="SemSegFPNHead",
            IN_FEATURES=["p2", "p3", "p4", "p5"],
            # Label in the semantic segmentation ground truth that is ignored,
            # i.e., no loss is calculated for the correposnding pixel.
            IGNORE_VALUE=255,
            # Number of classes in the semantic segmentation head
            NUM_CLASSES=54,
            # Number of channels in the 3x3 convs inside semantic-FPN heads.
            CONVS_DIM=128,
            # Outputs from semantic-FPN heads are up-scaled to the COMMON_STRIDE stride.
            COMMON_STRIDE=4,
            # Normalization method for the convolution layers. Options: "" (no norm), "GN".
            NORM="GN",
            LOSS_WEIGHT=0.5,
        ),
        PANOPTIC_FPN=dict(
            # Scaling of all losses from instance detection / segmentation head.
            INSTANCE_LOSS_WEIGHT=1.0,
            # options when combining instance & semantic segmentation outputs
            COMBINE=dict(
                ENABLED=True,
                OVERLAP_THRESH=0.5,
                STUFF_AREA_LIMIT=4096,
                INSTANCES_CONFIDENCE_THRESH=0.5,
            ),
        ),
    )
)


class PanopticSegmentationConfig(RCNNFPNConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = PanopticSegmentationConfig()
```

### cvpods/configs/rcnn_fpn_config.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from .rcnn_config import RCNNConfig

_config_dict = dict(
    MODEL=dict(
        # BACKBONE=dict(NAME='build_resnet_backbone',),
        RESNETS=dict(OUT_FEATURES=["res2", "res3", "res4", "res5"],),
        FPN=dict(IN_FEATURES=["res2", "res3", "res4", "res5"]),
        ANCHOR_GENERATOR=dict(
            SIZES=[[32], [64], [128], [256], [512]], ASPECT_RATIOS=[[0.5, 1.0, 2.0]],
        ),
        RPN=dict(
            IN_FEATURES=["p2", "p3", "p4", "p5", "p6"],
            PRE_NMS_TOPK_TRAIN=2000,
            PRE_NMS_TOPK_TEST=1000,
            POST_NMS_TOPK_TRAIN=1000,
            POST_NMS_TOPK_TEST=1000,
        ),
        ROI_HEADS=dict(
            # NAME: "StandardROIHeads"
            IN_FEATURES=["p2", "p3", "p4", "p5"],
        ),
        ROI_BOX_HEAD=dict(
            # NAME: "FastRCNNConvFCHead"
            NUM_FC=2,
            POOLER_RESOLUTION=7,
        ),
        ROI_MASK_HEAD=dict(
            # NAME: "MaskRCNNConvUpsampleHead"
            NUM_CONV=4,
            POOLER_RESOLUTION=14,
        ),
    ),
)


class RCNNFPNConfig(RCNNConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = RCNNFPNConfig()
```

### cvpods/configs/__init__.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from .base_classification_config import BaseClassificationConfig
from .base_config import BaseConfig
from .base_detection_config import BaseDetectionConfig
from .dynamic_routing_config import SemanticSegmentationConfig
from .efficientdet_config import EfficientDetConfig
from .fcos_config import FCOSConfig
from .keypoint_config import KeypointConfig
from .panoptic_seg_config import PanopticSegmentationConfig
from .pointrend_config import PointRendRCNNFPNConfig
from .rcnn_config import RCNNConfig
from .rcnn_fpn_config import RCNNFPNConfig
from .retinanet_config import RetinaNetConfig
from .segm_config import SegmentationConfig
from .solo_config import SOLOConfig
from .ssd_config import SSDConfig
from .yolo_config import YOLO3Config
```

### cvpods/configs/keypoint_config.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from .base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        KEYPOINT_ON=True,
        ROI_KEYPOINT_HEAD=dict(
            NAME="KRCNNConvDeconvUpsampleHead",
            POOLER_RESOLUTION=14,
            POOLER_SAMPLING_RATIO=0,
            CONV_DIMS=tuple(512 for _ in range(8)),
            NUM_KEYPOINTS=17,  # 17 is the number of keypoints in COCO
            # Images with too few (or no) keypoints are excluded from training.
            MIN_KEYPOINTS_PER_IMAGE=1,
            # Normalize by the total number of visible keypoints in the minibatch if True.
            # Otherwise, normalize by the total number of keypoints that could ever exist
            # in the minibatch.
            # The keypoint softmax loss is only calculated on visible keypoints.
            # Since the number of visible keypoints can vary significantly between
            # minibatches, this has the effect of up-weighting the importance of
            # minibatches with few visible keypoints. (Imagine the extreme case of
            # only one visible keypoint versus N: in the case of N, each one
            # contributes 1/N to the gradient compared to the single keypoint
            # determining the gradient direction). Instead, we can normalize the
            # loss by the total number of keypoints, if it were the case that all
            # keypoints were visible in a full minibatch. (Returning to the example,
            # this means that the one visible keypoint contributes as much as each
            # of the N keypoints.)
            NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS=True,
            # Multi-task loss weight to use for keypoints
            # Recommended values:
            #   - use 1.0 if NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS is True
            #   - use 4.0 if NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS is False
            LOSS_WEIGHT=1.0,
            # Type of pooling operation applied to the incoming feature map for each RoI
            POOLER_TYPE="ROIAlignV2",
        ),
    )
)


class KeypointConfig(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = KeypointConfig()
```

### cvpods/configs/base_classification_config.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from cvpods.configs.base_config import BaseConfig

_config_dict = dict(
    MODEL=dict(
        WEIGHTS="",
        PIXEL_MEAN=[0.485, 0.456, 0.406],  # RGB
        PIXEL_STD=[0.229, 0.224, 0.225],
        BACKBONE=dict(FREEZE_AT=-1, ),  # do not freeze
        RESNETS=dict(
            NUM_CLASSES=None,
            DEPTH=None,
            OUT_FEATURES=["linear"],
            NUM_GROUPS=1,
            # Options: FrozenBN, GN, "SyncBN", "BN"
            NORM="BN",
            ACTIVATION=dict(
                NAME="ReLU",
                INPLACE=True,
            ),
            # Whether init last bn weight of each BasicBlock or BottleneckBlock to 0
            ZERO_INIT_RESIDUAL=True,
            WIDTH_PER_GROUP=64,
            # Use True only for the original MSRA ResNet; use False for C2 and Torch models
            STRIDE_IN_1X1=False,
            RES5_DILATION=1,
            RES2_OUT_CHANNELS=256,
            STEM_OUT_CHANNELS=64,

            # Deep Stem
            DEEP_STEM=False,
        ),
    ),
    INPUT=dict(FORMAT="RGB"),
    SOLVER=dict(
        IMS_PER_DEVICE=32,  # defalut: 8 gpus x 32 = 256
    ),
)


class BaseClassificationConfig(BaseConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = BaseClassificationConfig()
```

### cvpods/configs/efficientdet_config.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from .base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        PIXEL_MEAN=[0.485, 0.456, 0.406],  # mean value from ImageNet
        PIXEL_STD=[0.229, 0.224, 0.225],
        EFFICIENTNET=dict(
            MODEL_NAME="efficientnet-b0",  # default setting for EfficientDet-D0
            NORM="BN",
            BN_MOMENTUM=1 - 0.99,
            BN_EPS=1e-3,
            DROP_CONNECT_RATE=1 - 0.8,  # survival_prob = 0.8
            DEPTH_DIVISOR=8,
            MIN_DEPTH=None,
            NUM_CLASSES=None,
            FIX_HEAD_STEAM=False,
            MEMORY_EFFICIENT_SWISH=True,
            OUT_FEATURES=["stage4", "stage6", "stage8"],
        ),
        BIFPN=dict(
            IN_FEATURES=["stage4", "stage6", "stage8"],
            NORM="BN",
            BN_MOMENTUM=0.01,  # 1 - 0.99
            BN_EPS=1e-3,
            MEMORY_EFFICIENT_SWISH=True,
            INPUT_SIZE=512,  # default setting for EfficientDet-D0
            NUM_LAYERS=3,  # default setting for EfficientDet-D0
            OUT_CHANNELS=60,  # default setting for EfficientDet-D0
            FUSE_TYPE="fast",  # select in ["softmax", "fast", "sum"]
        ),
        EFFICIENTDET=dict(
            IN_FEATURES=[f"p{i}" for i in range(3, 8)],  # p3-p7
            NUM_CLASSES=80,
            FREEZE_BACKBONE=False,
            FREEZE_BN=False,
            HEAD=dict(
                NUM_CONV=3,  # default setting for EfficientDet-D0
                NORM="BN",
                BN_MOMENTUM=1 - 0.99,
                BN_EPS=1e-3,
                PRIOR_PROB=0.01,
                MEMORY_EFFICIENT_SWISH=True,
            ),
            IOU_THRESHOLDS=[0.5, 0.5],
            IOU_LABELS=[0, -1, 1],
            SCORE_THRESH_TEST=0.05,
            TOPK_CANDIDATES_TEST=1000,
            NMS_THRESH_TEST=0.5,
            BBOX_REG_WEIGHTS=(1.0, 1.0, 1.0, 1.0),
            FOCAL_LOSS_GAMMA=1.5,
            FOCAL_LOSS_ALPHA=0.25,
            SMOOTH_L1_LOSS_BETA=0.1,
            REG_NORM=4.0,
            BOX_LOSS_WEIGHT=50.0,
        ),
        ANCHOR_GENERATOR=dict(
            SIZES=[
                [x, x * 2 ** (1.0 / 3), x * 2 ** (2.0 / 3)]
                for x in [4 * 2**i for i in range(3, 8)]
            ]
        ),
    ),
)


class EfficientDetConfig(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = EfficientDetConfig()
```

### cvpods/configs/segm_config.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from .base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        SEM_SEG_HEAD=dict(
            # NAME="SemSegFPNHead",
            IN_FEATURES=["p2", "p3", "p4", "p5"],
            # Label in the semantic segmentation ground truth that is ignored,
            # i.e., no loss is calculated for the correposnding pixel.
            IGNORE_VALUE=255,
            # Number of classes in the semantic segmentation head
            NUM_CLASSES=54,
            # Number of channels in the 3x3 convs inside semantic-FPN heads.
            CONVS_DIM=128,
            # Outputs from semantic-FPN heads are up-scaled to the COMMON_STRIDE stride.
            COMMON_STRIDE=4,
            # Normalization method for the convolution layers. Options: "" (no norm), "GN".
            NORM="GN",
            LOSS_WEIGHT=1.0,
        ),
        PANOPTIC_FPN=dict(
            # Scaling of all losses from instance detection / segmentation head.
            INSTANCE_LOSS_WEIGHT=1.0,
            # options when combining instance & semantic segmentation outputs
            COMBINE=dict(
                ENABLED=True,
                OVERLAP_THRESH=0.5,
                STUFF_AREA_LIMIT=4096,
                INSTANCES_CONFIDENCE_THRESH=0.5,
            ),
        ),
    ),
    DATALOADER=dict(FILTER_EMPTY_ANNOTATIONS=False,),
)


class SegmentationConfig(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = SegmentationConfig()
```

### cvpods/configs/fcos_config.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from .base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        # META_ARCHITECTURE="RetinaNet",
        RESNETS=dict(OUT_FEATURES=["res3", "res4", "res5"]),
        FPN=dict(IN_FEATURES=["res3", "res4", "res5"]),
        SHIFT_GENERATOR=dict(
            NUM_SHIFTS=1,
            # Relative offset between the center of the first shift and the top-left corner of img
            # Units: fraction of feature map stride (e.g., 0.5 means half stride)
            # Allowed values are floats in [0, 1) range inclusive.
            # Recommended value is 0.5, although it is not expected to affect model accuracy.
            OFFSET=0.0,
        ),
        FCOS=dict(
            NUM_CLASSES=80,
            IN_FEATURES=["p3", "p4", "p5", "p6", "p7"],
            NUM_CONVS=4,
            FPN_STRIDES=[8, 16, 32, 64, 128],
            PRIOR_PROB=0.01,
            CENTERNESS_ON_REG=False,
            NORM_REG_TARGETS=False,
            SCORE_THRESH_TEST=0.05,
            TOPK_CANDIDATES_TEST=1000,
            NMS_THRESH_TEST=0.6,
            BBOX_REG_WEIGHTS=(1.0, 1.0, 1.0, 1.0),
            FOCAL_LOSS_GAMMA=2.0,
            FOCAL_LOSS_ALPHA=0.25,
            IOU_LOSS_TYPE="iou",
            CENTER_SAMPLING_RADIUS=0.0,
            OBJECT_SIZES_OF_INTEREST=[
                [-1, 64],
                [64, 128],
                [128, 256],
                [256, 512],
                [512, float("inf")],
            ],
            NORM_SYNC=True,
        ),
    ),
)


class FCOSConfig(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = FCOSConfig()
```

### cvpods/configs/solo_config.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from .base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        MASK_ON=True,
        PIXEL_MEAN=[123.675, 116.28, 103.53],  # RGB FORMAT
        PIXEL_STD=[1.0, 1.0, 1.0],
        RESNETS=dict(
            DEPTH=50,
            OUT_FEATURES=["res2", "res3", "res4", "res5"],
        ),
        FPN=dict(
            IN_FEATURES=["res2", "res3", "res4", "res5"],
            OUT_CHANNELS=256,
        ),
        SOLO=dict(
            NUM_CLASSES=80,
            IN_FEATURES=["p2", "p3", "p4", "p5", "p6"],
            NUM_GRIDS=[40, 36, 24, 16, 12],  # per level
            SCALE_RANGES=((1, 64), (32, 128), (64, 256), (128, 512), (256, 2048)),
            FEATURE_STRIDES=[8, 8, 16, 32, 32],
            # Given a gt: (cx, cy, w, h), the center region is controlled by
            # constant scale factors sigma: (cx, cy, sigma*w, sigma*h)
            SIGMA=0.2,
            HEAD=dict(
                TYPE="SOLOHead",  # "SOLOHead", "DecoupledSOLOHead"
                SEG_FEAT_CHANNELS=256,
                STACKED_CONVS=7,
                PRIOR_PROB=0.01,
                NORM="GN",
                # The following two items are useful in the "DecoupledSOLOLightHead"
                USE_DCN_IN_TOWER=False,
                DCN_TYPE=None,
            ),
            # Loss parameters:
            LOSS_INS=dict(
                TYPE='DiceLoss',
                LOSS_WEIGHT=3.0
            ),
            LOSS_CAT=dict(
                TYPE='FocalLoss',
                GAMMA=2.0,
                ALPHA=0.25,
                LOSS_WEIGHT=1.0,
            ),
            # Inference parameters:
            SCORE_THRESH_TEST=0.1,
            MASK_THRESH_TEST=0.5,
            # NMS parameters:
            NMS_PER_IMAGE=500,
            NMS_KERNEL='gaussian',  # gaussian/linear
            NMS_SIGMA=2.0,
            UPDATE_THRESH=0.05,
            DETECTIONS_PER_IMAGE=100,
        ),
    ),
    INPUT=dict(
        # SOLO for instance segmenation does not work with "polygon" mask_format
        MASK_FORMAT="bitmask",
    )
)


class SOLOConfig(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = SOLOConfig()
```

### cvpods/configs/rcnn_config.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from .base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        # META_ARCHITECTURE='GeneralizedRCNN',
        LOAD_PROPOSALS=False,
        MASK_ON=False,
        KEYPOINT_ON=False,
        ANCHOR_GENERATOR=dict(
            SIZES=[[32, 64, 128, 256, 512]], ASPECT_RATIOS=[[0.5, 1.0, 2.0]],
        ),
        PROPOSAL_GENERATOR=dict(
            # Current proposal generators include "RPN", "RRPN" and "PrecomputedProposals"
            NAME="RPN",
            MIN_SIZE=0,
        ),
        RPN=dict(
            # HEAD_NAME="StandardRPNHead",
            # Names of the input feature maps to be used by RPN
            # e.g., ["p2", "p3", "p4", "p5", "p6"] for FPN
            IN_FEATURES=["res4"],
            # Remove RPN anchors that go outside the image by BOUNDARY_THRESH pixels
            # Set to -1 or a large value, e.g. 100000, to disable pruning anchors
            BOUNDARY_THRESH=-1,
            # IOU overlap ratios [BG_IOU_THRESHOLD, FG_IOU_THRESHOLD]
            # Minimum overlap required between an anchor and ground-truth box for the
            # (anchor, gt box) pair to be a positive example (IoU >= FG_IOU_THRESHOLD
            # ==> positive RPN example: 1)
            # Maximum overlap allowed between an anchor and ground-truth box for the
            # (anchor, gt box) pair to be a negative examples (IoU < BG_IOU_THRESHOLD
            # ==> negative RPN example: 0)
            # Anchors with overlap in between (BG_IOU_THRESHOLD <= IoU < FG_IOU_THRESHOLD)
            # are ignored (-1)
            IOU_THRESHOLDS=[0.3, 0.7],
            IOU_LABELS=[0, -1, 1],
            # Total number of RPN examples per image
            BATCH_SIZE_PER_IMAGE=256,
            # Target fraction of foreground (positive) examples per RPN minibatch
            POSITIVE_FRACTION=0.5,
            # Weights on (dx, dy, dw, dh) for normalizing RPN anchor regression targets
            BBOX_REG_WEIGHTS=(1.0, 1.0, 1.0, 1.0),
            # The transition point from L1 to L2 loss. Set to 0.0 to make the loss simply L1.
            SMOOTH_L1_BETA=0.0,
            LOSS_WEIGHT=1.0,
            # Number of top scoring RPN proposals to keep before applying NMS
            # When FPN is used, this is *per FPN level* (not total)
            PRE_NMS_TOPK_TRAIN=12000,
            PRE_NMS_TOPK_TEST=6000,
            # Number of top scoring RPN proposals to keep after applying NMS
            # When FPN is used, this limit is applied per level and then again to the union
            # of proposals from all levels
            # NOTE: When FPN is used, the meaning of this config is different from Detectron1.
            # It means per-batch topk in Detectron1, but per-image topk here.
            # See "modeling/rpn/rpn_outputs.py" for details.
            POST_NMS_TOPK_TRAIN=2000,
            POST_NMS_TOPK_TEST=1000,
            # NMS threshold used on RPN proposals
            NMS_THRESH=0.7,
            # NMS type for RPN
            # Format: str. (e.g., 'normal' means using normal nms)
            # Allowed values are 'normal', 'softnms-linear', 'softnms-gaussian'
            NMS_TYPE='normal'
        ),
        ROI_HEADS=dict(
            # ROI_HEADS type: "Res5ROIHeads",
            # Names of the input feature maps to be used by ROI heads
            # Currently all heads (box, mask, ...) use the same input feature map list
            # e.g., ["p2", "p3", "p4", "p5"] is commonly used for FPN
            IN_FEATURES=["res4"],
            # Number of foreground classes
            NUM_CLASSES=80,
            # IOU overlap ratios [IOU_THRESHOLD]
            # Overlap threshold for an RoI to be considered background (if < IOU_THRESHOLD)
            # Overlap threshold for an RoI to be considered foreground (if >= IOU_THRESHOLD)
            IOU_THRESHOLDS=[0.5],
            IOU_LABELS=[0, 1],
            # RoI minibatch size *per image* (number of regions of interest [ROIs])
            # Total number of RoIs per training minibatch =
            #   ROI_HEADS.BATCH_SIZE_PER_IMAGE * SOLVER.IMS_PER_BATCH
            # E.g., a common configuration is: 512 * 16 = 8192
            BATCH_SIZE_PER_IMAGE=512,
            # Target fraction of RoI minibatch that is labeled foreground (i.e. class > 0)
            POSITIVE_FRACTION=0.25,

            # Only used in test mode

            # Minimum score threshold (assuming scores in a [0, 1] range); a value chosen to
            # balance obtaining high recall with not having too many low precision
            # detections that will slow down inference post processing steps (like NMS)
            # A default threshold of 0.0 increases AP by ~0.2-0.3 but significantly slows down
            # inference.
            SCORE_THRESH_TEST=0.05,
            # Overlap threshold used for non-maximum suppression (suppress boxes with
            # IoU >= this threshold)
            NMS_THRESH_TEST=0.5,
            # If True, augment proposals with ground-truth boxes before sampling proposals to
            # train ROI heads.
            PROPOSAL_APPEND_GT=True,
        ),
        ROI_BOX_HEAD=dict(
            # C4 don't use head name option
            # Options for non-C4 models: FastRCNNConvFCHead,

            # Default weights on (dx, dy, dw, dh) for normalizing bbox regression targets
            # These are empirically chosen to approximately lead to unit variance targets
            BBOX_REG_WEIGHTS=(10.0, 10.0, 5.0, 5.0),
            # The transition point from L1 to L2 loss. Set to 0.0 to make the loss simply L1.
            SMOOTH_L1_BETA=0.0,
            POOLER_RESOLUTION=14,
            POOLER_SAMPLING_RATIO=0,
            # Type of pooling operation applied to the incoming feature map for each RoI
            POOLER_TYPE="ROIAlignV2",
            NUM_FC=0,
            # Hidden layer dimension for FC layers in the RoI box head
            FC_DIM=1024,
            NUM_CONV=0,
            # Channel dimension for Conv layers in the RoI box head
            CONV_DIM=256,
            # Normalization method for the convolution layers.
            # Options: "" (no norm), "GN", "SyncBN".
            NORM="",
            # Whether to use class agnostic for bbox regression
            CLS_AGNOSTIC_BBOX_REG=False,
            # If true, RoI heads use bounding boxes predicted by the box head
            # rather than proposal boxes
            TRAIN_ON_PRED_BOXES=False,
        ),
        ROI_BOX_CASCADE_HEAD=dict(
            # The number of cascade stages is implicitly defined by
            # the length of the following two configs.
            BBOX_REG_WEIGHTS=(
                (10.0, 10.0, 5.0, 5.0),
                (20.0, 20.0, 10.0, 10.0),
                (30.0, 30.0, 15.0, 15.0),
            ),
            IOUS=(0.5, 0.6, 0.7),
        ),
        ROI_MASK_HEAD=dict(
            # NAME="MaskRCNNConvUpsampleHead",
            POOLER_RESOLUTION=14,
            POOLER_SAMPLING_RATIO=0,
            # The number of convs in the mask head
            NUM_CONV=0,
            CONV_DIM=256,
            # Normalization method for the convolution layers.
            # Options: "" (no norm), "GN", "SyncBN".
            NORM="",
            # Whether to use class agnostic for mask prediction
            CLS_AGNOSTIC_MASK=False,
            # Type of pooling operation applied to the incoming feature map for each RoI
            POOLER_TYPE="ROIAlignV2",
        ),
    ),
)


class RCNNConfig(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = RCNNConfig()
```

### cvpods/configs/yolo_config.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from .base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        PIXEL_MEAN=(0.485, 0.456, 0.406),
        PIXEL_STD=(0.229, 0.224, 0.225),
        DARKNET=dict(
            DEPTH=53,
            STEM_OUT_CHANNELS=32,
            WEIGHTS="cvpods/ImageNetPretrained/custom/darknet53.mix.pth",
            OUT_FEATURES=["dark3", "dark4", "dark5"]
        ),
        YOLO=dict(
            CLASSES=80,
            IN_FEATURES=["dark3", "dark4", "dark5"],
            ANCHORS=[
                [[116, 90], [156, 198], [373, 326]],
                [[30, 61], [62, 45], [42, 119]],
                [[10, 13], [16, 30], [33, 23]],
            ],
            CONF_THRESHOLD=0.01,  # TEST
            NMS_THRESHOLD=0.5,
            IGNORE_THRESHOLD=0.7,
        ),
    ),
)


class YOLO3Config(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = YOLO3Config()
```

### cvpods/configs/ssd_config.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from .base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        PIXEL_MEAN=[123.675, 116.28, 103.53],  # RGB FORMAT
        PIXEL_STD=[1.0, 1.0, 1.0],
        VGG=dict(
            ARCH='D',
            NORM="",
            NUM_CLASSES=None,
            OUT_FEATURES=["Conv4_3", "Conv7"],
            POOL_ARGS=dict(
                pool3=(2, 2, 0, True),  # k, s, p, ceil_model
                pool5=(3, 1, 1, False)  # k, s, p, ceil_model
            ),
            FC_TO_CONV=True,
        ),
        SSD=dict(
            NUM_CLASSES=80,
            IN_FEATURES=["Conv4_3", "Conv7"],
            EXTRA_LAYER_ARCH={
                # the number after "S" and "S" to denote conv layer with stride=2
                "300": [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256],
                "512": [256, 'S', 512, 128, 'S', 256, 128, 'S', 256, 128, 'S', 256, 128, 256],
            },
            IOU_THRESHOLDS=[0.5, 0.5],
            IOU_LABELS=[0, -1, 1],
            BBOX_REG_WEIGHTS=(10.0, 10.0, 5.0, 5.0),
            L2NORM_SCALE=20.0,
            # Loss parameters:
            LOSS_ALPHA=1.0,
            SMOOTH_L1_LOSS_BETA=1.0,
            NEGATIVE_POSITIVE_RATIO=3.0,
            # Inference parameters:
            SCORE_THRESH_TEST=0.02,
            NMS_THRESH_TEST=0.45,
        ),
    )
)


class SSDConfig(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = SSDConfig()
```

### cvpods/configs/base_detection_config.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from .base_config import BaseConfig

_config_dict = dict(
    # Model state parameters
    MODEL=dict(
        LOAD_PROPOSALS=False,
        MASK_ON=False,
        KEYPOINT_ON=False,
        BACKBONE=dict(
            # Freeze parameters if FREEZE_AT >= 1
            FREEZE_AT=2,
        ),
        RESNETS=dict(
            NUM_CLASSES=None,
            DEPTH=None,
            # res4 for C4 backbone, res2..5 for FPN backbone
            OUT_FEATURES=["res4"],
            # Number of groups to use; 1 ==> ResNet; > 1 ==> ResNeXt
            NUM_GROUPS=1,
            # Options: FrozenBN, GN, "SyncBN", "BN"
            NORM="FrozenBN",
            ACTIVATION=dict(
                NAME="ReLU",
                INPLACE=True,
            ),
            # Whether init last bn weight of each BasicBlock or BottleneckBlock to 0
            ZERO_INIT_RESIDUAL=False,
            # Baseline width of each group.
            # Scaling this parameters will scale the width of all bottleneck layers.
            WIDTH_PER_GROUP=64,
            # Use True only for the original MSRA ResNet; use False for C2 and Torch models
            STRIDE_IN_1X1=True,
            # Output width of res2. Scaling this parameters will scale the width of all 1x1 convs
            # For R18 and R34, this needs to be set to 64
            RES5_DILATION=1,
            RES2_OUT_CHANNELS=256,
            STEM_OUT_CHANNELS=64,

            # Deep Stem
            DEEP_STEM=False,
        ),
        FPN=dict(
            # Names of the input feature maps to be used by FPN
            # They must have contiguous power of 2 strides
            # e.g., ["res2", "res3", "res4", "res5"]
            IN_FEATURES=[],
            OUT_CHANNELS=256,
            # Options: "" (no norm), "GN"
            NORM="",
            # Types for fusing the FPN top-down and lateral features. Can be either "sum" or "avg"
            FUSE_TYPE="sum",
            BLOCK_IN_FEATURES="p5",
        ),
        ANCHOR_GENERATOR=dict(
            # NAME="DefaultAnchorGenerator",
            # Anchor sizes (i.e. sqrt of area) in absolute pixels w.r.t. the network input.
            # Format: list[list[int]]. SIZES[i] specifies the list of sizes
            # to use for IN_FEATURES[i]; len(SIZES) == len(IN_FEATURES) must be true,
            # or len(SIZES) == 1 is true and size list SIZES[0] is used for all
            # IN_FEATURES.
            SIZES=[[32, 64, 128, 256, 512]],
            # Anchor aspect ratios. For each area given in `SIZES`, anchors with different aspect
            # ratios are generated by an anchor generator.
            # Format: list[list[int]]. ASPECT_RATIOS[i] specifies the list of aspect ratios
            # to use for IN_FEATURES[i]; len(ASPECT_RATIOS) == len(IN_FEATURES) must be true,
            # or len(ASPECT_RATIOS) == 1 is true and aspect ratio list ASPECT_RATIOS[0] is used
            # for all IN_FEATURES.
            ASPECT_RATIOS=[[0.5, 1.0, 2.0]],
            # Anchor angles.
            # list[float], the angle in degrees, for each input feature map.
            # ANGLES[i] specifies the list of angles for IN_FEATURES[i].
            ANGLES=[[-90, 0, 90]],
            # Relative offset between the center of the first anchor and the top-left corner of img
            # Units: fraction of feature map stride (e.g., 0.5 means half stride)
            # Allowed values are floats in [0, 1) range inclusive.
            # Recommended value is 0.5, although it is not expected to affect model accuracy.
            OFFSET=0.0,
        ),
        # NMS type during inference
        # Format: str. (e.g., 'normal' means using normal nms)
        # Allowed values are 'normal', 'softnms-linear', 'softnms-gaussian', 'cluster'
        NMS_TYPE='normal',
    ),
)


class BaseDetectionConfig(BaseConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = BaseDetectionConfig()
```

### cvpods/evaluation/eval_MR_multisetup.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import copy
import datetime
import time
from collections import defaultdict

import numpy as np


class COCOeval:
    # Interface for evaluating detection on the Microsoft COCO dataset.
    #
    # The usage for CocoEval is as follows:
    #  cocoGt=..., cocoDt=...       # load dataset and results
    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object
    #  E.params.recThrs = ...;      # set parameters as desired
    #  E.evaluate();                # run per image evaluation
    #  E.accumulate();              # accumulate per image results
    #  E.summarize();               # display summary metrics of results
    # For example usage see evalDemo.m and http://mscoco.org/.
    #
    # The evaluation parameters are as follows (defaults in brackets):
    #  imgIds     - [all] N img ids to use for evaluation
    #  catIds     - [all] K cat ids to use for evaluation
    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation
    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation
    #  areaRng    - [...] A=4 object area ranges for evaluation
    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image
    #  iouType    - ['segm'] set iouType to 'segm', 'bbox' or 'keypoints'
    #  iouType replaced the now DEPRECATED useSegm parameter.
    #  useCats    - [1] if true use category labels for evaluation
    # Note: if useCats=0 category labels are ignored as in proposal scoring.
    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.
    #
    # evaluate(): evaluates detections on every image and every category and
    # concats the results into the "evalImgs" with fields:
    #  dtIds      - [1xD] id for each of the D detections (dt)
    #  gtIds      - [1xG] id for each of the G ground truths (gt)
    #  dtMatches  - [TxD] matching gt id at each IoU or 0
    #  gtMatches  - [TxG] matching dt id at each IoU or 0
    #  dtScores   - [1xD] confidence of each dt
    #  gtIgnore   - [1xG] ignore flag for each gt
    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU
    #
    # accumulate(): accumulates the per-image, per-category evaluation
    # results in "evalImgs" into the dictionary "eval" with fields:
    #  params     - parameters used for evaluation
    #  date       - date evaluation was performed
    #  counts     - [T,R,K,A,M] parameter dimensions (see above)
    #  precision  - [TxRxKxAxM] precision for every evaluation setting
    #  recall     - [TxKxAxM] max recall for every evaluation setting
    # Note: precision and recall==-1 for settings with no gt objects.
    #
    # See also coco, mask, pycocoDemo, pycocoEvalDemo
    #
    # Microsoft COCO Toolbox.      version 2.0
    # Data, paper, and tutorials available at:  http://mscoco.org/
    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.
    # Licensed under the Simplified BSD License [see coco/license.txt]
    def __init__(self, cocoGt=None, cocoDt=None, iouType='segm'):
        '''
        Initialize CocoEval using coco APIs for gt and dt
        :param cocoGt: coco object with ground truth annotations
        :param cocoDt: coco object with detection results
        :return: None
        '''
        if not iouType:
            print('iouType not specified. use default iouType segm')
        self.cocoGt = cocoGt              # ground truth COCO API
        self.cocoDt = cocoDt              # detections COCO API
        self.params = {}                  # evaluation parameters
        # per-image per-category evaluation results [KxAxI] elements
        self.evalImgs = defaultdict(list)
        self.eval = {}                  # accumulated evaluation results
        self._gts = defaultdict(list)       # gt for evaluation
        self._dts = defaultdict(list)       # dt for evaluation
        self.params = Params(iouType=iouType)  # parameters
        self._paramsEval = {}               # parameters for evaluation
        self.stats = -1                      # result summarization
        if cocoGt is not None:
            self.params.imgIds = sorted(cocoGt.getImgIds())
            self.params.catIds = sorted(cocoGt.getCatIds())

    def _prepare(self, id_setup):
        '''
        Prepare ._gts and ._dts for evaluation based on params
        :return: None
        '''
        p = self.params
        if p.useCats:
            gts = self.cocoGt.loadAnns(
                self.cocoGt.getAnnIds(
                    imgIds=p.imgIds,
                    catIds=p.catIds))
            dts = self.cocoDt.loadAnns(
                self.cocoDt.getAnnIds(
                    imgIds=p.imgIds,
                    catIds=p.catIds))
        else:
            gts = self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))
            dts = self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))

        # set ignore flag
        for gt in gts:
            gt['ignore'] = gt['ignore'] if 'ignore' in gt else 0
            gt['ignore'] = 1 if (
                gt['height'] < self.params.HtRng[id_setup][0]
                or gt['height'] > self.params.HtRng[id_setup][1]
            ) or (
                gt['vis_ratio'] < self.params.VisRng[id_setup][0]
                or gt['vis_ratio'] > self.params.VisRng[id_setup][1]
            ) else gt['ignore']

        self._gts = defaultdict(list)       # gt for evaluation
        self._dts = defaultdict(list)       # dt for evaluation
        for gt in gts:
            self._gts[gt['image_id'], gt['category_id']].append(gt)
        for dt in dts:
            self._dts[dt['image_id'], dt['category_id']].append(dt)
        # per-image per-category evaluation results
        self.evalImgs = defaultdict(list)
        self.eval = {}                  # accumulated evaluation results

    def evaluate(self, id_setup):
        '''
        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs
        :return: None
        '''
        tic = time.time()
        print('Running per image evaluation...')
        p = self.params
        # add backward compatibility if useSegm is specified in params
        if p.useSegm is not None:
            p.iouType = 'segm' if p.useSegm == 1 else 'bbox'
            print(
                'useSegm (deprecated) is not None. Running {} evaluation'.format(
                    p.iouType))
        print('Evaluate annotation type *{}*'.format(p.iouType))
        p.imgIds = list(np.unique(p.imgIds))
        if p.useCats:
            p.catIds = list(np.unique(p.catIds))
        p.maxDets = sorted(p.maxDets)
        self.params = p

        self._prepare(id_setup)
        # loop through images, area range, max detection number
        catIds = p.catIds if p.useCats else [-1]

        evaluateImg = self.evaluateImg
        maxDet = p.maxDets[-1]
        HtRng = self.params.HtRng[id_setup]
        VisRng = self.params.VisRng[id_setup]
        self.evalImgs = [evaluateImg(imgId, catId, HtRng, VisRng, maxDet)
                         for catId in catIds
                         for imgId in p.imgIds
                         ]
        self._paramsEval = copy.deepcopy(self.params)
        toc = time.time()
        print('DONE (t={:0.2f}s).'.format(toc - tic))

    def computeIoU(self, gt, dt):
        p = self.params
        if len(gt) == 0 and len(dt) == 0:
            return []
        inds = np.argsort([-d['score'] for d in dt], kind='mergesort')
        dt = [dt[i] for i in inds]
        if len(dt) > p.maxDets[-1]:
            dt = dt[0:p.maxDets[-1]]

        if p.iouType == 'segm':
            g = [g['segmentation'] for g in gt]
            d = [d['segmentation'] for d in dt]
        elif p.iouType == 'bbox':
            g = [g['bbox'] for g in gt]
            d = [d['bbox'] for d in dt]
        else:
            raise Exception('unknown iouType for iou computation')

        # compute iou between each dt and gt region
        iscrowd = [int(o['ignore']) for o in gt]
        ious = self.iou(d, g, iscrowd)
        return ious

    def iou(self, dts, gts, pyiscrowd):
        dts = np.asarray(dts)
        gts = np.asarray(gts)
        pyiscrowd = np.asarray(pyiscrowd)
        ious = np.zeros((len(dts), len(gts)))
        for j, gt in enumerate(gts):
            gx1 = gt[0]
            gy1 = gt[1]
            gx2 = gt[0] + gt[2]
            gy2 = gt[1] + gt[3]
            garea = gt[2] * gt[3]
            for i, dt in enumerate(dts):
                dx1 = dt[0]
                dy1 = dt[1]
                dx2 = dt[0] + dt[2]
                dy2 = dt[1] + dt[3]
                darea = dt[2] * dt[3]

                unionw = min(dx2, gx2) - max(dx1, gx1)
                if unionw <= 0:
                    continue
                unionh = min(dy2, gy2) - max(dy1, gy1)
                if unionh <= 0:
                    continue
                t = unionw * unionh
                if pyiscrowd[j]:
                    unionarea = darea
                else:
                    unionarea = darea + garea - t

                ious[i, j] = float(t) / unionarea
        return ious

    def evaluateImg(self, imgId, catId, hRng, vRng, maxDet):
        '''
        perform evaluation for single category and image
        :return: dict (single image results)
        '''
        p = self.params
        if p.useCats:
            gt = self._gts[imgId, catId]
            dt = self._dts[imgId, catId]
        else:
            gt = [_ for cId in p.catIds for _ in self._gts[imgId, cId]]
            dt = [_ for cId in p.catIds for _ in self._dts[imgId, cId]]
        if len(gt) == 0 and len(dt) == 0:
            return None

        for g in gt:
            if g['ignore']:
                g['_ignore'] = 1
            else:
                g['_ignore'] = 0
        # sort dt highest score first, sort gt ignore last
        gtind = np.argsort([g['_ignore'] for g in gt], kind='mergesort')
        gt = [gt[i] for i in gtind]
        dtind = np.argsort([-d['score'] for d in dt], kind='mergesort')
        dt = [dt[i] for i in dtind[0:maxDet]]
        # exclude dt out of height range
        dt = [
            d for d in dt if d['height'] >= hRng[0] / self.params.expFilter
            and d['height'] < hRng[1] * self.params.expFilter
        ]
        dtind = np.array([int(d['id'] - dt[0]['id']) for d in dt])

        # load computed ious
        if len(dtind) > 0:
            ious = self.computeIoU(gt, dt)
        else:
            ious = []

        T = len(p.iouThrs)
        G = len(gt)
        D = len(dt)
        gtm = np.zeros((T, G))
        dtm = np.zeros((T, D))
        gtIg = np.array([g['_ignore'] for g in gt])
        dtIg = np.zeros((T, D))
        if not len(ious) == 0:
            for tind, t in enumerate(p.iouThrs):
                for dind, d in enumerate(dt):
                    # information about best match so far (m=-1 -> unmatched)
                    iou = min([t, 1 - 1e-10])
                    bstOa = iou
                    bstg = -2
                    bstm = -2
                    for gind, g in enumerate(gt):
                        m = gtm[tind, gind]
                        # if this gt already matched, and not a crowd, continue
                        if m > 0:
                            continue
                        # if dt matched to reg gt, and on ignore gt, stop
                        if bstm != -2 and gtIg[gind] == 1:
                            break
                        # continue to next gt unless better match made
                        if ious[dind, gind] < bstOa:
                            continue
                        # if match successful and best so far, store
                        # appropriately
                        bstOa = ious[dind, gind]
                        bstg = gind
                        if gtIg[gind] == 0:
                            bstm = 1
                        else:
                            bstm = -1

                    # if match made store id of match for both dt and gt
                    if bstg == -2:
                        continue
                    dtIg[tind, dind] = gtIg[bstg]
                    dtm[tind, dind] = gt[bstg]['id']
                    if bstm == 1:
                        gtm[tind, bstg] = d['id']

        # store results for given image and category
        return {
            'image_id': imgId,
            'category_id': catId,
            'hRng': hRng,
            'vRng': vRng,
            'maxDet': maxDet,
            'dtIds': [d['id'] for d in dt],
            'gtIds': [g['id'] for g in gt],
            'dtMatches': dtm,
            'gtMatches': gtm,
            'dtScores': [d['score'] for d in dt],
            'gtIgnore': gtIg,
            'dtIgnore': dtIg,
        }

    def accumulate(self, p=None):
        '''
        Accumulate per image evaluation results and store the result in self.eval
        :param p: input params for evaluation
        :return: None
        '''
        print('Accumulating evaluation results...')
        tic = time.time()
        if not self.evalImgs:
            print('Please run evaluate() first')
        # allows input customized parameters
        if p is None:
            p = self.params
        p.catIds = p.catIds if p.useCats == 1 else [-1]
        T = len(p.iouThrs)
        R = len(p.fppiThrs)
        K = len(p.catIds) if p.useCats else 1
        M = len(p.maxDets)
        # -1 for the precision of absent categories
        ys = -np.ones((T, R, K, M))

        # create dictionary for future indexing
        _pe = self._paramsEval
        catIds = [1]  # _pe.catIds if _pe.useCats else [-1]
        setK = set(catIds)
        setM = set(_pe.maxDets)
        setI = set(_pe.imgIds)
        # get inds to evaluate
        k_list = [n for n, k in enumerate(p.catIds) if k in setK]

        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]
        i_list = [n for n, i in enumerate(p.imgIds) if i in setI]
        I0 = len(_pe.imgIds)

        # retrieve E at each category, area range, and max number of detections
        for k, k0 in enumerate(k_list):
            Nk = k0 * I0
            for m, maxDet in enumerate(m_list):
                E = [self.evalImgs[Nk + i] for i in i_list]
                E = [e for e in E if e is not None]
                if len(E) == 0:
                    continue

                dtScores = np.concatenate([e['dtScores'][0:maxDet] for e in E])

                # different sorting method generates slightly different results.
                # mergesort is used to be consistent as Matlab implementation.

                inds = np.argsort(-dtScores, kind='mergesort')

                dtm = np.concatenate([e['dtMatches'][:, 0:maxDet]
                                      for e in E], axis=1)[:, inds]
                dtIg = np.concatenate([e['dtIgnore'][:, 0:maxDet]
                                       for e in E], axis=1)[:, inds]
                gtIg = np.concatenate([e['gtIgnore'] for e in E])
                npig = np.count_nonzero(gtIg == 0)
                if npig == 0:
                    continue
                tps = np.logical_and(dtm, np.logical_not(dtIg))
                fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg))
                # print(tps,fps)
                inds = np.where(dtIg == 0)[1]
                # print(inds)
                tps = tps[:, inds]
                fps = fps[:, inds]
                # print(tps,fps)
                tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)
                fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)
                # print(tp_sum,fp_sum)
                for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):
                    tp = np.array(tp)
                    fppi = np.array(fp) / I0
                    nd = len(tp)
                    recall = tp / npig
                    q = np.zeros((R,))

                    # numpy is slow without cython optimization for accessing elements
                    # use python array gets significant speed improvement
                    recall = recall.tolist()
                    q = q.tolist()

                    for i in range(nd - 1, 0, -1):
                        if recall[i] < recall[i - 1]:
                            recall[i - 1] = recall[i]

                    inds = np.searchsorted(fppi, p.fppiThrs, side='right') - 1
                    try:
                        for ri, pi in enumerate(inds):
                            q[ri] = recall[pi]
                    except BaseException:
                        pass
                    ys[t, :, k, m] = np.array(q)
        self.eval = {
            'params': p,
            'counts': [T, R, K, M],
            'date': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'TP': ys,
        }
        toc = time.time()
        print('DONE (t={:0.2f}s).'.format(toc - tic))

    def summarize(self, id_setup):
        '''
        Compute and display summary metrics for evaluation results.
        Note this functin can *only* be applied on the default parameter setting
        '''
        def _summarize(iouThr=None, maxDets=100):
            p = self.params
            iStr = ' {:<18} {} @ {:<18}[ IoU={:<9}| height={:>6s} | visibility={:>6s} ] = {:0.2f}%'
            titleStr = 'Average Miss Rate'
            typeStr = '(MR)'
            setupStr = p.SetupLbl[id_setup]
            iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) \
                if iouThr is None else '{:0.2f}'.format(iouThr)
            heightStr = '[{:0.0f}:{:0.0f}]'.format(
                p.HtRng[id_setup][0], p.HtRng[id_setup][1])
            occlStr = '[{:0.2f}:{:0.2f}]'.format(
                p.VisRng[id_setup][0], p.VisRng[id_setup][1])

            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]

            # dimension of precision: [TxRxKxAxM]
            s = self.eval['TP']
            # IoU
            if iouThr is not None:
                t = np.where(iouThr == p.iouThrs)[0]
                s = s[t]
            mrs = 1 - s[:, :, :, mind]

            if len(mrs[mrs < 2]) == 0:
                mean_s = -1
            else:
                mean_s = np.log(mrs[mrs < 2])
                mean_s = np.mean(mean_s)
                mean_s = np.exp(mean_s)
            print(
                iStr.format(
                    titleStr,
                    typeStr,
                    setupStr,
                    iouStr,
                    heightStr,
                    occlStr,
                    mean_s * 100
                )
            )
            return mean_s

        if not self.eval:
            raise Exception('Please run accumulate() first')
        self.stats = _summarize(iouThr=.5, maxDets=1000)

    def __str__(self):
        self.summarize()


class Params:
    '''
    Params for coco evaluation api
    '''

    def setDetParams(self):
        self.imgIds = []
        self.catIds = []
        # np.arange causes trouble.  the data point on arange is slightly larger than the true value

        self.recThrs = np.linspace(.0,
                                   1.00,
                                   int(np.round((1.00 - .0) / .01)) + 1,
                                   endpoint=True)
        self.fppiThrs = np.array(
            [0.0100, 0.0178, 0.0316, 0.0562, 0.1000, 0.1778, 0.3162, 0.5623, 1.0000])
        self.maxDets = [1000]
        self.expFilter = 1.25
        self.useCats = 1

        # np.linspace(.5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)
        self.iouThrs = np.array([0.5])

        self.HtRng = [[50, 1e5 ** 2], [50, 75], [50, 1e5 ** 2], [20, 1e5 ** 2]]
        self.VisRng = [[0.65, 1e5 ** 2], [0.65, 1e5 ** 2],
                       [0.2, 0.65], [0.2, 1e5 ** 2]]
        self.SetupLbl = [
            'Reasonable',
            'Reasonable_small',
            'Reasonable_occ=heavy',
            'All']

    def __init__(self, iouType='segm'):
        if iouType == 'segm' or iouType == 'bbox':
            self.setDetParams()
        else:
            raise Exception('iouType not supported')
        self.iouType = iouType
        # useSegm is deprecated
        self.useSegm = None
```

### cvpods/evaluation/citypersons_evaluation.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved
import contextlib
import copy
import io
import itertools
import json
import logging
import os
from collections import OrderedDict

import numpy as np
import pycocotools.mask as mask_util
from pycocotools.coco import COCO

import torch

from cvpods.data.datasets.coco import convert_to_coco_json
from cvpods.structures import BoxMode
from cvpods.utils import PathManager, comm, create_small_table

# from pycocotools.cocoeval import COCOeval
from .eval_MR_multisetup import COCOeval
from .evaluator import DatasetEvaluator
from .registry import EVALUATOR


@EVALUATOR.register()
class CityPersonsEvaluator(DatasetEvaluator):
    """
    Evaluate object proposal, instance detection/segmentation, keypoint detection
    outputs using COCO's metrics and APIs.
    """

    def __init__(
            self,
            dataset_name,
            meta,
            cfg,
            distributed,
            output_dir=None,
            dump=False):
        """
        Args:
            dataset_name (str): name of the dataset to be evaluated.
                It must have either the following corresponding metadata:

                    "json_file": the path to the COCO format annotation

                Or it must be in cvpods's standard dataset format
                so it can be converted to COCO format automatically.
            meta (SimpleNamespace): dataset metadata.
            cfg (config dict): cvpods Config instance.
            distributed (True): if True, will collect results from all ranks for evaluation.
                Otherwise, will evaluate the results in the current process.
            output_dir (str): optional, an output directory to dump results.
            dump (bool): If True, after the evaluation is completed, a Markdown file
                that records the model evaluation metrics and corresponding scores
                will be generated in the working directory.
        """
        self._dump = dump
        self._tasks = self._tasks_from_config(cfg)
        self._distributed = distributed
        self._output_dir = output_dir

        self._cpu_device = torch.device("cpu")
        self._logger = logging.getLogger(__name__)

        self._metadata = meta
        if not hasattr(self._metadata, "json_file"):
            self._logger.warning(
                f"json_file was not found in MetaDataCatalog for '{dataset_name}'")

            cache_path = convert_to_coco_json(dataset_name, output_dir)
            self._metadata.json_file = cache_path

        json_file = PathManager.get_local_path(self._metadata.json_file)
        with contextlib.redirect_stdout(io.StringIO()):
            self._coco_api = COCO(json_file)

        self._kpt_oks_sigmas = cfg.TEST.KEYPOINT_OKS_SIGMAS
        # Test set json files do not contain annotations (evaluation must be
        # performed using the COCO evaluation server).
        self._do_evaluation = "annotations" in self._coco_api.dataset

    def reset(self):
        self._predictions = []
        self._coco_results = []
        self._dump_infos = []  # per task

    def _tasks_from_config(self, cfg):
        """
        Returns:
            tuple[str]: tasks that can be evaluated under the given configuration.
        """
        tasks = ("bbox",)
        if cfg.MODEL.MASK_ON:
            tasks = tasks + ("segm",)
        if cfg.MODEL.KEYPOINT_ON:
            tasks = tasks + ("keypoints",)
        return tasks

    def process(self, inputs, outputs):
        """
        Args:
            inputs: the inputs to a COCO model (e.g., GeneralizedRCNN).
                It is a list of dict. Each dict corresponds to an image and
                contains keys like "height", "width", "file_name", "image_id".
            outputs: the outputs of a COCO model. It is a list of dicts with key
                "instances" that contains :class:`Instances`.
        """
        for input, output in zip(inputs, outputs):
            prediction = {"image_id": input["image_id"]}

            # TODO this is ugly
            if "instances" in output:
                instances = output["instances"].to(self._cpu_device)
                prediction["instances"] = instances_to_coco_json(
                    instances, input["image_id"])
            if "proposals" in output:
                prediction["proposals"] = output["proposals"].to(
                    self._cpu_device)
            self._predictions.append(prediction)

    def evaluate(self):
        if self._distributed:
            comm.synchronize()
            self._predictions = comm.gather(self._predictions, dst=0)
            self._predictions = list(itertools.chain(*self._predictions))

            if not comm.is_main_process():
                return {}

        if len(self._predictions) == 0:
            self._logger.warning(
                "[COCOEvaluator] Did not receive valid predictions.")
            return {}

        if self._output_dir:
            PathManager.mkdirs(self._output_dir)
            file_path = os.path.join(
                self._output_dir, "instances_predictions.pth")
            with PathManager.open(file_path, "wb") as f:
                torch.save(self._predictions, f)

        self._results = OrderedDict()
        if "instances" in self._predictions[0]:
            self._eval_predictions(set(self._tasks))

        if self._dump:
            _dump_to_markdown(self._dump_infos)

        # Copy so the caller can do whatever with results
        return copy.deepcopy(self._results)

    def _eval_predictions(self, tasks):
        """
        Evaluate self._predictions on the given tasks.
        Fill self._results with the metrics of the tasks.
        """
        self._logger.info("Preparing results for COCO format ...")
        self._coco_results = list(itertools.chain(
            *[x["instances"] for x in self._predictions]))

        # unmap the category ids for COCO
        if hasattr(self._metadata, "thing_dataset_id_to_contiguous_id"):
            reverse_id_mapping = {
                v: k for k,
                v in self._metadata.thing_dataset_id_to_contiguous_id.items()}
            for result in self._coco_results:
                category_id = result["category_id"]
                assert (
                    category_id in reverse_id_mapping
                ), "A prediction has category_id={}, which is not available in the dataset.".format(
                    category_id
                )
                result["category_id"] = reverse_id_mapping[category_id]

        if self._output_dir:
            file_path = os.path.join(
                self._output_dir, "coco_instances_results.json")
            self._logger.info("Saving results to {}".format(file_path))
            with PathManager.open(file_path, "w") as f:
                f.write(json.dumps(self._coco_results))
                f.flush()

        if not self._do_evaluation:
            self._logger.info("Annotations are not available for evaluation.")
            return

        self._logger.info("Evaluating predictions ...")
        for task in sorted(tasks):
            coco_eval = (
                _evaluate_predictions_on_coco(
                    self._coco_api, self._coco_results, task, kpt_oks_sigmas=self._kpt_oks_sigmas
                )
                if len(self._coco_results) > 0
                else None  # cocoapi does not handle empty results very well
            )
            if task == "bbox":
                task = "MR"
            res = self._derive_coco_results(coco_eval, task)
            self._results[task] = res

    def _derive_coco_results(self, coco_eval, iou_type):
        """
        Derive the desired score numbers from summarized COCOeval.

        Args:
            coco_eval (None or COCOEval): None represents no predictions from model.
            iou_type (str):
            class_names (None or list[str]): if provided, will use it to predict
                per-category AP.

        Returns:
            a dict of {metric name: score}
        """

        metrics = [
            "Reasonable",
            "Reasonable_small",
            "Reasonable_occ=heavy",
            "All"]

        if coco_eval is None:
            self._logger.warn(
                "No predictions from the model! Set scores to -1")
            return {metric: -1 for metric in metrics}

        # the standard metrics
        results = {metric: coco_eval[idx]
                   for idx, metric in enumerate(metrics)}
        small_table = create_small_table(results)
        self._logger.info(
            "Evaluation results for {}: \n".format(iou_type) + small_table
        )

        if self._dump:
            dump_info_one_task = {
                "task": iou_type,
                "tables": [small_table],
            }
            self._dump_infos.append(dump_info_one_task)
        # if class_names is None or len(class_names) <= 1:
        return results


def instances_to_coco_json(instances, img_id):
    """
    Dump an "Instances" object to a COCO-format json that's used for evaluation.
    Args:
        instances (Instances):
        img_id (int): the image id
    Returns:
        list[dict]: list of json annotations in COCO format.
    """
    num_instance = len(instances)
    if num_instance == 0:
        return []

    boxes = instances.pred_boxes.tensor.numpy()
    boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)
    boxes = boxes.tolist()
    scores = instances.scores.tolist()
    classes = instances.pred_classes.tolist()

    has_mask = instances.has("pred_masks")
    if has_mask:
        # use RLE to encode the masks, because they are too large and takes memory
        # since this evaluator stores outputs of the entire dataset
        rles = [
            mask_util.encode(np.array(mask[:, :, None], order="F", dtype="uint8"))[0]
            for mask in instances.pred_masks
        ]
        for rle in rles:
            # "counts" is an array encoded by mask_util as a byte-stream. Python3's
            # json writer which always produces strings cannot serialize a bytestream
            # unless you decode it. Thankfully, utf-8 works out (which is also what
            # the pycocotools/_mask.pyx does).
            rle["counts"] = rle["counts"].decode("utf-8")

    has_keypoints = instances.has("pred_keypoints")
    if has_keypoints:
        keypoints = instances.pred_keypoints

    results = []
    for k in range(num_instance):
        result = {
            "image_id": img_id,
            "category_id": classes[k],
            "bbox": boxes[k],
            "height": boxes[k][3],
            "score": scores[k],
        }
        if has_mask:
            result["segmentation"] = rles[k]
        if has_keypoints:
            # In COCO annotations,
            # keypoints coordinates are pixel indices.
            # However our predictions are floating point coordinates.
            # Therefore we subtract 0.5 to be consistent with the annotation format.
            # This is the inverse of data loading logic in `datasets/coco.py`.
            keypoints[k][:, :2] -= 0.5
            result["keypoints"] = keypoints[k].flatten().tolist()
        results.append(result)
    return results


def _evaluate_predictions_on_coco(
        coco_gt,
        coco_results,
        iou_type,
        kpt_oks_sigmas=None):
    """
    Evaluate the coco results using COCOEval API.
    """
    assert len(coco_results) > 0

    if iou_type == "segm":
        coco_results = copy.deepcopy(coco_results)
        # When evaluating mask AP, if the results contain bbox, cocoapi will
        # use the box area as the area of the instance, instead of the mask area.
        # This leads to a different definition of small/medium/large.
        # We remove the bbox field to let mask AP use mask area.
        for c in coco_results:
            c.pop("bbox", None)

    coco_dt = coco_gt.loadRes(coco_results)
    coco_eval = COCOeval(coco_gt, coco_dt, iou_type)
    # Use the COCO default keypoint OKS sigmas unless overrides are specified
    if kpt_oks_sigmas:
        coco_eval.params.kpt_oks_sigmas = np.array(kpt_oks_sigmas)

    if iou_type == "keypoints":
        num_keypoints = len(coco_results[0]["keypoints"]) // 3
        assert len(coco_eval.params.kpt_oks_sigmas) == num_keypoints, (
            "[COCOEvaluator] The length of cfg.TEST.KEYPOINT_OKS_SIGMAS (default: 17) "
            "must be equal to the number of keypoints. However the prediction has {} "
            "keypoints! For more information please refer to "
            "http://cocodataset.org/#keypoints-eval.".format(num_keypoints)
        )
    coco_result = []
    for i in range(0, 4):
        coco_eval_new = copy.deepcopy(coco_eval)
        coco_eval_new.evaluate(i)
        coco_eval_new.accumulate()
        coco_eval_new.summarize(i)
        coco_result.append(coco_eval_new.stats)
    return coco_result


def _dump_to_markdown(dump_infos, md_file="README.md"):
    """
    Dump a Markdown file that records the model evaluation metrics and corresponding scores
    to the current working directory.

    Args:
        dump_infos (list[dict]): dump information for each task.
        md_file (str): markdown file path.
    """
    # The detection results of the citypersons dataset will conduct two evaluations,
    # the first time is `COCOEvaluator`, the second time is `CityPersonsEvaluator`,
    # the title of markdown file has been generated in the first evaluation process,
    # so special treatment will be done here:
    # 1. No title is added
    # 2. The file is written in "a+" mode
    with open(md_file, "a+") as f:
        for dump_info_per_task in dump_infos:
            task_name = dump_info_per_task["task"]
            tables = dump_info_per_task["tables"]
            tables = [table.replace("\n", "  \n") for table in tables]
            f.write("\n\n## Evaluation results for {}:  \n\n".format(task_name))
            f.write(tables[0])
            f.write("\n")
```

### cvpods/evaluation/build.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved
import os

import torch

from cvpods.utils import comm

from .evaluator import DatasetEvaluators
from .registry import EVALUATOR


def build_evaluator(cfg, dataset_name, dataset, output_folder=None, dump=False):
    """
    Create evaluator(s) for a given dataset.
    This uses the special metadata "evaluator_type" associated with each builtin dataset.
    For your own dataset, you can simply create an evaluator manually in your
    script and do not have to worry about the hacky if-else logic here.
    """
    if output_folder is None:
        output_folder = os.path.join(cfg.OUTPUT_DIR, "inference")

    evaluator_list = []
    meta = dataset.meta
    evaluator_type = meta.evaluator_type
    if evaluator_type in ["sem_seg", "coco_panoptic_seg"]:
        evaluator_list.append(
            EVALUATOR.get("SemSegEvaluator")(
                dataset_name,
                dataset,
                distributed=True,
                num_classes=cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES,
                ignore_label=cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE,
                output_dir=output_folder,
                dump=dump,
            )
        )
    if evaluator_type in ["coco", "coco_panoptic_seg", "citypersons"]:
        evaluator_list.append(
            EVALUATOR.get("COCOEvaluator")(dataset_name, meta, cfg, True, output_folder, dump)
        )

    if evaluator_type == "coco_panoptic_seg":
        evaluator_list.append(
            EVALUATOR.get("COCOPanopticEvaluator")(dataset_name, meta, output_folder, dump))
    elif evaluator_type == "cityscapes":
        assert (
            torch.cuda.device_count() >= comm.get_rank()
        ), "CityscapesEvaluator currently do not work with multiple machines."
        return EVALUATOR.get("CityscapesEvaluator")(dataset_name, meta, dump)
    elif evaluator_type == "pascal_voc":
        return EVALUATOR.get("PascalVOCDetectionEvaluator")(dataset_name, meta, dump)
    elif evaluator_type == "lvis":
        return EVALUATOR.get("LVISEvaluator")(dataset_name, meta, cfg, True, output_folder, dump)
    elif evaluator_type == "citypersons":
        evaluator_list.append(
            EVALUATOR.get("CityPersonsEvaluator")(
                dataset_name, meta, cfg, True, output_folder, dump)
        )

    if evaluator_type == "crowdhuman":
        return EVALUATOR.get("CrowdHumanEvaluator")(
            dataset_name, meta, cfg, True, output_folder, dump
        )
    elif evaluator_type == "widerface":
        return EVALUATOR.get("WiderFaceEvaluator")(
            dataset_name, meta, cfg, True, output_folder, dump)

    if evaluator_type == "classification":
        return EVALUATOR.get("ClassificationEvaluator")(
            dataset_name, meta, cfg, True, output_folder, dump)

    if hasattr(cfg, "EVALUATORS"):
        for evaluator in cfg.EVALUATORS:
            evaluator_list.append(evaluator(dataset_name, meta, True, output_folder, dump=True))
    if len(evaluator_list) == 0:
        raise NotImplementedError(
            "no Evaluator for the dataset {} with the type {}".format(
                dataset_name, evaluator_type
            )
        )
    elif len(evaluator_list) == 1:
        return evaluator_list[0]
    return DatasetEvaluators(evaluator_list)
```

### cvpods/evaluation/crowdhumantools.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import json
import os

import numpy as np

PERSON_CLASSES = ['background', 'person']


class Image(object):
    def __init__(self, mode):
        self.ID = None
        self._width = None
        self._height = None
        self.dtboxes = None
        self.gtboxes = None
        self.eval_mode = mode

        self._ignNum = None
        self._gtNum = None
        self._dtNum = None

    def load(self, record, body_key, head_key, class_names, gtflag):
        """
        :meth: read the object from a dict
        """
        if "ID" in record and self.ID is None:
            self.ID = record['ID']
        if "width" in record and self._width is None:
            self._width = record["width"]
        if "height" in record and self._height is None:
            self._height = record["height"]
        if gtflag:
            self._gtNum = len(record["gtboxes"])
            body_bbox, head_bbox = self.load_gt_boxes(record, 'gtboxes', class_names)
            if self.eval_mode == 0:
                self.gtboxes = body_bbox
                self._ignNum = (body_bbox[:, -1] == -1).sum()
            elif self.eval_mode == 1:
                self.gtboxes = head_bbox
                self._ignNum = (head_bbox[:, -1] == -1).sum()
            elif self.eval_mode == 2:
                gt_tag = np.array(
                    [body_bbox[i, -1] != -1 and head_bbox[i, -1] != -1
                     for i in range(len(body_bbox))]
                )
                self._ignNum = (gt_tag == 0).sum()
                self.gtboxes = np.hstack(
                    (body_bbox[:, :-1], head_bbox[:, :-1], gt_tag.reshape(-1, 1))
                )
            else:
                raise Exception('Unknown evaluation mode!')
        if not gtflag:
            self._dtNum = len(record["dtboxes"])
            if self.eval_mode == 0:
                self.dtboxes = self.load_det_boxes(record, 'dtboxes', body_key, 'score')
            elif self.eval_mode == 1:
                self.dtboxes = self.load_det_boxes(record, 'dtboxes', head_key, 'score')
            elif self.eval_mode == 2:
                body_dtboxes = self.load_det_boxes(record, 'dtboxes', body_key)
                head_dtboxes = self.load_det_boxes(record, 'dtboxes', head_key, 'score')
                self.dtboxes = np.hstack((body_dtboxes, head_dtboxes))
            else:
                raise Exception('Unknown evaluation mode!')

    def compare_caltech(self, thres):
        """
        :meth: match the detection results with the groundtruth by Caltech matching strategy
        :param thres: iou threshold
        :type thres: float
        :return: a list of tuples (dtbox, imageID), in the descending sort of dtbox.score
        """
        if self.dtboxes is None or self.gtboxes is None:
            return list()

        dtboxes = self.dtboxes if self.dtboxes is not None else list()
        gtboxes = self.gtboxes if self.gtboxes is not None else list()
        dt_matched = np.zeros(dtboxes.shape[0])
        gt_matched = np.zeros(gtboxes.shape[0])

        dtboxes = np.array(sorted(dtboxes, key=lambda x: x[-1], reverse=True))
        gtboxes = np.array(sorted(gtboxes, key=lambda x: x[-1], reverse=True))
        if len(dtboxes):
            overlap_iou = self.box_overlap_opr(dtboxes, gtboxes, True)
            overlap_ioa = self.box_overlap_opr(dtboxes, gtboxes, False)
        else:
            return list()

        scorelist = list()
        for i, dt in enumerate(dtboxes):
            maxpos = -1
            maxiou = thres
            for j, gt in enumerate(gtboxes):
                if gt_matched[j] == 1:
                    continue
                if gt[-1] > 0:
                    overlap = overlap_iou[i][j]
                    if overlap > maxiou:
                        maxiou = overlap
                        maxpos = j
                else:
                    if maxpos >= 0:
                        break
                    else:
                        overlap = overlap_ioa[i][j]
                        if overlap > thres:
                            maxiou = overlap
                            maxpos = j
            if maxpos >= 0:
                if gtboxes[maxpos, -1] > 0:
                    gt_matched[maxpos] = 1
                    dt_matched[i] = 1
                    scorelist.append((dt, 1, self.ID))
                else:
                    dt_matched[i] = -1
            else:
                dt_matched[i] = 0
                scorelist.append((dt, 0, self.ID))
        return scorelist

    def compare_caltech_union(self, thres):
        """
        :meth: match the detection results with the groundtruth by Caltech matching strategy
        :param thres: iou threshold
        :type thres: float
        :return: a list of tuples (dtbox, imageID), in the descending sort of dtbox.score
        """
        dtboxes = self.dtboxes if self.dtboxes is not None else list()
        gtboxes = self.gtboxes if self.gtboxes is not None else list()
        if len(dtboxes) == 0:
            return list()
        dt_matched = np.zeros(dtboxes.shape[0])
        gt_matched = np.zeros(gtboxes.shape[0])

        dtboxes = np.array(sorted(dtboxes, key=lambda x: x[-1], reverse=True))
        gtboxes = np.array(sorted(gtboxes, key=lambda x: x[-1], reverse=True))
        dt_body_boxes = np.hstack((dtboxes[:, :4], dtboxes[:, -1][:, None]))
        dt_head_boxes = dtboxes[:, 4:8]
        gt_body_boxes = np.hstack((gtboxes[:, :4], gtboxes[:, -1][:, None]))
        gt_head_boxes = gtboxes[:, 4:8]
        overlap_iou = self.box_overlap_opr(dt_body_boxes, gt_body_boxes, True)
        overlap_head = self.box_overlap_opr(dt_head_boxes, gt_head_boxes, True)
        overlap_ioa = self.box_overlap_opr(dt_body_boxes, gt_body_boxes, False)

        scorelist = list()
        for i, dt in enumerate(dtboxes):
            maxpos = -1
            maxiou = thres
            for j, gt in enumerate(gtboxes):
                if gt_matched[j] == 1:
                    continue
                if gt[-1] > 0:
                    o_body = overlap_iou[i][j]
                    o_head = overlap_head[i][j]
                    if o_body > maxiou and o_head > maxiou:
                        maxiou = o_body
                        maxpos = j
                else:
                    if maxpos >= 0:
                        break
                    else:
                        o_body = overlap_ioa[i][j]
                        if o_body > thres:
                            maxiou = o_body
                            maxpos = j
            if maxpos >= 0:
                if gtboxes[maxpos, -1] > 0:
                    gt_matched[maxpos] = 1
                    dt_matched[i] = 1
                    scorelist.append((dt, 1, self.ID))
                else:
                    dt_matched[i] = -1
            else:
                dt_matched[i] = 0
                scorelist.append((dt, 0, self.ID))
        return scorelist

    def box_overlap_opr(self, dboxes: np.ndarray, gboxes: np.ndarray, if_iou):
        eps = 1e-6
        assert dboxes.shape[-1] >= 4 and gboxes.shape[-1] >= 4
        N, K = dboxes.shape[0], gboxes.shape[0]
        dtboxes = np.tile(np.expand_dims(dboxes, axis=1), (1, K, 1))
        gtboxes = np.tile(np.expand_dims(gboxes, axis=0), (N, 1, 1))

        iw = (np.minimum(dtboxes[:, :, 2], gtboxes[:, :, 2])
              - np.maximum(dtboxes[:, :, 0], gtboxes[:, :, 0]))
        ih = (np.minimum(dtboxes[:, :, 3], gtboxes[:, :, 3])
              - np.maximum(dtboxes[:, :, 1], gtboxes[:, :, 1]))
        inter = np.maximum(0, iw) * np.maximum(0, ih)

        dtarea = (dtboxes[:, :, 2] - dtboxes[:, :, 0]) * (dtboxes[:, :, 3] - dtboxes[:, :, 1])
        if if_iou:
            gtarea = (gtboxes[:, :, 2] - gtboxes[:, :, 0]) * (gtboxes[:, :, 3] - gtboxes[:, :, 1])
            ious = inter / (dtarea + gtarea - inter + eps)
        else:
            ious = inter / (dtarea + eps)
        return ious

    def clip_all_boader(self):

        def _clip_boundary(boxes, height, width):
            assert boxes.shape[-1] >= 4
            boxes[:, 0] = np.minimum(np.maximum(boxes[:, 0], 0), width - 1)
            boxes[:, 1] = np.minimum(np.maximum(boxes[:, 1], 0), height - 1)
            boxes[:, 2] = np.maximum(np.minimum(boxes[:, 2], width), 0)
            boxes[:, 3] = np.maximum(np.minimum(boxes[:, 3], height), 0)
            return boxes

        assert self.dtboxes.shape[-1] >= 4
        assert self.gtboxes.shape[-1] >= 4
        assert self._width is not None and self._height is not None
        if self.eval_mode == 2:
            self.dtboxes[:, :4] = _clip_boundary(self.dtboxes[:, :4], self._height, self._width)
            self.gtboxes[:, :4] = _clip_boundary(self.gtboxes[:, :4], self._height, self._width)
            self.dtboxes[:, 4:8] = _clip_boundary(self.dtboxes[:, 4:8], self._height, self._width)
            self.gtboxes[:, 4:8] = _clip_boundary(self.gtboxes[:, 4:8], self._height, self._width)
        else:
            self.dtboxes = _clip_boundary(self.dtboxes, self._height, self._width)
            self.gtboxes = _clip_boundary(self.gtboxes, self._height, self._width)

    def load_gt_boxes(self, dict_input, key_name, class_names):
        assert key_name in dict_input
        if len(dict_input[key_name]) < 1:
            return np.empty([0, 5])
        head_bbox = []
        body_bbox = []
        for rb in dict_input[key_name]:
            if rb['tag'] in class_names:
                body_tag = class_names.index(rb['tag'])
                head_tag = 1
            else:
                body_tag = -1
                head_tag = -1
            if 'extra' in rb:
                if 'ignore' in rb['extra']:
                    if rb['extra']['ignore'] != 0:
                        body_tag = -1
                        head_tag = -1
            if 'head_attr' in rb:
                if 'ignore' in rb['head_attr']:
                    if rb['head_attr']['ignore'] != 0:
                        head_tag = -1
            head_bbox.append(np.hstack((rb['hbox'], head_tag)))
            body_bbox.append(np.hstack((rb['fbox'], body_tag)))
        head_bbox = np.array(head_bbox)
        head_bbox[:, 2:4] += head_bbox[:, :2]
        body_bbox = np.array(body_bbox)
        body_bbox[:, 2:4] += body_bbox[:, :2]
        return body_bbox, head_bbox

    def load_det_boxes(self, dict_input, key_name, key_box, key_score=None, key_tag=None):
        assert key_name in dict_input
        if len(dict_input[key_name]) < 1:
            return np.empty([0, 5])
        else:
            assert key_box in dict_input[key_name][0]
            if key_score:
                assert key_score in dict_input[key_name][0]
            if key_tag:
                assert key_tag in dict_input[key_name][0]
        if key_score:
            if key_tag:
                bboxes = np.vstack(
                    [
                        np.hstack(
                            (rb[key_box], rb[key_score], rb[key_tag])
                        ) for rb in dict_input[key_name]
                    ]
                )
            else:
                bboxes = np.vstack(
                    [np.hstack((rb[key_box], rb[key_score])) for rb in dict_input[key_name]]
                )
        else:
            if key_tag:
                bboxes = np.vstack(
                    [np.hstack((rb[key_box], rb[key_tag])) for rb in dict_input[key_name]]
                )
            else:
                bboxes = np.vstack([rb[key_box] for rb in dict_input[key_name]])
        bboxes[:, 2:4] += bboxes[:, :2]
        return bboxes

    def compare_voc(self, thres):
        """
        :meth: match the detection results with the groundtruth by VOC matching strategy
        :param thres: iou threshold
        :type thres: float
        :return: a list of tuples (dtbox, imageID), in the descending sort of dtbox.score
        """
        if self.dtboxes is None:
            return list()
        dtboxes = self.dtboxes
        gtboxes = self.gtboxes if self.gtboxes is not None else list()
        dtboxes.sort(key=lambda x: x.score, reverse=True)
        gtboxes.sort(key=lambda x: x.ign)

        scorelist = list()
        for i, dt in enumerate(dtboxes):
            maxpos = -1
            maxiou = thres

            for j, gt in enumerate(gtboxes):
                overlap = dt.iou(gt)
                if overlap > maxiou:
                    maxiou = overlap
                    maxpos = j

            if maxpos >= 0:
                if gtboxes[maxpos].ign == 0:
                    gtboxes[maxpos].matched = 1
                    dtboxes[i].matched = 1
                    scorelist.append((dt, self.ID))
                else:
                    dtboxes[i].matched = -1
            else:
                dtboxes[i].matched = 0
                scorelist.append((dt, self.ID))
        return scorelist


class Database(object):
    def __init__(self, gtpath=None, dtpath=None, body_key=None, head_key=None, mode=0):
        """
        mode=0: only body; mode=1: only head
        """
        self.images = dict()
        self.eval_mode = mode
        self.loadData(gtpath, body_key, head_key, if_gt=True)
        self.loadData(dtpath, body_key, head_key, if_gt=False)

        self._ignNum = sum([self.images[i]._ignNum for i in self.images])
        self._gtNum = sum([self.images[i]._gtNum for i in self.images])
        self._imageNum = len(self.images)
        self.scorelist = None

    def loadData(self, fpath, body_key=None, head_key=None, if_gt=True):
        assert os.path.isfile(fpath), fpath + " does not exist!"
        with open(fpath, "r") as f:
            lines = f.readlines()
        records = [json.loads(line.strip('\n')) for line in lines]
        if if_gt:
            for record in records:
                self.images[record["ID"]] = Image(self.eval_mode)
                self.images[record["ID"]].load(record, body_key, head_key, PERSON_CLASSES, True)
        else:
            for record in records:
                self.images[record["ID"]].load(record, body_key, head_key, PERSON_CLASSES, False)
                self.images[record["ID"]].clip_all_boader()

    def compare(self, thres=0.5, matching=None):
        """
        match the detection results with the groundtruth in the whole database
        """
        assert matching is None or matching == "VOC", matching
        scorelist = list()
        for ID in self.images:
            if matching == "VOC":
                result = self.images[ID].compare_voc(thres)
            else:
                result = self.images[ID].compare_caltech(thres)
            scorelist.extend(result)
        # In the descending sort of dtbox score.
        scorelist.sort(key=lambda x: x[0][-1], reverse=True)
        self.scorelist = scorelist

    def eval_MR(self, ref="CALTECH_-2"):
        """
        evaluate by Caltech-style log-average miss rate
        ref: str - "CALTECH_-2"/"CALTECH_-4"
        """
        # find greater_than
        def _find_gt(lst, target):
            for idx, item in enumerate(lst):
                if item >= target:
                    return idx
            return len(lst) - 1

        assert ref == "CALTECH_-2" or ref == "CALTECH_-4", ref
        if ref == "CALTECH_-2":
            # CALTECH_MRREF_2: anchor points (from 10^-2 to 1) as in P.Dollar's paper
            ref = [0.0100, 0.0178, 0.03160, 0.0562, 0.1000, 0.1778, 0.3162, 0.5623, 1.000]
        else:
            # CALTECH_MRREF_4: anchor points (from 10^-4 to 1) as in S.Zhang's paper
            ref = [0.0001, 0.0003, 0.00100, 0.0032, 0.0100, 0.0316, 0.1000, 0.3162, 1.000]

        if self.scorelist is None:
            self.compare()

        tp, fp = 0.0, 0.0
        fppiX, fppiY = list(), list()
        for i, item in enumerate(self.scorelist):
            if item[1] == 1:
                tp += 1.0
            elif item[1] == 0:
                fp += 1.0

            fn = (self._gtNum - self._ignNum) - tp
            recall = tp / (tp + fn)
            missrate = 1.0 - recall
            fppi = fp / self._imageNum
            fppiX.append(fppi)
            fppiY.append(missrate)

        score = list()
        for pos in ref:
            argmin = _find_gt(fppiX, pos)
            if argmin >= 0:
                score.append(fppiY[argmin])
        score = np.array(score)
        MR = np.exp(np.log(score).mean())
        return MR, (fppiX, fppiY)

    def eval_AP(self):
        """
        :meth: evaluate by average precision
        """
        # calculate general ap score
        def _calculate_map(recall, precision):
            assert len(recall) == len(precision)
            area = 0
            for i in range(1, len(recall)):
                delta_h = (precision[i - 1] + precision[i]) / 2
                delta_w = recall[i] - recall[i - 1]
                area += delta_w * delta_h
            return area

        tp, fp = 0.0, 0.0
        rpX, rpY = list(), list()
        total_gt = self._gtNum - self._ignNum
        total_images = self._imageNum

        fpn = []
        recalln = []
        thr = []
        fppi = []
        for i, item in enumerate(self.scorelist):
            if item[1] == 1:
                tp += 1.0
            elif item[1] == 0:
                fp += 1.0
            fn = total_gt - tp
            recall = tp / (tp + fn)
            precision = tp / (tp + fp)
            rpX.append(recall)
            rpY.append(precision)
            fpn.append(fp)
            recalln.append(tp)
            thr.append(item[0][-1])
            fppi.append(fp / total_images)

        AP = _calculate_map(rpX, rpY)
        return AP, recall, (rpX, rpY, thr, fpn, recalln, fppi)
```

### cvpods/evaluation/fast_eval_api.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import copy
import time

import numpy as np
from pycocotools.cocoeval import COCOeval

from cvpods import _C


class COCOeval_opt(COCOeval):
    """
    This is a slightly modified version of the original COCO API, where the functions evaluateImg()
    and accumulate() are implemented in C++ to speedup evaluation
    """

    def evaluate(self):
        """
        Run per image evaluation on given images and store results in self.evalImgs_cpp, a
        datastructure that isn't readable from Python but is used by a c++ implementation of
        accumulate().  Unlike the original COCO PythonAPI, we don't populate the datastructure
        self.evalImgs because this datastructure is a computational bottleneck.
        :return: None
        """
        tic = time.time()

        print("Running per image evaluation...")
        p = self.params
        # add backward compatibility if useSegm is specified in params
        if p.useSegm is not None:
            p.iouType = "segm" if p.useSegm == 1 else "bbox"
            print("useSegm (deprecated) is not None. Running {} evaluation".format(p.iouType))
        print("Evaluate annotation type *{}*".format(p.iouType))
        p.imgIds = list(np.unique(p.imgIds))
        if p.useCats:
            p.catIds = list(np.unique(p.catIds))
        p.maxDets = sorted(p.maxDets)
        self.params = p

        self._prepare()

        # loop through images, area range, max detection number
        catIds = p.catIds if p.useCats else [-1]

        if p.iouType == "segm" or p.iouType == "bbox":
            computeIoU = self.computeIoU
        elif p.iouType == "keypoints":
            computeIoU = self.computeOks
        self.ious = {
            (imgId, catId): computeIoU(imgId, catId) for imgId in p.imgIds for catId in catIds
        }

        maxDet = p.maxDets[-1]

        # <<<< Beginning of code differences with original COCO API
        def convert_instances_to_cpp(instances, is_det=False):
            # Convert annotations for a list of instances in an image to a format that's fast
            # to access in C++
            instances_cpp = []
            for instance in instances:
                instance_cpp = _C.InstanceAnnotation(
                    instance["id"],
                    instance["score"] if is_det else instance.get("score", 0.0),
                    instance["area"],
                    bool(instance.get("iscrowd", 0)),
                    bool(instance.get("ignore", 0)),
                )
                instances_cpp.append(instance_cpp)
            return instances_cpp

        # Convert GT annotations, detections, and IOUs to a format that's fast to access in C++
        ground_truth_instances = [
            [convert_instances_to_cpp(self._gts[imgId, catId]) for catId in p.catIds]
            for imgId in p.imgIds
        ]
        detected_instances = [
            [convert_instances_to_cpp(self._dts[imgId, catId], is_det=True) for catId in p.catIds]
            for imgId in p.imgIds
        ]
        ious = [[self.ious[imgId, catId] for catId in catIds] for imgId in p.imgIds]

        if not p.useCats:
            # For each image, flatten per-category lists into a single list
            ground_truth_instances = [[[o for c in i for o in c]] for i in ground_truth_instances]
            detected_instances = [[[o for c in i for o in c]] for i in detected_instances]

        # Call C++ implementation of self.evaluateImgs()
        self._evalImgs_cpp = _C.COCOevalEvaluateImages(
            p.areaRng, maxDet, p.iouThrs, ious, ground_truth_instances, detected_instances
        )
        self._evalImgs = None

        self._paramsEval = copy.deepcopy(self.params)
        toc = time.time()
        print("COCOeval_opt.evaluate() finished in {:0.2f} seconds.".format(toc - tic))
        # >>>> End of code differences with original COCO API

    def accumulate(self):
        """
        Accumulate per image evaluation results and store the result in self.eval.  Does not
        support changing parameter settings from those used by self.evaluate()
        """
        print("Accumulating evaluation results...")
        tic = time.time()
        if not hasattr(self, "_evalImgs_cpp"):
            print("Please run evaluate() first")

        self.eval = _C.COCOevalAccumulate(self._paramsEval, self._evalImgs_cpp)

        # recall is num_iou_thresholds X num_categories X num_area_ranges X num_max_detections
        self.eval["recall"] = np.array(self.eval["recall"]).reshape(
            self.eval["counts"][:1] + self.eval["counts"][2:]
        )

        # precision and scores are num_iou_thresholds X num_recall_thresholds X num_categories X
        # num_area_ranges X num_max_detections
        self.eval["precision"] = np.array(self.eval["precision"]).reshape(self.eval["counts"])
        self.eval["scores"] = np.array(self.eval["scores"]).reshape(self.eval["counts"])
        toc = time.time()
        print("COCOeval_opt.accumulate() finished in {:0.2f} seconds.".format(toc - tic))
```

### cvpods/evaluation/panoptic_evaluation.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import contextlib
import io
import itertools
import json
import logging
import os
import tempfile
from collections import OrderedDict
from tabulate import tabulate

from PIL import Image

from cvpods.utils import PathManager, comm

from .evaluator import DatasetEvaluator
from .registry import EVALUATOR

logger = logging.getLogger(__name__)


@EVALUATOR.register()
class COCOPanopticEvaluator(DatasetEvaluator):
    """
    Evaluate Panoptic Quality metrics on COCO using PanopticAPI.
    It saves panoptic segmentation prediction in `output_dir`

    It contains a synchronize call and has to be called from all workers.
    """

    def __init__(self, dataset_name, meta, output_dir, dump=False):
        """
        Args:
            dataset_name (str): name of the dataset.
            meta (SimpleNamespace): dataset metadata.
            output_dir (str): output directory to save results for evaluation.
            dump (bool): If True, after the evaluation is completed, a Markdown file
                that records the model evaluation metrics and corresponding scores
                will be generated in the working directory.
        """
        # TODO: really use dataset_name
        self.dataset_name = dataset_name
        self._dump = dump
        self._metadata = meta
        self._thing_contiguous_id_to_dataset_id = {
            v: k for k, v in self._metadata.thing_dataset_id_to_contiguous_id.items()
        }
        self._stuff_contiguous_id_to_dataset_id = {
            v: k for k, v in self._metadata.stuff_dataset_id_to_contiguous_id.items()
        }

        self._predictions_json = os.path.join(output_dir, "predictions.json")

    def reset(self):
        self._predictions = []

    def _convert_category_id(self, segment_info):
        isthing = segment_info.pop("isthing", None)
        if isthing is None:
            # the model produces panoptic category id directly. No more conversion needed
            return segment_info
        if isthing is True:
            segment_info["category_id"] = self._thing_contiguous_id_to_dataset_id[
                segment_info["category_id"]
            ]
        else:
            segment_info["category_id"] = self._stuff_contiguous_id_to_dataset_id[
                segment_info["category_id"]
            ]
        return segment_info

    def process(self, inputs, outputs):
        from panopticapi.utils import id2rgb

        for input, output in zip(inputs, outputs):
            panoptic_img, segments_info = output["panoptic_seg"]
            panoptic_img = panoptic_img.cpu().numpy()

            file_name = os.path.basename(input["file_name"])
            file_name_png = os.path.splitext(file_name)[0] + ".png"
            with io.BytesIO() as out:
                Image.fromarray(id2rgb(panoptic_img)).save(out, format="PNG")
                segments_info = [self._convert_category_id(x) for x in segments_info]
                self._predictions.append(
                    {
                        "image_id": input["image_id"],
                        "file_name": file_name_png,
                        "png_string": out.getvalue(),
                        "segments_info": segments_info,
                    }
                )

    def evaluate(self):
        comm.synchronize()

        self._predictions = comm.gather(self._predictions)
        self._predictions = list(itertools.chain(*self._predictions))
        if not comm.is_main_process():
            return

        gt_json = PathManager.get_local_path(self._metadata.panoptic_json)
        gt_folder = self._metadata.panoptic_root

        with tempfile.TemporaryDirectory(prefix="panoptic_eval") as pred_dir:
            logger.info("Writing all panoptic predictions to {} ...".format(pred_dir))
            for p in self._predictions:
                with open(os.path.join(pred_dir, p["file_name"]), "wb") as f:
                    f.write(p.pop("png_string"))

            with open(gt_json, "r") as f:
                json_data = json.load(f)
            json_data["annotations"] = self._predictions
            with PathManager.open(self._predictions_json, "w") as f:
                f.write(json.dumps(json_data))

            from panopticapi.evaluation import pq_compute

            with contextlib.redirect_stdout(io.StringIO()):
                pq_res = pq_compute(
                    gt_json,
                    PathManager.get_local_path(self._predictions_json),
                    gt_folder=gt_folder,
                    pred_folder=pred_dir,
                )

        res = {}
        res["PQ"] = 100 * pq_res["All"]["pq"]
        res["SQ"] = 100 * pq_res["All"]["sq"]
        res["RQ"] = 100 * pq_res["All"]["rq"]
        res["PQ_th"] = 100 * pq_res["Things"]["pq"]
        res["SQ_th"] = 100 * pq_res["Things"]["sq"]
        res["RQ_th"] = 100 * pq_res["Things"]["rq"]
        res["PQ_st"] = 100 * pq_res["Stuff"]["pq"]
        res["SQ_st"] = 100 * pq_res["Stuff"]["sq"]
        res["RQ_st"] = 100 * pq_res["Stuff"]["rq"]

        results = OrderedDict({"panoptic_seg": res})
        table = _print_panoptic_results(pq_res)

        if self._dump:
            dump_info_one_task = {
                "task": "panoptic_seg",
                "tables": [table],
            }
            _dump_to_markdown([dump_info_one_task])

        return results


def _print_panoptic_results(pq_res):
    """
    Print evaluation results in the form of a string table.

    Args:
        pq_res (dict): a dictionary containing the evaluation results.

    Returns:
        table (str): a string table containing the model evaluation metrics
            and corresponding scores.
    """
    headers = ["", "PQ", "SQ", "RQ", "#categories"]
    data = []
    for name in ["All", "Things", "Stuff"]:
        row = [name] + [pq_res[name][k] * 100 for k in ["pq", "sq", "rq"]] + [pq_res[name]["n"]]
        data.append(row)
    table = tabulate(
        data, headers=headers, tablefmt="pipe", floatfmt=".3f", stralign="center", numalign="center"
    )
    logger.info("Panoptic Evaluation Results:\n" + table)
    return table


def _dump_to_markdown(dump_infos, md_file="README.md"):
    """
    Dump a Markdown file that records the model evaluation metrics and corresponding scores
    to the current working directory.

    Args:
        dump_infos (list[dict]): dump information for each task.
        md_file (str): markdown file path.
    """
    title = os.getcwd().split("/")[-1]
    with open(md_file, "w") as f:
        f.write("# {}  ".format(title))
        for dump_info_per_task in dump_infos:
            task_name = dump_info_per_task["task"]
            tables = dump_info_per_task["tables"]
            tables = [table.replace("\n", "  \n") for table in tables]
            f.write("\n\n## Evaluation results for {}:  \n\n".format(task_name))
            f.write(tables[0])
            f.write("\n")


if __name__ == "__main__":
    from cvpods.utils import setup_logger

    logger = setup_logger()
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--gt-json")
    parser.add_argument("--gt-dir")
    parser.add_argument("--pred-json")
    parser.add_argument("--pred-dir")
    args = parser.parse_args()

    from panopticapi.evaluation import pq_compute

    with contextlib.redirect_stdout(io.StringIO()):
        pq_res = pq_compute(
            args.gt_json, args.pred_json, gt_folder=args.gt_dir, pred_folder=args.pred_dir
        )
        _print_panoptic_results(pq_res)
```

### cvpods/evaluation/registry.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-

from cvpods.utils import Registry

EVALUATOR = Registry("evaluator")
```

### cvpods/evaluation/classification_evaluation.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import copy
import itertools
import logging
import os
from collections import OrderedDict

import torch

from cvpods.utils import PathManager, comm, create_small_table

from .evaluator import DatasetEvaluator
from .registry import EVALUATOR


@EVALUATOR.register()
class ClassificationEvaluator(DatasetEvaluator):
    """
    Evaluate instance calssification results.
    """

    # TODO: unused_arguments: cfg
    def __init__(self, dataset_name, meta, cfg, distributed, output_dir=None, dump=False):
        """
        Args:
            dataset_name (str): name of the dataset to be evaluated.
            meta (SimpleNamespace): dataset metadata.
            cfg (config dict): cvpods Config instance.
            distributed (True): if True, will collect results from all ranks for evaluation.
                Otherwise, will evaluate the results in the current process.
            output_dir (str): optional, an output directory to dump all
                results predicted on the dataset. The dump contains:

                1. "instance_predictions.pth" a file in torch serialization
                   format that contains all the raw original predictions.

            dump (bool): If True, after the evaluation is completed, a Markdown file
                that records the model evaluation metrics and corresponding scores
                will be generated in the working directory.
        """
        super(ClassificationEvaluator, self).__init__()
        # TODO: really use dataset_name
        self.dataset_name = dataset_name
        self._dump = dump
        self._distributed = distributed
        self._output_dir = output_dir

        self._cpu_device = torch.device("cpu")
        self._logger = logging.getLogger(__name__)

        self._metadata = meta

        self._topk = (1, 5)

    def reset(self):
        self._predictions = []
        self._targets = []
        self._dump_infos = []  # per task

    def process(self, inputs, outputs):
        # Find the top max_k predictions for each sample
        _top_max_k_vals, top_max_k_inds = torch.topk(
            outputs.cpu(), max(self._topk), dim=1, largest=True, sorted=True
        )
        # (batch_size, max_k) -> (max_k, batch_size)
        top_max_k_inds = top_max_k_inds.t()

        self._targets.append(torch.tensor([i["category_id"] for i in inputs]))
        self._predictions.append(top_max_k_inds)

    def evaluate(self):
        if self._distributed:
            comm.synchronize()
            self._predictions = comm.gather(self._predictions, dst=0)
            self._predictions = list(itertools.chain(*self._predictions))

            self._targets = comm.gather(self._targets, dst=0)
            self._targets = list(itertools.chain(*self._targets))

            if not comm.is_main_process():
                return {}

        if len(self._predictions) == 0:
            self._logger.warning("[ClassificationEvaluator] Did not receive valid predictions.")
            return {}

        if self._output_dir:
            PathManager.mkdirs(self._output_dir)
            file_path = os.path.join(self._output_dir, "instances_predictions.pth")
            with PathManager.open(file_path, "wb") as f:
                torch.save(self._predictions, f)

        self._results = OrderedDict()
        assert len(self._predictions) == len(self._targets)
        if self._predictions[0] is not None:
            self._eval_classification_accuracy()

        if self._dump:
            _dump_to_markdown(self._dump_infos)

        # Copy so the caller can do whatever with results
        return copy.deepcopy(self._results)

    def _eval_classification_accuracy(self):
        """
        Evaluate self._predictions on the classification task.
        Fill self._results with the metrics of the tasks.
        """
        batch_size = len(self._targets)

        pred = torch.cat(self._predictions, dim=1)
        target = torch.cat(self._targets)

        correct = pred.eq(target.view(1, -1).expand_as(pred))

        results = {}
        for k in self._topk:
            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)
            results[f"Top_{k} Acc"] = correct_k.mul_(100.0 / batch_size).item()
        self._results["Accuracy"] = results

        small_table = create_small_table(results)
        self._logger.info("Evaluation results for classification: \n" + small_table)

        if self._dump:
            dump_info_one_task = {
                "task": "classification",
                "tables": [small_table],
            }
            self._dump_infos.append(dump_info_one_task)


def _dump_to_markdown(dump_infos, md_file="README.md"):
    """
    Dump a Markdown file that records the model evaluation metrics and corresponding scores
    to the current working directory.

    Args:
        dump_infos (list[dict]): dump information for each task.
        md_file (str): markdown file path.
    """
    with open(md_file, "w") as f:
        title = os.path.basename(os.getcwd())
        f.write("# {}  ".format(title))
        for dump_info_per_task in dump_infos:
            task_name = dump_info_per_task["task"]
            tables = dump_info_per_task["tables"]
            tables = [table.replace("\n", "  \n") for table in tables]
            f.write("\n\n## Evaluation results for {}:  \n\n".format(task_name))
            f.write(tables[0] + "\n")
```

### cvpods/evaluation/__init__.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from .build import build_evaluator
from .citypersons_evaluation import CityPersonsEvaluator
from .cityscapes_evaluation import CityscapesEvaluator
from .classification_evaluation import ClassificationEvaluator
from .coco_evaluation import COCOEvaluator
from .crowdhuman_evaluation import CrowdHumanEvaluator
from .evaluator import DatasetEvaluator, DatasetEvaluators, inference_context, inference_on_dataset
from .lvis_evaluation import LVISEvaluator
from .panoptic_evaluation import COCOPanopticEvaluator
from .pascal_voc_evaluation import PascalVOCDetectionEvaluator
from .registry import EVALUATOR
from .rotated_coco_evaluation import RotatedCOCOEvaluator
from .sem_seg_evaluation import SemSegEvaluator
from .testing import print_csv_format, verify_results
from .widerface_evaluation import WiderFaceEvaluator

__all__ = [k for k in globals().keys() if not k.startswith("_")]
```

### cvpods/evaluation/cityscapes_evaluation.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import glob
import logging
import os
import tempfile
from collections import OrderedDict
from tabulate import tabulate

from PIL import Image

import torch

from cvpods.utils import comm, create_small_table

from .evaluator import DatasetEvaluator
from .registry import EVALUATOR


@EVALUATOR.register()
class CityscapesEvaluator(DatasetEvaluator):
    """
    Evaluate instance segmentation results using cityscapes API.

    Note:
        * It does not work in multi-machine distributed training.
        * It contains a synchronization, therefore has to be used on all ranks.
    """

    def __init__(self, dataset_name, meta, dump=False):
        """
        Args:
            dataset_name (str): the name of the dataset.
                It must have the following metadata associated with it:
                "thing_classes", "gt_dir".
            meta (SimpleNamespace): dataset metadata.
            dump (bool): If True, after the evaluation is completed, a Markdown file
                that records the model evaluation metrics and corresponding scores
                will be generated in the working directory.
        """
        # TODO: really use dataset_name
        self.dataset_name = dataset_name
        self._dump = dump
        self._metadata = meta
        self._cpu_device = torch.device("cpu")
        self._logger = logging.getLogger(__name__)

    def reset(self):
        self._working_dir = tempfile.TemporaryDirectory(prefix="cityscapes_eval_")
        self._temp_dir = self._working_dir.name
        # All workers will write to the same results directory
        # TODO this does not work in distributed training
        self._temp_dir = comm.all_gather(self._temp_dir)[0]
        if self._temp_dir != self._working_dir.name:
            self._working_dir.cleanup()
        self._logger.info(
            "Writing cityscapes results to temporary directory {} ...".format(self._temp_dir)
        )

    def process(self, inputs, outputs):
        from cityscapesscripts.helpers.labels import name2label

        for input, output in zip(inputs, outputs):
            file_name = input["file_name"]
            basename = os.path.splitext(os.path.basename(file_name))[0]
            pred_txt = os.path.join(self._temp_dir, basename + "_pred.txt")

            output = output["instances"].to(self._cpu_device)
            num_instances = len(output)
            with open(pred_txt, "w") as fout:
                for i in range(num_instances):
                    pred_class = output.pred_classes[i]
                    classes = self._metadata.thing_classes[pred_class]
                    class_id = name2label[classes].id
                    score = output.scores[i]
                    mask = output.pred_masks[i].numpy().astype("uint8")
                    png_filename = os.path.join(
                        self._temp_dir, basename + "_{}_{}.png".format(i, classes)
                    )

                    Image.fromarray(mask * 255).save(png_filename)
                    fout.write("{} {} {}\n".format(os.path.basename(png_filename), class_id, score))

    def evaluate(self):
        """
        Returns:
            dict: has a key "segm", whose value is a dict of "AP" and "AP50".
        """
        comm.synchronize()
        if comm.get_rank() > 0:
            return
        os.environ["CITYSCAPES_DATASET"] = os.path.abspath(
            os.path.join(self._metadata.gt_dir, "..", "..")
        )
        # Load the Cityscapes eval script *after* setting the required env var,
        # since the script reads CITYSCAPES_DATASET into global variables at load time.
        import cityscapesscripts.evaluation.evalInstanceLevelSemanticLabeling as cityscapes_eval

        self._logger.info("Evaluating results under {} ...".format(self._temp_dir))

        # set some global states in cityscapes evaluation API, before evaluating
        cityscapes_eval.args.predictionPath = os.path.abspath(self._temp_dir)
        cityscapes_eval.args.predictionWalk = None
        cityscapes_eval.args.JSONOutput = False
        cityscapes_eval.args.colorized = False
        cityscapes_eval.args.gtInstancesFile = os.path.join(self._temp_dir, "gtInstances.json")

        # These lines are adopted from
        # https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/evaluation/evalInstanceLevelSemanticLabeling.py # noqa
        groundTruthImgList = glob.glob(cityscapes_eval.args.groundTruthSearch)
        assert len(
            groundTruthImgList
        ), "Cannot find any ground truth images to use for evaluation. Searched for: {}".format(
            cityscapes_eval.args.groundTruthSearch
        )
        predictionImgList = []
        for gt in groundTruthImgList:
            predictionImgList.append(cityscapes_eval.getPrediction(gt, cityscapes_eval.args))
        results = cityscapes_eval.evaluateImgLists(
            predictionImgList, groundTruthImgList, cityscapes_eval.args
        )["averages"]

        ret = OrderedDict()
        ret["segm"] = {"AP": results["allAp"] * 100, "AP50": results["allAp50%"] * 100}
        self._working_dir.cleanup()

        small_table = create_small_table(ret["segm"])
        self._logger.info("Evaluation results for segm: \n" + small_table)

        results_per_category = []
        for cat, ap in results["classes"].items():
            ap = [ap_i * 100 for ap_i in ap.values()]
            results_per_category.append([cat, *ap])

        table = tabulate(
            results_per_category,
            headers=["category", "AP", "AP50"],
            tablefmt="pipe",
            floatfmt=".3f",
            numalign="left"
        )
        self._logger.info("Per-category segm AP: \n" + table)

        if self._dump:
            dump_info_one_task = {
                "task": "segm",
                "tables": [small_table, table],
            }
            _dump_to_markdown([dump_info_one_task])
        return ret


def _dump_to_markdown(dump_infos, md_file="README.md"):
    """
    Dump a Markdown file that records the model evaluation metrics and corresponding scores
    to the current working directory.

    Args:
        dump_infos (list[dict]): dump information for each task.
        md_file (str): markdown file path.
    """
    title = os.getcwd().split("/")[-1]
    with open(md_file, "w") as f:
        f.write("# {}  ".format(title))
        for dump_info_per_task in dump_infos:
            task_name = dump_info_per_task["task"]
            tables = dump_info_per_task["tables"]
            tables = [table.replace("\n", "  \n") for table in tables]
            f.write("\n\n## Evaluation results for {}:  \n\n".format(task_name))
            f.write(tables[0])
            f.write("\n\n### Per-category {} AP:  \n\n".format(task_name))
            f.write(tables[1])
            f.write("\n")
```

### cvpods/evaluation/coco_evaluation.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import contextlib
import copy
import io
import itertools
import json
import logging
import os
import pickle
from collections import OrderedDict

import numpy as np
import pycocotools.mask as mask_util
from pycocotools.coco import COCO

import torch

from cvpods.data.datasets.coco import convert_to_coco_json
from cvpods.evaluation.fast_eval_api import COCOeval_opt as COCOeval
from cvpods.structures import Boxes, BoxMode, pairwise_iou
from cvpods.utils import PathManager, comm, create_small_table, create_table_with_header

from .evaluator import DatasetEvaluator
from .registry import EVALUATOR


@EVALUATOR.register()
class COCOEvaluator(DatasetEvaluator):
    """
    Evaluate object proposal, instance detection/segmentation, keypoint detection
    outputs using COCO's metrics and APIs.
    """

    def __init__(self, dataset_name, meta, cfg, distributed, output_dir=None, dump=False):
        """
        Args:
            dataset_name (str): name of the dataset to be evaluated.
                It must have either the following corresponding metadata:

                    "json_file": the path to the COCO format annotation

                Or it must be in cvpods's standard dataset format
                so it can be converted to COCO format automatically.
            meta (SimpleNamespace): dataset metadata.
            cfg (config dict): cvpods Config instance.
            distributed (True): if True, will collect results from all ranks for evaluation.
                Otherwise, will evaluate the results in the current process.
            output_dir (str): optional, an output directory to dump all
                results predicted on the dataset. The dump contains two files:

                1. "instance_predictions.pth" a file in torch serialization
                   format that contains all the raw original predictions.
                2. "coco_instances_results.json" a json file in COCO's result
                   format.

            dump (bool): If True, after the evaluation is completed, a Markdown file
                that records the model evaluation metrics and corresponding scores
                will be generated in the working directory.
        """
        self._dump = dump
        self.cfg = cfg
        self._tasks = self._tasks_from_config(cfg)
        self._distributed = distributed
        self._output_dir = output_dir

        self._cpu_device = torch.device("cpu")
        self._logger = logging.getLogger(__name__)

        self._metadata = meta
        if not hasattr(self._metadata, "json_file"):
            self._logger.warning(
                f"json_file was not found in MetaDataCatalog for '{dataset_name}'."
                " Trying to convert it to COCO format ..."
            )

            cache_path = os.path.join(output_dir, f"{dataset_name}_coco_format.json")
            self._metadata.json_file = cache_path
            convert_to_coco_json(dataset_name, cache_path)

        json_file = PathManager.get_local_path(self._metadata.json_file)
        with contextlib.redirect_stdout(io.StringIO()):
            self._coco_api = COCO(json_file)

        self._kpt_oks_sigmas = cfg.TEST.KEYPOINT_OKS_SIGMAS
        # Test set json files do not contain annotations (evaluation must be
        # performed using the COCO evaluation server).
        self._do_evaluation = "annotations" in self._coco_api.dataset

    def reset(self):
        self._predictions = []
        self._coco_results = []
        self._dump_infos = []  # per task

    def _tasks_from_config(self, cfg):
        """
        Returns:
            tuple[str]: tasks that can be evaluated under the given configuration.
        """
        tasks = ("bbox",)
        if cfg.MODEL.MASK_ON:
            tasks = tasks + ("segm",)
        if cfg.MODEL.KEYPOINT_ON:
            tasks = tasks + ("keypoints",)
        return tasks

    def process(self, inputs, outputs):
        """
        Args:
            inputs: the inputs to a COCO model (e.g., GeneralizedRCNN).
                It is a list of dict. Each dict corresponds to an image and
                contains keys like "height", "width", "file_name", "image_id".
            outputs: the outputs of a COCO model. It is a list of dicts with key
                "instances" that contains :class:`Instances`.
        """
        for input, output in zip(inputs, outputs):
            prediction = {"image_id": input["image_id"]}

            # TODO this is ugly
            if "instances" in output:
                instances = output["instances"].to(self._cpu_device)
                prediction["instances"] = instances_to_coco_json(instances, input["image_id"])
            if "proposals" in output:
                prediction["proposals"] = output["proposals"].to(self._cpu_device)
            self._predictions.append(prediction)

    def evaluate(self):
        if self._distributed:
            comm.synchronize()
            self._predictions = comm.gather(self._predictions, dst=0)
            self._predictions = list(itertools.chain(*self._predictions))

            if not comm.is_main_process():
                return {}

        if len(self._predictions) == 0:
            self._logger.warning("[COCOEvaluator] Did not receive valid predictions.")
            return {}

        if self._output_dir:
            PathManager.mkdirs(self._output_dir)
            file_path = os.path.join(self._output_dir, "instances_predictions.pth")
            with PathManager.open(file_path, "wb") as f:
                torch.save(self._predictions, f)

        self._results = OrderedDict()
        if "proposals" in self._predictions[0]:
            self._eval_box_proposals()
        if "instances" in self._predictions[0]:
            self._eval_predictions(set(self._tasks))

        if self._dump:
            extra_infos = {
                "title": os.path.basename(os.getcwd()),
                "seed": self.cfg.SEED,
            }
            _dump_to_markdown(extra_infos, self._dump_infos)

        # Copy so the caller can do whatever with results
        return copy.deepcopy(self._results)

    def _eval_predictions(self, tasks):
        """
        Evaluate self._predictions on the given tasks.
        Fill self._results with the metrics of the tasks.
        """
        self._logger.info("Preparing results for COCO format ...")
        self._coco_results = list(itertools.chain(*[x["instances"] for x in self._predictions]))

        # unmap the category ids for COCO
        if hasattr(self._metadata, "thing_dataset_id_to_contiguous_id"):
            reverse_id_mapping = {
                v: k for k, v in self._metadata.thing_dataset_id_to_contiguous_id.items()
            }
            for result in self._coco_results:
                category_id = result["category_id"]
                assert (
                    category_id in reverse_id_mapping
                ), "A prediction has category_id={}, which is not available in the dataset.".format(
                    category_id
                )
                result["category_id"] = reverse_id_mapping[category_id]

        if self._output_dir:
            file_path = os.path.join(self._output_dir, "coco_instances_results.json")
            self._logger.info("Saving results to {}".format(file_path))
            with PathManager.open(file_path, "w") as f:
                f.write(json.dumps(self._coco_results))
                f.flush()

        if not self._do_evaluation:
            self._logger.info("Annotations are not available for evaluation.")
            return

        self._logger.info("Evaluating predictions ...")
        for task in sorted(tasks):
            coco_eval, summary = (
                _evaluate_predictions_on_coco(
                    self._coco_api, self._coco_results, task, kpt_oks_sigmas=self._kpt_oks_sigmas
                )
                if len(self._coco_results) > 0
                else None  # cocoapi does not handle empty results very well
            )
            self._logger.info("\n" + summary.getvalue())
            res = self._derive_coco_results(
                coco_eval, task, summary, class_names=self._metadata.thing_classes
            )
            self._results[task] = res

    def _eval_box_proposals(self):
        """
        Evaluate the box proposals in self._predictions.
        Fill self._results with the metrics for "box_proposals" task.
        """
        if self._output_dir:
            # Saving generated box proposals to file.
            # Predicted box_proposals are in XYXY_ABS mode.
            bbox_mode = BoxMode.XYXY_ABS.value
            ids, boxes, objectness_logits = [], [], []
            for prediction in self._predictions:
                ids.append(prediction["image_id"])
                boxes.append(prediction["proposals"].proposal_boxes.tensor.numpy())
                objectness_logits.append(prediction["proposals"].objectness_logits.numpy())

            proposal_data = {
                "boxes": boxes,
                "objectness_logits": objectness_logits,
                "ids": ids,
                "bbox_mode": bbox_mode,
            }
            with PathManager.open(os.path.join(self._output_dir, "box_proposals.pkl"), "wb") as f:
                pickle.dump(proposal_data, f)

        if not self._do_evaluation:
            self._logger.info("Annotations are not available for evaluation.")
            return

        self._logger.info("Evaluating bbox proposals ...")
        res = {}
        areas = {"all": "", "small": "s", "medium": "m", "large": "l"}
        for limit in [100, 1000]:
            for area, suffix in areas.items():
                stats = _evaluate_box_proposals(
                    self._predictions, self._coco_api, area=area, limit=limit
                )
                key = "AR{}@{:d}".format(suffix, limit)
                res[key] = float(stats["ar"].item() * 100)
        self._logger.info("Proposal metrics: \n" + create_small_table(res))
        self._results["box_proposals"] = res

    def _derive_coco_results(self, coco_eval, iou_type, summary, class_names=None):
        """
        Derive the desired score numbers from summarized COCOeval.

        Args:
            coco_eval (None or COCOEval): None represents no predictions from model.
            iou_type (str): specific evaluation task,
                optional values are: "bbox", "segm", "keypoints".
            class_names (None or list[str]): if provided, will use it to predict
                per-category AP.

        Returns:
            a dict of {metric name: score}
        """

        metrics = {
            "bbox": ["AP", "AP50", "AP75", "APs", "APm", "APl"],
            "segm": ["AP", "AP50", "AP75", "APs", "APm", "APl"],
            "keypoints": ["AP", "AP50", "AP75", "APm", "APl"],
        }[iou_type]

        if coco_eval is None:
            self._logger.warn("No predictions from the model!")
            return {metric: float("nan") for metric in metrics}

        # the standard metrics
        results = {
            metric: float(coco_eval.stats[idx] * 100 if coco_eval.stats[idx] >= 0 else "nan")
            for idx, metric in enumerate(metrics)
        }
        small_table = create_small_table(results)
        self._logger.info("Evaluation results for {}: \n".format(iou_type) + small_table)
        if not np.isfinite(sum(results.values())):
            self._logger.info("Note that some metrics cannot be computed.")

        if class_names is None:  # or len(class_names) <= 1:
            return results
        # Compute per-category AP
        # from https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L222-L252 # noqa
        precisions = coco_eval.eval["precision"]
        # precision has dims (iou, recall, cls, area range, max dets)
        assert len(class_names) == precisions.shape[2]

        results_per_category = {}
        for idx, name in enumerate(class_names):
            # area range index 0: all area ranges
            # max dets index -1: typically 100 per image
            precision = precisions[:, :, idx, 0, -1]
            precision = precision[precision > -1]
            ap = np.mean(precision) if precision.size else float("nan")
            results_per_category[name] = float(ap * 100)
            # results_per_category.append(("{}".format(name), float(ap * 100)))

        # tabulate it
        table = create_table_with_header(results_per_category, headers=["category", "AP"])
        self._logger.info("Per-category {} AP: \n".format(iou_type) + table)

        results.update({"AP-" + name: ap for name, ap in results_per_category.items()})
        if self._dump:
            dump_info_one_task = {
                "task": iou_type,
                "summary": summary.getvalue(),
                "tables": [small_table, table],
            }
            self._dump_infos.append(dump_info_one_task)
        return results


def instances_to_coco_json(instances, img_id):
    """
    Dump an "Instances" object to a COCO-format json that's used for evaluation.
    Args:
        instances (Instances):
        img_id (int): the image id
    Returns:
        list[dict]: list of json annotations in COCO format.
    """
    num_instance = len(instances)
    if num_instance == 0:
        return []

    boxes = instances.pred_boxes.tensor.numpy()
    boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)
    boxes = boxes.tolist()
    scores = instances.scores.tolist()
    classes = instances.pred_classes.tolist()

    has_mask = instances.has("pred_masks")
    if has_mask:
        # use RLE to encode the masks, because they are too large and takes memory
        # since this evaluator stores outputs of the entire dataset
        rles = [
            mask_util.encode(np.array(mask[:, :, None], order="F", dtype="uint8"))[0]
            for mask in instances.pred_masks
        ]
        for rle in rles:
            # "counts" is an array encoded by mask_util as a byte-stream. Python3's
            # json writer which always produces strings cannot serialize a bytestream
            # unless you decode it. Thankfully, utf-8 works out (which is also what
            # the pycocotools/_mask.pyx does).
            rle["counts"] = rle["counts"].decode("utf-8")

    has_keypoints = instances.has("pred_keypoints")
    if has_keypoints:
        keypoints = instances.pred_keypoints

    results = []
    for k in range(num_instance):
        result = {
            "image_id": img_id,
            "category_id": classes[k],
            "bbox": boxes[k],
            "score": scores[k],
        }
        if has_mask:
            result["segmentation"] = rles[k]
        if has_keypoints:
            # In COCO annotations,
            # keypoints coordinates are pixel indices.
            # However our predictions are floating point coordinates.
            # Therefore we subtract 0.5 to be consistent with the annotation format.
            # This is the inverse of data loading logic in `datasets/coco.py`.
            keypoints[k][:, :2] -= 0.5
            result["keypoints"] = keypoints[k].flatten().tolist()
        results.append(result)
    return results


# inspired from Detectron:
# https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L255 # noqa
def _evaluate_box_proposals(dataset_predictions, coco_api, thresholds=None, area="all", limit=None):
    """
    Evaluate detection proposal recall metrics. This function is a much
    faster alternative to the official COCO API recall evaluation code. However,
    it produces slightly different results.
    """
    # Record max overlap value for each gt box
    # Return vector of overlap values
    areas = {
        "all": 0,
        "small": 1,
        "medium": 2,
        "large": 3,
        "96-128": 4,
        "128-256": 5,
        "256-512": 6,
        "512-inf": 7,
    }
    area_ranges = [
        [0 ** 2, 1e5 ** 2],  # all
        [0 ** 2, 32 ** 2],  # small
        [32 ** 2, 96 ** 2],  # medium
        [96 ** 2, 1e5 ** 2],  # large
        [96 ** 2, 128 ** 2],  # 96-128
        [128 ** 2, 256 ** 2],  # 128-256
        [256 ** 2, 512 ** 2],  # 256-512
        [512 ** 2, 1e5 ** 2],
    ]  # 512-inf
    assert area in areas, "Unknown area range: {}".format(area)
    area_range = area_ranges[areas[area]]
    gt_overlaps = []
    num_pos = 0

    for prediction_dict in dataset_predictions:
        predictions = prediction_dict["proposals"]

        # sort predictions in descending order
        # TODO maybe remove this and make it explicit in the documentation
        inds = predictions.objectness_logits.sort(descending=True)[1]
        predictions = predictions[inds]

        ann_ids = coco_api.getAnnIds(imgIds=prediction_dict["image_id"])
        anno = coco_api.loadAnns(ann_ids)
        gt_boxes = [
            BoxMode.convert(obj["bbox"], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)
            for obj in anno
            if obj["iscrowd"] == 0
        ]
        gt_boxes = torch.as_tensor(gt_boxes).reshape(-1, 4)  # guard against no boxes
        gt_boxes = Boxes(gt_boxes)
        gt_areas = torch.as_tensor([obj["area"] for obj in anno if obj["iscrowd"] == 0])

        if len(gt_boxes) == 0 or len(predictions) == 0:
            continue

        valid_gt_inds = (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])
        gt_boxes = gt_boxes[valid_gt_inds]

        num_pos += len(gt_boxes)

        if len(gt_boxes) == 0:
            continue

        if limit is not None and len(predictions) > limit:
            predictions = predictions[:limit]

        overlaps = pairwise_iou(predictions.proposal_boxes, gt_boxes)

        _gt_overlaps = torch.zeros(len(gt_boxes))
        for j in range(min(len(predictions), len(gt_boxes))):
            # find which proposal box maximally covers each gt box
            # and get the iou amount of coverage for each gt box
            max_overlaps, argmax_overlaps = overlaps.max(dim=0)

            # find which gt box is 'best' covered (i.e. 'best' = most iou)
            gt_ovr, gt_ind = max_overlaps.max(dim=0)
            assert gt_ovr >= 0
            # find the proposal box that covers the best covered gt box
            box_ind = argmax_overlaps[gt_ind]
            # record the iou coverage of this gt box
            _gt_overlaps[j] = overlaps[box_ind, gt_ind]
            assert _gt_overlaps[j] == gt_ovr
            # mark the proposal box and the gt box as used
            overlaps[box_ind, :] = -1
            overlaps[:, gt_ind] = -1

        # append recorded iou coverage level
        gt_overlaps.append(_gt_overlaps)
    gt_overlaps = torch.cat(gt_overlaps, dim=0)
    gt_overlaps, _ = torch.sort(gt_overlaps)

    if thresholds is None:
        step = 0.05
        thresholds = torch.arange(0.5, 0.95 + 1e-5, step, dtype=torch.float32)
    recalls = torch.zeros_like(thresholds)
    # compute recall for each iou threshold
    for i, t in enumerate(thresholds):
        recalls[i] = (gt_overlaps >= t).float().sum() / float(num_pos)
    # ar = 2 * np.trapz(recalls, thresholds)
    ar = recalls.mean()
    return {
        "ar": ar,
        "recalls": recalls,
        "thresholds": thresholds,
        "gt_overlaps": gt_overlaps,
        "num_pos": num_pos,
    }


def _evaluate_predictions_on_coco(coco_gt, coco_results, iou_type, kpt_oks_sigmas=None):
    """
    Evaluate the coco results using COCOEval API.
    """
    assert len(coco_results) > 0

    if iou_type == "segm":
        coco_results = copy.deepcopy(coco_results)
        # When evaluating mask AP, if the results contain bbox, cocoapi will
        # use the box area as the area of the instance, instead of the mask area.
        # This leads to a different definition of small/medium/large.
        # We remove the bbox field to let mask AP use mask area.
        for c in coco_results:
            c.pop("bbox", None)

    coco_dt = coco_gt.loadRes(coco_results)
    coco_eval = COCOeval(coco_gt, coco_dt, iou_type)
    # Use the COCO default keypoint OKS sigmas unless overrides are specified
    if kpt_oks_sigmas:
        coco_eval.params.kpt_oks_sigmas = np.array(kpt_oks_sigmas)

    if iou_type == "keypoints":
        num_keypoints = len(coco_results[0]["keypoints"]) // 3
        assert len(coco_eval.params.kpt_oks_sigmas) == num_keypoints, (
            "[COCOEvaluator] The length of cfg.TEST.KEYPOINT_OKS_SIGMAS (default: 17) "
            "must be equal to the number of keypoints. However the prediction has {} "
            "keypoints! For more information please refer to "
            "http://cocodataset.org/#keypoints-eval.".format(num_keypoints)
        )

    coco_eval.evaluate()
    coco_eval.accumulate()

    redirect_string = io.StringIO()
    with contextlib.redirect_stdout(redirect_string):
        coco_eval.summarize()
    redirect_string.getvalue()
    return coco_eval, redirect_string


def _dump_to_markdown(extra_infos, dump_infos, md_file="README.md"):
    """
    Dump a Markdown file that records the model evaluation metrics and corresponding scores
    to the current working directory.

    Args:
        extra_infos (dict): extra_infos to dump.
        dump_infos (list[dict]): dump information for each task.
        md_file (str): markdown file path.
    """
    with open(md_file, "w") as f:
        title = extra_infos["title"]
        seed = extra_infos["seed"]
        f.write("# {}  \n\nseed: {}".format(title, seed))
        for task_info in dump_infos:
            task_name = task_info["task"]
            f.write("\n\n## Evaluation results for {}:  \n\n".format(task_name))
            f.write("```  \n" + task_info["summary"] + "```  \n")
            overall_table, detail_table = [
                table.replace("\n", "  \n") for table in task_info["tables"]
            ]
            header = "\n\n### Per-category {} AP:  \n\n".format(task_name)
            f.write(overall_table + header + detail_table + "\n")
```

### cvpods/evaluation/crowdhuman_evaluation.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import copy
import itertools
import json
import logging
import os
from collections import OrderedDict

import numpy as np
import pycocotools.mask as mask_util

import torch

from cvpods.data.datasets.coco import convert_to_coco_json
from cvpods.structures import BoxMode
from cvpods.utils import PathManager, comm, create_small_table

from .crowdhumantools import Database
from .evaluator import DatasetEvaluator
from .registry import EVALUATOR


@EVALUATOR.register()
class CrowdHumanEvaluator(DatasetEvaluator):
    """
    Evaluate object proposal, instance detection/segmentation, keypoint detection
    outputs using COCO's metrics and APIs.
    """

    def __init__(
            self,
            dataset_name,
            meta,
            cfg,
            distributed,
            output_dir=None,
            dump=False):
        """
        Args:
            dataset_name (str): name of the dataset to be evaluated.
                It must have either the following corresponding metadata:
                    "json_file": the path to the COCO format annotation
                Or it must be in cvpods's standard dataset format
                so it can be converted to COCO format automatically.
            meta (dict): dataset meta.
            cfg (dict): config instance
            distributed (True): if True, will collect results from all ranks for evaluation.
                Otherwise, will evaluate the results in the current process.
            output_dir (str): optional, an output directory to dump results.
            dump (boolean): optional, whether dump predictions to disk.
        """
        self._dump = dump
        self._tasks = self._tasks_from_config(cfg)
        self._distributed = distributed
        self._output_dir = output_dir

        self._cpu_device = torch.device("cpu")
        self._logger = logging.getLogger(__name__)

        self._metadata = meta
        if not hasattr(self._metadata, "json_file"):
            self._logger.warning(
                f"json_file was not found in MetaDataCatalog for '{dataset_name}'")

            cache_path = convert_to_coco_json(dataset_name, output_dir)
            self._metadata.json_file = cache_path

        self._kpt_oks_sigmas = cfg.TEST.KEYPOINT_OKS_SIGMAS
        # Test set json files do not contain annotations (evaluation must be
        # performed using the COCO evaluation server).

    def reset(self):
        self._predictions = []
        self._coco_results = []

    def _tasks_from_config(self, cfg):
        """
        Returns:
            tuple[str]: tasks that can be evaluated under the given configuration.
        """
        # TODO@wangfeng02: next 4 line to a func
        if self._dump:
            with open("README.md", "w") as f:
                name = cfg.OUTPUT_DIR.split("/")[-1]
                f.write("# {}  \n".format(name))

        tasks = ("bbox",)
        if cfg.MODEL.MASK_ON:
            tasks = tasks + ("segm",)
        if cfg.MODEL.KEYPOINT_ON:
            tasks = tasks + ("keypoints",)
        return tasks

    def boxes_dump(self, boxes, is_gt=False):
        result = []
        boxes = boxes.tolist()
        for box in boxes:
            if is_gt:
                box_dict = {}
                box_dict['box'] = [box[0], box[1], box[2] - box[0],
                                   box[3] - box[1]]
                box_dict['tag'] = box[-1]
                result.append(box_dict)
            else:
                box_dict = {}
                box_dict['box'] = [box[0], box[1], box[2] - box[0],
                                   box[3] - box[1]]
                box_dict['tag'] = 1
                box_dict['score'] = box[-1]
                result.append(box_dict)
        return result

    def process(self, inputs, outputs):
        """
        Args:
            inputs: the inputs to a COCO model (e.g., GeneralizedRCNN).
                It is a list of dict. Each dict corresponds to an image and
                contains keys like "height", "width", "file_name", "image_id".
            outputs: the outputs of a COCO model. It is a list of dicts with key
                "instances" that contains :class:`Instances`.
        """
        for input, output in zip(inputs, outputs):
            prediction = {"image_id": input["image_id"]}

            # TODO this is ugly
            if "instances" in output:
                instances = output["instances"].to(self._cpu_device)
                prediction["instances"] = instances_to_coco_json(
                    instances, input["image_id"])
            if "proposals" in output:
                prediction["proposals"] = output["proposals"].to(
                    self._cpu_device)

            gt_boxes = input['instances'].gt_boxes.tensor.cpu().numpy()
            gt_classes = input['instances'].gt_classes.cpu().numpy()[:, np.newaxis]
            gt_boxes = np.concatenate([gt_boxes, gt_classes], axis=1)

            pred_boxes = output['instances'].pred_boxes.tensor.cpu().numpy()
            pred_score = output['instances'].scores.cpu().numpy()[:, np.newaxis]
            pred_boxes = np.concatenate([pred_boxes, pred_score], axis=1)

            result_dict = dict(
                ID=input['image_id'],
                height=int(input['height']),
                width=int(input['width']),
                dtboxes=self.boxes_dump(pred_boxes),
                gtboxes=self.boxes_dump(gt_boxes, is_gt=True)
            )
            # rois=misc_utils.boxes_dump(rois[:, 1:], True))
            self._predictions.append(result_dict)

    def evaluate(self):
        if self._distributed:
            comm.synchronize()
            self._predictions = comm.gather(self._predictions, dst=0)
            self._predictions = list(itertools.chain(*self._predictions))

            if not comm.is_main_process():
                return {}

        if len(self._predictions) == 0:
            self._logger.warning(
                "[COCOEvaluator] Did not receive valid predictions.")
            return {}

        if self._output_dir:
            PathManager.mkdirs(self._output_dir)
            file_path = os.path.join(
                self._output_dir, "instances_predictions.pth")
            with PathManager.open(file_path, "wb") as f:
                torch.save(self._predictions, f)

        self._results = OrderedDict()
        self._eval_predictions(set(self._tasks))
        # Copy so the caller can do whatever with results
        return copy.deepcopy(self._results)

    def _eval_predictions(self, tasks):
        """
        Evaluate self._predictions on the given tasks.
        Fill self._results with the metrics of the tasks.
        """
        self._logger.info("Preparing results for CrowdHuman format ...")
        self._coco_results = self._predictions

        if self._output_dir:
            file_path = os.path.join(
                self._output_dir, "coco_instances_results.json")
            self._logger.info("Saving results to {}".format(file_path))

            with PathManager.open(file_path, "w") as f:
                for db in self._coco_results:
                    line = json.dumps(db) + '\n'
                    f.write(line)

        self._logger.info("Evaluating predictions ...")
        for task in sorted(tasks):
            coco_eval = (
                _evaluate_predictions_on_crowdhuman(
                    self._metadata.json_file, file_path)
                if len(self._coco_results) > 0
                else None  # cocoapi does not handle empty results very well
            )
            res = self._derive_coco_results(coco_eval, task)
            self._results[task] = res

    def _derive_coco_results(self, coco_eval, iou_type):
        """
        Derive the desired score numbers from summarized COCOeval.

        Args:
            coco_eval (None or COCOEval): None represents no predictions from model.
            iou_type (str):
            class_names (None or list[str]): if provided, will use it to predict
                per-category AP.

        Returns:
            a dict of {metric name: score}
        """

        metrics = ["AP", "mMR", "Recall"]

        if coco_eval is None:
            self._logger.warn(
                "No predictions from the model! Set scores to -1")
            return {metric: -1 for metric in metrics}

        # the standard metrics
        results = {metric: coco_eval[idx]
                   for idx, metric in enumerate(metrics)}
        small_table = create_small_table(results)
        self._logger.info(
            "Evaluation results for {}: \n".format(iou_type) + small_table
        )

        # if class_names is None or len(class_names) <= 1:
        return results


def instances_to_coco_json(instances, img_id):
    """
    Dump an "Instances" object to a COCO-format json that's used for evaluation.
    Args:
        instances (Instances):
        img_id (int): the image id
    Returns:
        list[dict]: list of json annotations in COCO format.
    """
    num_instance = len(instances)
    if num_instance == 0:
        return []

    boxes = instances.pred_boxes.tensor.numpy()
    boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)
    boxes = boxes.tolist()
    scores = instances.scores.tolist()
    classes = instances.pred_classes.tolist()

    has_mask = instances.has("pred_masks")
    if has_mask:
        # use RLE to encode the masks, because they are too large and takes memory
        # since this evaluator stores outputs of the entire dataset
        rles = [
            mask_util.encode(np.array(mask[:, :, None], order="F", dtype="uint8"))[0]
            for mask in instances.pred_masks
        ]
        for rle in rles:
            # "counts" is an array encoded by mask_util as a byte-stream. Python3's
            # json writer which always produces strings cannot serialize a bytestream
            # unless you decode it. Thankfully, utf-8 works out (which is also what
            # the pycocotools/_mask.pyx does).
            rle["counts"] = rle["counts"].decode("utf-8")

    has_keypoints = instances.has("pred_keypoints")
    if has_keypoints:
        keypoints = instances.pred_keypoints

    results = []
    for k in range(num_instance):
        result = {
            "image_id": img_id,
            "category_id": classes[k],
            "bbox": boxes[k],
            "height": boxes[k][3],
            "score": scores[k],
        }
        if has_mask:
            result["segmentation"] = rles[k]
        if has_keypoints:
            # In COCO annotations,
            # keypoints coordinates are pixel indices.
            # However our predictions are floating point coordinates.
            # Therefore we subtract 0.5 to be consistent with the annotation format.
            # This is the inverse of data loading logic in `datasets/coco.py`.
            keypoints[k][:, :2] -= 0.5
            result["keypoints"] = keypoints[k].flatten().tolist()
        results.append(result)
    return results


def _evaluate_predictions_on_crowdhuman(gt_path, dt_path, target_key="box", mode=0):
    """
    Evaluate the coco results using COCOEval API.
    """
    database = Database(gt_path, dt_path, target_key, None, mode)
    database.compare()
    AP, recall, _ = database.eval_AP()
    mMR, _ = database.eval_MR()
    return AP, mMR, recall
```

### cvpods/evaluation/widerface_evaluation.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved
import contextlib
import copy
import io
import itertools
import json
import logging
import os
import pickle
import sys
from collections import OrderedDict

import numpy as np
import pycocotools.mask as mask_util
from pycocotools.coco import COCO

import torch

from cvpods.data.datasets.coco import convert_to_coco_json
from cvpods.structures import Boxes, BoxMode, pairwise_iou
from cvpods.utils import PathManager, comm, create_small_table, create_table_with_header

from .evaluator import DatasetEvaluator
from .registry import EVALUATOR
# from pycocotools.cocoeval import COCOeval
from .widerfacetools import COCOeval


@EVALUATOR.register()
class WiderFaceEvaluator(DatasetEvaluator):
    """
    Evaluate object proposal, instance detection/segmentation, keypoint detection
    outputs using COCO's metrics and APIs.
    """

    def __init__(self, dataset_name, meta, cfg, distributed, output_dir=None, dump=False):
        """
        Args:
            dataset_name (str): name of the dataset to be evaluated.
                It must have either the following corresponding metadata:

                    "json_file": the path to the COCO format annotation

                Or it must be in cvpods's standard dataset format
                so it can be converted to COCO format automatically.
            cfg (config dict): cvpods Config instance.
            distributed (True): if True, will collect results from all ranks for evaluation.
                Otherwise, will evaluate the results in the current process.
            output_dir (str): optional, an output directory to dump results.
            dump (bool): If True, after the evaluation is completed, a Markdown file
                that records the model evaluation metrics and corresponding scores
                will be generated in the working directory.
        """
        self._dump = dump
        self._tasks = self._tasks_from_config(cfg)
        self._distributed = distributed
        self._output_dir = output_dir

        self._cpu_device = torch.device("cpu")
        self._logger = logging.getLogger(__name__)

        self._metadata = meta
        if not hasattr(self._metadata, "json_file"):
            self._logger.warning(f"json_file was not found in MetaDataCatalog for '{dataset_name}'")

            cache_path = convert_to_coco_json(dataset_name, output_dir)
            self._metadata.json_file = cache_path

        json_file = PathManager.get_local_path(self._metadata.json_file)
        with contextlib.redirect_stdout(io.StringIO()):
            self._coco_api = COCO(json_file)

        self._kpt_oks_sigmas = cfg.TEST.KEYPOINT_OKS_SIGMAS
        # Test set json files do not contain annotations (evaluation must be
        # performed using the COCO evaluation server).
        self._do_evaluation = "annotations" in self._coco_api.dataset

    def reset(self):
        self._predictions = []
        self._coco_results = []
        self._dump_infos = []  # per task

    def _tasks_from_config(self, cfg):
        """
        Returns:
            tuple[str]: tasks that can be evaluated under the given configuration.
        """
        tasks = ("bbox",)
        if cfg.MODEL.MASK_ON:
            tasks = tasks + ("segm",)
        if cfg.MODEL.KEYPOINT_ON:
            tasks = tasks + ("keypoints",)
        return tasks

    def process(self, inputs, outputs):
        """
        Args:
            inputs: the inputs to a COCO model (e.g., GeneralizedRCNN).
                It is a list of dict. Each dict corresponds to an image and
                contains keys like "height", "width", "file_name", "image_id".
            outputs: the outputs of a COCO model. It is a list of dicts with key
                "instances" that contains :class:`Instances`.
        """
        for input, output in zip(inputs, outputs):
            prediction = {"image_id": input["image_id"]}

            # TODO this is ugly
            if "instances" in output:
                instances = output["instances"].to(self._cpu_device)
                prediction["instances"] = instances_to_coco_json(instances, input["image_id"])
            if "proposals" in output:
                prediction["proposals"] = output["proposals"].to(self._cpu_device)
            self._predictions.append(prediction)

    def evaluate(self):
        if self._distributed:
            comm.synchronize()
            self._predictions = comm.gather(self._predictions, dst=0)
            self._predictions = list(itertools.chain(*self._predictions))

            if not comm.is_main_process():
                return {}

        if len(self._predictions) == 0:
            self._logger.warning("[WiderFaceEvaluator] Did not receive valid predictions.")
            return {}

        if self._output_dir:
            PathManager.mkdirs(self._output_dir)
            file_path = os.path.join(self._output_dir, "instances_predictions.pth")
            with PathManager.open(file_path, "wb") as f:
                torch.save(self._predictions, f)

        self._results = OrderedDict()
        if "proposals" in self._predictions[0]:
            self._eval_box_proposals()
        if "instances" in self._predictions[0]:
            self._eval_predictions(set(self._tasks))

        if self._dump:
            _dump_to_markdown(self._dump_infos)

        # Copy so the caller can do whatever with results
        return copy.deepcopy(self._results)

    def _eval_predictions(self, tasks):
        """
        Evaluate self._predictions on the given tasks.
        Fill self._results with the metrics of the tasks.
        """
        self._logger.info("Preparing results for COCO format ...")
        self._coco_results = list(itertools.chain(*[x["instances"] for x in self._predictions]))

        # unmap the category ids for COCO
        if hasattr(self._metadata, "thing_dataset_id_to_contiguous_id"):
            reverse_id_mapping = {
                v: k for k, v in self._metadata.thing_dataset_id_to_contiguous_id.items()
            }
            for result in self._coco_results:
                category_id = result["category_id"]
                assert (
                    category_id in reverse_id_mapping
                ), "A prediction has category_id={}, which is not available in the dataset.".format(
                    category_id
                )
                result["category_id"] = reverse_id_mapping[category_id]

        if self._output_dir:
            file_path = os.path.join(self._output_dir, "coco_instances_results.json")
            self._logger.info("Saving results to {}".format(file_path))
            with PathManager.open(file_path, "w") as f:
                f.write(json.dumps(self._coco_results))
                f.flush()

        if not self._do_evaluation:
            self._logger.info("Annotations are not available for evaluation.")
            return

        self._logger.info("Evaluating predictions ...")
        for task in sorted(tasks):
            coco_eval = (
                _evaluate_predictions_on_coco(
                    self._coco_api, self._coco_results, task, kpt_oks_sigmas=self._kpt_oks_sigmas
                )
                if len(self._coco_results) > 0
                else None  # cocoapi does not handle empty results very well
            )
            res = self._derive_coco_results(
                coco_eval, task, class_names=self._metadata.thing_classes
            )
            self._results[task] = res

    def _eval_box_proposals(self):
        """
        Evaluate the box proposals in self._predictions.
        Fill self._results with the metrics for "box_proposals" task.
        """
        if self._output_dir:
            # Saving generated box proposals to file.
            # Predicted box_proposals are in XYXY_ABS mode.
            bbox_mode = BoxMode.XYXY_ABS.value
            ids, boxes, objectness_logits = [], [], []
            for prediction in self._predictions:
                ids.append(prediction["image_id"])
                boxes.append(prediction["proposals"].proposal_boxes.tensor.numpy())
                objectness_logits.append(prediction["proposals"].objectness_logits.numpy())

            proposal_data = {
                "boxes": boxes,
                "objectness_logits": objectness_logits,
                "ids": ids,
                "bbox_mode": bbox_mode,
            }
            with PathManager.open(os.path.join(self._output_dir, "box_proposals.pkl"), "wb") as f:
                pickle.dump(proposal_data, f)

        if not self._do_evaluation:
            self._logger.info("Annotations are not available for evaluation.")
            return

        self._logger.info("Evaluating bbox proposals ...")
        res = {}
        areas = {"all": "", "small": "s", "medium": "m", "large": "l"}
        for limit in [100, 1000]:
            for area, suffix in areas.items():
                stats = _evaluate_box_proposals(
                    self._predictions, self._coco_api, area=area, limit=limit
                )
                key = "AR{}@{:d}".format(suffix, limit)
                res[key] = float(stats["ar"].item() * 100)
        self._logger.info("Proposal metrics: \n" + create_small_table(res))
        self._results["box_proposals"] = res

    def _derive_coco_results(self, coco_eval, iou_type, class_names=None):
        """
        Derive the desired score numbers from summarized COCOeval.

        Args:
            coco_eval (None or COCOEval): None represents no predictions from model.
            iou_type (str):
            class_names (None or list[str]): if provided, will use it to predict
                per-category AP.

        Returns:
            a dict of {metric name: score}
        """

        metrics = {
            "bbox": ["AP", "AP50", "AP75", "APs", "APm", "APl"],
            "segm": ["AP", "AP50", "AP75", "APs", "APm", "APl"],
            "keypoints": ["AP", "AP50", "AP75", "APm", "APl"],
        }[iou_type]

        if coco_eval is None:
            self._logger.warn("No predictions from the model! Set scores to -1")
            return {metric: -1 for metric in metrics}

        # the standard metrics
        results = {metric: float(coco_eval.stats[idx] * 100) for idx, metric in enumerate(metrics)}
        small_table = create_small_table(results)
        self._logger.info(
            "Evaluation results for {}: \n".format(iou_type) + small_table
        )

        if class_names is None:  # or len(class_names) <= 1:
            return results
        # Compute per-category AP
        # from https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L222-L252 # noqa
        precisions = coco_eval.eval["precision"]
        # precision has dims (iou, recall, cls, area range, max dets)
        assert len(class_names) == precisions.shape[2]

        results_per_category = {}
        for idx, name in enumerate(class_names):
            # area range index 0: all area ranges
            # max dets index -1: typically 100 per image
            precision = precisions[:, :, idx, 0, -1]
            precision = precision[precision > -1]
            ap = np.mean(precision) if precision.size else float("nan")
            results_per_category[name] = float(ap * 100)
            # results_per_category.append(("{}".format(name), float(ap * 100)))

        # tabulate it
        table = create_table_with_header(results_per_category, headers=["category", "AP"])
        self._logger.info("Per-category {} AP: \n".format(iou_type) + table)

        results.update({"AP-" + name: ap for name, ap in results_per_category.items()})

        if self._dump:
            dump_info_one_task = {
                "task": iou_type,
                "coco_eval": coco_eval,
                "tables": [small_table, table],
            }
            self._dump_infos.append(dump_info_one_task)
        return results


def instances_to_coco_json(instances, img_id):
    """
    Dump an "Instances" object to a COCO-format json that's used for evaluation.
    Args:
        instances (Instances):
        img_id (int): the image id
    Returns:
        list[dict]: list of json annotations in COCO format.
    """
    num_instance = len(instances)
    if num_instance == 0:
        return []

    boxes = instances.pred_boxes.tensor.numpy()
    boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)
    boxes = boxes.tolist()
    scores = instances.scores.tolist()
    classes = instances.pred_classes.tolist()

    has_mask = instances.has("pred_masks")
    if has_mask:
        # use RLE to encode the masks, because they are too large and takes memory
        # since this evaluator stores outputs of the entire dataset
        rles = [
            mask_util.encode(np.array(mask[:, :, None], order="F", dtype="uint8"))[0]
            for mask in instances.pred_masks
        ]
        for rle in rles:
            # "counts" is an array encoded by mask_util as a byte-stream. Python3's
            # json writer which always produces strings cannot serialize a bytestream
            # unless you decode it. Thankfully, utf-8 works out (which is also what
            # the pycocotools/_mask.pyx does).
            rle["counts"] = rle["counts"].decode("utf-8")

    has_keypoints = instances.has("pred_keypoints")
    if has_keypoints:
        keypoints = instances.pred_keypoints

    results = []
    for k in range(num_instance):
        result = {
            "image_id": img_id,
            "category_id": classes[k],
            "bbox": boxes[k],
            "height": boxes[k][3],
            "score": scores[k],
        }
        if has_mask:
            result["segmentation"] = rles[k]
        if has_keypoints:
            # In COCO annotations,
            # keypoints coordinates are pixel indices.
            # However our predictions are floating point coordinates.
            # Therefore we subtract 0.5 to be consistent with the annotation format.
            # This is the inverse of data loading logic in `datasets/coco.py`.
            keypoints[k][:, :2] -= 0.5
            result["keypoints"] = keypoints[k].flatten().tolist()
        results.append(result)
    return results


# inspired from Detectron:
# https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L255 # noqa
def _evaluate_box_proposals(dataset_predictions, coco_api, thresholds=None, area="all", limit=None):
    """
    Evaluate detection proposal recall metrics. This function is a much
    faster alternative to the official COCO API recall evaluation code. However,
    it produces slightly different results.
    """
    # Record max overlap value for each gt box
    # Return vector of overlap values
    areas = {
        "all": 0,
        "small": 1,
        "medium": 2,
        "large": 3,
        "96-128": 4,
        "128-256": 5,
        "256-512": 6,
        "512-inf": 7,
    }
    area_ranges = [
        [0 ** 2, 1e5 ** 2],  # all
        [0 ** 2, 32 ** 2],  # small
        [32 ** 2, 96 ** 2],  # medium
        [96 ** 2, 1e5 ** 2],  # large
        [96 ** 2, 128 ** 2],  # 96-128
        [128 ** 2, 256 ** 2],  # 128-256
        [256 ** 2, 512 ** 2],  # 256-512
        [512 ** 2, 1e5 ** 2],
    ]  # 512-inf
    assert area in areas, "Unknown area range: {}".format(area)
    area_range = area_ranges[areas[area]]
    gt_overlaps = []
    num_pos = 0

    for prediction_dict in dataset_predictions:
        predictions = prediction_dict["proposals"]

        # sort predictions in descending order
        # TODO maybe remove this and make it explicit in the documentation
        inds = predictions.objectness_logits.sort(descending=True)[1]
        predictions = predictions[inds]

        ann_ids = coco_api.getAnnIds(imgIds=prediction_dict["image_id"])
        anno = coco_api.loadAnns(ann_ids)
        gt_boxes = [
            BoxMode.convert(obj["bbox"], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)
            for obj in anno
            if obj["iscrowd"] == 0
        ]
        gt_boxes = torch.as_tensor(gt_boxes).reshape(-1, 4)  # guard against no boxes
        gt_boxes = Boxes(gt_boxes)
        gt_areas = torch.as_tensor([obj["area"] for obj in anno if obj["iscrowd"] == 0])

        if len(gt_boxes) == 0 or len(predictions) == 0:
            continue

        valid_gt_inds = (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])
        gt_boxes = gt_boxes[valid_gt_inds]

        num_pos += len(gt_boxes)

        if len(gt_boxes) == 0:
            continue

        if limit is not None and len(predictions) > limit:
            predictions = predictions[:limit]

        overlaps = pairwise_iou(predictions.proposal_boxes, gt_boxes)

        _gt_overlaps = torch.zeros(len(gt_boxes))
        for j in range(min(len(predictions), len(gt_boxes))):
            # find which proposal box maximally covers each gt box
            # and get the iou amount of coverage for each gt box
            max_overlaps, argmax_overlaps = overlaps.max(dim=0)

            # find which gt box is 'best' covered (i.e. 'best' = most iou)
            gt_ovr, gt_ind = max_overlaps.max(dim=0)
            assert gt_ovr >= 0
            # find the proposal box that covers the best covered gt box
            box_ind = argmax_overlaps[gt_ind]
            # record the iou coverage of this gt box
            _gt_overlaps[j] = overlaps[box_ind, gt_ind]
            assert _gt_overlaps[j] == gt_ovr
            # mark the proposal box and the gt box as used
            overlaps[box_ind, :] = -1
            overlaps[:, gt_ind] = -1

        # append recorded iou coverage level
        gt_overlaps.append(_gt_overlaps)
    gt_overlaps = torch.cat(gt_overlaps, dim=0)
    gt_overlaps, _ = torch.sort(gt_overlaps)

    if thresholds is None:
        step = 0.05
        thresholds = torch.arange(0.5, 0.95 + 1e-5, step, dtype=torch.float32)
    recalls = torch.zeros_like(thresholds)
    # compute recall for each iou threshold
    for i, t in enumerate(thresholds):
        recalls[i] = (gt_overlaps >= t).float().sum() / float(num_pos)
    # ar = 2 * np.trapz(recalls, thresholds)
    ar = recalls.mean()
    return {
        "ar": ar,
        "recalls": recalls,
        "thresholds": thresholds,
        "gt_overlaps": gt_overlaps,
        "num_pos": num_pos,
    }


def _evaluate_predictions_on_coco(coco_gt, coco_results, iou_type, kpt_oks_sigmas=None):
    """
    Evaluate the coco results using COCOEval API.
    """
    assert len(coco_results) > 0

    if iou_type == "segm":
        coco_results = copy.deepcopy(coco_results)
        # When evaluating mask AP, if the results contain bbox, cocoapi will
        # use the box area as the area of the instance, instead of the mask area.
        # This leads to a different definition of small/medium/large.
        # We remove the bbox field to let mask AP use mask area.
        for c in coco_results:
            c.pop("bbox", None)

    coco_dt = coco_gt.loadRes(coco_results)
    coco_eval = COCOeval(coco_gt, coco_dt, iou_type)
    # Use the COCO default keypoint OKS sigmas unless overrides are specified
    if kpt_oks_sigmas:
        coco_eval.params.kpt_oks_sigmas = np.array(kpt_oks_sigmas)

    if iou_type == "keypoints":
        num_keypoints = len(coco_results[0]["keypoints"]) // 3
        assert len(coco_eval.params.kpt_oks_sigmas) == num_keypoints, (
            "[WiderFaceEvaluator] The length of cfg.TEST.KEYPOINT_OKS_SIGMAS (default: 17) "
            "must be equal to the number of keypoints. However the prediction has {} "
            "keypoints! For more information please refer to "
            "http://cocodataset.org/#keypoints-eval.".format(num_keypoints)
        )

    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()
    return coco_eval


def _dump_to_markdown(dump_infos, md_file="README.md"):
    """
    Dump a Markdown file that records the model evaluation metrics and corresponding scores
    to the current working directory.

    Args:
        dump_infos (list[dict]): dump information for each task.
        md_file (str): markdown file path.
    """
    title = os.getcwd().split("/")[-1]
    with open(md_file, "w") as f:
        f.write("# {}  ".format(title))
        for dump_info_per_task in dump_infos:
            task_name = dump_info_per_task["task"]
            coco_eval = dump_info_per_task["coco_eval"]
            tables = dump_info_per_task["tables"]
            tables = [table.replace("\n", "  \n") for table in tables]
            f.write("\n\n## Evaluation results for {}:  \n\n".format(task_name))
            terminal = sys.stdout
            f.write("```  \n")
            sys.stdout = f
            coco_eval.summarize()
            sys.stdout.flush()
            sys.stdout = terminal
            f.write("```  \n")
            f.write(tables[0])
            f.write("\n\n### Per-category {} AP:  \n\n".format(task_name))
            f.write(tables[1])
            f.write("\n")
```

### cvpods/evaluation/sem_seg_evaluation.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import itertools
import json
import logging
import os
from collections import OrderedDict

import numpy as np
import PIL.Image as Image
import pycocotools.mask as mask_util

import torch

from cvpods.utils import PathManager, comm, create_small_table

from .evaluator import DatasetEvaluator
from .registry import EVALUATOR


@EVALUATOR.register()
class SemSegEvaluator(DatasetEvaluator):
    """
    Evaluate semantic segmentation results.
    """

    def __init__(self,
                 dataset_name,
                 dataset,
                 distributed,
                 num_classes,
                 ignore_label=255,
                 output_dir=None,
                 dump=False):
        """
        Args:
            dataset_name (str): name of the dataset to be evaluated.
            dataset (Dataset): the dataset used for evaluation.
            distributed (True): if True, will collect results from all ranks for evaluation.
                Otherwise, will evaluate the results in the current process.
            num_classes (int): number of classes.
            ignore_label (int): value in semantic segmentation ground truth. Predictions for the
                corresponding pixels should be ignored.
            output_dir (str): an output directory to dump results.
            dump (bool): If True, after the evaluation is completed, a Markdown file
                that records the model evaluation metrics and corresponding scores
                will be generated in the working directory.
        """
        self._dump = dump
        self._dataset_name = dataset_name
        self._distributed = distributed
        self._output_dir = output_dir
        self._num_classes = num_classes
        self._ignore_label = ignore_label
        self._N = num_classes + 1

        self._cpu_device = torch.device("cpu")
        self._logger = logging.getLogger(__name__)

        dataset_dicts = []
        if hasattr(dataset, "datasets"):
            for d in dataset.datasets:
                dataset_dicts += d.dataset_dicts
        else:
            dataset_dicts = dataset.dataset_dicts
        self.input_file_to_gt_file = {
            dataset_record["file_name"]: dataset_record["sem_seg_file_name"]
            for dataset_record in dataset_dicts
        }

        # Dict that maps contiguous training ids to COCO category ids
        meta = dataset.meta
        try:
            c2d = meta.stuff_dataset_id_to_contiguous_id
            self._contiguous_id_to_dataset_id = {v: k for k, v in c2d.items()}
        except AttributeError:
            self._contiguous_id_to_dataset_id = None

    def reset(self):
        self._conf_matrix = np.zeros((self._N, self._N), dtype=np.int64)
        self._predictions = []

    def process(self, inputs, outputs):
        """
        Args:
            inputs: the inputs to a model.
                It is a list of dicts. Each dict corresponds to an image and
                contains keys like "height", "width", "file_name".
            outputs: the outputs of a model. It is either list of semantic segmentation predictions
                (Tensor [H, W]) or list of dicts with key "sem_seg" that contains semantic
                segmentation prediction in the same format.
        """
        for input, output in zip(inputs, outputs):
            output = output["sem_seg"].argmax(dim=0).to(self._cpu_device)
            pred = np.array(output, dtype=np.int)
            with PathManager.open(
                    self.input_file_to_gt_file[input["file_name"]], "rb") as f:
                gt = np.array(Image.open(f), dtype=np.int)

            gt[gt == self._ignore_label] = self._num_classes

            self._conf_matrix += np.bincount(
                self._N * pred.reshape(-1) + gt.reshape(-1),
                minlength=self._N**2).reshape(self._N, self._N)

            self._predictions.extend(
                self.encode_json_sem_seg(pred, input["file_name"]))

    def evaluate(self):
        """
        Evaluates standard semantic segmentation metrics (http://cocodataset.org/#stuff-eval):

        * Mean intersection-over-union averaged across classes (mIoU)
        * Frequency Weighted IoU (fwIoU)
        * Mean pixel accuracy averaged across classes (mACC)
        * Pixel Accuracy (pACC)
        """
        if self._distributed:
            comm.synchronize()
            conf_matrix_list = comm.all_gather(self._conf_matrix)
            self._predictions = comm.all_gather(self._predictions)
            self._predictions = list(itertools.chain(*self._predictions))
            if not comm.is_main_process():
                return

            self._conf_matrix = np.zeros_like(self._conf_matrix)
            for conf_matrix in conf_matrix_list:
                self._conf_matrix += conf_matrix

        if self._output_dir:
            PathManager.mkdirs(self._output_dir)
            file_path = os.path.join(self._output_dir,
                                     "sem_seg_predictions.json")
            with PathManager.open(file_path, "w") as f:
                f.write(json.dumps(self._predictions))

        acc = np.zeros(self._num_classes, dtype=np.float)
        iou = np.zeros(self._num_classes, dtype=np.float)
        tp = self._conf_matrix.diagonal()[:-1].astype(np.float)
        pos_gt = np.sum(self._conf_matrix[:-1, :-1], axis=0).astype(np.float)
        class_weights = pos_gt / np.sum(pos_gt)
        pos_pred = np.sum(self._conf_matrix[:-1, :-1], axis=1).astype(np.float)
        acc_valid = pos_gt > 0
        acc[acc_valid] = tp[acc_valid] / pos_gt[acc_valid]
        iou_valid = (pos_gt + pos_pred) > 0
        union = pos_gt + pos_pred - tp
        iou[acc_valid] = tp[acc_valid] / union[acc_valid]
        macc = np.sum(acc) / np.sum(acc_valid)
        miou = np.sum(iou) / np.sum(iou_valid)
        fiou = np.sum(iou * class_weights)
        pacc = np.sum(tp) / np.sum(pos_gt)

        res = {}
        res["mIoU"] = 100 * miou
        res["fwIoU"] = 100 * fiou
        res["mACC"] = 100 * macc
        res["pACC"] = 100 * pacc

        if self._output_dir:
            file_path = os.path.join(self._output_dir,
                                     "sem_seg_evaluation.pth")
            with PathManager.open(file_path, "wb") as f:
                torch.save(res, f)
        results = OrderedDict({"sem_seg": res})

        small_table = create_small_table(res)
        self._logger.info("Evaluation results for sem_seg: \n" + small_table)

        if self._dump:
            dump_info_one_task = {
                "task": "sem_seg",
                "tables": [small_table],
            }
            _dump_to_markdown([dump_info_one_task])

        return results

    def encode_json_sem_seg(self, sem_seg, input_file_name):
        """
        Convert semantic segmentation to COCO stuff format with segments encoded as RLEs.
        See http://cocodataset.org/#format-results
        """
        json_list = []
        for label in np.unique(sem_seg):
            if self._contiguous_id_to_dataset_id is not None:
                assert (
                    label in self._contiguous_id_to_dataset_id
                ), "Label {} is not in the metadata info for {}".format(
                    label, self._dataset_name)
                dataset_id = self._contiguous_id_to_dataset_id[label]
            else:
                dataset_id = int(label)
            mask = (sem_seg == label).astype(np.uint8)
            mask_rle = mask_util.encode(np.array(mask[:, :, None],
                                                 order="F"))[0]
            mask_rle["counts"] = mask_rle["counts"].decode("utf-8")
            json_list.append({
                "file_name": input_file_name,
                "category_id": dataset_id,
                "segmentation": mask_rle
            })
        return json_list


def _dump_to_markdown(dump_infos, md_file="README.md"):
    """
    Dump a Markdown file that records the model evaluation metrics and corresponding scores
    to the current working directory.

    Args:
        dump_infos (list[dict]): dump information for each task.
        md_file (str): markdown file path.
    """
    title = os.getcwd().split("/")[-1]
    with open(md_file, "w") as f:
        f.write("# {}  ".format(title))
        for dump_info_per_task in dump_infos:
            task_name = dump_info_per_task["task"]
            tables = dump_info_per_task["tables"]
            tables = [table.replace("\n", "  \n") for table in tables]
            f.write("\n\n## Evaluation results for {}:  \n\n".format(task_name))
            f.write(tables[0])
            f.write("\n")
```

### cvpods/evaluation/widerfacetools.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved
__author__ = 'tsungyi'

import copy
import datetime
import time
from collections import defaultdict

import numpy as np
from pycocotools import mask as maskUtils


class COCOeval:
    # Interface for evaluating detection on the Microsoft COCO dataset.
    #
    # The usage for CocoEval is as follows:
    #  cocoGt=..., cocoDt=...       # load dataset and results
    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object
    #  E.params.recThrs = ...;      # set parameters as desired
    #  E.evaluate();                # run per image evaluation
    #  E.accumulate();              # accumulate per image results
    #  E.summarize();               # display summary metrics of results
    # For example usage see evalDemo.m and http://mscoco.org/.
    #
    # The evaluation parameters are as follows (defaults in brackets):
    #  imgIds     - [all] N img ids to use for evaluation
    #  catIds     - [all] K cat ids to use for evaluation
    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation
    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation
    #  HtRng    - [...] A=4 object height ranges for evaluation
    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image
    #  iouType    - ['segm'] set iouType to 'segm', 'bbox' or 'keypoints'
    #  iouType replaced the now DEPRECATED useSegm parameter.
    #  useCats    - [1] if true use category labels for evaluation
    # Note: if useCats=0 category labels are ignored as in proposal scoring.
    # Note: multiple HtRngs [Ax2] and maxDets [Mx1] can be specified.
    #
    # evaluate(): evaluates detections on every image and every category and
    # concats the results into the "evalImgs" with fields:
    #  dtIds      - [1xD] id for each of the D detections (dt)
    #  gtIds      - [1xG] id for each of the G ground truths (gt)
    #  dtMatches  - [TxD] matching gt id at each IoU or 0
    #  gtMatches  - [TxG] matching dt id at each IoU or 0
    #  dtScores   - [1xD] confidence of each dt
    #  gtIgnore   - [1xG] ignore flag for each gt
    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU
    #
    # accumulate(): accumulates the per-image, per-category evaluation
    # results in "evalImgs" into the dictionary "eval" with fields:
    #  params     - parameters used for evaluation
    #  date       - date evaluation was performed
    #  counts     - [T,R,K,A,M] parameter dimensions (see above)
    #  precision  - [TxRxKxAxM] precision for every evaluation setting
    #  recall     - [TxKxAxM] max recall for every evaluation setting
    # Note: precision and recall==-1 for settings with no gt objects.
    #
    # See also coco, mask, pycocoDemo, pycocoEvalDemo
    #
    # Microsoft COCO Toolbox.      version 2.0
    # Data, paper, and tutorials available at:  http://mscoco.org/
    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.
    # Licensed under the Simplified BSD License [see coco/license.txt]
    def __init__(self, cocoGt=None, cocoDt=None, iouType='segm'):
        '''
        Initialize CocoEval using coco APIs for gt and dt
        :param cocoGt: coco object with ground truth annotations
        :param cocoDt: coco object with detection results
        :return: None
        '''
        if not iouType:
            print('iouType not specified. use default iouType segm')
        self.cocoGt = cocoGt    # ground truth COCO API
        self.cocoDt = cocoDt    # detections COCO API
        self.evalImgs = defaultdict(
            list
        )    # per-image per-category evaluation results [KxAxI] elements
        self.eval = {}    # accumulated evaluation results
        self._gts = defaultdict(list)    # gt for evaluation
        self._dts = defaultdict(list)    # dt for evaluation
        self.params = Params(iouType=iouType)    # parameters
        self._paramsEval = {}    # parameters for evaluation
        self.stats = []    # result summarization
        self.ious = {}    # ious between all gts and dts
        if cocoGt is not None:
            self.params.imgIds = sorted(cocoGt.getImgIds())
            self.params.catIds = sorted(cocoGt.getCatIds())

    def _prepare(self):
        '''
        Prepare ._gts and ._dts for evaluation based on params
        :return: None
        '''
        def _toMask(anns, coco):
            # modify ann['segmentation'] by reference
            for ann in anns:
                rle = coco.annToRLE(ann)
                ann['segmentation'] = rle

        p = self.params
        if p.useCats:
            gts = self.cocoGt.loadAnns(
                self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))
            dts = self.cocoDt.loadAnns(
                self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))
        else:
            gts = self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))
            dts = self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))

        # convert ground truth to mask if iouType == 'segm'
        if p.iouType == 'segm':
            _toMask(gts, self.cocoGt)
            _toMask(dts, self.cocoDt)
        # set ignore flag
        for gt in gts:
            gt['ignore'] = gt['ignore'] if 'ignore' in gt else 0
            gt['ignore'] = 'iscrowd' in gt and gt['iscrowd']
            if p.iouType == 'keypoints':
                gt['ignore'] = (gt['num_keypoints'] == 0) or gt['ignore']
        self._gts = defaultdict(list)    # gt for evaluation
        self._dts = defaultdict(list)    # dt for evaluation
        for gt in gts:
            self._gts[gt['image_id'], gt['category_id']].append(gt)
        for dt in dts:
            self._dts[dt['image_id'], dt['category_id']].append(dt)
        self.evalImgs = defaultdict(
            list)    # per-image per-category evaluation results
        self.eval = {}    # accumulated evaluation results

    def evaluate(self):
        '''
        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs
        :return: None
        '''
        tic = time.time()
        print('Running per image evaluation...')
        p = self.params
        # add backward compatibility if useSegm is specified in params
        if p.useSegm is not None:
            p.iouType = 'segm' if p.useSegm == 1 else 'bbox'
            print('useSegm (deprecated) is not None. Running {} evaluation'.
                  format(p.iouType))
        print('Evaluate annotation type *{}*'.format(p.iouType))
        p.imgIds = list(np.unique(p.imgIds))
        if p.useCats:
            p.catIds = list(np.unique(p.catIds))
        p.maxDets = sorted(p.maxDets)
        self.params = p

        self._prepare()
        # loop through images, height range, max detection number
        catIds = p.catIds if p.useCats else [-1]

        if p.iouType == 'segm' or p.iouType == 'bbox':
            computeIoU = self.computeIoU
        elif p.iouType == 'keypoints':
            computeIoU = self.computeOks
        self.ious = {(imgId, catId): computeIoU(imgId, catId) for imgId in p.imgIds
                     for catId in catIds}

        evaluateImg = self.evaluateImg
        maxDet = p.maxDets[-1]
        self.evalImgs = [
            evaluateImg(imgId, catId, HtRng, maxDet) for catId in catIds
            for HtRng in p.HtRng for imgId in p.imgIds
        ]
        self._paramsEval = copy.deepcopy(self.params)
        toc = time.time()
        print('DONE (t={:0.2f}s).'.format(toc - tic))

    def computeIoU(self, imgId, catId):
        p = self.params
        if p.useCats:
            gt = self._gts[imgId, catId]
            dt = self._dts[imgId, catId]
        else:
            gt = [_ for cId in p.catIds for _ in self._gts[imgId, cId]]
            dt = [_ for cId in p.catIds for _ in self._dts[imgId, cId]]
        if len(gt) == 0 and len(dt) == 0:
            return []
        inds = np.argsort([-d['score'] for d in dt], kind='mergesort')
        dt = [dt[i] for i in inds]
        if len(dt) > p.maxDets[-1]:
            dt = dt[0:p.maxDets[-1]]

        if p.iouType == 'segm':
            g = [g['segmentation'] for g in gt]
            d = [d['segmentation'] for d in dt]
        elif p.iouType == 'bbox':
            g = [g['bbox'] for g in gt]
            d = [d['bbox'] for d in dt]
        else:
            raise Exception('unknown iouType for iou computation')

        # compute iou between each dt and gt region
        iscrowd = [int(o['iscrowd']) for o in gt]
        ious = maskUtils.iou(d, g, iscrowd)
        return ious

    def computeOks(self, imgId, catId):
        p = self.params
        # dimention here should be Nxm
        gts = self._gts[imgId, catId]
        dts = self._dts[imgId, catId]
        inds = np.argsort([-d['score'] for d in dts], kind='mergesort')
        dts = [dts[i] for i in inds]
        if len(dts) > p.maxDets[-1]:
            dts = dts[0:p.maxDets[-1]]
        # if len(gts) == 0 and len(dts) == 0:
        if len(gts) == 0 or len(dts) == 0:
            return []
        ious = np.zeros((len(dts), len(gts)))
        sigmas = p.kpt_oks_sigmas
        vars = (sigmas * 2)**2
        k = len(sigmas)
        # compute oks between each detection and ground truth object
        for j, gt in enumerate(gts):
            # create bounds for ignore regions(double the gt bbox)
            g = np.array(gt['keypoints'])
            xg = g[0::3]
            yg = g[1::3]
            vg = g[2::3]
            k1 = np.count_nonzero(vg > 0)
            bb = gt['bbox']
            x0 = bb[0] - bb[2]
            x1 = bb[0] + bb[2] * 2
            y0 = bb[1] - bb[3]
            y1 = bb[1] + bb[3] * 2
            for i, dt in enumerate(dts):
                d = np.array(dt['keypoints'])
                xd = d[0::3]
                yd = d[1::3]
                if k1 > 0:
                    # measure the per-keypoint distance if keypoints visible
                    dx = xd - xg
                    dy = yd - yg
                else:
                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)
                    z = np.zeros((k))
                    dx = np.max((z, x0 - xd), axis=0) + np.max(
                        (z, xd - x1), axis=0)
                    dy = np.max((z, y0 - yd), axis=0) + np.max(
                        (z, yd - y1), axis=0)
                e = (dx**2 + dy**2) / vars / (gt['area'] + np.spacing(1)) / 2
                if k1 > 0:
                    e = e[vg > 0]
                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]
        return ious

    def evaluateImg(self, imgId, catId, aRng, maxDet):
        '''
        perform evaluation for single category and image
        :return: dict (single image results)
        '''
        p = self.params
        if p.useCats:
            gt = self._gts[imgId, catId]
            dt = self._dts[imgId, catId]
        else:
            gt = [_ for cId in p.catIds for _ in self._gts[imgId, cId]]
            dt = [_ for cId in p.catIds for _ in self._dts[imgId, cId]]
        if len(gt) == 0 and len(dt) == 0:
            return None

        for g in gt:
            box_height = g['bbox'][2]
            if g['ignore'] or (box_height < aRng[0] or box_height > aRng[1]):
                g['_ignore'] = 1
            else:
                g['_ignore'] = 0

        # sort dt highest score first, sort gt ignore last
        gtind = np.argsort([g['_ignore'] for g in gt], kind='mergesort')
        gt = [gt[i] for i in gtind]
        dtind = np.argsort([-d['score'] for d in dt], kind='mergesort')
        dt = [dt[i] for i in dtind[0:maxDet]]
        iscrowd = [int(o['iscrowd']) for o in gt]
        # load computed ious
        ious = self.ious[imgId, catId][:, gtind] if len(
            self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]

        T = len(p.iouThrs)
        G = len(gt)
        D = len(dt)
        gtm = np.zeros((T, G))
        dtm = np.zeros((T, D))
        gtIg = np.array([g['_ignore'] for g in gt])
        dtIg = np.zeros((T, D))
        if not len(ious) == 0:
            for tind, t in enumerate(p.iouThrs):
                for dind, d in enumerate(dt):
                    # information about best match so far (m=-1 -> unmatched)
                    iou = min([t, 1 - 1e-10])
                    m = -1
                    for gind, g in enumerate(gt):
                        # if this gt already matched, and not a crowd, continue
                        if gtm[tind, gind] > 0 and not iscrowd[gind]:
                            continue
                        # if dt matched to reg gt, and on ignore gt, stop
                        if m > -1 and gtIg[m] == 0 and gtIg[gind] == 1:
                            break
                        # continue to next gt unless better match made
                        if ious[dind, gind] < iou:
                            continue
                        # if match successful and best so far, store appropriately
                        iou = ious[dind, gind]
                        m = gind
                    # if match made store id of match for both dt and gt
                    if m == -1:
                        continue
                    dtIg[tind, dind] = gtIg[m]
                    dtm[tind, dind] = gt[m]['id']
                    gtm[tind, m] = d['id']
        # set unmatched detections outside of height range to ignore
        a = np.array([
            d['height'] < aRng[0] or d['height'] > aRng[1] for d in dt
        ]).reshape((1, len(dt)))
        dtIg = np.logical_or(dtIg, np.logical_and(dtm == 0, np.repeat(a, T,
                                                                      0)))
        # store results for given image and category
        return {
            'image_id': imgId,
            'category_id': catId,
            'aRng': aRng,
            'maxDet': maxDet,
            'dtIds': [d['id'] for d in dt],
            'gtIds': [g['id'] for g in gt],
            'dtMatches': dtm,
            'gtMatches': gtm,
            'dtScores': [d['score'] for d in dt],
            'gtIgnore': gtIg,
            'dtIgnore': dtIg,
        }

    def accumulate(self, p=None):
        '''
        Accumulate per image evaluation results and store the result in self.eval
        :param p: input params for evaluation
        :return: None
        '''
        print('Accumulating evaluation results...')
        tic = time.time()
        if not self.evalImgs:
            print('Please run evaluate() first')
        # allows input customized parameters
        if p is None:
            p = self.params
        p.catIds = p.catIds if p.useCats == 1 else [-1]
        T = len(p.iouThrs)
        R = len(p.recThrs)
        K = len(p.catIds) if p.useCats else 1
        A = len(p.HtRng)
        M = len(p.maxDets)
        precision = -np.ones(
            (T, R, K, A, M))    # -1 for the precision of absent categories
        recall = -np.ones((T, K, A, M))
        scores = -np.ones((T, R, K, A, M))

        # create dictionary for future indexing
        _pe = self._paramsEval
        catIds = _pe.catIds if _pe.useCats else [-1]
        setK = set(catIds)
        setA = set(map(tuple, _pe.HtRng))
        setM = set(_pe.maxDets)
        setI = set(_pe.imgIds)
        # get inds to evaluate
        k_list = [n for n, k in enumerate(p.catIds) if k in setK]
        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]
        a_list = [
            n for n, a in enumerate(map(lambda x: tuple(x), p.HtRng))
            if a in setA
        ]
        i_list = [n for n, i in enumerate(p.imgIds) if i in setI]
        I0 = len(_pe.imgIds)
        A0 = len(_pe.HtRng)
        # retrieve E at each category, height range, and max number of detections
        for k, k0 in enumerate(k_list):
            Nk = k0 * A0 * I0
            for a, a0 in enumerate(a_list):
                Na = a0 * I0
                for m, maxDet in enumerate(m_list):
                    E = [self.evalImgs[Nk + Na + i] for i in i_list]
                    E = [e for e in E if e is not None]
                    if len(E) == 0:
                        continue
                    dtScores = np.concatenate(
                        [e['dtScores'][0:maxDet] for e in E])

                    # different sorting method generates slightly different results.
                    # mergesort is used to be consistent as Matlab implementation.
                    inds = np.argsort(-dtScores, kind='mergesort')
                    dtScoresSorted = dtScores[inds]

                    dtm = np.concatenate(
                        [e['dtMatches'][:, 0:maxDet] for e in E], axis=1
                    )[:, inds]
                    dtIg = np.concatenate(
                        [e['dtIgnore'][:, 0:maxDet] for e in E], axis=1
                    )[:, inds]
                    gtIg = np.concatenate([e['gtIgnore'] for e in E])
                    npig = np.count_nonzero(gtIg == 0)
                    if npig == 0:
                        continue
                    tps = np.logical_and(dtm, np.logical_not(dtIg))
                    fps = np.logical_and(np.logical_not(dtm),
                                         np.logical_not(dtIg))

                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)
                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)
                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):
                        tp = np.array(tp)
                        fp = np.array(fp)
                        nd = len(tp)
                        rc = tp / npig
                        pr = tp / (fp + tp + np.spacing(1))
                        q = np.zeros((R, ))
                        ss = np.zeros((R, ))

                        if nd:
                            recall[t, k, a, m] = rc[-1]
                        else:
                            recall[t, k, a, m] = 0

                        # numpy is slow without cython optimization for accessing elements
                        # use python array gets significant speed improvement
                        pr = pr.tolist()
                        q = q.tolist()

                        for i in range(nd - 1, 0, -1):
                            if pr[i] > pr[i - 1]:
                                pr[i - 1] = pr[i]

                        inds = np.searchsorted(rc, p.recThrs, side='left')
                        try:
                            for ri, pi in enumerate(inds):
                                q[ri] = pr[pi]
                                ss[ri] = dtScoresSorted[pi]
                        except:  # noqa
                            pass
                        precision[t, :, k, a, m] = np.array(q)
                        scores[t, :, k, a, m] = np.array(ss)
        self.eval = {
            'params': p,
            'counts': [T, R, K, A, M],
            'date': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'precision': precision,
            'recall': recall,
            'scores': scores,
        }
        toc = time.time()
        print('DONE (t={:0.2f}s).'.format(toc - tic))

    def summarize(self):
        '''
        Compute and display summary metrics for evaluation results.
        Note this functin can *only* be applied on the default parameter setting
        '''
        def _summarize(ap=1, iouThr=None, HtRng='all', maxDets=100):
            p = self.params
            iStr = ' {:<18} {} @[ IoU={:<9} | height={:>6s} | maxDets={:>4d} ] = {:0.3f}'
            titleStr = 'Average Precision' if ap == 1 else 'Average Recall'
            typeStr = '(AP)' if ap == 1 else '(AR)'
            iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) \
                if iouThr is None else '{:0.2f}'.format(iouThr)

            aind = [i for i, aRng in enumerate(p.HtRngLbl) if aRng == HtRng]
            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]
            if ap == 1:
                # dimension of precision: [TxRxKxAxM]
                s = self.eval['precision']
                # IoU
                if iouThr is not None:
                    t = np.where(iouThr == p.iouThrs)[0]
                    s = s[t]
                s = s[:, :, :, aind, mind]
            else:
                # dimension of recall: [TxKxAxM]
                s = self.eval['recall']
                if iouThr is not None:
                    t = np.where(iouThr == p.iouThrs)[0]
                    s = s[t]
                s = s[:, :, aind, mind]
            if len(s[s > -1]) == 0:
                mean_s = -1
            else:
                mean_s = np.mean(s[s > -1])
            print(
                iStr.format(titleStr, typeStr, iouStr, HtRng, maxDets, mean_s))
            return mean_s

        def _summarizeDets():
            stats = np.zeros((12, ))
            stats[0] = _summarize(1, maxDets=self.params.maxDets[2])
            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])
            stats[2] = _summarize(1,
                                  iouThr=.75,
                                  maxDets=self.params.maxDets[2])
            stats[3] = _summarize(1,
                                  HtRng='small',
                                  maxDets=self.params.maxDets[2])
            stats[4] = _summarize(1,
                                  HtRng='medium',
                                  maxDets=self.params.maxDets[2])
            stats[5] = _summarize(1,
                                  HtRng='large',
                                  maxDets=self.params.maxDets[2])
            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])
            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])
            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])
            stats[9] = _summarize(0,
                                  HtRng='small',
                                  maxDets=self.params.maxDets[2])
            stats[10] = _summarize(0,
                                   HtRng='medium',
                                   maxDets=self.params.maxDets[2])
            stats[11] = _summarize(0,
                                   HtRng='large',
                                   maxDets=self.params.maxDets[2])
            return stats

        def _summarizeKps():
            stats = np.zeros((10, ))
            stats[0] = _summarize(1, maxDets=20)
            stats[1] = _summarize(1, maxDets=20, iouThr=.5)
            stats[2] = _summarize(1, maxDets=20, iouThr=.75)
            stats[3] = _summarize(1, maxDets=20, HtRng='medium')
            stats[4] = _summarize(1, maxDets=20, HtRng='large')
            stats[5] = _summarize(0, maxDets=20)
            stats[6] = _summarize(0, maxDets=20, iouThr=.5)
            stats[7] = _summarize(0, maxDets=20, iouThr=.75)
            stats[8] = _summarize(0, maxDets=20, HtRng='medium')
            stats[9] = _summarize(0, maxDets=20, HtRng='large')
            return stats

        if not self.eval:
            raise Exception('Please run accumulate() first')
        iouType = self.params.iouType
        if iouType == 'segm' or iouType == 'bbox':
            summarize = _summarizeDets
        elif iouType == 'keypoints':
            summarize = _summarizeKps
        self.stats = summarize()

    def __str__(self):
        self.summarize()


class Params:
    '''
    Params for coco evaluation api
    '''
    def setDetParams(self):
        self.imgIds = []
        self.catIds = []
        # np.arange causes trouble.  the data point on arange is slightly larger than the true value
        self.iouThrs = np.linspace(.5,
                                   0.95,
                                   int(np.round((0.95 - .5) / .05)) + 1,
                                   endpoint=True)
        self.recThrs = np.linspace(.0,
                                   1.00,
                                   int(np.round((1.00 - .0) / .01)) + 1,
                                   endpoint=True)
        self.maxDets = [10, 100, 1000]
        self.HtRng = [[10, 1e5], [10, 50], [50, 300], [300, 1e5]]
        self.HtRngLbl = ['all', 'small', 'medium', 'large']
        self.useCats = 1

    def setKpParams(self):
        self.imgIds = []
        self.catIds = []
        # np.arange causes trouble.  the data point on arange is slightly larger than the true value
        self.iouThrs = np.linspace(.5,
                                   0.95,
                                   int(np.round((0.95 - .5) / .05)) + 1,
                                   endpoint=True)
        self.recThrs = np.linspace(.0,
                                   1.00,
                                   int(np.round((1.00 - .0) / .01)) + 1,
                                   endpoint=True)
        self.maxDets = [20]
        self.HtRng = [[10, 1e5], [50, 300], [300, 1e5]]
        self.HtRngLbl = ['all', 'medium', 'large']
        self.useCats = 1
        self.kpt_oks_sigmas = np.array([
            .26, .25, .25, .35, .35, .79, .79, .72, .72, .62, .62, 1.07, 1.07,
            .87, .87, .89, .89
        ]) / 10.0

    def __init__(self, iouType='segm'):
        if iouType == 'segm' or iouType == 'bbox':
            self.setDetParams()
        elif iouType == 'keypoints':
            self.setKpParams()
        else:
            raise Exception('iouType not supported')
        self.iouType = iouType
        # useSegm is deprecated
        self.useSegm = None
```

### cvpods/evaluation/pascal_voc_evaluation.py

```python
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

import logging
import os
import tempfile
import xml.etree.ElementTree as ET
from collections import OrderedDict, defaultdict
from functools import lru_cache

import numpy as np

import torch

from cvpods.utils import comm, create_small_table

from .evaluator import DatasetEvaluator
from .registry import EVALUATOR


@EVALUATOR.register()
class PascalVOCDetectionEvaluator(DatasetEvaluator):
    """
    Evaluate Pascal VOC AP.
    It contains a synchronization, therefore has to be called from all ranks.

    Note that this is a rewrite of the official Matlab API.
    The results should be similar, but not identical to the one produced by
    the official API.
    """

    def __init__(self, dataset_name, meta, dump=False):
        """
        Args:
            dataset_name (str): name of the dataset, e.g., "voc_2007_test".
            meta (SimpleNamespace): dataset metadata.
            dump (bool): If True, after the evaluation is completed, a Markdown file
                that records the model evaluation metrics and corresponding scores
                will be generated in the working directory.
        """
        self._dump = dump
        self._dataset_name = dataset_name
        self._anno_file_template = os.path.join(meta.dirname, "Annotations", "{}.xml")
        self._image_set_path = os.path.join(meta.dirname, "ImageSets", "Main", meta.split + ".txt")
        self._class_names = meta.thing_classes
        assert meta.year in [2007, 2012], meta.year
        self._is_2007 = meta.year == 2007
        self._cpu_device = torch.device("cpu")
        self._logger = logging.getLogger(__name__)

    def reset(self):
        self._predictions = defaultdict(list)  # class name -> list of prediction strings

    def process(self, inputs, outputs):
        for input, output in zip(inputs, outputs):
            image_id = input["image_id"]
            instances = output["instances"].to(self._cpu_device)
            boxes = instances.pred_boxes.tensor.numpy()
            scores = instances.scores.tolist()
            classes = instances.pred_classes.tolist()
            for box, score, cls in zip(boxes, scores, classes):
                xmin, ymin, xmax, ymax = box
                # The inverse of data loading logic in `datasets/pascal_voc.py`
                xmin += 1
                ymin += 1
                self._predictions[cls].append(
                    f"{image_id} {score:.3f} {xmin:.1f} {ymin:.1f} {xmax:.1f} {ymax:.1f}"
                )

    def evaluate(self):
        """
        Returns:
            dict: has a key "segm", whose value is a dict of "AP", "AP50", and "AP75".
        """
        all_predictions = comm.gather(self._predictions, dst=0)
        if not comm.is_main_process():
            return
        predictions = defaultdict(list)
        for predictions_per_rank in all_predictions:
            for clsid, lines in predictions_per_rank.items():
                predictions[clsid].extend(lines)
        del all_predictions

        self._logger.info(
            "Evaluating {} using {} metric. "
            "Note that results do not use the official Matlab API.".format(
                self._dataset_name, 2007 if self._is_2007 else 2012
            )
        )

        with tempfile.TemporaryDirectory(prefix="pascal_voc_eval_") as dirname:
            res_file_template = os.path.join(dirname, "{}.txt")

            aps = defaultdict(list)  # iou -> ap per class
            for cls_id, cls_name in enumerate(self._class_names):
                lines = predictions.get(cls_id, [""])

                with open(res_file_template.format(cls_name), "w") as f:
                    f.write("\n".join(lines))

                for thresh in range(50, 100, 5):
                    rec, prec, ap = voc_eval(
                        res_file_template,
                        self._anno_file_template,
                        self._image_set_path,
                        cls_name,
                        ovthresh=thresh / 100.0,
                        use_07_metric=self._is_2007,
                    )
                    aps[thresh].append(ap * 100)

        ret = OrderedDict()
        mAP = {iou: np.mean(x) for iou, x in aps.items()}
        ret["bbox"] = {"AP": np.mean(list(mAP.values())), "AP50": mAP[50], "AP75": mAP[75]}

        small_table = create_small_table(ret["bbox"])
        self._logger.info("Evaluation results for bbox: \n" + small_table)

        if self._dump:
            dump_info_one_task = {
                "task": "bbox",
                "tables": [small_table],
            }
            _dump_to_markdown([dump_info_one_task])
        return ret


def _dump_to_markdown(dump_infos, md_file="README.md"):
    """
    Dump a Markdown file that records the model evaluation metrics and corresponding scores
    to the current working directory.

    Args:
        dump_infos (list[dict]): dump information for each task.
        md_file (str): markdown file path.
    """
    title = os.getcwd().split("/")[-1]
    with open(md_file, "w") as f:
        f.write("# {}  ".format(title))
        for dump_info_per_task in dump_infos:
            task_name = dump_info_per_task["task"]
            tables = dump_info_per_task["tables"]
            tables = [table.replace("\n", "  \n") for table in tables]
            f.write("\n\n## Evaluation results for {}:  \n\n".format(task_name))
            f.write(tables[0])
            f.write("\n")


##############################################################################
#
# Below code is modified from
# https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/datasets/voc_eval.py
# --------------------------------------------------------
# Fast/er R-CNN
# Licensed under The MIT License [see LICENSE for details]
# Written by Bharath Hariharan
# --------------------------------------------------------

"""Python implementation of the PASCAL VOC devkit's AP evaluation code."""


@lru_cache(maxsize=None)
def parse_rec(filename):
    """Parse a PASCAL VOC xml file."""
    tree = ET.parse(filename)
    objects = []
    for obj in tree.findall("object"):
        obj_struct = {}
        obj_struct["name"] = obj.find("name").text
        obj_struct["pose"] = obj.find("pose").text
        obj_struct["truncated"] = int(obj.find("truncated").text)
        obj_struct["difficult"] = int(obj.find("difficult").text)
        bbox = obj.find("bndbox")
        obj_struct["bbox"] = [
            int(bbox.find("xmin").text),
            int(bbox.find("ymin").text),
            int(bbox.find("xmax").text),
            int(bbox.find("ymax").text),
        ]
        objects.append(obj_struct)

    return objects


def voc_ap(rec, prec, use_07_metric=False):
    """Compute VOC AP given precision and recall. If use_07_metric is true, uses
    the VOC 07 11-point method (default:False).
    """
    if use_07_metric:
        # 11 point metric
        ap = 0.0
        for t in np.arange(0.0, 1.1, 0.1):
            if np.sum(rec >= t) == 0:
                p = 0
            else:
                p = np.max(prec[rec >= t])
            ap = ap + p / 11.0
    else:
        # correct AP calculation
        # first append sentinel values at the end
        mrec = np.concatenate(([0.0], rec, [1.0]))
        mpre = np.concatenate(([0.0], prec, [0.0]))

        # compute the precision envelope
        for i in range(mpre.size - 1, 0, -1):
            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])

        # to calculate area under PR curve, look for points
        # where X axis (recall) changes value
        i = np.where(mrec[1:] != mrec[:-1])[0]

        # and sum (\Delta recall) * prec
        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])
    return ap


def voc_eval(detpath, annopath, imagesetfile, classname, ovthresh=0.5, use_07_metric=False):
    """rec, prec, ap = voc_eval(detpath,
                                annopath,
                                imagesetfile,
                                classname,
                                [ovthresh],
                                [use_07_metric])

    Top level function that does the PASCAL VOC evaluation.

    detpath: Path to detections
        detpath.format(classname) should produce the detection results file.
    annopath: Path to annotations
        annopath.format(imagename) should be the xml annotations file.
    imagesetfile: Text file containing the list of images, one image per line.
    classname: Category name (duh)
    [ovthresh]: Overlap threshold (default = 0.5)
    [use_07_metric]: Whether to use VOC07's 11 point AP computation
        (default False)
    """
    # assumes detections are in detpath.format(classname)
    # assumes annotations are in annopath.format(imagename)
    # assumes imagesetfile is a text file with each line an image name

    # first load gt
    # read list of images
    with open(imagesetfile, "r") as f:
        lines = f.readlines()
    imagenames = [x.strip() for x in lines]

    # load annots
    recs = {}
    for imagename in imagenames:
        recs[imagename] = parse_rec(annopath.format(imagename))

    # extract gt objects for this class
    class_recs = {}
    npos = 0
    for imagename in imagenames:
        R = [obj for obj in recs[imagename] if obj["name"] == classname]
        bbox = np.array([x["bbox"] for x in R])
        difficult = np.array([x["difficult"] for x in R]).astype(np.bool)
        # difficult = np.array([False for x in R]).astype(np.bool)  # treat all "difficult" as GT
        det = [False] * len(R)
        npos = npos + sum(~difficult)
        class_recs[imagename] = {"bbox": bbox, "difficult": difficult, "det": det}

    # read dets
    detfile = detpath.format(classname)
    with open(detfile, "r") as f:
        lines = f.readlines()

    splitlines = [x.strip().split(" ") for x in lines]
    image_ids = [x[0] for x in splitlines]
    confidence = np.array([float(x[1]) for x in splitlines])
    BB = np.array([[float(z) for z in x[2:]] for x in splitlines]).reshape(-1, 4)

    # sort by confidence
    sorted_ind = np.argsort(-confidence)
    BB = BB[sorted_ind, :]
    image_ids = [image_ids[x] for x in sorted_ind]

    # go down dets and mark TPs and FPs
    nd = len(image_ids)
    tp = np.zeros(nd)
    fp = np.zeros(nd)
    for d in range(nd):
        R = class_recs[image_ids[d]]
        bb = BB[d, :].astype(float)
        ovmax = -np.inf
        BBGT = R["bbox"].astype(float)

        if BBGT.size > 0:
            # compute overlaps
            # intersection
            ixmin = np.maximum(BBGT[:, 0], bb[0])
            iymin = np.maximum(BBGT[:, 1], bb[1])
            ixmax = np.minimum(BBGT[:, 2], bb[2])
            iymax = np.minimum(BBGT[:, 3], bb[3])
            iw = np.maximum(ixmax - ixmin + 1.0, 0.0)
            ih = np.maximum(iymax - iymin + 1.0, 0.0)
            inters = iw * ih

            # union
            uni = (
                (bb[2] - bb[0] + 1.0) * (bb[3] - bb[1] + 1.0)
                + (BBGT[:, 2] - BBGT[:, 0] + 1.0) * (BBGT[:, 3] - BBGT[:, 1] + 1.0)
                - inters
            )

            overlaps = inters / uni
            ovmax = np.max(overlaps)
            jmax = np.argmax(overlaps)

        if ovmax > ovthresh:
            if not R["difficult"][jmax]:
                if not R["det"][jmax]:
                    tp[d] = 1.0
                    R["det"][jmax] = 1
                else:
                    fp[d] = 1.0
        else:
            fp[d] = 1.0

    # compute precision recall
    fp = np.cumsum(fp)
    tp = np.cumsum(tp)
    rec = tp / float(npos)
    # avoid divide by zero in case the first detection matches a difficult
    # ground truth
    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)
    ap = voc_ap(rec, prec, use_07_metric)

    return rec, prec, ap
```

### cvpods/evaluation/lvis_evaluation.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import contextlib
import copy
import io
import itertools
import json
import logging
import os
import pickle
from collections import OrderedDict

from lvis import LVISEval, LVISResults

import torch

from cvpods.structures import Boxes, BoxMode, pairwise_iou
from cvpods.utils import PathManager, comm, create_small_table

from .coco_evaluation import instances_to_coco_json
from .evaluator import DatasetEvaluator
from .registry import EVALUATOR


@EVALUATOR.register()
class LVISEvaluator(DatasetEvaluator):
    """
    Evaluate object proposal and instance detection/segmentation outputs using
    LVIS's metrics and evaluation API.
    """

    def __init__(self, dataset_name, meta, cfg, distributed, output_dir=None, dump=False):
        """
        Args:
            dataset_name (str): name of the dataset to be evaluated.
                It must have the following corresponding metadata:

                    "json_file": the path to the LVIS format annotation

            meta (SimpleNamespace): dataset metadata.
            cfg (config dict): cvpods Config instance.
            distributed (True): if True, will collect results from all ranks for evaluation.
                Otherwise, will evaluate the results in the current process.
            output_dir (str): optional, an output directory to dump results.
            dump (bool): If True, after the evaluation is completed, a Markdown file
                that records the model evaluation metrics and corresponding scores
                will be generated in the working directory.
        """
        from lvis import LVIS
        # TODO: really use dataset_name
        self.dataset_name = dataset_name
        self._dump = dump
        self._tasks = self._tasks_from_config(cfg)
        self._distributed = distributed
        self._output_dir = output_dir

        self._cpu_device = torch.device("cpu")
        self._logger = logging.getLogger(__name__)

        self._metadata = meta
        json_file = PathManager.get_local_path(self._metadata.json_file)
        self._lvis_api = LVIS(json_file)
        # Test set json files do not contain annotations (evaluation must be
        # performed using the LVIS evaluation server).
        self._do_evaluation = len(self._lvis_api.get_ann_ids()) > 0

    def reset(self):
        self._predictions = []
        self._lvis_results = []
        self._dump_infos = []  # per task

    def _tasks_from_config(self, cfg):
        """
        Returns:
            tuple[str]: tasks that can be evaluated under the given configuration.
        """
        tasks = ("bbox",)
        if cfg.MODEL.MASK_ON:
            tasks = tasks + ("segm",)
        return tasks

    def process(self, inputs, outputs):
        """
        Args:
            inputs: the inputs to a LVIS model (e.g., GeneralizedRCNN).
                It is a list of dict. Each dict corresponds to an image and
                contains keys like "height", "width", "file_name", "image_id".
            outputs: the outputs of a LVIS model. It is a list of dicts with key
                "instances" that contains :class:`Instances`.
        """
        for input, output in zip(inputs, outputs):
            prediction = {"image_id": input["image_id"]}

            if "instances" in output:
                instances = output["instances"].to(self._cpu_device)
                prediction["instances"] = instances_to_coco_json(instances, input["image_id"])
            if "proposals" in output:
                prediction["proposals"] = output["proposals"].to(self._cpu_device)
            self._predictions.append(prediction)

    def evaluate(self):
        if self._distributed:
            comm.synchronize()
            self._predictions = comm.gather(self._predictions, dst=0)
            self._predictions = list(itertools.chain(*self._predictions))

            if not comm.is_main_process():
                return

        if len(self._predictions) == 0:
            self._logger.warning("[LVISEvaluator] Did not receive valid predictions.")
            return {}

        if self._output_dir:
            PathManager.mkdirs(self._output_dir)
            file_path = os.path.join(self._output_dir, "instances_predictions.pth")
            with PathManager.open(file_path, "wb") as f:
                torch.save(self._predictions, f)

        self._results = OrderedDict()
        if "proposals" in self._predictions[0]:
            self._eval_box_proposals()
        if "instances" in self._predictions[0]:
            self._eval_predictions(set(self._tasks))

        if self._dump:
            _dump_to_markdown(self._dump_infos)

        # Copy so the caller can do whatever with results
        return copy.deepcopy(self._results)

    def _eval_predictions(self, tasks):
        """
        Evaluate self._predictions on the given tasks.
        Fill self._results with the metrics of the tasks.
        """
        self._logger.info("Preparing results in the LVIS format ...")
        self._lvis_results = list(itertools.chain(*[x["instances"] for x in self._predictions]))

        # LVIS evaluator can be used to evaluate results for COCO dataset categories.
        # In this case `_metadata` variable will have a field with COCO-specific category mapping.
        if hasattr(self._metadata, "thing_dataset_id_to_contiguous_id"):
            reverse_id_mapping = {
                v: k for k, v in self._metadata.thing_dataset_id_to_contiguous_id.items()
            }
            for result in self._lvis_results:
                result["category_id"] = reverse_id_mapping[result["category_id"]]
        else:
            # unmap the category ids for LVIS (from 0-indexed to 1-indexed)
            for result in self._lvis_results:
                result["category_id"] += 1

        if self._output_dir:
            file_path = os.path.join(self._output_dir, "lvis_instances_results.json")
            self._logger.info("Saving results to {}".format(file_path))
            with PathManager.open(file_path, "w") as f:
                f.write(json.dumps(self._lvis_results))
                f.flush()

        if not self._do_evaluation:
            self._logger.info("Annotations are not available for evaluation.")
            return

        self._logger.info("Evaluating predictions ...")
        for task in sorted(tasks):
            lvis_eval, summary = (
                _evaluate_predictions_on_lvis(
                    self._lvis_api, self._lvis_results, task
                )
                if len(self._lvis_results) > 0
                else None
            )
            self._logger.info("\n" + summary.getvalue())
            res = self._derive_lvis_results(lvis_eval, task, summary)
            self._results[task] = res

    def _eval_box_proposals(self):
        """
        Evaluate the box proposals in self._predictions.
        Fill self._results with the metrics for "box_proposals" task.
        """
        if self._output_dir:
            # Saving generated box proposals to file.
            # Predicted box_proposals are in XYXY_ABS mode.
            bbox_mode = BoxMode.XYXY_ABS.value
            ids, boxes, objectness_logits = [], [], []
            for prediction in self._predictions:
                ids.append(prediction["image_id"])
                boxes.append(prediction["proposals"].proposal_boxes.tensor.numpy())
                objectness_logits.append(prediction["proposals"].objectness_logits.numpy())

            proposal_data = {
                "boxes": boxes,
                "objectness_logits": objectness_logits,
                "ids": ids,
                "bbox_mode": bbox_mode,
            }
            with PathManager.open(os.path.join(self._output_dir, "box_proposals.pkl"), "wb") as f:
                pickle.dump(proposal_data, f)

        if not self._do_evaluation:
            self._logger.info("Annotations are not available for evaluation.")
            return

        self._logger.info("Evaluating bbox proposals ...")
        res = {}
        areas = {"all": "", "small": "s", "medium": "m", "large": "l"}
        for limit in [100, 1000]:
            for area, suffix in areas.items():
                stats = _evaluate_box_proposals(
                    self._predictions, self._lvis_api, area=area, limit=limit
                )
                key = "AR{}@{:d}".format(suffix, limit)
                res[key] = float(stats["ar"].item() * 100)
        self._logger.info("Proposal metrics: \n" + create_small_table(res))
        self._results["box_proposals"] = res

    def _derive_lvis_results(self, lvis_eval, iou_type, summary):
        """
        Derive the desired score numbers from summarized LVISEval.

        Args:
            lvis_eval (None or LVISEval): None represents no predictions from model.
            iou_type (str): specific evaluation task, optional values are: "bbox", "segm".

        Returns:
            a dict of {metric name: score}
        """
        metrics = {
            "bbox": ["AP", "AP50", "AP75", "APs", "APm", "APl", "APr", "APc", "APf"],
            "segm": ["AP", "AP50", "AP75", "APs", "APm", "APl", "APr", "APc", "APf"],
        }[iou_type]

        if lvis_eval is None:
            self._logger.warn("No predictions from the model!")
            return {metric: float("nan") for metric in metrics}

        # Pull the standard metrics from the LVIS results
        results = lvis_eval.get_results()
        results = {metric: float(results[metric] * 100) for metric in metrics}
        small_table = create_small_table(results)
        self._logger.info("Evaluation results for {}: \n".format(iou_type) + small_table)

        if self._dump:
            dump_info_one_task = {
                "task": iou_type,
                "summary": summary.getvalue(),
                "tables": [small_table],
            }
            self._dump_infos.append(dump_info_one_task)
        return results


# inspired from Detectron:
# https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L255 # noqa
def _evaluate_box_proposals(dataset_predictions, lvis_api, thresholds=None, area="all", limit=None):
    """
    Evaluate detection proposal recall metrics. This function is a much
    faster alternative to the official LVIS API recall evaluation code. However,
    it produces slightly different results.
    """
    # Record max overlap value for each gt box
    # Return vector of overlap values
    areas = {
        "all": 0,
        "small": 1,
        "medium": 2,
        "large": 3,
        "96-128": 4,
        "128-256": 5,
        "256-512": 6,
        "512-inf": 7,
    }
    area_ranges = [
        [0 ** 2, 1e5 ** 2],  # all
        [0 ** 2, 32 ** 2],  # small
        [32 ** 2, 96 ** 2],  # medium
        [96 ** 2, 1e5 ** 2],  # large
        [96 ** 2, 128 ** 2],  # 96-128
        [128 ** 2, 256 ** 2],  # 128-256
        [256 ** 2, 512 ** 2],  # 256-512
        [512 ** 2, 1e5 ** 2],
    ]  # 512-inf
    assert area in areas, "Unknown area range: {}".format(area)
    area_range = area_ranges[areas[area]]
    gt_overlaps = []
    num_pos = 0

    for prediction_dict in dataset_predictions:
        predictions = prediction_dict["proposals"]

        # sort predictions in descending order
        # TODO maybe remove this and make it explicit in the documentation
        inds = predictions.objectness_logits.sort(descending=True)[1]
        predictions = predictions[inds]

        ann_ids = lvis_api.get_ann_ids(img_ids=[prediction_dict["image_id"]])
        anno = lvis_api.load_anns(ann_ids)
        gt_boxes = [
            BoxMode.convert(obj["bbox"], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS) for obj in anno
        ]
        gt_boxes = torch.as_tensor(gt_boxes).reshape(-1, 4)  # guard against no boxes
        gt_boxes = Boxes(gt_boxes)
        gt_areas = torch.as_tensor([obj["area"] for obj in anno])

        if len(gt_boxes) == 0 or len(predictions) == 0:
            continue

        valid_gt_inds = (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])
        gt_boxes = gt_boxes[valid_gt_inds]

        num_pos += len(gt_boxes)

        if len(gt_boxes) == 0:
            continue

        if limit is not None and len(predictions) > limit:
            predictions = predictions[:limit]

        overlaps = pairwise_iou(predictions.proposal_boxes, gt_boxes)

        _gt_overlaps = torch.zeros(len(gt_boxes))
        for j in range(min(len(predictions), len(gt_boxes))):
            # find which proposal box maximally covers each gt box
            # and get the iou amount of coverage for each gt box
            max_overlaps, argmax_overlaps = overlaps.max(dim=0)

            # find which gt box is 'best' covered (i.e. 'best' = most iou)
            gt_ovr, gt_ind = max_overlaps.max(dim=0)
            assert gt_ovr >= 0
            # find the proposal box that covers the best covered gt box
            box_ind = argmax_overlaps[gt_ind]
            # record the iou coverage of this gt box
            _gt_overlaps[j] = overlaps[box_ind, gt_ind]
            assert _gt_overlaps[j] == gt_ovr
            # mark the proposal box and the gt box as used
            overlaps[box_ind, :] = -1
            overlaps[:, gt_ind] = -1

        # append recorded iou coverage level
        gt_overlaps.append(_gt_overlaps)
    gt_overlaps = torch.cat(gt_overlaps, dim=0)
    gt_overlaps, _ = torch.sort(gt_overlaps)

    if thresholds is None:
        step = 0.05
        thresholds = torch.arange(0.5, 0.95 + 1e-5, step, dtype=torch.float32)
    recalls = torch.zeros_like(thresholds)
    # compute recall for each iou threshold
    for i, t in enumerate(thresholds):
        recalls[i] = (gt_overlaps >= t).float().sum() / float(num_pos)
    # ar = 2 * np.trapz(recalls, thresholds)
    ar = recalls.mean()
    return {
        "ar": ar,
        "recalls": recalls,
        "thresholds": thresholds,
        "gt_overlaps": gt_overlaps,
        "num_pos": num_pos,
    }


def _evaluate_predictions_on_lvis(lvis_gt, lvis_results, iou_type):
    """
    Evaluate the lvis results using LVISEval API.
    """
    assert len(lvis_results) > 0

    if iou_type == "segm":
        lvis_results = copy.deepcopy(lvis_results)
        # When evaluating mask AP, if the results contain bbox, LVIS API will
        # use the box area as the area of the instance, instead of the mask area.
        # This leads to a different definition of small/medium/large.
        # We remove the bbox field to let mask AP use mask area.
        for c in lvis_results:
            c.pop("bbox", None)

    lvis_results = LVISResults(lvis_gt, lvis_results)
    lvis_eval = LVISEval(lvis_gt, lvis_results, iou_type)
    lvis_eval.run()
    summary = io.StringIO()
    with contextlib.redirect_stdout(summary):
        lvis_eval.print_results()
    return lvis_eval, summary


def _dump_to_markdown(dump_infos, md_file="README.md"):
    """
    Dump a Markdown file that records the model evaluation metrics and corresponding scores
    to the current working directory.

    Args:
        dump_infos (list[dict]): dump information for each task.
        md_file (str): markdown file path.
    """
    with open(md_file, "w") as f:
        title = os.path.basename(os.getcwd())
        f.write("# {}  ".format(title))
        for dump_info_per_task in dump_infos:
            task_name = dump_info_per_task["task"]
            summary = dump_info_per_task["summary"]
            tables = dump_info_per_task["tables"]
            tables = [table.replace("\n", "  \n") for table in tables]
            f.write("\n\n## Evaluation results for {}:  \n\n".format(task_name))
            f.write("```  \n" + summary + "```  \n" + tables[0] + "\n")
```

### cvpods/evaluation/testing.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import logging
import pprint
import sys
from collections import Mapping, OrderedDict

import numpy as np


def print_csv_format(results):
    """
    Print main metrics in a format similar to Detectron,
    so that they are easy to copypaste into a spreadsheet.

    Args:
        results (OrderedDict[dict]): task_name -> {metric -> score}
    """
    assert isinstance(results, OrderedDict), results  # unordered results cannot be properly printed
    logger = logging.getLogger(__name__)
    for task, res in results.items():
        # Don't print "AP-category" metrics since they are usually not tracked.
        important_res = [(k, v) for k, v in res.items() if "-" not in k]
        logger.info("copypaste: Task: {}".format(task))
        logger.info("copypaste: " + ",".join([k[0] for k in important_res]))
        logger.info("copypaste: " + ",".join(["{0:.4f}".format(k[1]) for k in important_res]))


def verify_results(cfg, results):
    """
    Args:
        results (OrderedDict[dict]): task_name -> {metric -> score}

    Returns:
        bool: whether the verification succeeds or not
    """
    expected_results = cfg.TEST.EXPECTED_RESULTS
    if not len(expected_results):
        return True

    ok = True
    for task, metric, expected, tolerance in expected_results:
        actual = results[task][metric]
        if not np.isfinite(actual):
            ok = False
        diff = abs(actual - expected)
        if diff > tolerance:
            ok = False

    logger = logging.getLogger(__name__)
    if not ok:
        logger.error("Result verification failed!")
        logger.error("Expected Results: " + str(expected_results))
        logger.error("Actual Results: " + pprint.pformat(results))

        sys.exit(1)
    else:
        logger.info("Results verification passed.")
    return ok


def flatten_results_dict(results):
    """
    Expand a hierarchical dict of scalars into a flat dict of scalars.
    If results[k1][k2][k3] = v, the returned dict will have the entry
    {"k1/k2/k3": v}.

    Args:
        results (dict):
    """
    r = {}
    for k, v in results.items():
        if isinstance(v, Mapping):
            v = flatten_results_dict(v)
            for kk, vv in v.items():
                r[k + "/" + kk] = vv
        else:
            r[k] = v
    return r
```

### cvpods/evaluation/evaluator.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import datetime
import logging
import time
from collections import OrderedDict
from contextlib import contextmanager

import torch

from cvpods.utils import comm, log_every_n_seconds

from .registry import EVALUATOR


@EVALUATOR.register()
class DatasetEvaluator:
    """
    Base class for a dataset evaluator.

    The function :func:`inference_on_dataset` runs the model over
    all samples in the dataset, and have a DatasetEvaluator to process the inputs/outputs.

    This class will accumulate information of the inputs/outputs (by :meth:`process`),
    and produce evaluation results in the end (by :meth:`evaluate`).
    """

    def reset(self):
        """
        Preparation for a new round of evaluation.
        Should be called before starting a round of evaluation.
        """
        pass

    def process(self, input, output):
        """
        Process an input/output pair.

        Args:
            input: the input that's used to call the model.
            output: the return value of `model(input)`
        """
        pass

    def evaluate(self):
        """
        Evaluate/summarize the performance, after processing all input/output pairs.

        Returns:
            dict:
                A new evaluator class can return a dict of arbitrary format
                as long as the user can process the results.
                In our train_net.py, we expect the following format:

                * key: the name of the task (e.g., bbox)
                * value: a dict of {metric name: score}, e.g.: {"AP50": 80}
        """
        pass


class DatasetEvaluators(DatasetEvaluator):
    def __init__(self, evaluators):
        assert len(evaluators)
        super().__init__()
        self._evaluators = evaluators

    def reset(self):
        for evaluator in self._evaluators:
            evaluator.reset()

    def process(self, input, output):
        for evaluator in self._evaluators:
            evaluator.process(input, output)

    def evaluate(self):
        results = OrderedDict()
        for evaluator in self._evaluators:
            result = evaluator.evaluate()
            if comm.is_main_process() and result is not None:
                for k, v in result.items():
                    assert (
                        k not in results
                    ), "Different evaluators produce results with the same key {}".format(k)
                    results[k] = v
        return results


def inference_on_dataset(model, data_loader, evaluator):
    """
    Run model on the data_loader and evaluate the metrics with evaluator.
    The model will be used in eval mode.

    Args:
        model (nn.Module): a module which accepts an object from
            `data_loader` and returns some outputs. It will be temporarily set to `eval` mode.

            If you wish to evaluate a model in `training` mode instead, you can
            wrap the given model and override its behavior of `.eval()` and `.train()`.
        data_loader: an iterable object with a length.
            The elements it generates will be the inputs to the model.
        evaluator (DatasetEvaluator): the evaluator to run. Use `None` if you only want
            to benchmark, but don't want to do any evaluation.

    Returns:
        The return value of `evaluator.evaluate()`
    """
    num_devices = torch.distributed.get_world_size() if torch.distributed.is_initialized() else 1
    logger = logging.getLogger(__name__)
    logger.info("Start inference on {} data samples".format(len(data_loader)))

    total = len(data_loader)  # inference data loader must have a fixed length
    if evaluator is None:
        # create a no-op evaluator
        evaluator = DatasetEvaluators([])
    evaluator.reset()

    num_warmup = min(5, total - 1)

    start_time = time.perf_counter()
    total_compute_time = 0
    with inference_context(model), torch.no_grad():
        for idx, inputs in enumerate(data_loader):
            if idx == num_warmup:
                start_time = time.perf_counter()
                total_compute_time = 0

            start_compute_time = time.perf_counter()
            outputs = model(inputs)
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            total_compute_time += time.perf_counter() - start_compute_time
            evaluator.process(inputs, outputs)

            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)
            seconds_per_img = total_compute_time / iters_after_start
            if idx >= num_warmup * 2 or seconds_per_img > 5:
                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start
                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))
                log_every_n_seconds(
                    logging.INFO,
                    "Inference done {}/{}. {:.4f} s / sample. ETA={}".format(
                        idx + 1, total, seconds_per_img, str(eta)
                    ),
                    n=5,
                )

    # Measure the time only for this worker (before the synchronization barrier)
    total_time = time.perf_counter() - start_time
    total_time_str = str(datetime.timedelta(seconds=total_time))
    # NOTE this format is parsed by grep
    logger.info(
        "Total inference time: {} ({:.6f} s / sample per device, on {} devices)".format(
            total_time_str, total_time / (total - num_warmup), num_devices
        )
    )
    total_compute_time_str = str(datetime.timedelta(seconds=int(total_compute_time)))
    logger.info(
        "Total inference pure compute time: {} ({:.6f} s / sample per device, "
        "on {} devices)".format(
            total_compute_time_str, total_compute_time / (total - num_warmup), num_devices
        )
    )

    results = evaluator.evaluate()
    # An evaluator may return None when not in main process.
    # Replace it by an empty dict instead to make it easier for downstream code to handle
    if results is None:
        results = {}
    return results


@contextmanager
def inference_context(model):
    """
    A context where the model is temporarily changed to eval mode,
    and restored to previous mode afterwards.

    Args:
        model: a torch Module
    """
    training_mode = model.training
    model.eval()
    yield
    model.train(training_mode)
```

### cvpods/evaluation/rotated_coco_evaluation.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

import itertools
import json
import os

import numpy as np
from pycocotools.cocoeval import COCOeval, maskUtils

import torch

from cvpods.structures import BoxMode, RotatedBoxes, pairwise_iou_rotated
from cvpods.utils import PathManager

from .coco_evaluation import COCOEvaluator
from .registry import EVALUATOR


@EVALUATOR.register()
class RotatedCOCOeval(COCOeval):
    @staticmethod
    def is_rotated(box_list):
        if type(box_list) == np.ndarray:
            return box_list.shape[1] == 5
        elif type(box_list) == list:
            if box_list == []:  # cannot decide the box_dim
                return False
            return np.all(
                np.array(
                    [
                        (len(obj) == 5) and ((type(obj) == list) or (type(obj) == np.ndarray))
                        for obj in box_list
                    ]
                )
            )
        return False

    @staticmethod
    def boxlist_to_tensor(boxlist, output_box_dim):
        if type(boxlist) == np.ndarray:
            box_tensor = torch.from_numpy(boxlist)
        elif type(boxlist) == list:
            if boxlist == []:
                return torch.zeros((0, output_box_dim), dtype=torch.float32)
            else:
                box_tensor = torch.FloatTensor(boxlist)
        else:
            raise Exception("Unrecognized boxlist type")

        input_box_dim = box_tensor.shape[1]
        if input_box_dim != output_box_dim:
            if input_box_dim == 4 and output_box_dim == 5:
                box_tensor = BoxMode.convert(box_tensor, BoxMode.XYWH_ABS, BoxMode.XYWHA_ABS)
            else:
                raise Exception(
                    "Unable to convert from {}-dim box to {}-dim box".format(
                        input_box_dim, output_box_dim
                    )
                )
        return box_tensor

    def compute_iou_dt_gt(self, dt, gt, is_crowd):
        if self.is_rotated(dt) or self.is_rotated(gt):
            # TODO: take is_crowd into consideration
            assert all(c == 0 for c in is_crowd)
            dt = RotatedBoxes(self.boxlist_to_tensor(dt, output_box_dim=5))
            gt = RotatedBoxes(self.boxlist_to_tensor(gt, output_box_dim=5))
            return pairwise_iou_rotated(dt, gt)
        else:
            # This is the same as the classical COCO evaluation
            return maskUtils.iou(dt, gt, is_crowd)

    def computeIoU(self, imgId, catId):
        p = self.params
        if p.useCats:
            gt = self._gts[imgId, catId]
            dt = self._dts[imgId, catId]
        else:
            gt = [_ for cId in p.catIds for _ in self._gts[imgId, cId]]
            dt = [_ for cId in p.catIds for _ in self._dts[imgId, cId]]
        if len(gt) == 0 and len(dt) == 0:
            return []
        inds = np.argsort([-d["score"] for d in dt], kind="mergesort")
        dt = [dt[i] for i in inds]
        if len(dt) > p.maxDets[-1]:
            dt = dt[0: p.maxDets[-1]]

        assert p.iouType == "bbox", "unsupported iouType for iou computation"

        g = [g["bbox"] for g in gt]
        d = [d["bbox"] for d in dt]

        # compute iou between each dt and gt region
        iscrowd = [int(o["iscrowd"]) for o in gt]

        # Note: this function is copied from cocoeval.py in cocoapi
        # and the major difference is here.
        ious = self.compute_iou_dt_gt(d, g, iscrowd)
        return ious


class RotatedCOCOEvaluator(COCOEvaluator):
    """
    Evaluate object proposal/instance detection outputs using COCO-like metrics and APIs,
    with rotated boxes support.
    Note: this uses IOU only and does not consider angle differences.
    """

    def process(self, inputs, outputs):
        """
        Args:
            inputs: the inputs to a COCO model (e.g., GeneralizedRCNN).
                It is a list of dict. Each dict corresponds to an image and
                contains keys like "height", "width", "file_name", "image_id".
            outputs: the outputs of a COCO model. It is a list of dicts with key
                "instances" that contains :class:`Instances`.
        """
        for input, output in zip(inputs, outputs):
            prediction = {"image_id": input["image_id"]}

            if "instances" in output:
                instances = output["instances"].to(self._cpu_device)

                prediction["instances"] = self.instances_to_json(instances, input["image_id"])
            if "proposals" in output:
                prediction["proposals"] = output["proposals"].to(self._cpu_device)
            self._predictions.append(prediction)

    def instances_to_json(self, instances, img_id):
        num_instance = len(instances)
        if num_instance == 0:
            return []

        boxes = instances.pred_boxes.tensor.numpy()
        if boxes.shape[1] == 4:
            boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)
        boxes = boxes.tolist()
        scores = instances.scores.tolist()
        classes = instances.pred_classes.tolist()

        results = []
        for k in range(num_instance):
            result = {
                "image_id": img_id,
                "category_id": classes[k],
                "bbox": boxes[k],
                "score": scores[k],
            }

            results.append(result)
        return results

    def _eval_predictions(self, tasks):
        """
        Evaluate self._predictions on the given tasks.
        Fill self._results with the metrics of the tasks.
        """
        self._logger.info("Preparing results for COCO format ...")
        self._coco_results = list(itertools.chain(*[x["instances"] for x in self._predictions]))

        # unmap the category ids for COCO
        if hasattr(self._metadata, "thing_dataset_id_to_contiguous_id"):
            reverse_id_mapping = {
                v: k for k, v in self._metadata.thing_dataset_id_to_contiguous_id.items()
            }
            for result in self._coco_results:
                result["category_id"] = reverse_id_mapping[result["category_id"]]

        if self._output_dir:
            file_path = os.path.join(self._output_dir, "coco_instances_results.json")
            self._logger.info("Saving results to {}".format(file_path))
            with PathManager.open(file_path, "w") as f:
                f.write(json.dumps(self._coco_results))
                f.flush()

        if not self._do_evaluation:
            self._logger.info("Annotations are not available for evaluation.")
            return

        self._logger.info("Evaluating predictions ...")
        for task in sorted(tasks):
            assert task == "bbox", "Task {} is not supported".format(task)
            coco_eval = (
                self._evaluate_predictions_on_coco(self._coco_api, self._coco_results)
                if len(self._coco_results) > 0
                else None  # cocoapi does not handle empty results very well
            )

            res = self._derive_coco_results(
                coco_eval, task, class_names=self._metadata.get("thing_classes")
            )
            self._results[task] = res

    def _evaluate_predictions_on_coco(self, coco_gt, coco_results):
        """
        Evaluate the coco results using COCOEval API.
        """
        assert len(coco_results) > 0

        coco_dt = coco_gt.loadRes(coco_results)

        # Only bbox is supported for now
        coco_eval = RotatedCOCOeval(coco_gt, coco_dt, iouType="bbox")

        coco_eval.evaluate()
        coco_eval.accumulate()
        coco_eval.summarize()

        return coco_eval
```

### cvpods/data/build.py

```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

import logging

import numpy as np

import torch.utils.data

from cvpods.utils import comm, seed_all_rng

from .detection_utils import check_sample_valid
from .registry import DATASETS, PATH_ROUTES, SAMPLERS, TRANSFORMS

"""
This file contains the default logic to build a dataloader for training or testing.
"""

__all__ = [
    "build_dataset",
    "build_train_loader",
    "build_test_loader",
    "build_transform_gens",
]

logger = logging.getLogger(__name__)


def build_transform_gens(pipelines):
    """
    Create a list of :class:`TransformGen` from config.

    Transform list is a list of tuple which includes Transform name and parameters.
    Args:
        pipelines: cfg.INPUT.TRAIN_PIPELINES and cfg.INPUT.TEST_PIPELINES are used here

    Returns:
        list[TransformGen]: a list of several TransformGen.
    """

    def build(pipeline):
        tfm_gens = []

        for (aug, args) in pipeline:
            if "List" in aug:
                assert "transforms" in args, "List Transforms must contain a `transforms` key"
                sub_pipelines = args["transforms"]
                args["transforms"] = build_transform_gens(sub_pipelines)
                tfm = TRANSFORMS.get(aug)(**args)
            else:
                if aug == "ResizeShortestEdge":
                    check_sample_valid(args)
                if aug.startswith("Torch_"):
                    tfm = TRANSFORMS.get("TorchTransformGen")(args)
                else:
                    tfm = TRANSFORMS.get(aug)(**args)
            tfm_gens.append(tfm)

        return tfm_gens

    if isinstance(pipelines, dict):
        tfm_gens_dict = {}
        for key, tfms in pipelines.items():
            tfm_gens_dict[key] = build(tfms)
        return tfm_gens_dict
    else:
        return build(pipelines)


def build_dataset(config, dataset_names, transforms=[], is_train=True):
    """
    dataset_names: List[str], in which elemements must be in format of "dataset_task_version"
    """

    def _build_single_dataset(config, dataset_name, transforms=[], is_train=True):
        """
        Build a single dataset according to dataset_name.

        Args:
            config (BaseConfig): config.
            dataset_name (str): dataset_name should be of 'dataset_xxx_xxx' format,
                so that corresponding dataset can be acquired from the first token in this argument.
            transforms (List[TransformGen]): list of transforms configured in config file.
            is_train (bool): whether is in training mode or not.
        """
        dataset_type = dataset_name.split("_")[0].upper()
        assert dataset_type in PATH_ROUTES, "{} not found in PATH_ROUTES".format(dataset_type)
        name = PATH_ROUTES.get(dataset_type)["dataset_type"]
        dataset = DATASETS.get(name)(config, dataset_name, transforms=transforms, is_train=is_train)
        return dataset

    datasets = [
        _build_single_dataset(config, dataset_name, transforms=transforms, is_train=is_train)
        for dataset_name in dataset_names
    ]
    custom_type, args = config.DATASETS.CUSTOM_TYPE
    # wrap all datasets, Dataset concat is the default behaviour
    dataset = DATASETS.get(custom_type)(datasets, **args)
    return dataset


def build_train_loader(cfg):
    """
    A data loader is created by the following steps:
    1. Use the dataset names in config to query :class:`DatasetCatalog`, and obtain a list of dicts.
    2. Start workers to work on the dicts. Each worker will:

       * Map each metadata dict into another format to be consumed by the model.
       * Batch them by simply putting dicts into a list.

    The batched ``list[mapped_dict]`` is what this dataloader will return.

    Args:
        cfg (config dict): the config

    Returns:
        an infinite iterator of training data
    """
    # For simulate large batch training
    num_devices = comm.get_world_size()
    rank = comm.get_rank()

    # use subdivision batchsize
    images_per_minibatch = cfg.SOLVER.IMS_PER_DEVICE // cfg.SOLVER.BATCH_SUBDIVISIONS

    logger = logging.getLogger(__name__)

    transform_gens = build_transform_gens(cfg.INPUT.AUG.TRAIN_PIPELINES)
    logger.info(f"TransformGens used: {transform_gens} in training")
    dataset = build_dataset(
        cfg, cfg.DATASETS.TRAIN, transforms=transform_gens, is_train=True
    )

    sampler_name = cfg.DATALOADER.SAMPLER_TRAIN
    logger.info("Using training sampler {}".format(sampler_name))

    assert sampler_name in SAMPLERS, "{} not found in SAMPLERS".format(sampler_name)
    if sampler_name == "DistributedGroupSampler":
        sampler = SAMPLERS.get(sampler_name)(
            dataset, images_per_minibatch, num_devices, rank)
    elif sampler_name == "RepeatFactorTrainingSampler":
        sampler = SAMPLERS.get(sampler_name)(
            dataset, cfg.DATALOADER.REPEAT_THRESHOLD)

    data_loader = torch.utils.data.DataLoader(
        dataset,
        batch_size=images_per_minibatch,
        sampler=sampler,
        num_workers=cfg.DATALOADER.NUM_WORKERS,
        collate_fn=trivial_batch_collator,
        worker_init_fn=worker_init_reset_seed,
    )

    return data_loader


def build_test_loader(cfg):
    """
    Similar to `build_train_loader`.
    But this function uses the given `dataset_name` argument (instead of the names in cfg),
    and uses batch size 1.

    Args:
        cfg: a cvpods config dict

    Returns:
        DataLoader: a torch DataLoader, that loads the given detection
        dataset, with test-time transformation and batching.
    """
    transform_gens = build_transform_gens(cfg.INPUT.AUG.TEST_PIPELINES)
    logger = logging.getLogger(__name__)
    logger.info(f"TransformGens used: {transform_gens} in testing")
    dataset = build_dataset(cfg,
                            cfg.DATASETS.TEST,
                            transforms=transform_gens,
                            is_train=False)
    sampler = SAMPLERS.get("InferenceSampler")(len(dataset))
    # Always use 1 image per worker during inference since this is the
    # standard when reporting inference time in papers.
    batch_sampler = torch.utils.data.sampler.BatchSampler(sampler, 1, drop_last=False)

    data_loader = torch.utils.data.DataLoader(
        dataset,
        num_workers=cfg.DATALOADER.NUM_WORKERS,
        batch_sampler=batch_sampler,
        collate_fn=trivial_batch_collator,
        pin_memory=True,
    )
    return data_loader


def trivial_batch_collator(batch):
    """
    A batch collator that does nothing.
    """
    return batch


def worker_init_reset_seed(worker_id):
    seed_all_rng(np.random.randint(2**31) + worker_id)
```

### cvpods/data/base_dataset.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import itertools
import logging
import os.path as osp
import pickle
from copy import deepcopy
from tabulate import tabulate
from termcolor import colored

import numpy as np

from torch.utils.data import Dataset

import cvpods
from cvpods.data.detection_utils import check_metadata_consistency, read_image
from cvpods.structures import BoxMode
from cvpods.utils import PathManager, log_first_n

from .registry import DATASETS


@DATASETS.register()
class BaseDataset(Dataset):
    """Abstract class representing a pytorch-like Dataset.
    All other datasets should be subclasses of it.
    All subclasses should override:
        ``__len__`` that provides the size of the dataset,
        ``__getitem__`` that supports integer indexing in the range from 0 to length,
        ``_get_metadata`` that stores dataset meta such as category lists,
        ``_apply_transforms`` that specifies how to apply transformation onto data,
        ``_load_annotations`` that specfies how to access label files,
        ``evaluate`` that is responsible for evaluate predictions of this dataset.

    Default annotation type:
    [
        {
            'file_name': 'a.jpg',
            'width': 1280,
            'height': 720,
            'image_id': if necessary
            'annotations': {
                'bboxes': <np.ndarray> (n, 4),
                'bboxes_ignore': <np.ndarray> (k, 4), (optional field)
                'masks': polygon or mle (optional)
                'semantic_seg': xxx (optional)
                'labels': <np.ndarray> (n, ), (optional)
                'labels_ignore': <np.ndarray> (k, 4) (optional field)
            }
        },
        ...
    ]
    """

    def __init__(self, cfg, dataset_name, transforms=[], is_train=True):
        """
        BaseDataset should have the following properties:
            * data_root (contains data and annotations)
            * transforms list
            * evaluators list

        Args:
            cfg (BaseConfig): config
            dataset_name (str): name of the dataset
            transforms (List[TransformGen]): list of transforms to get network input.
            is_train (bool): whether in training mode.
        """
        super(BaseDataset, self).__init__()

        self.name = dataset_name
        self.data_root = osp.join(
            osp.split(osp.split(cvpods.__file__)[0])[0], "datasets")
        self.transforms = transforms
        self.is_train = is_train

        self.data_format = cfg.INPUT.FORMAT

    def __getitem__(self, index):
        """Load data, apply transforms, converto to Instances.
        """
        raise NotImplementedError

    def __len__(self):
        raise NotImplementedError

    def _load_annotations(self):
        raise NotImplementedError

    def _get_metadata(self):
        raise NotImplementedError

    def _read_data(self, file_name):
        return read_image(file_name, format=self.data_format)

    def _apply_transforms(self, image, annotations=None, **kwargs):
        """
        Apply a list of :class:`TransformGen` on the input image, and
        returns the transformed image and a list of transforms.

        We cannot simply create and return all transforms without
        applying it to the image, because a subsequent transform may
        need the output of the previous one.

        Args:
            transform_gens (list): list of :class:`TransformGen` instance to
                be applied.
            img (ndarray): uint8 or floating point images with 1 or 3 channels.
            annotations (list): annotations
        Returns:
            ndarray: the transformed image
            TransformList: contain the transforms that's used.
        """

        if isinstance(self.transforms, dict):
            dataset_dict = {}
            for key, tfms in self.transforms.items():
                img = deepcopy(image)
                annos = deepcopy(annotations)
                for tfm in tfms:
                    img, annos = tfm(img, annos, **kwargs)
                dataset_dict[key] = (img, annos)
            return dataset_dict, None
        else:
            for tfm in self.transforms:
                image, annotations = tfm(image, annotations, **kwargs)

            return image, annotations

    def _filter_annotations(self, filter_empty=True, min_keypoints=0, proposal_files=None):
        """
        Load and prepare dataset dicts for instance detection/segmentation and
        semantic segmentation.

        Args:
            dataset_names (list[str]): a list of dataset names
            filter_empty (bool): whether to filter out images without instance annotations
            min_keypoints (int): filter out images with fewer keypoints than
                `min_keypoints`. Set to 0 to do nothing.
            proposal_files (list[str]): if given, a list of object proposal files
                that match each dataset in `dataset_names`.
        """
        dataset_dicts = self.dataset_dicts

        if proposal_files is not None:
            assert len(self.name) == len(proposal_files)
            # load precomputed proposals from proposal files
            dataset_dicts = [
                load_proposals_into_dataset(dataset_i_dicts, proposal_file)
                for dataset_i_dicts, proposal_file in zip(dataset_dicts, proposal_files)
            ]

        has_instances = "annotations" in dataset_dicts[0]
        # Keep images without instance-level GT if the dataset has semantic labels
        # unless the task is panoptic segmentation.
        if filter_empty and has_instances:
            dataset_dicts = filter_images_with_only_crowd_annotations(dataset_dicts)

        if min_keypoints > 0 and has_instances:
            dataset_dicts = filter_images_with_few_keypoints(dataset_dicts, min_keypoints)

        if has_instances:
            try:
                class_names = self.meta["thing_classes"]
                check_metadata_consistency("thing_classes", self.name, self.meta)
                print_instances_class_histogram(dataset_dicts, class_names)
            except AttributeError:  # class names are not available for this dataset
                pass

        return dataset_dicts

    def _set_group_flag(self):
        """Set flag according to image aspect ratio.
        Images with aspect ratio greater than 1 will be set as group 1,
        otherwise group 0.
        """
        self.aspect_ratios = np.zeros(len(self), dtype=np.uint8)
        if "width" in self.dataset_dicts[0] and "height" in self.dataset_dicts[0]:
            for i in range(len(self)):
                dataset_dict = self.dataset_dicts[i]
                if dataset_dict['width'] / dataset_dict['height'] > 1:
                    self.aspect_ratios[i] = 1

    def evaluate(self, predictions):
        """Dataset must provide a evaluation function to evaluate model."""
        raise NotImplementedError

    @property
    def ground_truth_annotations(self):
        raise NotImplementedError


def filter_images_with_only_crowd_annotations(dataset_dicts):
    """
    Filter out images with none annotations or only crowd annotations
    (i.e., images without non-crowd annotations).
    A common training-time preprocessing on COCO dataset.

    Args:
        dataset_dicts (list[dict]): annotations in cvpods Dataset format.

    Returns:
        list[dict]: the same format, but filtered.
    """
    num_before = len(dataset_dicts)

    def valid(anns):
        for ann in anns:
            if ann.get("iscrowd", 0) == 0:
                return True
        return False

    dataset_dicts = [x for x in dataset_dicts if valid(x["annotations"])]
    num_after = len(dataset_dicts)
    logger = logging.getLogger(__name__)
    logger.info(
        "Removed {} images with no usable annotations. {} images left.".format(
            num_before - num_after, num_after
        )
    )
    return dataset_dicts


def filter_images_with_few_keypoints(dataset_dicts, min_keypoints_per_image):
    """
    Filter out images with too few number of keypoints.

    Args:
        dataset_dicts (list[dict]): annotations in cvpods Dataset format.

    Returns:
        list[dict]: the same format as dataset_dicts, but filtered.
    """
    num_before = len(dataset_dicts)

    def visible_keypoints_in_image(dic):
        # Each keypoints field has the format [x1, y1, v1, ...], where v is visibility
        annotations = dic["annotations"]
        return sum(
            (np.array(ann["keypoints"][2::3]) > 0).sum()
            for ann in annotations if "keypoints" in ann
        )

    dataset_dicts = [
        x for x in dataset_dicts
        if visible_keypoints_in_image(x) >= min_keypoints_per_image
    ]
    num_after = len(dataset_dicts)
    logger = logging.getLogger(__name__)
    logger.info("Removed {} images with fewer than {} keypoints.".format(
        num_before - num_after, min_keypoints_per_image))
    return dataset_dicts


def load_proposals_into_dataset(dataset_dicts, proposal_file):
    r"""
    Load precomputed object proposals into the dataset.

    The proposal file should be a pickled dict with the following keys:

    - "ids": list[int] or list[str], the image ids
    - "boxes": list[np.ndarray], each is an Nx4 array of boxes corresponding to the image id
    - "objectness_logits": list[np.ndarray], each is an N sized array of objectness scores
      corresponding to the boxes.
    - "bbox_mode": the BoxMode of the boxes array. Defaults to ``BoxMode.XYXY_ABS``.

    Args:
        dataset_dicts (list[dict]): annotations in cvpods Dataset format.
        proposal_file (str): file path of pre-computed proposals, in pkl format.

    Returns:
        list[dict]: the same format as dataset_dicts, but added proposal field.
    """
    logger = logging.getLogger(__name__)
    logger.info("Loading proposals from: {}".format(proposal_file))

    with PathManager.open(proposal_file, "rb") as f:
        proposals = pickle.load(f, encoding="latin1")

    # Rename the key names in D1 proposal files
    rename_keys = {"indexes": "ids", "scores": "objectness_logits"}
    for key in rename_keys:
        if key in proposals:
            proposals[rename_keys[key]] = proposals.pop(key)

    # Fetch the indexes of all proposals that are in the dataset
    # Convert image_id to str since they could be int.
    img_ids = set({str(record["image_id"]) for record in dataset_dicts})
    id_to_index = {
        str(id): i
        for i, id in enumerate(proposals["ids"]) if str(id) in img_ids
    }

    # Assuming default bbox_mode of precomputed proposals are 'XYXY_ABS'
    bbox_mode = BoxMode(proposals["bbox_mode"]) if "bbox_mode" in proposals else BoxMode.XYXY_ABS

    for record in dataset_dicts:
        # Get the index of the proposal
        i = id_to_index[str(record["image_id"])]

        boxes = proposals["boxes"][i]
        objectness_logits = proposals["objectness_logits"][i]
        # Sort the proposals in descending order of the scores
        inds = objectness_logits.argsort()[::-1]
        record["proposal_boxes"] = boxes[inds]
        record["proposal_objectness_logits"] = objectness_logits[inds]
        record["proposal_bbox_mode"] = bbox_mode

    return dataset_dicts


def print_instances_class_histogram(dataset_dicts, class_names):
    """
    Args:
        dataset_dicts (list[dict]): list of dataset dicts.
        class_names (list[str]): list of class names (zero-indexed).
    """
    num_classes = len(class_names)
    hist_bins = np.arange(num_classes + 1)
    histogram = np.zeros((num_classes, ), dtype=np.int)
    for entry in dataset_dicts:
        annos = entry["annotations"]
        classes = [x["category_id"] for x in annos if not x.get("iscrowd", 0)]
        histogram += np.histogram(classes, bins=hist_bins)[0]

    N_COLS = min(6, len(class_names) * 2)

    def short_name(x):
        # make long class names shorter. useful for lvis
        if len(x) > 13:
            return x[:11] + ".."
        return x

    data = list(
        itertools.chain(
            *[[short_name(class_names[i]), int(v)]
              for i, v in enumerate(histogram)]))
    total_num_instances = sum(data[1::2])
    data.extend([None] * (N_COLS - (len(data) % N_COLS)))
    if num_classes > 1:
        data.extend(["total", total_num_instances])
    data = itertools.zip_longest(*[data[i::N_COLS] for i in range(N_COLS)])
    table = tabulate(
        data,
        headers=["category", "#instances"] * (N_COLS // 2),
        tablefmt="pipe",
        numalign="left",
        stralign="center",
    )
    log_first_n(
        logging.INFO,
        "Distribution of instances among all {} categories:\n".format(
            num_classes) + colored(table, "cyan"),
        key="message",
    )
```

### cvpods/data/wrapped_dataset.py

```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
from types import SimpleNamespace

import numpy as np

from torch.utils.data.dataset import ConcatDataset as _ConcatDataset

from .registry import DATASETS


@DATASETS.register()
class ConcatDataset(_ConcatDataset):
    """A wrapper of concatenated dataset.
    Same as :obj:`torch.utils.data.dataset.ConcatDataset`, but
    concat the group flag for image aspect ratio.
    Args:
        datasets (list[:obj:`Dataset`]): A list of datasets.
    """

    def __init__(self, datasets):
        super(ConcatDataset, self).__init__(datasets)
        if hasattr(self.datasets[0], 'aspect_ratios'):
            aspect_ratios = [d.aspect_ratios for d in self.datasets]
            self.aspect_ratios = np.concatenate(aspect_ratios)
        if hasattr(self.datasets[0], 'meta'):
            self.meta = {}
            for d in self.datasets:
                self.meta.update(d.meta)
            self.meta = SimpleNamespace(**self.meta)


@DATASETS.register()
class RepeatDataset(object):
    """A wrapper of repeated dataset.
    The length of repeated dataset will be `times` larger than the original
    dataset. This is useful when the data loading time is long but the dataset
    is small. Using RepeatDataset can reduce the data loading time between
    epochs.
    Args:
        dataset (:obj:`Dataset`): The dataset to be repeated.
        times (int): Repeat times.
    """

    def __init__(self, dataset, times):
        self.dataset = dataset
        self.times = times
        if hasattr(self.dataset, 'aspect_ratios'):
            self.aspect_ratios = np.tile(self.dataset.aspect_ratios, times)

        self._ori_len = len(self.dataset)

    def __getitem__(self, idx):
        return self.dataset[idx % self._ori_len]

    def __len__(self):
        return self.times * self._ori_len
```

### cvpods/data/registry.py

```python
from cvpods.utils import Registry

DATASETS = Registry("datasets")
TRANSFORMS = Registry("transforms")
SAMPLERS = Registry("samplers")
PATH_ROUTES = Registry("path_routes")
```

### cvpods/data/__init__.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from .build import build_dataset, build_test_loader, build_train_loader, build_transform_gens
from .registry import DATASETS, SAMPLERS, TRANSFORMS
from .wrapped_dataset import ConcatDataset, RepeatDataset

from . import transforms  # isort:skip
# ensure the builtin datasets are registered
from . import datasets, samplers  # isort:skip


__all__ = [k for k in globals().keys() if not k.startswith("_")]
```

### cvpods/data/detection_utils.py

```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

import logging

import cv2
import numpy as np
from PIL import Image
import pycocotools.mask as mask_util

import torch

from cvpods.structures import (
    BitMasks,
    Boxes,
    BoxMode,
    Instances,
    Keypoints,
    PolygonMasks,
    RotatedBoxes,
    polygons_to_bitmask
)
from cvpods.utils import PathManager

from . import transforms as T


class SizeMismatchError(ValueError):
    """
    When loaded image has difference width/height compared with annotation.
    """


# https://en.wikipedia.org/wiki/YUV#SDTV_with_BT.601
_M_RGB2YUV = [[0.299, 0.587, 0.114], [-0.14713, -0.28886, 0.436], [0.615, -0.51499, -0.10001]]
_M_YUV2RGB = [[1.0, 0.0, 1.13983], [1.0, -0.39465, -0.58060], [1.0, 2.03211, 0.0]]

# https://www.exiv2.org/tags.html
_EXIF_ORIENT = 274  # exif 'Orientation' tag


def convert_PIL_to_numpy(image, format):
    """
    Convert PIL image to numpy array of target format.
    Args:
        image (PIL.Image): a PIL image
        format (str): the format of output image
    Returns:
        (np.ndarray): also see `read_image`
    """
    if format is not None:
        # PIL only supports RGB, so convert to RGB and flip channels over below
        conversion_format = format
        if format in ["BGR", "YUV-BT.601"]:
            conversion_format = "RGB"
        image = image.convert(conversion_format)
    image = np.asarray(image)
    # PIL squeezes out the channel dimension for "L", so make it HWC
    if format == "L":
        image = np.expand_dims(image, -1)

    # handle formats not supported by PIL
    elif format == "BGR":
        # flip channels if needed
        image = image[:, :, ::-1]
    elif format == "YUV-BT.601":
        image = image / 255.0
        image = np.dot(image, np.array(_M_RGB2YUV).T)

    return image


def convert_image_to_rgb(image, format):
    """
    Convert an image from given format to RGB.
    Args:
        image (np.ndarray or Tensor): an HWC image
        format (str): the format of input image, also see `read_image`
    Returns:
        (np.ndarray): (H,W,3) RGB image in 0-255 range, can be either float or uint8
    """
    if isinstance(image, torch.Tensor):
        image = image.cpu().numpy()
    if format == "BGR":
        image = image[:, :, [2, 1, 0]]
    elif format == "YUV-BT.601":
        image = np.dot(image, np.array(_M_YUV2RGB).T)
        image = image * 255.0
    else:
        if format == "L":
            image = image[:, :, 0]
        image = image.astype(np.uint8)
        image = np.asarray(Image.fromarray(image, mode=format).convert("RGB"))
    return image


def _apply_exif_orientation(image):
    """
    Applies the exif orientation correctly.
    This code exists per the bug:
      https://github.com/python-pillow/Pillow/issues/3973
    with the function `ImageOps.exif_transpose`. The Pillow source raises errors with
    various methods, especially `tobytes`
    Function based on:
      https://github.com/wkentaro/labelme/blob/v4.5.4/labelme/utils/image.py#L59
      https://github.com/python-pillow/Pillow/blob/7.1.2/src/PIL/ImageOps.py#L527
    Args:
        image (PIL.Image): a PIL image
    Returns:
        (PIL.Image): the PIL image with exif orientation applied, if applicable
    """
    if not hasattr(image, "getexif"):
        return image

    try:
        exif = image.getexif()
    except Exception:  # https://github.com/facebookresearch/detectron2/issues/1885
        exif = None

    if exif is None:
        return image

    orientation = exif.get(_EXIF_ORIENT)

    method = {
        2: Image.FLIP_LEFT_RIGHT,
        3: Image.ROTATE_180,
        4: Image.FLIP_TOP_BOTTOM,
        5: Image.TRANSPOSE,
        6: Image.ROTATE_270,
        7: Image.TRANSVERSE,
        8: Image.ROTATE_90,
    }.get(orientation)

    if method is not None:
        return image.transpose(method)
    return image


def read_image(file_name, format=None):
    """
    Read an image into the given format.
    Will apply rotation and flipping if the image has such exif information.
    Args:
        file_name (str): image file path
        format (str): one of the supported image modes in PIL, or "BGR" or "YUV-BT.601".
    Returns:
        image (np.ndarray): an HWC image in the given format, which is 0-255, uint8 for
            supported image modes in PIL or "BGR"; float (0-1 for Y) for YUV-BT.601.
    """

    with PathManager.open(file_name, "rb") as f:
        image = Image.open(f)
        if format == ("RGB"):
            image = image.convert("RGB")
            return np.array(image)
        else:
            # work around this bug: https://github.com/python-pillow/Pillow/issues/3973
            image = _apply_exif_orientation(image)
            return convert_PIL_to_numpy(image, format)


def check_image_size(dataset_dict, image):
    """
    Raise an error if the image does not match the size specified in the dict.
    """
    if "width" in dataset_dict or "height" in dataset_dict:
        image_wh = (image.shape[1], image.shape[0])
        expected_wh = (dataset_dict["width"], dataset_dict["height"])
        if not image_wh == expected_wh:
            raise SizeMismatchError(
                "Mismatched image shape{}, got {}, expect {}.".format(
                    " for image " + dataset_dict["file_name"]
                    if "file_name" in dataset_dict
                    else "",
                    image_wh,
                    expected_wh,
                )
                + " Please check the width/height in your annotation."
            )

    # To ensure bbox always remap to original image size
    if "width" not in dataset_dict:
        dataset_dict["width"] = image.shape[1]
    if "height" not in dataset_dict:
        dataset_dict["height"] = image.shape[0]


def transform_proposals(dataset_dict, image_shape, transforms,
                        min_box_side_len, proposal_topk):
    """
    Apply transformations to the proposals in dataset_dict, if any.

    Args:
        dataset_dict (dict): a dict read from the dataset, possibly
            contains fields "proposal_boxes", "proposal_objectness_logits", "proposal_bbox_mode"
        image_shape (tuple): height, width
        transforms (TransformList):
        min_box_side_len (int): keep proposals with at least this size
        proposal_topk (int): only keep top-K scoring proposals

    The input dict is modified in-place, with abovementioned keys removed. A new
    key "proposals" will be added. Its value is an `Instances`
    object which contains the transformed proposals in its field
    "proposal_boxes" and "objectness_logits".
    """
    if "proposal_boxes" in dataset_dict:
        # Transform proposal boxes
        boxes = transforms.apply_box(
            BoxMode.convert(
                dataset_dict.pop("proposal_boxes"),
                dataset_dict.pop("proposal_bbox_mode"),
                BoxMode.XYXY_ABS,
            ))
        boxes = Boxes(boxes)
        objectness_logits = torch.as_tensor(
            dataset_dict.pop("proposal_objectness_logits").astype("float32"))

        boxes.clip(image_shape)
        keep = boxes.nonempty(threshold=min_box_side_len)
        boxes = boxes[keep]
        objectness_logits = objectness_logits[keep]

        proposals = Instances(image_shape)
        proposals.proposal_boxes = boxes[:proposal_topk]
        proposals.objectness_logits = objectness_logits[:proposal_topk]
        dataset_dict["proposals"] = proposals


def annotations_to_instances(annos, image_size, mask_format="polygon"):
    """
    Create an :class:`Instances` object used by the models,
    from instance annotations in the dataset dict.

    Args:
        annos (list[dict]): a list of instance annotations in one image, each
            element for one instance.
        image_size (tuple): height, width

    Returns:
        Instances:
            It will contain fields "gt_boxes", "gt_classes",
            "gt_masks", "gt_keypoints", if they can be obtained from `annos`.
            This is the format that builtin models expect.
    """
    boxes = [
        BoxMode.convert(obj["bbox"], obj["bbox_mode"], BoxMode.XYXY_ABS)
        for obj in annos
    ]
    target = Instances(image_size)
    boxes = target.gt_boxes = Boxes(boxes)
    boxes.clip(image_size)

    classes = [obj["category_id"] for obj in annos]
    classes = torch.tensor(classes, dtype=torch.int64)
    target.gt_classes = classes

    if len(annos) and "segmentation" in annos[0]:
        segms = [obj["segmentation"] for obj in annos]
        if mask_format == "polygon":
            masks = PolygonMasks(segms)
        else:
            assert mask_format == "bitmask", mask_format
            masks = []
            for segm in segms:
                if isinstance(segm, list):
                    # polygon
                    masks.append(polygons_to_bitmask(segm, *image_size))
                elif isinstance(segm, dict):
                    # COCO RLE
                    masks.append(mask_util.decode(segm))
                elif isinstance(segm, np.ndarray):
                    assert segm.ndim == 2, "Expect segmentation of 2 dimensions, got {}.".format(
                        segm.ndim)
                    # mask array
                    masks.append(segm)
                else:
                    raise ValueError(
                        "Cannot convert segmentation of type '{}' to BitMasks!"
                        "Supported types are: polygons as list[list[float] or ndarray],"
                        " COCO-style RLE as a dict, or a full-image segmentation mask "
                        "as a 2D ndarray.".format(type(segm)))
            # torch.from_numpy does not support array with negative stride.
            masks = BitMasks(
                torch.stack([
                    torch.from_numpy(np.ascontiguousarray(x)) for x in masks
                ]))
        target.gt_masks = masks

    if len(annos) and "keypoints" in annos[0]:
        kpts = np.array([obj.get("keypoints", []) for obj in annos])  # (N, K, 3)
        # Set all out-of-boundary points to "unlabeled"
        kpts_xy = kpts[:, :, :2]
        inside = (kpts_xy >= np.array([0, 0])) & (kpts_xy <= np.array(image_size[::-1]))
        inside = inside.all(axis=2)
        kpts[:, :, :2] = kpts_xy
        kpts[:, :, 2][~inside] = 0
        target.gt_keypoints = Keypoints(kpts)

    return target


def annotations_to_instances_rotated(annos, image_size):
    """
    Create an :class:`Instances` object used by the models,
    from instance annotations in the dataset dict.
    Compared to `annotations_to_instances`, this function is for rotated boxes only

    Args:
        annos (list[dict]): a list of instance annotations in one image, each
            element for one instance.
        image_size (tuple): height, width

    Returns:
        Instances:
            Containing fields "gt_boxes", "gt_classes",
            if they can be obtained from `annos`.
            This is the format that builtin models expect.
    """
    boxes = [obj["bbox"] for obj in annos]
    target = Instances(image_size)
    boxes = target.gt_boxes = RotatedBoxes(boxes)
    boxes.clip(image_size)

    classes = [obj["category_id"] for obj in annos]
    classes = torch.tensor(classes, dtype=torch.int64)
    target.gt_classes = classes

    return target


def filter_empty_instances(instances, by_box=True, by_mask=True):
    """
    Filter out empty instances in an `Instances` object.

    Args:
        instances (Instances):
        by_box (bool): whether to filter out instances with empty boxes
        by_mask (bool): whether to filter out instances with empty masks

    Returns:
        Instances: the filtered instances.
    """
    assert by_box or by_mask
    r = []
    if by_box:
        r.append(instances.gt_boxes.nonempty())
    if instances.has("gt_masks") and by_mask:
        r.append(instances.gt_masks.nonempty())

    # TODO: can also filter visible keypoints

    if not r:
        return instances
    m = r[0]
    for x in r[1:]:
        m = m & x
    return instances[m]


def create_keypoint_hflip_indices(dataset_names, meta):
    """
    Args:
        dataset_names (list[str]): list of dataset names
    Returns:
        ndarray[int]: a vector of size=#keypoints, storing the
        horizontally-flipped keypoint indices.
    """

    check_metadata_consistency("keypoint_names", dataset_names, meta)
    check_metadata_consistency("keypoint_flip_map", dataset_names, meta)

    names = meta["keypoint_names"]
    # TODO flip -> hflip
    flip_map = dict(meta["keypoint_flip_map"])
    flip_map.update({v: k for k, v in flip_map.items()})
    flipped_names = [i if i not in flip_map else flip_map[i] for i in names]
    flip_indices = [names.index(i) for i in flipped_names]
    return np.asarray(flip_indices)


def gen_crop_transform_with_instance(crop_size, image_size, instance):
    """
    Generate a CropTransform so that the cropping region contains
    the center of the given instance.

    Args:
        crop_size (tuple): h, w in pixels
        image_size (tuple): h, w
        instance (dict): an annotation dict of one instance, in cvpods's
            dataset format.
    """
    crop_size = np.asarray(crop_size, dtype=np.int32)
    bbox = BoxMode.convert(instance["bbox"], instance["bbox_mode"],
                           BoxMode.XYXY_ABS)
    center_yx = (bbox[1] + bbox[3]) * 0.5, (bbox[0] + bbox[2]) * 0.5

    assert (image_size[0] >= center_yx[0] and image_size[1] >= center_yx[1]
            ), "The annotation bounding box is outside of the image!"
    assert (image_size[0] >= crop_size[0] and image_size[1] >= crop_size[1]
            ), "Crop size is larger than image size!"

    min_yx = np.maximum(np.ceil(center_yx).astype(np.int32) - crop_size, 0)
    max_yx = np.maximum(np.asarray(image_size, dtype=np.int32) - crop_size, 0)
    max_yx = np.minimum(max_yx, np.floor(center_yx).astype(np.int32))

    y0 = np.random.randint(min_yx[0], max_yx[0] + 1)
    x0 = np.random.randint(min_yx[1], max_yx[1] + 1)
    return T.CropTransform(x0, y0, crop_size[1], crop_size[0])


def check_metadata_consistency(key, dataset_names, meta):
    """
    Check that the datasets have consistent metadata.

    Args:
        key (str): a metadata key
        dataset_names (list[str]): a list of dataset names

    Raises:
        AttributeError: if the key does not exist in the metadata
        ValueError: if the given datasets do not have the same metadata values defined by key
    """
    if len(dataset_names) == 0:
        return
    logger = logging.getLogger(__name__)
    entries_per_dataset = [meta.get(key) for d in dataset_names]
    for idx, entry in enumerate(entries_per_dataset):
        if entry != entries_per_dataset[0]:
            logger.error("Metadata '{}' for dataset '{}' is '{}'".format(
                key, dataset_names[idx], str(entry)))
            logger.error("Metadata '{}' for dataset '{}' is '{}'".format(
                key, dataset_names[0], str(entries_per_dataset[0])))
            raise ValueError(
                "Datasets have different metadata '{}'!".format(key))


def check_sample_valid(args):
    if args["sample_style"] == "range":
        assert (
            len(args["short_edge_length"]) == 2
        ), f"more than 2 ({len(args['short_edge_length'])}) " \
            "short_edge_length(s) are provided for ranges"


def imdecode(data, *, require_chl3=True, require_alpha=False):
    """decode images in common formats (jpg, png, etc.)
    :param data: encoded image data
    :type data: :class:`bytes`
    :param require_chl3: whether to convert gray image to 3-channel BGR image
    :param require_alpha: whether to add alpha channel to BGR image
    :rtype: :class:`numpy.ndarray`
    """
    img = cv2.imdecode(np.fromstring(data, np.uint8), cv2.IMREAD_UNCHANGED)

    if img is None and len(data) >= 3 and data[:3] == b'GIF':
        # cv2 doesn't support GIF, try PIL
        img = _gif_decode(data)

    assert img is not None, 'failed to decode'
    if img.ndim == 2 and require_chl3:
        img = img.reshape(img.shape + (1, ))
    if img.shape[2] == 1 and require_chl3:
        img = np.tile(img, (1, 1, 3))
    if img.ndim == 3 and img.shape[2] == 3 and require_alpha:
        assert img.dtype == np.uint8
        img = np.concatenate([img, np.ones_like(img[:, :, :1]) * 255], axis=2)
    return img


def _gif_decode(data):
    try:
        import io
        from PIL import Image

        im = Image.open(io.BytesIO(data))
        im = im.convert('RGB')
        return cv2.cvtColor(np.array(im), cv2.COLOR_RGB2BGR)
    except Exception:
        return
```

#### cvpods/data/datasets/coco.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

import contextlib
import copy
import datetime
import io
import json
import logging
import os
import os.path as osp

import numpy as np
from PIL import Image

import torch

from cvpods.structures import Boxes, BoxMode, PolygonMasks
from cvpods.utils import PathManager, Timer, file_lock

from ..base_dataset import BaseDataset
from ..detection_utils import (
    annotations_to_instances,
    check_image_size,
    create_keypoint_hflip_indices,
    filter_empty_instances,
    read_image
)
from ..registry import DATASETS
from .builtin_meta import _get_builtin_metadata
from .paths_route import _PREDEFINED_SPLITS_COCO

"""
This file contains functions to parse COCO-format annotations into dicts in "cvpods format".
"""

logger = logging.getLogger(__name__)


@DATASETS.register()
class COCODataset(BaseDataset):
    def __init__(self, cfg, dataset_name, transforms=[], is_train=True):
        super(COCODataset, self).__init__(cfg, dataset_name, transforms, is_train)

        if "panoptic" in dataset_name:
            self.task_key = "panoptic"      # for task: panoptic/semantic segmentation
        elif "keypoints" in dataset_name:
            self.task_key = "coco_person"   # for task: keypoints detection
        else:
            self.task_key = "coco"          # for task: instance detection/segmentation

        self.meta = self._get_metadata()

        if self.task_key in ["coco", "coco_person"]:
            self.dataset_dicts = self._load_annotations(
                self.meta["json_file"],
                self.meta["image_root"],
                dataset_name)
        elif self.task_key in ["panoptic"]:
            # panoptic segmentation task, support below dataset names:
            #  * coco_2017_train_panoptic_separated
            #  * coco_2017_val_panoptic_separated
            #  * coco_2017_val_100_panoptic_separated
            if "_separated" in dataset_name:
                self.dataset_dicts = self._load_annotations(
                    self.meta["json_file"],
                    self.meta["image_root"],
                    dataset_name
                )
                dicts4seg = load_sem_seg(
                    self.meta["sem_seg_root"],
                    self.meta["image_root"],
                )

                assert len(self.dataset_dicts) == len(dicts4seg), \
                    "len(self.dataset_dicts): {}, len(dicts4seg): {}".format(
                        len(self.dataset_dicts), len(dicts4seg))

                for idx, (dataset_dict, dict4seg) in enumerate(zip(self.dataset_dicts, dicts4seg)):
                    assert dataset_dict['file_name'] == dict4seg['file_name'], \
                        "idx: {}, dataset_dict['file_name']: {}, dict4seg['file_name']: {}".format(
                            idx, dataset_dict['file_name'], dict4seg['file_name'])

                    assert "sem_seg_file_name" not in dataset_dict

                    dataset_dict["sem_seg_file_name"] = dict4seg["sem_seg_file_name"]

            # semantic segmentation task, support below dataset names:
            #  * coco_2017_train_panoptic_stuffonly
            #  * coco_2017_val_panoptic_stuffonly
            #  * coco_2017_val_100_panoptic_stuffonly
            elif "_stuffonly" in dataset_name:
                self.dataset_dicts = load_sem_seg(
                    self.meta["sem_seg_root"],
                    self.meta["image_root"],
                )
            else:
                raise ValueError(f"Unknow dataset name: {self.name}.")

        # fmt: off
        self.data_format = cfg.INPUT.FORMAT
        self.mask_on = cfg.MODEL.MASK_ON
        self.mask_format = cfg.INPUT.MASK_FORMAT
        self.filter_empty = cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS
        self.keypoint_on = cfg.MODEL.KEYPOINT_ON
        self.load_proposals = cfg.MODEL.LOAD_PROPOSALS
        self.proposal_files = cfg.DATASETS.PROPOSAL_FILES_TRAIN
        # fmt: on
        ######################
        self.mosaic = cfg.INPUT.get('MOSAIC', None)
        ######################

        if is_train:
            # Remove images without instance-level GT even though the dataset has semantic labels.
            self.dataset_dicts = self._filter_annotations(
                filter_empty=self.filter_empty,
                min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE
                if self.keypoint_on else 0,
                proposal_files=self.proposal_files if self.load_proposals else None,
            )
            self._set_group_flag()

        self.eval_with_gt = cfg.TEST.get("WITH_GT", False)

        if self.keypoint_on:
            # Flip only makes sense in training
            self.keypoint_hflip_indices = create_keypoint_hflip_indices(
                cfg.DATASETS.TRAIN, self.meta)
        else:
            self.keypoint_hflip_indices = None

    def __getitem__(self, index):
        """Load data, apply transforms, converto to Instances.
        """
        dataset_dict = copy.deepcopy(self.dataset_dicts[index])

        # read image
        image = read_image(dataset_dict["file_name"], format=self.data_format)
        check_image_size(dataset_dict, image)

        if "annotations" in dataset_dict:
            annotations = dataset_dict.pop("annotations")
            annotations = [
                ann for ann in annotations if ann.get("iscrowd", 0) == 0]
        else:
            annotations = None

        if "sem_seg_file_name" in dataset_dict:
            if annotations is None:
                annotations = []
            with PathManager.open(dataset_dict.get("sem_seg_file_name"), "rb") as f:
                sem_seg_gt = Image.open(f)
                sem_seg_gt = np.asarray(sem_seg_gt, dtype="uint8")

            annotations.insert(0, {"sem_seg": sem_seg_gt})

        # apply transfrom
        image, annotations = self._apply_transforms(
            image, annotations, keypoint_hflip_indices=self.keypoint_hflip_indices)

        # mosaic transform
        mosaic_flag = np.random.randint(2)
        if self.is_train and self.mosaic is not None and mosaic_flag == 1:
            min_offset = self.mosaic.get('MIN_OFFSET', 0.2)
            mosaic_width = self.mosaic.get('MOSAIC_WIDTH', 640)
            mosaic_height = self.mosaic.get('MOSAIC_HEIGHT', 640)
            cut_x = np.random.randint(int(mosaic_width * min_offset),
                                      int(mosaic_width * (1 - min_offset)))
            cut_y = np.random.randint(int(mosaic_height * min_offset),
                                      int(mosaic_height * (1 - min_offset)))
            # init out image
            out_image = np.zeros([mosaic_height, mosaic_width, 3],
                                 dtype=np.float32)
            out_annotations = []
            # mosaic transform
            for m_idx in range(4):
                if m_idx != 0:
                    new_index = np.random.choice(
                        range(len(self.dataset_dicts)))
                    dataset_dict = copy.deepcopy(self.dataset_dicts[new_index])
                    # read image
                    image = read_image(dataset_dict["file_name"],
                                       format=self.data_format)
                    check_image_size(dataset_dict, image)
                    if "annotations" in dataset_dict:
                        annotations = dataset_dict.pop("annotations")
                        annotations = [
                            ann for ann in annotations if
                            ann.get("iscrowd", 0) == 0]
                    else:
                        annotations = None
                    # apply transfrom
                    image, annotations = self._apply_transforms(image,
                                                                annotations)

                image_size = image.shape[:2]  # h, w
                # as all meta_infos are the same, we just keep the first one
                meta_infos = \
                [annotation.pop("meta_infos") for annotation in annotations][0]
                pleft = meta_infos.get('jitter_pad_left', 0)
                pright = meta_infos.get('jitter_pad_right', 0)
                ptop = meta_infos.get('jitter_pad_top', 0)
                pbot = meta_infos.get('jitter_pad_bot', 0)
                swidth = meta_infos.get('jitter_swidth', image_size[1])
                sheight = meta_infos.get('jitter_sheight', image_size[0])
                # get shifts
                left_shift = int(
                    min(cut_x, max(0, (-int(pleft) * image_size[1] / swidth))))
                top_shift = int(
                    min(cut_y, max(0, (-int(ptop) * image_size[0] / sheight))))
                right_shift = int(min(image_size[1] - cut_x, max(0, (
                            -int(pright) * image_size[1] / swidth))))
                bot_shift = int(min(image_size[0] - cut_y, max(0, (
                            -int(pbot) * image_size[0] / sheight))))
                out_image, annos = self._blend_moasic(cut_x, cut_y,
                                                      out_image, image,
                                                      copy.deepcopy(
                                                          annotations),
                                                      (mosaic_height,
                                                       mosaic_width), m_idx,
                                                      (left_shift, top_shift,
                                                       right_shift, bot_shift))
                out_annotations.extend(annos)
            # replace image and annotation with out_image and out_annotation
            image = out_image
            annotations = out_annotations

        if "sem_seg_file_name" in dataset_dict:
            dataset_dict.pop("sem_seg_file_name")
            sem_seg_gt = annotations[0].pop("sem_seg")
            sem_seg_gt = torch.as_tensor(sem_seg_gt.astype("long"))
            dataset_dict["sem_seg"] = sem_seg_gt
            annotations = annotations[1:]

            if not annotations:
                annotations = None

        if annotations is not None:  # got instances in annotations
            image_shape = image.shape[:2]  # h, w

            instances = annotations_to_instances(
                annotations, image_shape, mask_format=self.mask_format
            )

            # # Create a tight bounding box from masks, useful when image is cropped
            # if self.crop_gen and instances.has("gt_masks"):
            #     instances.gt_boxes = instances.gt_masks.get_bounding_boxes()

            dataset_dict["instances"] = filter_empty_instances(instances)

        # convert to Instance type
        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,
        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.
        # Therefore it's important to use torch.Tensor.
        # h, w, c -> c, h, w
        dataset_dict["image"] = torch.as_tensor(
            np.ascontiguousarray(image.transpose(2, 0, 1)))

        return dataset_dict

    def __reset__(self):
        raise NotImplementedError

    def __len__(self):
        return len(self.dataset_dicts)

    def _apply_boxes(self, annotations, left_shift, top_shift, cut_width,
                     cut_height, cut_start_x, cut_start_y):
        for annotation in annotations:
            bboxes = BoxMode.convert(annotation["bbox"],
                                     annotation["bbox_mode"],
                                     BoxMode.XYXY_ABS)
            bboxes = np.asarray(bboxes)
            bboxes[0::2] -= left_shift
            bboxes[1::2] -= top_shift

            bboxes[0::2] = np.clip(bboxes[0::2], 0, cut_width)
            bboxes[1::2] = np.clip(bboxes[1::2], 0, cut_height)
            bboxes[0::2] += cut_start_x
            bboxes[1::2] += cut_start_y
            annotation["bbox"] = bboxes
            annotation["bbox_mode"] = BoxMode.XYXY_ABS
        return annotations

    def _blend_moasic(self, cut_x, cut_y, target_img, img, annos, img_size, blend_index, four_shifts):
        h, w = img_size
        img_h, img_w = img.shape[:2]
        left_shift = min(four_shifts[0], img_w - cut_x)
        top_shift = min(four_shifts[1], img_h - cut_y)
        right_shift = min(four_shifts[2], img_w - (w - cut_x))
        bot_shift = min(four_shifts[3], img_h - (h - cut_y))

        if blend_index == 0:
            annos = self._apply_boxes(annos,
                                    left_shift, top_shift,
                                    cut_x, cut_y,
                                    0, 0)
            target_img[:cut_y, :cut_x] = img[top_shift:top_shift + cut_y,
                                            left_shift:left_shift + cut_x]
        if blend_index == 1:
            annos = self._apply_boxes(annos,
                                    img_w + cut_x - w - right_shift, top_shift,
                                    w - cut_x, cut_y,
                                    cut_x, 0)
            target_img[:cut_y, cut_x:] = \
                img[top_shift:top_shift + cut_y, img_w + cut_x - w - right_shift:img_w - right_shift]
        if blend_index == 2:
            annos = self._apply_boxes(annos,
                                    left_shift, img_h + cut_y - h - bot_shift,
                                    cut_x, h - cut_y,
                                    0, cut_y)
            target_img[cut_y:, :cut_x] = \
                img[img_h + cut_y - h - bot_shift:img_h - bot_shift, left_shift:left_shift + cut_x]
        if blend_index == 3:
            annos = self._apply_boxes(annos,
                                    img_w + cut_x - w - right_shift, img_h + cut_y - h - bot_shift,
                                    w - cut_x, h - cut_y,
                                    cut_x, cut_y)
            target_img[cut_y:, cut_x:] = \
                img[img_h + cut_y - h - bot_shift:img_h - bot_shift, img_w + cut_x - w - right_shift:img_w - right_shift]
        return target_img, annos


    def _load_annotations(self,
                          json_file,
                          image_root,
                          dataset_name=None,
                          extra_annotation_keys=None):
        """
        Load a json file with COCO's instances annotation format.
        Currently supports instance detection, instance segmentation,
        and person keypoints annotations.

        Args:
            json_file (str): full path to the json file in COCO instances annotation format.
            image_root (str): the directory where the images in this json file exists.
            dataset_name (str): the name of the dataset (e.g., coco_2017_train).
                If provided, this function will also put "thing_classes" into
                the metadata associated with this dataset.
            extra_annotation_keys (list[str]): list of per-annotation keys that should also be
                loaded into the dataset dict (besides "iscrowd", "bbox", "keypoints",
                "category_id", "segmentation"). The values for these keys will be returned as-is.
                For example, the densepose annotations are loaded in this way.

        Returns:
            list[dict]: a list of dicts in cvpods standard format. (See
            `Using Custom Datasets </tutorials/datasets.html>`_ )

        Notes:
            1. This function does not read the image files.
            The results do not have the "image" field.
        """
        from pycocotools.coco import COCO

        timer = Timer()
        json_file = PathManager.get_local_path(json_file)
        with contextlib.redirect_stdout(io.StringIO()):
            coco_api = COCO(json_file)
        if timer.seconds() > 1:
            logger.info("Loading {} takes {:.2f} seconds.".format(
                json_file, timer.seconds()))

        id_map = None
        if dataset_name is not None:
            cat_ids = sorted(coco_api.getCatIds())
            cats = coco_api.loadCats(cat_ids)
            # The categories in a custom json file may not be sorted.
            thing_classes = [
                c["name"] for c in sorted(cats, key=lambda x: x["id"])
            ]
            self.meta["thing_classes"] = thing_classes

            # In COCO, certain category ids are artificially removed,
            # and by convention they are always ignored.
            # We deal with COCO's id issue and translate
            # the category ids to contiguous ids in [0, 80).

            # It works by looking at the "categories" field in the json, therefore
            # if users' own json also have incontiguous ids, we'll
            # apply this mapping as well but print a warning.
            if not (min(cat_ids) == 1 and max(cat_ids) == len(cat_ids)):
                if "coco" not in dataset_name:
                    logger.warning("""
    Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.
    """)
            id_map = {v: i for i, v in enumerate(cat_ids)}
            self.meta["thing_dataset_id_to_contiguous_id"] = id_map

        # sort indices for reproducible results
        img_ids = sorted(coco_api.imgs.keys())
        # imgs is a list of dicts, each looks something like:
        # {'license': 4,
        #  'url': 'http://farm6.staticflickr.com/5454/9413846304_881d5e5c3b_z.jpg',
        #  'file_name': 'COCO_val2014_000000001268.jpg',
        #  'height': 427,
        #  'width': 640,
        #  'date_captured': '2013-11-17 05:57:24',
        #  'id': 1268}
        imgs = coco_api.loadImgs(img_ids)
        # anns is a list[list[dict]], where each dict is an annotation
        # record for an object. The inner list enumerates the objects in an image
        # and the outer list enumerates over images. Example of anns[0]:
        # [{'segmentation': [[192.81,
        #     247.09,
        #     ...
        #     219.03,
        #     249.06]],
        #   'area': 1035.749,
        #   'iscrowd': 0,
        #   'image_id': 1268,
        #   'bbox': [192.81, 224.8, 74.73, 33.43],
        #   'category_id': 16,
        #   'id': 42986},
        #  ...]
        anns = [coco_api.imgToAnns[img_id] for img_id in img_ids]

        if "minival" not in json_file:
            # The popular valminusminival & minival annotations for COCO2014 contain this bug.
            # However the ratio of buggy annotations there is tiny and does not affect accuracy.
            # Therefore we explicitly white-list them.
            ann_ids = [
                ann["id"] for anns_per_image in anns for ann in anns_per_image
            ]
            assert len(set(ann_ids)) == len(
                ann_ids), "Annotation ids in '{}' are not unique!".format(
                    json_file)

        imgs_anns = list(zip(imgs, anns))

        logger.info("Loaded {} images in COCO format from {}".format(
            len(imgs_anns), json_file))

        dataset_dicts = []

        ann_keys = ["iscrowd", "bbox", "keypoints", "category_id"
                    ] + (extra_annotation_keys or [])

        num_instances_without_valid_segmentation = 0

        for (img_dict, anno_dict_list) in imgs_anns:
            record = {}
            record["file_name"] = os.path.join(image_root,
                                               img_dict["file_name"])
            record["height"] = img_dict["height"]
            record["width"] = img_dict["width"]
            image_id = record["image_id"] = img_dict["id"]

            objs = []
            for anno in anno_dict_list:
                # Check that the image_id in this annotation is the same as
                # the image_id we're looking at.
                # This fails only when the data parsing logic or the annotation file is buggy.

                # The original COCO valminusminival2014 & minival2014 annotation files
                # actually contains bugs that, together with certain ways of using COCO API,
                # can trigger this assertion.
                assert anno["image_id"] == image_id
                assert anno.get("ignore", 0) == 0

                obj = {key: anno[key] for key in ann_keys if key in anno}

                segm = anno.get("segmentation", None)
                if segm:    # either list[list[float]] or dict(RLE)
                    if not isinstance(segm, dict):
                        # filter out invalid polygons (< 3 points)
                        segm = [
                            poly for poly in segm
                            if len(poly) % 2 == 0 and len(poly) >= 6
                        ]
                        if len(segm) == 0:
                            num_instances_without_valid_segmentation += 1
                            continue    # ignore this instance
                    obj["segmentation"] = segm

                keypts = anno.get("keypoints", None)
                if keypts:    # list[int]
                    for idx, v in enumerate(keypts):
                        if idx % 3 != 2:
                            # COCO's segmentation coordinates are floating points in [0, H or W],
                            # but keypoint coordinates are integers in [0, H-1 or W-1]
                            # Therefore we assume the coordinates are "pixel indices" and
                            # add 0.5 to convert to floating point coordinates.
                            keypts[idx] = v + 0.5
                    obj["keypoints"] = keypts

                obj["bbox_mode"] = BoxMode.XYWH_ABS
                if id_map:
                    obj["category_id"] = id_map[obj["category_id"]]
                objs.append(obj)
            record["annotations"] = objs
            dataset_dicts.append(record)

        if num_instances_without_valid_segmentation > 0:
            logger.warning(
                "Filtered out {} instances without valid segmentation. "
                "There might be issues in your dataset generation process.".
                format(num_instances_without_valid_segmentation))
        return dataset_dicts

    def _get_metadata(self):
        if self.task_key in ["coco", "coco_person"]:
            meta = _get_builtin_metadata(self.task_key)
            image_root, json_file = _PREDEFINED_SPLITS_COCO[self.task_key][self.name]
            meta["image_root"] = osp.join(self.data_root, image_root) \
                if "://" not in image_root else image_root
            meta["json_file"] = osp.join(self.data_root, json_file) \
                if "://" not in image_root else osp.join(image_root, json_file)
            meta["evaluator_type"] = _PREDEFINED_SPLITS_COCO["evaluator_type"][self.task_key]
        elif self.task_key in ["panoptic"]:
            meta = _get_builtin_metadata("coco_panoptic_separated")
            prefix_instances = self.name[: -len("_panoptic_separated")]
            prefix_panoptic = self.name[: -len("_separated")]
            eval_key = self.name[-len("panoptic_separated"):]

            image_root, json_file = _PREDEFINED_SPLITS_COCO["coco"][prefix_instances]
            panoptic_root, panoptic_json, semantic_root = \
                _PREDEFINED_SPLITS_COCO[self.task_key][prefix_panoptic]
            meta["image_root"] = osp.join(self.data_root, image_root) \
                if "://" not in image_root else image_root
            meta["sem_seg_root"] = os.path.join(self.data_root, semantic_root) \
                if "://" not in semantic_root else semantic_root
            meta["evaluator_type"] = _PREDEFINED_SPLITS_COCO["evaluator_type"][eval_key]

            if "_separated" in self.name:
                meta["json_file"] = osp.join(self.data_root, json_file) \
                    if "://" not in image_root else osp.join(image_root, json_file)
                meta["panoptic_root"] = os.path.join(self.data_root, panoptic_root) \
                    if "://" not in panoptic_root else panoptic_root
                meta["panoptic_json"] = os.path.join(self.data_root, panoptic_json) \
                    if "://" not in panoptic_root else osp.join(panoptic_root, panoptic_json)
        return meta

    def evaluate(self, predictions):
        """Dataset must provide a evaluation function to evaluate model."""
        raise NotImplementedError

    @property
    def ground_truth_annotations(self):
        return self.dataset_dicts


# TODO this function is not specific to COCO, except for the "image_id" logic.
def load_sem_seg(gt_root, image_root, gt_ext="png", image_ext="jpg"):
    """
    Load semantic segmentation datasets. All files under "gt_root" with "gt_ext" extension are
    treated as ground truth annotations and all files under "image_root" with "image_ext" extension
    as input images. Ground truth and input images are matched using file paths relative to
    "gt_root" and "image_root" respectively without taking into account file extensions.
    This works for COCO as well as some other datasets.

    Args:
        gt_root (str): full path to ground truth semantic segmentation files. Semantic segmentation
            annotations are stored as images with integer values in pixels that represent
            corresponding semantic labels.
        image_root (str): the directory where the input images are.
        gt_ext (str): file extension for ground truth annotations.
        image_ext (str): file extension for input images.

    Returns:
        list[dict]:
            a list of dicts in cvpods standard format without instance-level
            annotation.

    Notes:
        1. This function does not read the image and ground truth files.
           The results do not have the "image" and "sem_seg" fields.
    """

    # We match input images with ground truth based on their relative filepaths (without file
    # extensions) starting from 'image_root' and 'gt_root' respectively.
    def file2id(folder_path, file_path):
        # extract relative path starting from `folder_path`
        image_id = os.path.normpath(
            os.path.relpath(file_path, start=folder_path))
        # remove file extension
        image_id = os.path.splitext(image_id)[0]
        return image_id

    input_files = sorted(
        (os.path.join(image_root, f)
         for f in PathManager.ls(image_root) if f.endswith(image_ext)),
        key=lambda file_path: file2id(image_root, file_path),
    )
    gt_files = sorted(
        (os.path.join(gt_root, f)
         for f in PathManager.ls(gt_root) if f.endswith(gt_ext)),
        key=lambda file_path: file2id(gt_root, file_path),
    )

    assert len(gt_files) > 0, "No annotations found in {}.".format(gt_root)

    # Use the intersection, so that val2017_100 annotations can run smoothly with val2017 images
    if len(input_files) != len(gt_files):
        logger.warn(
            "Directory {} and {} has {} and {} files, respectively.".format(
                image_root, gt_root, len(input_files), len(gt_files)))
        input_basenames = [
            os.path.basename(f)[:-len(image_ext)] for f in input_files
        ]
        gt_basenames = [os.path.basename(f)[:-len(gt_ext)] for f in gt_files]
        intersect = list(set(input_basenames) & set(gt_basenames))
        # sort, otherwise each worker may obtain a list[dict] in different order
        intersect = sorted(intersect)
        logger.warn("Will use their intersection of {} files.".format(
            len(intersect)))
        input_files = [
            os.path.join(image_root, f + image_ext) for f in intersect
        ]
        gt_files = [os.path.join(gt_root, f + gt_ext) for f in intersect]

    logger.info("Loaded {} images with semantic segmentation from {}".format(
        len(input_files), image_root))

    dataset_dicts = []
    for (img_path, gt_path) in zip(input_files, gt_files):
        record = {}
        record["file_name"] = img_path
        record["sem_seg_file_name"] = gt_path
        dataset_dicts.append(record)

    return dataset_dicts


def convert_to_coco_dict(dataset_name, dataset_dicts, metadata):
    """
    Convert a dataset in cvpods's standard format into COCO json format
    COCO data format description can be found here:
    http://cocodataset.org/#format-data
    Args:
        dataset_name:
            name of the source dataset
            must be registered in DatastCatalog and in cvpods's standard format
    Returns:
        coco_dict: serializable dict in COCO json format
    """
    if dataset_name not in [
        "citypersons_train",
        "citypersons_val",
        "crowdhuman_train",
        "crowdhuman_val",
        "coco_2017_train",
        "coco_2017_val",
        "widerface_2019_train",
        "widerface_2019_val"
    ]:
        raise NotImplementedError("Dataset name '{}' not supported".format(dataset_name))

    # unmap the category mapping ids for COCO
    if hasattr(metadata, "thing_dataset_id_to_contiguous_id"):
        reverse_id_mapping = {
            v: k
            for k, v in metadata.thing_dataset_id_to_contiguous_id.items()
        }

        def reverse_id_mapper(contiguous_id): return reverse_id_mapping[contiguous_id]  # noqa
    else:
        def reverse_id_mapper(contiguous_id): return contiguous_id    # noqa

    categories = [{
        "id": reverse_id_mapper(id),
        "name": name
    } for id, name in enumerate(metadata.thing_classes)]

    logger.info("Converting dataset dicts into COCO format")
    coco_images = []
    coco_annotations = []

    for image_id, image_dict in enumerate(dataset_dicts):
        coco_image = {
            "id": image_dict.get("image_id", image_id),
            "width": image_dict["width"],
            "height": image_dict["height"],
            "file_name": image_dict["file_name"],
        }
        coco_images.append(coco_image)

        anns_per_image = image_dict["annotations"]
        for annotation in anns_per_image:
            # create a new dict with only COCO fields
            coco_annotation = {}

            # COCO requirement: XYWH box format
            bbox = annotation["bbox"]
            bbox_mode = annotation["bbox_mode"]
            bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYWH_ABS)

            # COCO requirement: instance area
            if "segmentation" in annotation:
                # Computing areas for instances by counting the pixels
                segmentation = annotation["segmentation"]
                # TODO: check segmentation type: RLE, BinaryMask or Polygon
                polygons = PolygonMasks([segmentation])
                area = polygons.area()[0].item()
            else:
                # Computing areas using bounding boxes
                bbox_xy = BoxMode.convert(bbox, BoxMode.XYWH_ABS,
                                          BoxMode.XYXY_ABS)
                area = Boxes([bbox_xy]).area()[0].item()

            if "keypoints" in annotation:
                keypoints = annotation["keypoints"]    # list[int]
                for idx, v in enumerate(keypoints):
                    if idx % 3 != 2:
                        # COCO's segmentation coordinates are floating points in [0, H or W],
                        # but keypoint coordinates are integers in [0, H-1 or W-1]
                        # For COCO format consistency we substract 0.5
                        # https://github.com/facebookresearch/detectron2/pull/175#issuecomment-551202163
                        keypoints[idx] = v - 0.5
                if "num_keypoints" in annotation:
                    num_keypoints = annotation["num_keypoints"]
                else:
                    num_keypoints = sum(kp > 0 for kp in keypoints[2::3])

            # COCO requirement:
            #   linking annotations to images
            #   "id" field must start with 1
            coco_annotation["id"] = len(coco_annotations) + 1
            coco_annotation["image_id"] = coco_image["id"]
            coco_annotation["bbox"] = [round(float(x), 3) for x in bbox]
            coco_annotation["area"] = area
            coco_annotation["category_id"] = reverse_id_mapper(
                annotation["category_id"])
            coco_annotation["iscrowd"] = annotation.get("iscrowd", 0)

            # Add optional fields
            if "keypoints" in annotation:
                coco_annotation["keypoints"] = keypoints
                coco_annotation["num_keypoints"] = num_keypoints

            if "segmentation" in annotation:
                coco_annotation["segmentation"] = annotation["segmentation"]

            coco_annotations.append(coco_annotation)

    logger.info(
        "Conversion finished, "
        f"num images: {len(coco_images)}, num annotations: {len(coco_annotations)}"
    )

    info = {
        "date_created": str(datetime.datetime.now()),
        "description":
        "Automatically generated COCO json file for cvpods.",
    }
    coco_dict = {
        "info": info,
        "images": coco_images,
        "annotations": coco_annotations,
        "categories": categories,
        "licenses": None,
    }
    return coco_dict


def convert_to_coco_json(dataset_name, output_file, allow_cached=True):
    """
    Converts dataset into COCO format and saves it to a json file.
    dataset_name must be registered in DatasetCatalog and in cvpods's standard format.
    Args:
        dataset_name:
            reference from the config file to the catalogs
            must be registered in DatasetCatalog and in cvpods's standard format
        output_file: path of json file that will be saved to
        allow_cached: if json file is already present then skip conversion
    """

    # TODO: The dataset or the conversion script *may* change,
    # a checksum would be useful for validating the cached data

    PathManager.mkdirs(os.path.dirname(output_file))
    with file_lock(output_file):
        if PathManager.exists(output_file) and allow_cached:
            logger.info(
                f"Cached annotations in COCO format already exist: {output_file}"
            )
        else:
            logger.info(
                f"Converting dataset annotations in '{dataset_name}' to COCO format ...)"
            )
            coco_dict = convert_to_coco_dict(dataset_name)

            with PathManager.open(output_file, "w") as json_file:
                logger.info(
                    f"Caching annotations in COCO format: {output_file}")
                json.dump(coco_dict, json_file)
```

#### cvpods/data/datasets/cityscapes.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

import copy
import functools
import glob
import json
import logging
import multiprocessing as mp
import os
import os.path as osp
from itertools import chain

import numpy as np
from PIL import Image
import pycocotools.mask as mask_util

import torch

from cvpods.structures import BoxMode
from cvpods.utils import PathManager, comm

from ..base_dataset import BaseDataset
from ..detection_utils import (
    annotations_to_instances,
    check_image_size,
    create_keypoint_hflip_indices,
    filter_empty_instances,
    read_image
)
from ..registry import DATASETS
from .builtin_meta import _get_builtin_metadata
from .paths_route import _PREDEFINED_SPLITS_CITYSCAPES

try:
    import cv2  # noqa
except ImportError:
    # OpenCV is an optional dependency at the moment
    pass

"""
This file contains functions to parse COCO-format annotations into dicts in "cvpods format".
"""

logger = logging.getLogger(__name__)


@DATASETS.register()
class CityScapesDataset(BaseDataset):

    def __init__(self, cfg, dataset_name, transforms=[], is_train=True):
        super(CityScapesDataset, self).__init__(cfg, dataset_name, transforms, is_train)
        info = self.name.split("_")
        self.task = info[info.index("seg") - 1]
        assert self.task in ["instance", "sem"], "unsupported task {}".format(self.task)

        image_root, json_file = _PREDEFINED_SPLITS_CITYSCAPES["cityscapes"][self.name]
        self.json_file = osp.join(self.data_root, json_file) \
            if "://" not in image_root else osp.join(image_root, json_file)
        self.image_root = osp.join(self.data_root, image_root) \
            if "://" not in image_root else image_root

        self.meta = self._get_metadata()

        self.dataset_dicts = self._load_annotations(
            self.image_root,
            self.json_file,
        )

        # fmt: off
        self.data_format = cfg.INPUT.FORMAT
        self.mask_on = cfg.MODEL.MASK_ON
        self.mask_format = cfg.INPUT.MASK_FORMAT
        self.filter_empty = cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS
        self.keypoint_on = cfg.MODEL.KEYPOINT_ON
        self.load_proposals = cfg.MODEL.LOAD_PROPOSALS
        self.proposal_files = cfg.DATASETS.PROPOSAL_FILES_TRAIN
        # fmt: on

        if is_train:
            self.dataset_dicts = self._filter_annotations(
                filter_empty=self.filter_empty,
                min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE
                if self.keypoint_on else 0,
                proposal_files=self.proposal_files if self.load_proposals else None,
            )
            self._set_group_flag()

        self.eval_with_gt = cfg.TEST.get("WITH_GT", False)

        if self.keypoint_on:
            # Flip only makes sense in training
            self.keypoint_hflip_indices = create_keypoint_hflip_indices(
                cfg.DATASETS.TRAIN)
        else:
            self.keypoint_hflip_indices = None

    def __getitem__(self, index):
        """Load data, apply transforms, converto to Instances.
        """
        dataset_dict = copy.deepcopy(self.dataset_dicts[index])

        # read image
        image = read_image(dataset_dict["file_name"], format=self.data_format)
        check_image_size(dataset_dict, image)

        if "annotations" in dataset_dict:
            annotations = dataset_dict.pop("annotations")
            annotations = [
                ann for ann in annotations if ann.get("iscrowd", 0) == 0]
        else:
            annotations = None

        if "sem_seg_file_name" in dataset_dict:
            assert annotations is None
            annotations = []
            with PathManager.open(dataset_dict.get("sem_seg_file_name"), "rb") as f:
                sem_seg_gt = Image.open(f)
                sem_seg_gt = np.asarray(sem_seg_gt, dtype="uint8")
            annotations.append({"sem_seg": sem_seg_gt})

        # apply transfrom
        image, annotations = self._apply_transforms(
            image, annotations)

        if "sem_seg_file_name" in dataset_dict:
            dataset_dict.pop("sem_seg_file_name")
            sem_seg_gt = annotations[0].pop("sem_seg")
            sem_seg_gt = torch.as_tensor(sem_seg_gt.astype("long"))
            dataset_dict["sem_seg"] = sem_seg_gt
            annotations = None

        if annotations is not None:
            image_shape = image.shape[:2]  # h, w

            instances = annotations_to_instances(
                annotations, image_shape, mask_format=self.mask_format
            )

            # # Create a tight bounding box from masks, useful when image is cropped
            # if self.crop_gen and instances.has("gt_masks"):
            #     instances.gt_boxes = instances.gt_masks.get_bounding_boxes()

            dataset_dict["instances"] = filter_empty_instances(instances)

        # convert to Instance type
        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,
        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.
        # Therefore it's important to use torch.Tensor.
        # h, w, c -> c, h, w
        dataset_dict["image"] = torch.as_tensor(
            np.ascontiguousarray(image.transpose(2, 0, 1)))

        return dataset_dict

    def __reset__(self):
        raise NotImplementedError

    def __len__(self):
        return len(self.dataset_dicts)

    def _load_annotations(self, image_dir, gt_dir, from_json=True, to_polygons=True):
        if self.task == "instance":
            return self._load_instance_annotations(image_dir, gt_dir, from_json, to_polygons)
        else:
            return self._load_semantic_annotations(image_dir, gt_dir)

    def _load_instance_annotations(self, image_dir, gt_dir, from_json=True, to_polygons=True):
        """
        Args:
            image_dir (str): path to the raw dataset. e.g., "~/cityscapes/leftImg8bit/train".
            gt_dir (str): path to the raw annotations. e.g., "~/cityscapes/gtFine/train".
            from_json (bool): whether to read annotations from the raw json file or the png files.
            to_polygons (bool): whether to represent the segmentation as polygons
                (COCO's format) instead of masks (cityscapes's format).

        Returns:
            list[dict]: a list of dicts in cvpods standard format. (See
            `Using Custom Datasets </tutorials/datasets.html>`_ )
        """
        if from_json:
            assert to_polygons, (
                "Cityscapes's json annotations are in polygon format. "
                "Converting to mask format is not supported now."
            )
        files = []
        for image_file in glob.glob(os.path.join(image_dir, "**/*.png")):
            suffix = "leftImg8bit.png"
            assert image_file.endswith(suffix)
            prefix = image_dir
            instance_file = (gt_dir + image_file[len(prefix): -len(suffix)]
                             + "gtFine_instanceIds.png")
            assert os.path.isfile(instance_file), instance_file

            label_file = gt_dir + image_file[len(prefix): -len(suffix)] + "gtFine_labelIds.png"
            assert os.path.isfile(label_file), label_file

            json_file = gt_dir + image_file[len(prefix): -len(suffix)] + "gtFine_polygons.json"
            files.append((image_file, instance_file, label_file, json_file))
        assert len(files), "No images found in {}".format(image_dir)

        logger = logging.getLogger(__name__)
        logger.info("Preprocessing cityscapes annotations ...")
        # This is still not fast: all workers will execute duplicate works and will
        # take up to 10m on a 8GPU server.
        pool = mp.Pool(processes=max(mp.cpu_count() // comm.get_world_size() // 2, 4))

        ret = pool.map(
            functools.partial(
                cityscapes_files_to_dict,
                from_json=from_json,
                to_polygons=to_polygons
            ),
            files,
        )
        logger.info("Loaded {} images from {}".format(len(ret), image_dir))

        # Map cityscape ids to contiguous ids
        from cityscapesscripts.helpers.labels import labels

        labels = [label for label in labels if label.hasInstances and not label.ignoreInEval]
        dataset_id_to_contiguous_id = {l.id: idx for idx, l in enumerate(labels)}
        for dict_per_image in ret:
            for anno in dict_per_image["annotations"]:
                anno["category_id"] = dataset_id_to_contiguous_id[anno["category_id"]]
        return ret

    def _load_semantic_annotations(self, image_dir, gt_dir):
        """
        Args:
            image_dir (str): path to the raw dataset. e.g., "~/cityscapes/leftImg8bit/train".
            gt_dir (str): path to the raw annotations. e.g., "~/cityscapes/gtFine/train".

        Returns:
            list[dict]: a list of dict, each has "file_name" and
                "sem_seg_file_name".
        """
        ret = []
        for image_file in glob.glob(os.path.join(image_dir, "**/*.png")):
            suffix = "leftImg8bit.png"
            assert image_file.endswith(suffix)
            prefix = image_dir

            label_file = (gt_dir + image_file[len(prefix): -len(suffix)]
                          + "gtFine_labelTrainIds.png")
            assert os.path.isfile(
                label_file
            ), "Please generate labelTrainIds.png with cityscapesscripts/preparation/createTrainIdLabelImgs.py"  # noqa

            json_file = gt_dir + image_file[len(prefix): -len(suffix)] + "gtFine_polygons.json"

            with PathManager.open(json_file, "r") as f:
                jsonobj = json.load(f)
            ret.append(
                {
                    "file_name": image_file,
                    "sem_seg_file_name": label_file,
                    "height": jsonobj["imgHeight"],
                    "width": jsonobj["imgWidth"],
                }
            )
        return ret

    def _get_metadata(self):
        meta = _get_builtin_metadata("cityscapes")
        meta["evaluator_type"] = "cityscapes" if self.task == "instance" else "sem_seg"
        meta["image_dir"] = self.image_root
        meta["gt_dir"] = self.json_file

        return meta

    def evaluate(self, predictions):
        """Dataset must provide a evaluation function to evaluate model."""
        raise NotImplementedError

    @property
    def ground_truth_annotations(self):
        return self.dataset_dicts


def cityscapes_files_to_dict(files, from_json, to_polygons):
    """
    Parse cityscapes annotation files to a dict.

    Args:
        files (tuple): consists of (image_file, instance_id_file, label_id_file, json_file)
        from_json (bool): whether to read annotations from the raw json file or the png files.
        to_polygons (bool): whether to represent the segmentation as polygons
            (COCO's format) instead of masks (cityscapes's format).

    Returns:
        A dict in cvpods Dataset format.
    """
    from cityscapesscripts.helpers.labels import id2label, name2label

    image_file, instance_id_file, _, json_file = files

    annos = []

    if from_json:
        from shapely.geometry import MultiPolygon, Polygon

        with PathManager.open(json_file, "r") as f:
            jsonobj = json.load(f)
        ret = {
            "file_name": image_file,
            "image_id": os.path.basename(image_file),
            "height": jsonobj["imgHeight"],
            "width": jsonobj["imgWidth"],
        }

        # `polygons_union` contains the union of all valid polygons.
        polygons_union = Polygon()

        # CityscapesScripts draw the polygons in sequential order
        # and each polygon *overwrites* existing ones. See
        # (https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/preparation/json2instanceImg.py) # noqa
        # We use reverse order, and each polygon *avoids* early ones.
        # This will resolve the ploygon overlaps in the same way as CityscapesScripts.
        for obj in jsonobj["objects"][::-1]:
            if "deleted" in obj:  # cityscapes data format specific
                continue
            label_name = obj["label"]

            try:
                label = name2label[label_name]
            except KeyError:
                if label_name.endswith("group"):  # crowd area
                    label = name2label[label_name[: -len("group")]]
                else:
                    raise
            if label.id < 0:  # cityscapes data format
                continue

            # Cityscapes's raw annotations uses integer coordinates
            # Therefore +0.5 here
            poly_coord = np.asarray(obj["polygon"], dtype="f4") + 0.5
            # CityscapesScript uses PIL.ImageDraw.polygon to rasterize
            # polygons for evaluation. This function operates in integer space
            # and draws each pixel whose center falls into the polygon.
            # Therefore it draws a polygon which is 0.5 "fatter" in expectation.
            # We therefore dilate the input polygon by 0.5 as our input.
            poly = Polygon(poly_coord).buffer(0.5, resolution=4)

            if not label.hasInstances or label.ignoreInEval:
                # even if we won't store the polygon it still contributes to overlaps resolution
                polygons_union = polygons_union.union(poly)
                continue

            # Take non-overlapping part of the polygon
            poly_wo_overlaps = poly.difference(polygons_union)
            if poly_wo_overlaps.is_empty:
                continue
            polygons_union = polygons_union.union(poly)

            anno = {}
            anno["iscrowd"] = label_name.endswith("group")
            anno["category_id"] = label.id

            if isinstance(poly_wo_overlaps, Polygon):
                poly_list = [poly_wo_overlaps]
            elif isinstance(poly_wo_overlaps, MultiPolygon):
                poly_list = poly_wo_overlaps.geoms
            else:
                raise NotImplementedError("Unknown geometric structure {}".format(poly_wo_overlaps))

            poly_coord = []
            for poly_el in poly_list:
                # COCO API can work only with exterior boundaries now, hence we store only them.
                # TODO: store both exterior and interior boundaries once other parts of the
                # codebase support holes in polygons.
                poly_coord.append(list(chain(*poly_el.exterior.coords)))
            anno["segmentation"] = poly_coord
            (xmin, ymin, xmax, ymax) = poly_wo_overlaps.bounds

            anno["bbox"] = (xmin, ymin, xmax, ymax)
            anno["bbox_mode"] = BoxMode.XYXY_ABS

            annos.append(anno)
    else:
        # See also the official annotation parsing scripts at
        # https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/evaluation/instances2dict.py  # noqa
        with PathManager.open(instance_id_file, "rb") as f:
            inst_image = np.asarray(Image.open(f), order="F")
        # ids < 24 are stuff labels (filtering them first is about 5% faster)
        flattened_ids = np.unique(inst_image[inst_image >= 24])

        ret = {
            "file_name": image_file,
            "image_id": os.path.basename(image_file),
            "height": inst_image.shape[0],
            "width": inst_image.shape[1],
        }

        for instance_id in flattened_ids:
            # For non-crowd annotations, instance_id // 1000 is the label_id
            # Crowd annotations have <1000 instance ids
            label_id = instance_id // 1000 if instance_id >= 1000 else instance_id
            label = id2label[label_id]
            if not label.hasInstances or label.ignoreInEval:
                continue

            anno = {}
            anno["iscrowd"] = instance_id < 1000
            anno["category_id"] = label.id

            mask = np.asarray(inst_image == instance_id, dtype=np.uint8, order="F")

            inds = np.nonzero(mask)
            ymin, ymax = inds[0].min(), inds[0].max()
            xmin, xmax = inds[1].min(), inds[1].max()
            anno["bbox"] = (xmin, ymin, xmax, ymax)
            if xmax <= xmin or ymax <= ymin:
                continue
            anno["bbox_mode"] = BoxMode.XYXY_ABS
            if to_polygons:
                # This conversion comes from D4809743 and D5171122,
                # when Mask-RCNN was first developed.
                contours = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[
                    -2
                ]
                polygons = [c.reshape(-1).tolist() for c in contours if len(c) >= 3]
                # opencv's can produce invalid polygons
                if len(polygons) == 0:
                    continue
                anno["segmentation"] = polygons
            else:
                anno["segmentation"] = mask_util.encode(mask[:, :, None])[0]
            annos.append(anno)
    ret["annotations"] = annos
    return ret
```

#### cvpods/data/datasets/paths_route.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

from ..registry import PATH_ROUTES

"""
This file registers pre-defined datasets at hard-coded paths, and their metadata.

We hard-code metadata for common datasets. This will enable:
1. Consistency check when loading the datasets
2. Use models on these standard datasets directly and run demos,
   without having to download the dataset annotations

We hard-code some paths to the dataset that's assumed to
exist in "./datasets/".

Users SHOULD NOT use this file to create new dataset / metadata for new dataset.
To add new dataset, refer to the tutorial "docs/DATASETS.md".
"""

# ==== Predefined datasets and splits for COCO ==========

_PREDEFINED_SPLITS_COCO = {}
_PREDEFINED_SPLITS_COCO["dataset_type"] = "COCODataset"
_PREDEFINED_SPLITS_COCO["evaluator_type"] = {
    "coco": "coco",
    "coco_person": "coco",
    "panoptic_separated": "coco_panoptic_seg",
    "panoptic_stuffonly": "sem_seg",
}
_PREDEFINED_SPLITS_COCO["coco"] = {
    "coco_2014_train":
    ("coco/train2014", "coco/annotations/instances_train2014.json"),
    "coco_2014_val":
    ("coco/val2014", "coco/annotations/instances_val2014.json"),
    "coco_2014_minival":
    ("coco/val2014", "coco/annotations/instances_minival2014.json"),
    "coco_2014_minival_100":
    ("coco/val2014", "coco/annotations/instances_minival2014_100.json"),
    "coco_2014_valminusminival": (
        "coco/val2014",
        "coco/annotations/instances_valminusminival2014.json",
    ),
    "coco_2017_train": ("coco/train2017",
                        "coco/annotations/instances_train2017.json"),
    "coco_2017_val": ("coco/val2017",
                      "coco/annotations/instances_val2017.json"),
    "coco_2017_test": ("coco/test2017",
                       "coco/annotations/image_info_test2017.json"),
    "coco_2017_test-dev": ("coco/test2017",
                           "coco/annotations/image_info_test-dev2017.json"),
    "coco_2017_val_100": ("coco/val2017",
                          "coco/annotations/instances_val2017_100.json"),
}

_PREDEFINED_SPLITS_COCO["coco_person"] = {
    "coco_person_keypoints_2014_train": (
        "coco/train2014",
        "coco/annotations/person_keypoints_train2014.json",
    ),
    "coco_person_keypoints_2014_val":
    ("coco/val2014", "coco/annotations/person_keypoints_val2014.json"),
    "coco_person_keypoints_2014_minival": (
        "coco/val2014",
        "coco/annotations/person_keypoints_minival2014.json",
    ),
    "coco_person_keypoints_2014_valminusminival": (
        "coco/val2014",
        "coco/annotations/person_keypoints_valminusminival2014.json",
    ),
    "coco_person_keypoints_2014_minival_100": (
        "coco/val2014",
        "coco/annotations/person_keypoints_minival2014_100.json",
    ),
    "coco_person_keypoints_2017_train": (
        "coco/train2017",
        "coco/annotations/person_keypoints_train2017.json",
    ),
    "coco_person_keypoints_2017_val":
    ("coco/val2017", "coco/annotations/person_keypoints_val2017.json"),
    "coco_person_keypoints_2017_val_100": (
        "coco/val2017",
        "coco/annotations/person_keypoints_val2017_100.json",
    ),
}

_PREDEFINED_SPLITS_COCO["panoptic"] = {
    "coco_2017_train_panoptic": (
        # This is the original panoptic annotation directory
        "coco/panoptic_train2017",
        "coco/annotations/panoptic_train2017.json",
        # This directory contains semantic annotations that are
        # converted from panoptic annotations.
        # It is used by PanopticFPN.
        # You can use the script at cvpods/datasets/prepare_panoptic_fpn.py
        # to create these directories.
        "coco/panoptic_stuff_train2017",
    ),
    "coco_2017_val_panoptic": (
        "coco/panoptic_val2017",
        "coco/annotations/panoptic_val2017.json",
        "coco/panoptic_stuff_val2017",
    ),
    "coco_2017_val_100_panoptic": (
        "coco/panoptic_val2017_100",
        "coco/annotations/panoptic_val2017_100.json",
        "coco/panoptic_stuff_val2017_100",
    ),
}

PATH_ROUTES.register(_PREDEFINED_SPLITS_COCO, "COCO")

# ==== Predefined datasets and splits for LVIS ==========

_PREDEFINED_SPLITS_LVIS = {
    "dataset_type": "LVISDataset",
    "evaluator_type": {
        "lvis_v0.5": "lvis",
        "lvis_v0.5_cocofied": "lvis",
    },
    "lvis_v0.5": {
        "lvis_v0.5_train": ("coco/train2017", "lvis/lvis_v0.5_train.json"),
        "lvis_v0.5_val": ("coco/val2017", "lvis/lvis_v0.5_val.json"),
        "lvis_v0.5_val_rand_100": ("coco/val2017", "lvis/lvis_v0.5_val_rand_100.json"),
        "lvis_v0.5_test": ("coco/test2017", "lvis/lvis_v0.5_image_info_test.json"),
    },
    "lvis_v0.5_cocofied": {
        "lvis_v0.5_train_cocofied": ("coco/train2017", "lvis/lvis_v0.5_train_cocofied.json"),
        "lvis_v0.5_val_cocofied": ("coco/val2017", "lvis/lvis_v0.5_val_cocofied.json"),
    },
}

PATH_ROUTES.register(_PREDEFINED_SPLITS_LVIS, "LVIS")

# ==== Predefined splits for raw cityscapes images ===========

_PREDEFINED_SPLITS_CITYSCAPES = {
    "dataset_type": "CityScapesDataset",
    "evaluator_type": {
        "cityscapes_instance": "cityscapes",
        "cityscapes_sem": "sem_seg",
    },
    "cityscapes": {
        "cityscapes_fine_instance_seg_train":
        ("cityscapes/leftImg8bit/train", "cityscapes/gtFine/train"),
        "cityscapes_fine_instance_seg_val":
        ("cityscapes/leftImg8bit/val", "cityscapes/gtFine/val"),
        "cityscapes_fine_instance_seg_test":
        ("cityscapes/leftImg8bit/test", "cityscapes/gtFine/test"),

        "cityscapes_fine_sem_seg_train":
        ("cityscapes/leftImg8bit/train", "cityscapes/gtFine/train"),
        "cityscapes_fine_sem_seg_val":
        ("cityscapes/leftImg8bit/val", "cityscapes/gtFine/val"),
        "cityscapes_fine_sem_seg_test":
        ("cityscapes/leftImg8bit/test", "cityscapes/gtFine/test"),
    },
}

PATH_ROUTES.register(_PREDEFINED_SPLITS_CITYSCAPES, "CITYSCAPES")

# ==== Predefined splits for PASCAL VOC ===========

_PREDEFINED_SPLITS_VOC = {
    "dataset_type": "VOCDataset",
    "evaluator_type": {
        # "voc_2007": "pascal_voc",
        # "voc_2012": "pascal_voc",
        "voc": "pascal_voc",
    },
    "voc": {
        "voc_2007_trainval": ("voc/VOC2007", "trainval"),
        "voc_2007_train": ("voc/VOC2007", "train"),
        "voc_2007_val": ("voc/VOC2007", "val"),
        "voc_2007_test": ("voc/VOC2007", "test"),
        "voc_2012_trainval": ("voc/VOC2012", "trainval"),
        "voc_2012_train": ("voc/VOC2012", "train"),
        "voc_2012_val": ("voc/VOC2012", "val"),
    },
}

PATH_ROUTES.register(_PREDEFINED_SPLITS_VOC, "VOC")

# ==== Predefined splits for citypersons ===========

_PREDEFINED_SPLITS_CITYPERSONS = {
    "dataset_type": "CityPersonsDataset",
    "evaluator_type": {
        "citypersons": "citypersons",
    },
    "citypersons": {
        "citypersons_train":
        ("citypersons/train", "citypersons/annotations/train.json"),
        "citypersons_val":
        ("citypersons/val", "citypersons/annotations/val.json"),
    }
}

PATH_ROUTES.register(_PREDEFINED_SPLITS_CITYPERSONS, "CITYPERSONS")

# ==== Predefined splits for Objects365 ===========

_PREDEFINED_SPLITS_OBJECTS365 = {
    "dataset_type": "Objects365Dataset",
    "evaluator_type": {
        "objects365": "coco",
    },
    "objects365": {
        "objects365_train":
        ("objects365/train",
         "objects365/annotations/objects365_train_20190423.json"),
        "objects365_val":
        ("objects365/val",
         "objects365/annotations/objects365_val_20190423.json"),
        "objects365_test":
        ("objects365/test",
         "objects365/annotations/objects365_test_20190423.json"),
        "objects365_tiny_train":
        ("objects365/train",
         "objects365/annotations/objects365_Tiny_train.json"),
        "objects365_tiny_val":
        ("objects365/val", "objects365/annotations/objects365_Tiny_val.json"),
    }
}

PATH_ROUTES.register(_PREDEFINED_SPLITS_OBJECTS365, "OBJECTS365")

# ==== Predefined splits for WiderFace ===========

_PREDEFINED_SPLITS_WIDERFACE = {
    "dataset_type": "WiderFaceDataset",
    "evaluator_type": {
        "widerface_2019": "widerface",
    },
    "widerface_2019": {
        "widerface_2019_train":
        ("widerface/train",
         "widerface/annotations/widerface2019_train_cocostyle.json"),
        "widerface_2019_val":
        ("widerface/val",
         "widerface/annotations/widerface2019_val_cocostyle.json"),
    }
}

PATH_ROUTES.register(_PREDEFINED_SPLITS_WIDERFACE, "WIDERFACE")

# ==== Predefined datasets and splits for ImageNet ==========

_PREDEFINED_SPLITS_IMAGENET = {
    "dataset_type": "ImageNetDataset",
    "evaluator_type": {
        "imagenet": "classification"
    },
    "imagenet": {
        "imagenet_train": ("imagenet", "train"),
        "imagenet_val": ("imagenet", "val"),
    }
}

PATH_ROUTES.register(_PREDEFINED_SPLITS_IMAGENET, "IMAGENET")

# ==== Predefined splits for CrowdHuman ===========

_PREDEFINED_SPLITS_CROWDHUMAN = {
    "dataset_type": "CrowdHumanDataset",
    "evaluator_type": {
        "crowdhuman": "crowdhuman",
    },
    "crowdhuman": {
        "crowdhuman_train":
        ("crowdhuman/Images",
         "crowdhuman/annotations/annotation_train.odgt"),
        "crowdhuman_val":
        ("crowdhuman/Images",
         "crowdhuman/annotations/annotation_val.odgt"),
    }
}

PATH_ROUTES.register(_PREDEFINED_SPLITS_CROWDHUMAN, "CROWDHUMAN")
```

#### cvpods/data/datasets/objects365.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import contextlib
import copy
import io
import logging
import os
import os.path as osp

import numpy as np

import torch

from cvpods.structures import BoxMode
from cvpods.utils import PathManager, Timer

from ..base_dataset import BaseDataset
from ..detection_utils import (
    annotations_to_instances,
    check_image_size,
    create_keypoint_hflip_indices,
    filter_empty_instances,
    read_image
)
from ..registry import DATASETS
from .objects365_categories import OBJECTS365_CATEGORIES
from .paths_route import _PREDEFINED_SPLITS_OBJECTS365

"""
This file contains functions to parse COCO-format annotations into dicts in "cvpods format".
"""

logger = logging.getLogger(__name__)


@DATASETS.register()
class Objects365Dataset(BaseDataset):

    def __init__(self, cfg, dataset_name, transforms=[], is_train=True):
        super(Objects365Dataset, self).__init__(cfg, dataset_name, transforms, is_train)

        self.task_key = "objects365"          # for task: instance detection/segmentation

        self.meta = self._get_metadata()
        self.dataset_dicts = self._load_annotations(
            self.meta["json_file"],
            self.meta["image_root"],
            dataset_name
        )

        # fmt: off
        self.data_format = cfg.INPUT.FORMAT
        self.filter_empty = cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS
        self.keypoint_on = cfg.MODEL.KEYPOINT_ON
        self.load_proposals = cfg.MODEL.LOAD_PROPOSALS
        self.proposal_files = cfg.DATASETS.PROPOSAL_FILES_TRAIN
        # fmt: on

        if is_train:
            self.dataset_dicts = self._filter_annotations(
                filter_empty=self.filter_empty,
                min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE
                if self.keypoint_on else 0,
                proposal_files=self.proposal_files if self.load_proposals else None,
            )
            self._set_group_flag()

        self.eval_with_gt = cfg.TEST.get("WITH_GT", False)

        if self.keypoint_on:
            # Flip only makes sense in training
            self.keypoint_hflip_indices = create_keypoint_hflip_indices(
                cfg.DATASETS.TRAIN)
        else:
            self.keypoint_hflip_indices = None

    def __getitem__(self, index):
        """Load data, apply transforms, converto to Instances.
        """
        dataset_dict = copy.deepcopy(self.dataset_dicts[index])

        # read image
        image = read_image(dataset_dict["file_name"], format=self.data_format)
        check_image_size(dataset_dict, image)

        if "annotations" in dataset_dict:
            annotations = dataset_dict.pop("annotations")
            annotations = [ann for ann in annotations if ann.get("iscrowd", 0) == 0]
        else:
            annotations = None

        # apply transfrom
        image, annotations = self._apply_transforms(image, annotations)

        if annotations is not None:
            image_shape = image.shape[:2]  # h, w

            instances = annotations_to_instances(annotations, image_shape)

            # # Create a tight bounding box from masks, useful when image is cropped
            # if self.crop_gen and instances.has("gt_masks"):
            #     instances.gt_boxes = instances.gt_masks.get_bounding_boxes()

            dataset_dict["instances"] = filter_empty_instances(instances)

        # convert to Instance type
        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,
        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.
        # Therefore it's important to use torch.Tensor.
        # h, w, c -> c, h, w
        dataset_dict["image"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))

        return dataset_dict

    def __reset__(self):
        raise NotImplementedError

    def __len__(self):
        return len(self.dataset_dicts)

    def _load_annotations(
        self, json_file, image_root, dataset_name=None, extra_annotation_keys=None
    ):
        """
        Load a json file with COCO's instances annotation format.
        Currently supports instance detection, instance segmentation,
        and person keypoints annotations.

        Args:
            json_file (str): full path to the json file in COCO instances annotation format.
            image_root (str): the directory where the images in this json file exists.
            dataset_name (str): the name of the dataset (e.g., coco_2017_train).
                If provided, this function will also put "thing_classes" into
                the metadata associated with this dataset.
            extra_annotation_keys (list[str]): list of per-annotation keys that should also be
                loaded into the dataset dict (besides "iscrowd", "bbox", "keypoints",
                "category_id", "segmentation"). The values for these keys will be returned as-is.
                For example, the densepose annotations are loaded in this way.

        Returns:
            list[dict]: a list of dicts in cvpods standard format. (See
            `Using Custom Datasets </tutorials/datasets.html>`_ )

        Notes:
            1. This function does not read the image files.
               The results do not have the "image" field.
        """
        from pycocotools.coco import COCO

        timer = Timer()
        json_file = PathManager.get_local_path(json_file)
        with contextlib.redirect_stdout(io.StringIO()):
            coco_api = COCO(json_file)
        if timer.seconds() > 1:
            logger.info("Loading {} takes {:.2f} seconds.".format(json_file, timer.seconds()))

        id_map = None
        if dataset_name is not None:
            meta = self.meta
            cat_ids = sorted(coco_api.getCatIds())
            # The categories in a custom json file may not be sorted.

            # In COCO, certain category ids are artificially removed,
            # and by convention they are always ignored.
            # We deal with COCO's id issue and translate
            # the category ids to contiguous ids in [0, 80).

            # It works by looking at the "categories" field in the json, therefore
            # if users' own json also have incontiguous ids, we'll
            # apply this mapping as well but print a warning.
            if not (min(cat_ids) == 1 and max(cat_ids) == len(cat_ids)):
                if "coco" not in dataset_name:
                    logger.warning(
                        """ \
                        Category ids in annotations are not \
                        in [1, #categories]! We'll apply a mapping for you. \
                        """
                    )
            id_map = {v: i for i, v in enumerate(cat_ids)}
            meta["thing_dataset_id_to_contiguous_id"] = id_map

        # sort indices for reproducible results
        img_ids = sorted(list(coco_api.imgs.keys()))
        # imgs is a list of dicts, each looks something like:
        # {'license': 4,
        #  'url': 'http://farm6.staticflickr.com/5454/9413846304_881d5e5c3b_z.jpg',
        #  'file_name': 'COCO_val2014_000000001268.jpg',
        #  'height': 427,
        #  'width': 640,
        #  'date_captured': '2013-11-17 05:57:24',
        #  'id': 1268}
        imgs = coco_api.loadImgs(img_ids)
        # anns is a list[list[dict]], where each dict is an annotation
        # record for an object. The inner list enumerates the objects in an image
        # and the outer list enumerates over images. Example of anns[0]:
        # [{'segmentation': [[192.81,
        #     247.09,
        #     ...
        #     219.03,
        #     249.06]],
        #   'area': 1035.749,
        #   'iscrowd': 0,
        #   'image_id': 1268,
        #   'bbox': [192.81, 224.8, 74.73, 33.43],
        #   'category_id': 16,
        #   'id': 42986},
        #  ...]
        anns = [coco_api.imgToAnns[img_id] for img_id in img_ids]

        if "minival" not in json_file:
            # The popular valminusminival & minival annotations for COCO2014 contain this bug.
            # However the ratio of buggy annotations there is tiny and does not affect accuracy.
            # Therefore we explicitly white-list them.
            ann_ids = [ann["id"] for anns_per_image in anns for ann in anns_per_image]
            assert len(set(ann_ids)) == len(ann_ids), (
                "Annotation ids in '{}' are not unique!".format(json_file)
            )

        imgs_anns = list(zip(imgs, anns))

        logger.info("Loaded {} images in COCO format from {}".format(len(imgs_anns), json_file))

        dataset_dicts = []

        ann_keys = ["iscrowd", "bbox", "keypoints", "category_id"] + (extra_annotation_keys or [])

        num_instances_without_valid_segmentation = 0

        for (img_dict, anno_dict_list) in imgs_anns:
            record = {}
            record["file_name"] = os.path.join(image_root, img_dict["file_name"])
            record["height"] = img_dict["height"]
            record["width"] = img_dict["width"]
            image_id = record["image_id"] = img_dict["id"]

            objs = []
            for anno in anno_dict_list:
                # Check that the image_id in this annotation is the same as
                # the image_id we're looking at.
                # This fails only when the data parsing logic or the annotation file is buggy.

                # The original COCO valminusminival2014 & minival2014 annotation files
                # actually contains bugs that, together with certain ways of using COCO API,
                # can trigger this assertion.
                assert anno["image_id"] == image_id

                if anno.get("ignore", 0) != 0:
                    continue

                obj = {key: anno[key] for key in ann_keys if key in anno}

                segm = anno.get("segmentation", None)
                if segm:  # either list[list[float]] or dict(RLE)
                    if not isinstance(segm, dict):
                        # filter out invalid polygons (< 3 points)
                        segm = [poly for poly in segm if len(poly) % 2 == 0 and len(poly) >= 6]
                        if len(segm) == 0:
                            num_instances_without_valid_segmentation += 1
                            continue  # ignore this instance
                    obj["segmentation"] = segm

                keypts = anno.get("keypoints", None)
                if keypts:  # list[int]
                    for idx, v in enumerate(keypts):
                        if idx % 3 != 2:
                            # COCO's segmentation coordinates are floating points in [0, H or W],
                            # but keypoint coordinates are integers in [0, H-1 or W-1]
                            # Therefore we assume the coordinates are "pixel indices" and
                            # add 0.5 to convert to floating point coordinates.
                            keypts[idx] = v + 0.5
                    obj["keypoints"] = keypts

                obj["bbox_mode"] = BoxMode.XYWH_ABS
                if id_map:
                    obj["category_id"] = id_map[obj["category_id"]]
                objs.append(obj)
            record["annotations"] = objs
            dataset_dicts.append(record)

        if num_instances_without_valid_segmentation > 0:
            logger.warn(
                "Filtered out {} instances without valid segmentation. "
                "There might be issues in your dataset generation process.".format(
                    num_instances_without_valid_segmentation
                )
            )
        return dataset_dicts

    def _get_metadata(self):
        thing_classes = [k["name"] for k in OBJECTS365_CATEGORIES]
        meta = {"thing_classes": thing_classes}
        image_root, json_file = _PREDEFINED_SPLITS_OBJECTS365[self.task_key][self.name]
        meta["image_root"] = osp.join(self.data_root, image_root) \
            if "://" not in image_root else image_root
        meta["json_file"] = osp.join(self.data_root, json_file) \
            if "://" not in image_root else osp.join(image_root, json_file)
        meta["evaluator_type"] = _PREDEFINED_SPLITS_OBJECTS365["evaluator_type"][self.task_key]

        return meta

    def evaluate(self, predictions):
        """Dataset must provide a evaluation function to evaluate model."""
        raise NotImplementedError

    @property
    def ground_truth_annotations(self):
        return self.dataset_dicts


# TODO this function is not specific to COCO, except for the "image_id" logic.
def load_sem_seg(gt_root, image_root, gt_ext="png", image_ext="jpg"):
    """
    Load semantic segmentation datasets. All files under "gt_root" with "gt_ext" extension are
    treated as ground truth annotations and all files under "image_root" with "image_ext" extension
    as input images. Ground truth and input images are matched using file paths relative to
    "gt_root" and "image_root" respectively without taking into account file extensions.
    This works for COCO as well as some other datasets.

    Args:
        gt_root (str): full path to ground truth semantic segmentation files. Semantic segmentation
            annotations are stored as images with integer values in pixels that represent
            corresponding semantic labels.
        image_root (str): the directory where the input images are.
        gt_ext (str): file extension for ground truth annotations.
        image_ext (str): file extension for input images.

    Returns:
        list[dict]:
            a list of dicts in cvpods standard format without instance-level
            annotation.

    Notes:
        1. This function does not read the image and ground truth files.
           The results do not have the "image" and "sem_seg" fields.
    """

    # We match input images with ground truth based on their relative filepaths (without file
    # extensions) starting from 'image_root' and 'gt_root' respectively.
    def file2id(folder_path, file_path):
        # extract relative path starting from `folder_path`
        image_id = os.path.normpath(
            os.path.relpath(file_path, start=folder_path))
        # remove file extension
        image_id = os.path.splitext(image_id)[0]
        return image_id

    input_files = sorted(
        (os.path.join(image_root, f)
         for f in PathManager.ls(image_root) if f.endswith(image_ext)),
        key=lambda file_path: file2id(image_root, file_path),
    )
    gt_files = sorted(
        (os.path.join(gt_root, f)
         for f in PathManager.ls(gt_root) if f.endswith(gt_ext)),
        key=lambda file_path: file2id(gt_root, file_path),
    )

    assert len(gt_files) > 0, "No annotations found in {}.".format(gt_root)

    # Use the intersection, so that val2017_100 annotations can run smoothly with val2017 images
    if len(input_files) != len(gt_files):
        logger.warn(
            "Directory {} and {} has {} and {} files, respectively.".format(
                image_root, gt_root, len(input_files), len(gt_files)))
        input_basenames = [
            os.path.basename(f)[:-len(image_ext)] for f in input_files
        ]
        gt_basenames = [os.path.basename(f)[:-len(gt_ext)] for f in gt_files]
        intersect = list(set(input_basenames) & set(gt_basenames))
        # sort, otherwise each worker may obtain a list[dict] in different order
        intersect = sorted(intersect)
        logger.warn("Will use their intersection of {} files.".format(
            len(intersect)))
        input_files = [
            os.path.join(image_root, f + image_ext) for f in intersect
        ]
        gt_files = [os.path.join(gt_root, f + gt_ext) for f in intersect]

    logger.info("Loaded {} images with semantic segmentation from {}".format(
        len(input_files), image_root))

    dataset_dicts = []
    for (img_path, gt_path) in zip(input_files, gt_files):
        record = {}
        record["file_name"] = img_path
        record["sem_seg_file_name"] = gt_path
        dataset_dicts.append(record)

    return dataset_dicts
```

#### cvpods/data/datasets/imagenet.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import logging
import os
import os.path as osp
from copy import deepcopy

import numpy as np

import torch

from cvpods.utils import Timer

from ..base_dataset import BaseDataset
from ..registry import DATASETS
from .paths_route import _PREDEFINED_SPLITS_IMAGENET

"""
This file contains functions to parse ImageNet-format annotations into dicts in "cvpods format".
"""

logger = logging.getLogger(__name__)


@DATASETS.register()
class ImageNetDataset(BaseDataset):
    def __init__(self, cfg, dataset_name, transforms=[], is_train=True):
        super(ImageNetDataset, self).__init__(cfg, dataset_name, transforms, is_train)

        image_root, label_file = _PREDEFINED_SPLITS_IMAGENET["imagenet"][self.name]
        self.label_file = osp.join(self.data_root, image_root, label_file) \
            if "://" not in image_root else osp.join(image_root, label_file)
        self.image_root = osp.join(self.data_root, image_root) \
            if "://" not in image_root else image_root

        self.meta = self._get_metadata()
        self.dataset_dicts = self._load_annotations()
        self._set_group_flag()

        self.eval_with_gt = cfg.TEST.get("WITH_GT", False)

    def __getitem__(self, index):
        """Load data, apply transforms, converto to Instances.
        """
        dataset_dict = deepcopy(self.dataset_dicts[index])

        # read image
        image = self._read_data(dataset_dict["file_name"])

        annotations = dataset_dict.get("annotations", None)

        # apply transfrom
        images, annotations = self._apply_transforms(
            image, annotations)

        def process(dd, img, annos):
            if isinstance(annos, list):
                annos = [a for a in annos if a is not None]

            # image shape: CHW / NCHW
            # TODO: fix hack
            if img.shape[0] == 3:  # CHW
                dd["image"] = torch.as_tensor(np.ascontiguousarray(img))
            elif len(img.shape) == 3 and img.shape[-1] == 3:
                dd["image"] = torch.as_tensor(
                    np.ascontiguousarray(img.transpose(2, 0, 1)))
            elif len(img.shape) == 4 and img.shape[-1] == 3:
                # NHWC -> NCHW
                dd["image"] = torch.as_tensor(
                    np.ascontiguousarray(img.transpose(0, 3, 1, 2)))
            return dd

        if isinstance(images, dict):
            ret = {}
            # multiple input pipelines
            for desc, item in images.items():
                img, anno = item
                ret[desc] = process(deepcopy(dataset_dict), img, anno)
            return ret
        else:
            return process(dataset_dict, images, annotations)

    def __len__(self):
        return len(self.dataset_dicts)

    def _get_metadata(self):
        assert len(IMAGENET_CATEGORIES.keys()) == 1000
        cat_ids = [v[0] for v in IMAGENET_CATEGORIES.values()]
        assert min(cat_ids) == 1 and max(cat_ids) == len(cat_ids), \
            "Category ids are not in [1, #categories], as expected"
        # Ensure that the category list is sroted by id
        imagenet_categories = sorted(IMAGENET_CATEGORIES.items(), key=lambda x: x[1][0])
        thing_classes = [v[1][1] for v in imagenet_categories]
        meta = {
            "thing_classes": thing_classes,
            "evaluator_type": _PREDEFINED_SPLITS_IMAGENET["evaluator_type"]["imagenet"],
        }
        return meta

    def _load_annotations(self):
        timer = Timer()

        """Constructs the imdb."""
        # Compile the split data path
        logger.info('{} data path: {}'.format(self.name, self.label_file))
        # Images are stored per class in subdirs (format: n<number>)
        class_ids = [k for k, v in IMAGENET_CATEGORIES.items()]
        class_id_cont_id = {k: v[0] - 1 for k, v in IMAGENET_CATEGORIES.items()}
        # class_ids = sorted([
        #     f for f in os.listdir(split_path) if re.match(r'^n[0-9]+$', f)
        # ])
        # # Map ImageNet class ids to contiguous ids
        # class_id_cont_id = {v: i for i, v in enumerate(class_ids)}
        # Construct the image db
        imdb = []
        if "://" not in self.image_root:
            for class_id in class_ids:
                cont_id = class_id_cont_id[class_id]
                im_dir = os.path.join(self.label_file, class_id)
                for im_name in os.listdir(im_dir):
                    imdb.append({
                        'im_path': os.path.join(im_dir, im_name),
                        'class': cont_id,
                    })

        logging.info("Loading {} takes {:.2f} seconds.".format(self.label_file, timer.seconds()))

        dataset_dicts = []
        for i, item in enumerate(imdb):
            dataset_dicts.append({
                "image_id": i,
                "category_id": item["class"],
                "file_name": item["im_path"],
            })

        return dataset_dicts


IMAGENET_CATEGORIES = {
    'n01440764': (449, 'tench'),
    'n01443537': (450, 'goldfish'),
    'n01484850': (442, 'great_white_shark'),
    'n01491361': (443, 'tiger_shark'),
    'n01494475': (444, 'hammerhead'),
    'n01496331': (445, 'electric_ray'),
    'n01498041': (446, 'stingray'),
    'n01514668': (383, 'cock'),
    'n01514859': (384, 'hen'),
    'n01518878': (385, 'ostrich'),
    'n01530575': (386, 'brambling'),
    'n01531178': (387, 'goldfinch'),
    'n01532829': (388, 'house_finch'),
    'n01534433': (389, 'junco'),
    'n01537544': (390, 'indigo_bunting'),
    'n01558993': (391, 'robin'),
    'n01560419': (392, 'bulbul'),
    'n01580077': (393, 'jay'),
    'n01582220': (394, 'magpie'),
    'n01592084': (395, 'chickadee'),
    'n01601694': (396, 'water_ouzel'),
    'n01608432': (397, 'kite'),
    'n01614925': (398, 'bald_eagle'),
    'n01616318': (399, 'vulture'),
    'n01622779': (400, 'great_grey_owl'),
    'n01629819': (494, 'European_fire_salamander'),
    'n01630670': (495, 'common_newt'),
    'n01631663': (496, 'eft'),
    'n01632458': (497, 'spotted_salamander'),
    'n01632777': (498, 'axolotl'),
    'n01641577': (499, 'bullfrog'),
    'n01644373': (500, 'tree_frog'),
    'n01644900': (501, 'tailed_frog'),
    'n01664065': (458, 'loggerhead'),
    'n01665541': (459, 'leatherback_turtle'),
    'n01667114': (460, 'mud_turtle'),
    'n01667778': (461, 'terrapin'),
    'n01669191': (462, 'box_turtle'),
    'n01675722': (463, 'banded_gecko'),
    'n01677366': (464, 'common_iguana'),
    'n01682714': (465, 'American_chameleon'),
    'n01685808': (466, 'whiptail'),
    'n01687978': (467, 'agama'),
    'n01688243': (468, 'frilled_lizard'),
    'n01689811': (469, 'alligator_lizard'),
    'n01692333': (470, 'Gila_monster'),
    'n01693334': (471, 'green_lizard'),
    'n01694178': (472, 'African_chameleon'),
    'n01695060': (473, 'Komodo_dragon'),
    'n01697457': (475, 'African_crocodile'),
    'n01698640': (476, 'American_alligator'),
    'n01704323': (474, 'triceratops'),
    'n01728572': (477, 'thunder_snake'),
    'n01728920': (478, 'ringneck_snake'),
    'n01729322': (479, 'hognose_snake'),
    'n01729977': (480, 'green_snake'),
    'n01734418': (481, 'king_snake'),
    'n01735189': (482, 'garter_snake'),
    'n01737021': (483, 'water_snake'),
    'n01739381': (484, 'vine_snake'),
    'n01740131': (485, 'night_snake'),
    'n01742172': (486, 'boa_constrictor'),
    'n01744401': (487, 'rock_python'),
    'n01748264': (488, 'Indian_cobra'),
    'n01749939': (489, 'green_mamba'),
    'n01751748': (490, 'sea_snake'),
    'n01753488': (491, 'horned_viper'),
    'n01755581': (492, 'diamondback'),
    'n01756291': (493, 'sidewinder'),
    'n01768244': (601, 'trilobite'),
    'n01770081': (602, 'harvestman'),
    'n01770393': (603, 'scorpion'),
    'n01773157': (604, 'black_and_gold_garden_spider'),
    'n01773549': (605, 'barn_spider'),
    'n01773797': (606, 'garden_spider'),
    'n01774384': (607, 'black_widow'),
    'n01774750': (608, 'tarantula'),
    'n01775062': (609, 'wolf_spider'),
    'n01776313': (610, 'tick'),
    'n01784675': (611, 'centipede'),
    'n01795545': (401, 'black_grouse'),
    'n01796340': (402, 'ptarmigan'),
    'n01797886': (403, 'ruffed_grouse'),
    'n01798484': (404, 'prairie_chicken'),
    'n01806143': (405, 'peacock'),
    'n01806567': (406, 'quail'),
    'n01807496': (407, 'partridge'),
    'n01817953': (408, 'African_grey'),
    'n01818515': (409, 'macaw'),
    'n01819313': (410, 'sulphur-crested_cockatoo'),
    'n01820546': (411, 'lorikeet'),
    'n01824575': (412, 'coucal'),
    'n01828970': (413, 'bee_eater'),
    'n01829413': (414, 'hornbill'),
    'n01833805': (415, 'hummingbird'),
    'n01843065': (416, 'jacamar'),
    'n01843383': (417, 'toucan'),
    'n01847000': (418, 'drake'),
    'n01855032': (419, 'red-breasted_merganser'),
    'n01855672': (420, 'goose'),
    'n01860187': (421, 'black_swan'),
    'n01871265': (214, 'tusker'),
    'n01872401': (215, 'echidna'),
    'n01873310': (217, 'platypus'),
    'n01877812': (216, 'wallaby'),
    'n01882714': (213, 'koala'),
    'n01883070': (218, 'wombat'),
    'n01910747': (647, 'jellyfish'),
    'n01914609': (648, 'sea_anemone'),
    'n01917289': (649, 'brain_coral'),
    'n01924916': (650, 'flatworm'),
    'n01930112': (651, 'nematode'),
    'n01943899': (652, 'conch'),
    'n01944390': (653, 'snail'),
    'n01945685': (654, 'slug'),
    'n01950731': (655, 'sea_slug'),
    'n01955084': (656, 'chiton'),
    'n01968897': (226, 'chambered_nautilus'),
    'n01978287': (613, 'Dungeness_crab'),
    'n01978455': (614, 'rock_crab'),
    'n01980166': (615, 'fiddler_crab'),
    'n01981276': (616, 'king_crab'),
    'n01983481': (617, 'American_lobster'),
    'n01984695': (618, 'spiny_lobster'),
    'n01985128': (619, 'crayfish'),
    'n01986214': (620, 'hermit_crab'),
    'n01990800': (612, 'isopod'),
    'n02002556': (422, 'white_stork'),
    'n02002724': (423, 'black_stork'),
    'n02006656': (424, 'spoonbill'),
    'n02007558': (425, 'flamingo'),
    'n02009229': (427, 'little_blue_heron'),
    'n02009912': (426, 'American_egret'),
    'n02011460': (428, 'bittern'),
    'n02012849': (429, 'crane'),
    'n02013706': (430, 'limpkin'),
    'n02017213': (438, 'European_gallinule'),
    'n02018207': (431, 'American_coot'),
    'n02018795': (432, 'bustard'),
    'n02025239': (433, 'ruddy_turnstone'),
    'n02027492': (434, 'red-backed_sandpiper'),
    'n02028035': (435, 'redshank'),
    'n02033041': (436, 'dowitcher'),
    'n02037110': (437, 'oystercatcher'),
    'n02051845': (439, 'pelican'),
    'n02056570': (440, 'king_penguin'),
    'n02058221': (441, 'albatross'),
    'n02066245': (6, 'grey_whale'),
    'n02071294': (22, 'killer_whale'),
    'n02074367': (193, 'dugong'),
    'n02077923': (14, 'sea_lion'),
    'n02085620': (173, 'Chihuahua'),
    'n02085782': (99, 'Japanese_spaniel'),
    'n02085936': (87, 'Maltese_dog'),
    'n02086079': (69, 'Pekinese'),
    'n02086240': (116, 'Shih-Tzu'),
    'n02086646': (198, 'Blenheim_spaniel'),
    'n02086910': (43, 'papillon'),
    'n02087046': (89, 'toy_terrier'),
    'n02087394': (200, 'Rhodesian_ridgeback'),
    'n02088094': (98, 'Afghan_hound'),
    'n02088238': (161, 'basset'),
    'n02088364': (132, 'beagle'),
    'n02088466': (32, 'bloodhound'),
    'n02088632': (180, 'bluetick'),
    'n02089078': (42, 'black-and-tan_coonhound'),
    'n02089867': (18, 'Walker_hound'),
    'n02089973': (207, 'English_foxhound'),
    'n02090379': (181, 'redbone'),
    'n02090622': (105, 'borzoi'),
    'n02090721': (77, 'Irish_wolfhound'),
    'n02091032': (189, 'Italian_greyhound'),
    'n02091134': (20, 'whippet'),
    'n02091244': (204, 'Ibizan_hound'),
    'n02091467': (63, 'Norwegian_elkhound'),
    'n02091635': (31, 'otterhound'),
    'n02091831': (66, 'Saluki'),
    'n02092002': (21, 'Scottish_deerhound'),
    'n02092339': (25, 'Weimaraner'),
    'n02093256': (45, 'Staffordshire_bullterrier'),
    'n02093428': (170, 'American_Staffordshire_terrier'),
    'n02093647': (119, 'Bedlington_terrier'),
    'n02093754': (210, 'Border_terrier'),
    'n02093859': (107, 'Kerry_blue_terrier'),
    'n02093991': (126, 'Irish_terrier'),
    'n02094114': (88, 'Norfolk_terrier'),
    'n02094258': (145, 'Norwich_terrier'),
    'n02094433': (59, 'Yorkshire_terrier'),
    'n02095314': (160, 'wire-haired_fox_terrier'),
    'n02095570': (152, 'Lakeland_terrier'),
    'n02095889': (72, 'Sealyham_terrier'),
    'n02096051': (33, 'Airedale'),
    'n02096177': (91, 'cairn'),
    'n02096294': (4, 'Australian_terrier'),
    'n02096437': (27, 'Dandie_Dinmont'),
    'n02096585': (113, 'Boston_bull'),
    'n02097047': (123, 'miniature_schnauzer'),
    'n02097130': (36, 'giant_schnauzer'),
    'n02097209': (156, 'standard_schnauzer'),
    'n02097298': (109, 'Scotch_terrier'),
    'n02097474': (158, 'Tibetan_terrier'),
    'n02097658': (131, 'silky_terrier'),
    'n02098105': (26, 'soft-coated_wheaten_terrier'),
    'n02098286': (71, 'West_Highland_white_terrier'),
    'n02098413': (56, 'Lhasa'),
    'n02099267': (146, 'flat-coated_retriever'),
    'n02099429': (144, 'curly-coated_retriever'),
    'n02099601': (125, 'golden_retriever'),
    'n02099712': (176, 'Labrador_retriever'),
    'n02099849': (139, 'Chesapeake_Bay_retriever'),
    'n02100236': (134, 'German_short-haired_pointer'),
    'n02100583': (90, 'vizsla'),
    'n02100735': (2, 'English_setter'),
    'n02100877': (192, 'Irish_setter'),
    'n02101006': (154, 'Gordon_setter'),
    'n02101388': (150, 'Brittany_spaniel'),
    'n02101556': (94, 'clumber'),
    'n02102040': (5, 'English_springer'),
    'n02102177': (19, 'Welsh_springer_spaniel'),
    'n02102318': (191, 'cocker_spaniel'),
    'n02102480': (196, 'Sussex_spaniel'),
    'n02102973': (117, 'Irish_water_spaniel'),
    'n02104029': (141, 'kuvasz'),
    'n02104365': (68, 'schipperke'),
    'n02105056': (93, 'groenendael'),
    'n02105162': (51, 'malinois'),
    'n02105251': (208, 'briard'),
    'n02105412': (184, 'kelpie'),
    'n02105505': (97, 'komondor'),
    'n02105641': (29, 'Old_English_sheepdog'),
    'n02105855': (171, 'Shetland_sheepdog'),
    'n02106030': (124, 'collie'),
    'n02106166': (128, 'Border_collie'),
    'n02106382': (47, 'Bouvier_des_Flandres'),
    'n02106550': (64, 'Rottweiler'),
    'n02106662': (211, 'German_shepherd'),
    'n02107142': (112, 'Doberman'),
    'n02107312': (187, 'miniature_pinscher'),
    'n02107574': (114, 'Greater_Swiss_Mountain_dog'),
    'n02107683': (86, 'Bernese_mountain_dog'),
    'n02107908': (115, 'Appenzeller'),
    'n02108000': (79, 'EntleBucher'),
    'n02108089': (130, 'boxer'),
    'n02108422': (140, 'bull_mastiff'),
    'n02108551': (110, 'Tibetan_mastiff'),
    'n02108915': (82, 'French_bulldog'),
    'n02109047': (17, 'Great_Dane'),
    'n02109525': (177, 'Saint_Bernard'),
    'n02109961': (149, 'Eskimo_dog'),
    'n02110063': (15, 'malamute'),
    'n02110185': (3, 'Siberian_husky'),
    'n02110341': (41, 'dalmatian'),
    'n02110627': (127, 'affenpinscher'),
    'n02110806': (84, 'basenji'),
    'n02110958': (143, 'pug'),
    'n02111129': (133, 'Leonberg'),
    'n02111277': (60, 'Newfoundland'),
    'n02111500': (172, 'Great_Pyrenees'),
    'n02111889': (179, 'Samoyed'),
    'n02112018': (118, 'Pomeranian'),
    'n02112137': (168, 'chow'),
    'n02112350': (148, 'keeshond'),
    'n02112706': (70, 'Brabancon_griffon'),
    'n02113023': (197, 'Pembroke'),
    'n02113186': (50, 'Cardigan'),
    'n02113624': (106, 'toy_poodle'),
    'n02113712': (49, 'miniature_poodle'),
    'n02113799': (151, 'standard_poodle'),
    'n02113978': (46, 'Mexican_hairless'),
    'n02114367': (205, 'timber_wolf'),
    'n02114548': (102, 'white_wolf'),
    'n02114712': (28, 'red_wolf'),
    'n02114855': (58, 'coyote'),
    'n02115641': (155, 'dingo'),
    'n02115913': (136, 'dhole'),
    'n02116738': (202, 'African_hunting_dog'),
    'n02117135': (34, 'hyena'),
    'n02119022': (62, 'red_fox'),
    'n02119789': (1, 'kit_fox'),
    'n02120079': (159, 'Arctic_fox'),
    'n02120505': (67, 'grey_fox'),
    'n02123045': (174, 'tabby'),
    'n02123159': (55, 'tiger_cat'),
    'n02123394': (10, 'Persian_cat'),
    'n02123597': (95, 'Siamese_cat'),
    'n02124075': (8, 'Egyptian_cat'),
    'n02125311': (11, 'cougar'),
    'n02127052': (201, 'lynx'),
    'n02128385': (85, 'leopard'),
    'n02128757': (153, 'snow_leopard'),
    'n02128925': (30, 'jaguar'),
    'n02129165': (190, 'lion'),
    'n02129604': (76, 'tiger'),
    'n02130308': (206, 'cheetah'),
    'n02132136': (61, 'brown_bear'),
    'n02133161': (163, 'American_black_bear'),
    'n02134084': (103, 'ice_bear'),
    'n02134418': (209, 'sloth_bear'),
    'n02137549': (74, 'mongoose'),
    'n02138441': (35, 'meerkat'),
    'n02165105': (621, 'tiger_beetle'),
    'n02165456': (622, 'ladybug'),
    'n02167151': (623, 'ground_beetle'),
    'n02168699': (624, 'long-horned_beetle'),
    'n02169497': (625, 'leaf_beetle'),
    'n02172182': (626, 'dung_beetle'),
    'n02174001': (627, 'rhinoceros_beetle'),
    'n02177972': (628, 'weevil'),
    'n02190166': (629, 'fly'),
    'n02206856': (630, 'bee'),
    'n02219486': (224, 'ant'),
    'n02226429': (631, 'grasshopper'),
    'n02229544': (632, 'cricket'),
    'n02231487': (633, 'walking_stick'),
    'n02233338': (634, 'cockroach'),
    'n02236044': (635, 'mantis'),
    'n02256656': (636, 'cicada'),
    'n02259212': (637, 'leafhopper'),
    'n02264363': (638, 'lacewing'),
    'n02268443': (639, 'dragonfly'),
    'n02268853': (640, 'damselfly'),
    'n02276258': (641, 'admiral'),
    'n02277742': (642, 'ringlet'),
    'n02279972': (643, 'monarch'),
    'n02280649': (644, 'cabbage_butterfly'),
    'n02281406': (645, 'sulphur_butterfly'),
    'n02281787': (646, 'lycaenid'),
    'n02317335': (225, 'starfish'),
    'n02319095': (657, 'sea_urchin'),
    'n02321529': (658, 'sea_cucumber'),
    'n02325366': (188, 'wood_rabbit'),
    'n02326432': (129, 'hare'),
    'n02328150': (164, 'Angora'),
    'n02342885': (157, 'hamster'),
    'n02346627': (13, 'porcupine'),
    'n02356798': (53, 'fox_squirrel'),
    'n02361337': (183, 'marmot'),
    'n02363005': (195, 'beaver'),
    'n02364673': (101, 'guinea_pig'),
    'n02389026': (39, 'sorrel'),
    'n02391049': (80, 'zebra'),
    'n02395406': (147, 'hog'),
    'n02396427': (78, 'wild_boar'),
    'n02397096': (120, 'warthog'),
    'n02398521': (167, 'hippopotamus'),
    'n02403003': (108, 'ox'),
    'n02408429': (162, 'water_buffalo'),
    'n02410509': (165, 'bison'),
    'n02412080': (81, 'ram'),
    'n02415577': (52, 'bighorn'),
    'n02417914': (9, 'ibex'),
    'n02422106': (65, 'hartebeest'),
    'n02422699': (57, 'impala'),
    'n02423022': (12, 'gazelle'),
    'n02437312': (121, 'Arabian_camel'),
    'n02437616': (186, 'llama'),
    'n02441942': (48, 'weasel'),
    'n02442845': (23, 'mink'),
    'n02443114': (182, 'polecat'),
    'n02443484': (40, 'black-footed_ferret'),
    'n02444819': (212, 'otter'),
    'n02445715': (44, 'skunk'),
    'n02447366': (16, 'badger'),
    'n02454379': (178, 'armadillo'),
    'n02457408': (38, 'three-toed_sloth'),
    'n02480495': (83, 'orangutan'),
    'n02480855': (104, 'gorilla'),
    'n02481823': (96, 'chimpanzee'),
    'n02483362': (185, 'gibbon'),
    'n02483708': (122, 'siamang'),
    'n02484975': (73, 'guenon'),
    'n02486261': (135, 'patas'),
    'n02486410': (137, 'baboon'),
    'n02487347': (138, 'macaque'),
    'n02488291': (203, 'langur'),
    'n02488702': (54, 'colobus'),
    'n02489166': (100, 'proboscis_monkey'),
    'n02490219': (175, 'marmoset'),
    'n02492035': (142, 'capuchin'),
    'n02492660': (166, 'howler_monkey'),
    'n02493509': (37, 'titi'),
    'n02493793': (111, 'spider_monkey'),
    'n02494079': (92, 'squirrel_monkey'),
    'n02497673': (199, 'Madagascar_cat'),
    'n02500267': (75, 'indri'),
    'n02504013': (194, 'Indian_elephant'),
    'n02504458': (24, 'African_elephant'),
    'n02509815': (7, 'lesser_panda'),
    'n02510455': (169, 'giant_panda'),
    'n02514041': (447, 'barracouta'),
    'n02526121': (451, 'eel'),
    'n02536864': (448, 'coho'),
    'n02606052': (452, 'rock_beauty'),
    'n02607072': (453, 'anemone_fish'),
    'n02640242': (456, 'sturgeon'),
    'n02641379': (457, 'gar'),
    'n02643566': (454, 'lionfish'),
    'n02655020': (455, 'puffer'),
    'n02666196': (547, 'abacus'),
    'n02667093': (853, 'abaya'),
    'n02669723': (896, 'academic_gown'),
    'n02672831': (223, 'accordion'),
    'n02676566': (345, 'acoustic_guitar'),
    'n02687172': (246, 'aircraft_carrier'),
    'n02690373': (230, 'airliner'),
    'n02692877': (232, 'airship'),
    'n02699494': (677, 'altar'),
    'n02701002': (265, 'ambulance'),
    'n02704792': (264, 'amphibian'),
    'n02708093': (522, 'analog_clock'),
    'n02727426': (688, 'apiary'),
    'n02730930': (845, 'apron'),
    'n02747177': (752, 'ashcan'),
    'n02749479': (540, 'assault_rifle'),
    'n02769748': (847, 'backpack'),
    'n02776631': (704, 'bakery'),
    'n02777292': (767, 'balance_beam'),
    'n02782093': (233, 'balloon'),
    'n02783161': (907, 'ballpoint'),
    'n02786058': (967, 'Band_Aid'),
    'n02787622': (341, 'banjo'),
    'n02788148': (718, 'bannister'),
    'n02790996': (916, 'barbell'),
    'n02791124': (307, 'barber_chair'),
    'n02791270': (705, 'barbershop'),
    'n02793495': (683, 'barn'),
    'n02794156': (518, 'barometer'),
    'n02795169': (905, 'barrel'),
    'n02797295': (258, 'barrow'),
    'n02799071': (807, 'baseball'),
    'n02802426': (908, 'basketball'),
    'n02804414': (296, 'bassinet'),
    'n02804610': (353, 'bassoon'),
    'n02807133': (785, 'bathing_cap'),
    'n02808304': (909, 'bath_towel'),
    'n02808440': (884, 'bathtub'),
    'n02814533': (266, 'beach_wagon'),
    'n02814860': (733, 'beacon'),
    'n02815834': (991, 'beaker'),
    'n02817516': (849, 'bearskin'),
    'n02823428': (777, 'beer_bottle'),
    'n02823750': (811, 'beer_glass'),
    'n02825657': (933, 'bell_cote'),
    'n02834397': (941, 'bib'),
    'n02835271': (254, 'bicycle-built-for-two'),
    'n02837789': (985, 'bikini'),
    'n02840245': (835, 'binder'),
    'n02841315': (533, 'binoculars'),
    'n02843684': (839, 'birdhouse'),
    'n02859443': (689, 'boathouse'),
    'n02860847': (252, 'bobsled'),
    'n02865351': (940, 'bolo_tie'),
    'n02869837': (805, 'bonnet'),
    'n02870880': (300, 'bookcase'),
    'n02871525': (706, 'bookshop'),
    'n02877765': (779, 'bottlecap'),
    'n02879718': (538, 'bow'),
    'n02883205': (817, 'bow_tie'),
    'n02892201': (716, 'brass'),
    'n02892767': (872, 'brassiere'),
    'n02894605': (719, 'breakwater'),
    'n02895154': (949, 'breastplate'),
    'n02906734': (851, 'broom'),
    'n02909870': (820, 'bucket'),
    'n02910353': (580, 'buckle'),
    'n02916936': (833, 'bulletproof_vest'),
    'n02917067': (887, 'bullet_train'),
    'n02927161': (707, 'butcher_shop'),
    'n02930766': (267, 'cab'),
    'n02939185': (673, 'caldron'),
    'n02948072': (591, 'candle'),
    'n02950826': (539, 'cannon'),
    'n02951358': (239, 'canoe'),
    'n02951585': (377, 'can_opener'),
    'n02963159': (836, 'cardigan'),
    'n02965783': (576, 'car_mirror'),
    'n02966193': (568, 'carousel'),
    'n02966687': (891, "carpenter's_kit"),
    'n02971356': (749, 'carton'),
    'n02974003': (563, 'car_wheel'),
    'n02977058': (548, 'cash_machine'),
    'n02978881': (890, 'cassette'),
    'n02979186': (929, 'cassette_player'),
    'n02980441': (701, 'castle'),
    'n02981792': (241, 'catamaran'),
    'n02988304': (987, 'CD_player'),
    'n02992211': (342, 'cello'),
    'n02992529': (914, 'cellular_telephone'),
    'n02999410': (827, 'chain'),
    'n03000134': (721, 'chainlink_fence'),
    'n03000247': (902, 'chain_mail'),
    'n03000684': (382, 'chain_saw'),
    'n03014705': (762, 'chest'),
    'n03016953': (303, 'chiffonier'),
    'n03017168': (335, 'chime'),
    'n03018349': (301, 'china_cabinet'),
    'n03026506': (801, 'Christmas_stocking'),
    'n03028079': (690, 'church'),
    'n03032252': (695, 'cinema'),
    'n03041632': (370, 'cleaver'),
    'n03042490': (713, 'cliff_dwelling'),
    'n03045698': (797, 'cloak'),
    'n03047690': (979, 'clog'),
    'n03062245': (761, 'cocktail_shaker'),
    'n03063599': (996, 'coffee_mug'),
    'n03063689': (674, 'coffeepot'),
    'n03065424': (698, 'coil'),
    'n03075370': (583, 'combination_lock'),
    'n03085013': (543, 'computer_keyboard'),
    'n03089624': (708, 'confectionery'),
    'n03095699': (243, 'container_ship'),
    'n03100240': (268, 'convertible'),
    'n03109150': (376, 'corkscrew'),
    'n03110669': (347, 'cornet'),
    'n03124043': (910, 'cowboy_boot'),
    'n03124170': (881, 'cowboy_hat'),
    'n03125729': (297, 'cradle'),
    'n03126707': (545, 'crane'),
    'n03127747': (778, 'crash_helmet'),
    'n03127925': (898, 'crate'),
    'n03131574': (298, 'crib'),
    'n03133878': (670, 'Crock_Pot'),
    'n03134739': (756, 'croquet_ball'),
    'n03141823': (856, 'crutch'),
    'n03146219': (865, 'cuirass'),
    'n03160309': (720, 'dam'),
    'n03179701': (313, 'desk'),
    'n03180011': (550, 'desktop_computer'),
    'n03187595': (959, 'dial_telephone'),
    'n03188531': (966, 'diaper'),
    'n03196217': (523, 'digital_clock'),
    'n03197337': (529, 'digital_watch'),
    'n03201208': (315, 'dining_table'),
    'n03207743': (821, 'dishrag'),
    'n03207941': (667, 'dishwasher'),
    'n03208938': (579, 'disk_brake'),
    'n03216828': (715, 'dock'),
    'n03218198': (253, 'dogsled'),
    'n03220513': (897, 'dome'),
    'n03223299': (972, 'doormat'),
    'n03240683': (834, 'drilling_platform'),
    'n03249569': (336, 'drum'),
    'n03250847': (799, 'drumstick'),
    'n03255030': (1000, 'dumbbell'),
    'n03259280': (662, 'Dutch_oven'),
    'n03271574': (512, 'electric_fan'),
    'n03272010': (346, 'electric_guitar'),
    'n03272562': (262, 'electric_locomotive'),
    'n03290653': (316, 'entertainment_center'),
    'n03291819': (879, 'envelope'),
    'n03297495': (660, 'espresso_maker'),
    'n03314780': (808, 'face_powder'),
    'n03325584': (796, 'feather_boa'),
    'n03337140': (305, 'file'),
    'n03344393': (235, 'fireboat'),
    'n03345487': (279, 'fire_engine'),
    'n03347037': (919, 'fire_screen'),
    'n03355925': (995, 'flagpole'),
    'n03372029': (356, 'flute'),
    'n03376595': (309, 'folding_chair'),
    'n03379051': (784, 'football_helmet'),
    'n03384352': (261, 'forklift'),
    'n03388043': (712, 'fountain'),
    'n03388183': (934, 'fountain_pen'),
    'n03388549': (299, 'four-poster'),
    'n03393912': (256, 'freight_car'),
    'n03394916': (348, 'French_horn'),
    'n03400231': (671, 'frying_pan'),
    'n03404251': (757, 'fur_coat'),
    'n03417042': (280, 'garbage_truck'),
    'n03424325': (971, 'gasmask'),
    'n03425413': (567, 'gas_pump'),
    'n03443371': (955, 'goblet'),
    'n03444034': (275, 'go-kart'),
    'n03445777': (792, 'golf_ball'),
    'n03445924': (276, 'golfcart'),
    'n03447447': (236, 'gondola'),
    'n03447721': (337, 'gong'),
    'n03450230': (911, 'gown'),
    'n03452741': (227, 'grand_piano'),
    'n03457902': (684, 'greenhouse'),
    'n03459775': (725, 'grille'),
    'n03461385': (703, 'grocery_store'),
    'n03467068': (517, 'guillotine'),
    'n03476684': (581, 'hair_slide'),
    'n03476991': (895, 'hair_spray'),
    'n03478589': (249, 'half_track'),
    'n03481172': (375, 'hammer'),
    'n03482405': (840, 'hamper'),
    'n03483316': (505, 'hand_blower'),
    'n03485407': (551, 'hand-held_computer'),
    'n03485794': (750, 'handkerchief'),
    'n03492542': (573, 'hard_disc'),
    'n03494278': (350, 'harmonica'),
    'n03495258': (344, 'harp'),
    'n03496892': (554, 'harvester'),
    'n03498962': (369, 'hatchet'),
    'n03527444': (787, 'holster'),
    'n03529860': (696, 'home_theater'),
    'n03530642': (730, 'honeycomb'),
    'n03532672': (562, 'hook'),
    'n03534580': (802, 'hoopskirt'),
    'n03535780': (924, 'horizontal_bar'),
    'n03538406': (293, 'horse_cart'),
    'n03544143': (525, 'hourglass'),
    'n03584254': (980, 'iPod'),
    'n03584829': (659, 'iron'),
    'n03590841': (592, "jack-o'-lantern"),
    'n03594734': (748, 'jean'),
    'n03594945': (269, 'jeep'),
    'n03595614': (961, 'jersey'),
    'n03598930': (963, 'jigsaw_puzzle'),
    'n03599486': (294, 'jinrikisha'),
    'n03602883': (560, 'joystick'),
    'n03617480': (770, 'kimono'),
    'n03623198': (773, 'knee_pad'),
    'n03627232': (582, 'knot'),
    'n03630383': (918, 'lab_coat'),
    'n03633091': (892, 'ladle'),
    'n03637318': (814, 'lampshade'),
    'n03642806': (228, 'laptop'),
    'n03649909': (374, 'lawn_mower'),
    'n03657121': (988, 'lens_cap'),
    'n03658185': (371, 'letter_opener'),
    'n03661043': (687, 'library'),
    'n03662601': (238, 'lifeboat'),
    'n03666591': (546, 'lighter'),
    'n03670208': (270, 'limousine'),
    'n03673027': (244, 'liner'),
    'n03676483': (867, 'lipstick'),
    'n03680355': (973, 'Loafer'),
    'n03690938': (894, 'lotion'),
    'n03691459': (508, 'loudspeaker'),
    'n03692522': (536, 'loupe'),
    'n03697007': (697, 'lumbermill'),
    'n03706229': (532, 'magnetic_compass'),
    'n03709823': (818, 'mailbag'),
    'n03710193': (917, 'mailbox'),
    'n03710637': (782, 'maillot'),
    'n03710721': (977, 'maillot'),
    'n03717622': (763, 'manhole_cover'),
    'n03720891': (338, 'maraca'),
    'n03721384': (339, 'marimba'),
    'n03724870': (781, 'mask'),
    'n03729826': (984, 'matchstick'),
    'n03733131': (598, 'maypole'),
    'n03733281': (922, 'maze'),
    'n03733805': (946, 'measuring_cup'),
    'n03742115': (302, 'medicine_chest'),
    'n03743016': (717, 'megalith'),
    'n03759954': (509, 'microphone'),
    'n03761084': (661, 'microwave'),
    'n03763968': (866, 'military_uniform'),
    'n03764736': (875, 'milk_can'),
    'n03769881': (920, 'minibus'),
    'n03770439': (880, 'miniskirt'),
    'n03770679': (271, 'minivan'),
    'n03773504': (251, 'missile'),
    'n03775071': (871, 'mitten'),
    'n03775546': (829, 'mixing_bowl'),
    'n03776460': (290, 'mobile_home'),
    'n03777568': (272, 'Model_T'),
    'n03777754': (764, 'modem'),
    'n03781244': (686, 'monastery'),
    'n03782006': (869, 'monitor'),
    'n03785016': (277, 'moped'),
    'n03786901': (824, 'mortar'),
    'n03787032': (854, 'mortarboard'),
    'n03788195': (691, 'mosque'),
    'n03788365': (852, 'mosquito_net'),
    'n03791053': (260, 'motor_scooter'),
    'n03792782': (255, 'mountain_bike'),
    'n03792972': (728, 'mountain_tent'),
    'n03793489': (511, 'mouse'),
    'n03794056': (599, 'mousetrap'),
    'n03796401': (284, 'moving_van'),
    'n03803284': (588, 'muzzle'),
    'n03804744': (585, 'nail'),
    'n03814639': (595, 'neck_brace'),
    'n03814906': (755, 'necklace'),
    'n03825788': (915, 'nipple'),
    'n03832673': (552, 'notebook'),
    'n03837869': (699, 'obelisk'),
    'n03838899': (354, 'oboe'),
    'n03840681': (351, 'ocarina'),
    'n03841143': (520, 'odometer'),
    'n03843555': (513, 'oil_filter'),
    'n03854065': (333, 'organ'),
    'n03857828': (870, 'oscilloscope'),
    'n03866082': (937, 'overskirt'),
    'n03868242': (295, 'oxcart'),
    'n03868863': (506, 'oxygen_mask'),
    'n03871628': (921, 'packet'),
    'n03873416': (826, 'paddle'),
    'n03874293': (564, 'paddlewheel'),
    'n03874599': (584, 'padlock'),
    'n03876231': (504, 'paintbrush'),
    'n03877472': (759, 'pajama'),
    'n03877845': (685, 'palace'),
    'n03884397': (352, 'panpipe'),
    'n03887697': (877, 'paper_towel'),
    'n03888257': (942, 'parachute'),
    'n03888605': (994, 'parallel_bars'),
    'n03891251': (306, 'park_bench'),
    'n03891332': (527, 'parking_meter'),
    'n03895866': (257, 'passenger_car'),
    'n03899768': (679, 'patio'),
    'n03902125': (843, 'pay-phone'),
    'n03903868': (732, 'pedestal'),
    'n03908618': (842, 'pencil_box'),
    'n03908714': (850, 'pencil_sharpener'),
    'n03916031': (883, 'perfume'),
    'n03920288': (783, 'Petri_dish'),
    'n03924679': (789, 'photocopier'),
    'n03929660': (575, 'pick'),
    'n03929855': (926, 'pickelhaube'),
    'n03930313': (722, 'picket_fence'),
    'n03930630': (281, 'pickup'),
    'n03933933': (596, 'pier'),
    'n03935335': (931, 'piggy_bank'),
    'n03937543': (901, 'pill_bottle'),
    'n03938244': (888, 'pillow'),
    'n03942813': (841, 'ping-pong_ball'),
    'n03944341': (565, 'pinwheel'),
    'n03947888': (245, 'pirate'),
    'n03950228': (983, 'pitcher'),
    'n03954731': (372, 'plane'),
    'n03956157': (693, 'planetarium'),
    'n03958227': (964, 'plastic_bag'),
    'n03961711': (731, 'plate_rack'),
    'n03967562': (381, 'plow'),
    'n03970156': (378, 'plunger'),
    'n03976467': (857, 'Polaroid_camera'),
    'n03976657': (923, 'pole'),
    'n03977966': (285, 'police_van'),
    'n03980874': (855, 'poncho'),
    'n03982430': (314, 'pool_table'),
    'n03983396': (788, 'pop_bottle'),
    'n03991062': (838, 'pot'),
    'n03992509': (566, "potter's_wheel"),
    'n03995372': (373, 'power_drill'),
    'n03998194': (769, 'prayer_rug'),
    'n04004767': (556, 'printer'),
    'n04005630': (702, 'prison'),
    'n04008634': (542, 'projectile'),
    'n04009552': (534, 'projector'),
    'n04019541': (572, 'puck'),
    'n04023962': (846, 'punching_bag'),
    'n04026417': (939, 'purse'),
    'n04033901': (862, 'quill'),
    'n04033995': (976, 'quilt'),
    'n04037443': (273, 'racer'),
    'n04039381': (860, 'racket'),
    'n04040759': (571, 'radiator'),
    'n04041544': (863, 'radio'),
    'n04044716': (537, 'radio_telescope'),
    'n04049303': (927, 'rain_barrel'),
    'n04065272': (286, 'recreational_vehicle'),
    'n04067472': (570, 'reel'),
    'n04069434': (965, 'reflex_camera'),
    'n04070727': (668, 'refrigerator'),
    'n04074963': (578, 'remote_control'),
    'n04081281': (694, 'restaurant'),
    'n04086273': (219, 'revolver'),
    'n04090263': (541, 'rifle'),
    'n04099969': (310, 'rocking_chair'),
    'n04111531': (663, 'rotisserie'),
    'n04116512': (997, 'rubber_eraser'),
    'n04118538': (876, 'rugby_ball'),
    'n04118776': (519, 'rule'),
    'n04120489': (760, 'running_shoe'),
    'n04125021': (753, 'safe'),
    'n04127249': (586, 'safety_pin'),
    'n04131690': (952, 'saltshaker'),
    'n04133789': (751, 'sandal'),
    'n04136333': (938, 'sarong'),
    'n04141076': (355, 'sax'),
    'n04141327': (809, 'scabbard'),
    'n04141975': (521, 'scale'),
    'n04146614': (962, 'school_bus'),
    'n04147183': (221, 'schooner'),
    'n04149813': (729, 'scoreboard'),
    'n04152593': (510, 'screen'),
    'n04153751': (587, 'screw'),
    'n04154565': (379, 'screwdriver'),
    'n04162706': (589, 'seat_belt'),
    'n04179913': (559, 'sewing_machine'),
    'n04192698': (800, 'shield'),
    'n04200800': (709, 'shoe_shop'),
    'n04201297': (832, 'shoji'),
    'n04204238': (950, 'shopping_basket'),
    'n04204347': (259, 'shopping_cart'),
    'n04208210': (380, 'shovel'),
    'n04209133': (868, 'shower_cap'),
    'n04209239': (747, 'shower_curtain'),
    'n04228054': (590, 'ski'),
    'n04229816': (776, 'ski_mask'),
    'n04235860': (943, 'sleeping_bag'),
    'n04238763': (549, 'slide_rule'),
    'n04239074': (726, 'sliding_door'),
    'n04243546': (557, 'slot'),
    'n04251144': (507, 'snorkel'),
    'n04252077': (288, 'snowmobile'),
    'n04252225': (278, 'snowplow'),
    'n04254120': (960, 'soap_dispenser'),
    'n04254680': (222, 'soccer_ball'),
    'n04254777': (986, 'sock'),
    'n04258138': (577, 'solar_dish'),
    'n04259630': (925, 'sombrero'),
    'n04263257': (822, 'soup_bowl'),
    'n04264628': (858, 'space_bar'),
    'n04265275': (515, 'space_heater'),
    'n04266014': (234, 'space_shuttle'),
    'n04270147': (676, 'spatula'),
    'n04273569': (237, 'speedboat'),
    'n04275548': (600, 'spider_web'),
    'n04277352': (775, 'spindle'),
    'n04285008': (274, 'sports_car'),
    'n04286575': (593, 'spotlight'),
    'n04296562': (804, 'stage'),
    'n04310018': (263, 'steam_locomotive'),
    'n04311004': (680, 'steel_arch_bridge'),
    'n04311174': (340, 'steel_drum'),
    'n04317175': (530, 'stethoscope'),
    'n04325704': (998, 'stole'),
    'n04326547': (724, 'stone_wall'),
    'n04328186': (528, 'stopwatch'),
    'n04330267': (516, 'stove'),
    'n04332243': (514, 'strainer'),
    'n04335435': (287, 'streetcar'),
    'n04336792': (957, 'stretcher'),
    'n04344873': (311, 'studio_couch'),
    'n04346328': (692, 'stupa'),
    'n04347754': (247, 'submarine'),
    'n04350905': (794, 'suit'),
    'n04355338': (526, 'sundial'),
    'n04355933': (574, 'sunglass'),
    'n04356056': (535, 'sunglasses'),
    'n04357314': (810, 'sunscreen'),
    'n04366367': (681, 'suspension_bridge'),
    'n04367480': (828, 'swab'),
    'n04370456': (837, 'sweatshirt'),
    'n04371430': (945, 'swimming_trunks'),
    'n04371774': (569, 'swing'),
    'n04372370': (561, 'switch'),
    'n04376876': (531, 'syringe'),
    'n04380533': (304, 'table_lamp'),
    'n04389033': (250, 'tank'),
    'n04392985': (978, 'tape_player'),
    'n04398044': (675, 'teapot'),
    'n04399382': (786, 'teddy'),
    'n04404412': (944, 'television'),
    'n04409515': (970, 'tennis_ball'),
    'n04417672': (989, 'thatch'),
    'n04418357': (903, 'theater_curtain'),
    'n04423845': (758, 'thimble'),
    'n04428191': (555, 'thresher'),
    'n04429376': (308, 'throne'),
    'n04435653': (780, 'tile_roof'),
    'n04442312': (664, 'toaster'),
    'n04443257': (710, 'tobacco_shop'),
    'n04447861': (312, 'toilet_seat'),
    'n04456115': (594, 'torch'),
    'n04458633': (700, 'totem_pole'),
    'n04461696': (282, 'tow_truck'),
    'n04462240': (711, 'toyshop'),
    'n04465501': (289, 'tractor'),
    'n04467665': (283, 'trailer_truck'),
    'n04476259': (766, 'tray'),
    'n04479046': (825, 'trench_coat'),
    'n04482393': (291, 'tricycle'),
    'n04483307': (242, 'trimaran'),
    'n04485082': (597, 'tripod'),
    'n04486054': (678, 'triumphal_arch'),
    'n04487081': (882, 'trolleybus'),
    'n04487394': (349, 'trombone'),
    'n04493381': (765, 'tub'),
    'n04501370': (727, 'turnstile'),
    'n04505470': (544, 'typewriter_keyboard'),
    'n04507155': (220, 'umbrella'),
    'n04509417': (292, 'unicycle'),
    'n04515003': (334, 'upright'),
    'n04517823': (666, 'vacuum'),
    'n04522168': (874, 'vase'),
    'n04523525': (990, 'vault'),
    'n04525038': (969, 'velvet'),
    'n04525305': (558, 'vending_machine'),
    'n04532106': (790, 'vestment'),
    'n04532670': (682, 'viaduct'),
    'n04536866': (343, 'violin'),
    'n04540053': (936, 'volleyball'),
    'n04542943': (665, 'waffle_iron'),
    'n04548280': (524, 'wall_clock'),
    'n04548362': (928, 'wallet'),
    'n04550184': (317, 'wardrobe'),
    'n04552348': (231, 'warplane'),
    'n04553703': (906, 'washbasin'),
    'n04554684': (669, 'washer'),
    'n04557648': (958, 'water_bottle'),
    'n04560804': (819, 'water_jug'),
    'n04562935': (795, 'water_tower'),
    'n04579145': (772, 'whiskey_jug'),
    'n04579432': (502, 'whistle'),
    'n04584207': (899, 'wig'),
    'n04589890': (912, 'window_screen'),
    'n04590129': (904, 'window_shade'),
    'n04591157': (935, 'Windsor_tie'),
    'n04591713': (831, 'wine_bottle'),
    'n04592741': (503, 'wing'),
    'n04596742': (672, 'wok'),
    'n04597913': (951, 'wooden_spoon'),
    'n04599235': (815, 'wool'),
    'n04604644': (723, 'worm_fence'),
    'n04606251': (248, 'wreck'),
    'n04612504': (240, 'yawl'),
    'n04613696': (714, 'yurt'),
    'n06359193': (553, 'web_site'),
    'n06596364': (930, 'comic_book'),
    'n06785654': (791, 'crossword_puzzle'),
    'n06794110': (932, 'street_sign'),
    'n06874185': (861, 'traffic_light'),
    'n07248320': (774, 'book_jacket'),
    'n07565083': (803, 'menu'),
    'n07579787': (754, 'plate'),
    'n07583066': (813, 'guacamole'),
    'n07584110': (844, 'consomme'),
    'n07590611': (771, 'hot_pot'),
    'n07613480': (793, 'trifle'),
    'n07614500': (974, 'ice_cream'),
    'n07615774': (968, 'ice_lolly'),
    'n07684084': (873, 'French_loaf'),
    'n07693725': (768, 'bagel'),
    'n07695742': (975, 'pretzel'),
    'n07697313': (993, 'cheeseburger'),
    'n07697537': (885, 'hotdog'),
    'n07711569': (734, 'mashed_potato'),
    'n07714571': (736, 'head_cabbage'),
    'n07714990': (737, 'broccoli'),
    'n07715103': (738, 'cauliflower'),
    'n07716358': (739, 'zucchini'),
    'n07716906': (740, 'spaghetti_squash'),
    'n07717410': (741, 'acorn_squash'),
    'n07717556': (742, 'butternut_squash'),
    'n07718472': (743, 'cucumber'),
    'n07718747': (744, 'artichoke'),
    'n07720875': (735, 'bell_pepper'),
    'n07730033': (745, 'cardoon'),
    'n07734744': (746, 'mushroom'),
    'n07742313': (318, 'Granny_Smith'),
    'n07745940': (229, 'strawberry'),
    'n07747607': (319, 'orange'),
    'n07749582': (320, 'lemon'),
    'n07753113': (321, 'fig'),
    'n07753275': (322, 'pineapple'),
    'n07753592': (323, 'banana'),
    'n07754684': (324, 'jackfruit'),
    'n07760859': (325, 'custard_apple'),
    'n07768694': (326, 'pomegranate'),
    'n07802026': (816, 'hay'),
    'n07831146': (999, 'carbonara'),
    'n07836838': (953, 'chocolate_sauce'),
    'n07860988': (864, 'dough'),
    'n07871810': (806, 'meat_loaf'),
    'n07873807': (948, 'pizza'),
    'n07875152': (830, 'potpie'),
    'n07880968': (900, 'burrito'),
    'n07892512': (798, 'red_wine'),
    'n07920052': (947, 'espresso'),
    'n07930864': (859, 'cup'),
    'n07932039': (823, 'eggnog'),
    'n09193705': (361, 'alp'),
    'n09229709': (992, 'bubble'),
    'n09246464': (359, 'cliff'),
    'n09256479': (365, 'coral_reef'),
    'n09288635': (368, 'geyser'),
    'n09332890': (366, 'lakeside'),
    'n09399592': (363, 'promontory'),
    'n09421951': (364, 'sandbar'),
    'n09428293': (367, 'seashore'),
    'n09468604': (360, 'valley'),
    'n09472597': (362, 'volcano'),
    'n09835506': (954, 'ballplayer'),
    'n10148035': (848, 'groom'),
    'n10565667': (982, 'scuba_diver'),
    'n11879895': (330, 'rapeseed'),
    'n11939491': (357, 'daisy'),
    'n12057211': (358, "yellow_lady's_slipper"),
    'n12144580': (331, 'corn'),
    'n12267677': (327, 'acorn'),
    'n12620546': (328, 'hip'),
    'n12768682': (332, 'buckeye'),
    'n12985857': (886, 'coral_fungus'),
    'n12998815': (913, 'agaric'),
    'n13037406': (956, 'gyromitra'),
    'n13040303': (893, 'stinkhorn'),
    'n13044778': (878, 'earthstar'),
    'n13052670': (812, 'hen-of-the-woods'),
    'n13054560': (981, 'bolete'),
    'n13133613': (329, 'ear'),
    'n15075141': (889, 'toilet_tissue')}
```

#### cvpods/data/datasets/__init__.py

```python
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

from .citypersons import CityPersonsDataset
from .cityscapes import CityScapesDataset
from .coco import COCODataset
from .crowdhuman import CrowdHumanDataset
from .imagenet import ImageNetDataset
from .lvis import LVISDataset
from .objects365 import Objects365Dataset
from .torchvision_datasets import CIFAR10Dataset, STL10Datasets
from .voc import VOCDataset
from .widerface import WiderFaceDataset

__all__ = [
    "COCODataset",
    "VOCDataset",
    "CityScapesDataset",
    "ImageNetDataset",
    "WiderFaceDataset",
    "LVISDataset",
    "CityPersonsDataset",
    "Objects365Dataset",
    "CrowdHumanDataset",
    "CIFAR10Dataset",
    "STL10Datasets",
]
```

#### cvpods/data/datasets/lvis.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

import copy
import logging
import os
import os.path as osp

import numpy as np

import torch

from cvpods.structures import BoxMode
from cvpods.utils import PathManager, Timer

from ..base_dataset import BaseDataset
from ..detection_utils import (
    annotations_to_instances,
    check_image_size,
    create_keypoint_hflip_indices,
    filter_empty_instances,
    read_image
)
from ..registry import DATASETS
from .lvis_categories import LVIS_CATEGORIES
from .paths_route import _PREDEFINED_SPLITS_LVIS

"""
This file contains functions to parse LVIS-format annotations into dicts in "cvpods format".
"""

logger = logging.getLogger(__name__)


@DATASETS.register()
class LVISDataset(BaseDataset):
    def __init__(self, cfg, dataset_name, transforms=[], is_train=True):
        super(LVISDataset, self).__init__(cfg, dataset_name, transforms, is_train)

        assert (
            self.name.startswith("lvis_v0.5")
        ), "Only lvis_v0.5 is now supported, lvis_v1 will be supported in the future."

        image_root, json_file = _PREDEFINED_SPLITS_LVIS["lvis_v0.5"][self.name]
        self.json_file = osp.join(self.data_root, json_file) \
            if "://" not in image_root else osp.join(image_root, json_file)
        self.image_root = osp.join(self.data_root, image_root) \
            if "://" not in image_root else image_root

        self.meta = self._get_metadata()

        self.dataset_dicts = self._load_annotations(
            self.json_file,
            self.image_root)

        # fmt: off
        self.data_format = cfg.INPUT.FORMAT
        self.mask_on = cfg.MODEL.MASK_ON
        self.mask_format = cfg.INPUT.MASK_FORMAT
        self.filter_empty = cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS
        self.keypoint_on = cfg.MODEL.KEYPOINT_ON
        self.load_proposals = cfg.MODEL.LOAD_PROPOSALS
        self.proposal_files = cfg.DATASETS.PROPOSAL_FILES_TRAIN
        # fmt: on

        if is_train:
            self.dataset_dicts = self._filter_annotations(
                filter_empty=self.filter_empty,
                min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE
                if self.keypoint_on else 0,
                proposal_files=self.proposal_files if self.load_proposals else None,
            )
            self._set_group_flag()

        self.eval_with_gt = cfg.TEST.get("WITH_GT", False)

        if self.keypoint_on:
            # Flip only makes sense in training
            self.keypoint_hflip_indices = create_keypoint_hflip_indices(
                cfg.DATASETS.TRAIN)
        else:
            self.keypoint_hflip_indices = None

    def __getitem__(self, index):
        """Load data, apply transforms, converto to Instances.
        """
        dataset_dict = copy.deepcopy(self.dataset_dicts[index])

        # read image
        image = read_image(dataset_dict["file_name"], format=self.data_format)
        check_image_size(dataset_dict, image)

        if "annotations" in dataset_dict:
            annotations = dataset_dict.pop("annotations")
            annotations = [
                ann for ann in annotations if ann.get("iscrowd", 0) == 0]
        else:
            annotations = None

        # apply transfrom
        image, annotations = self._apply_transforms(
            image, annotations)

        if annotations is not None:
            image_shape = image.shape[:2]  # h, w

            instances = annotations_to_instances(
                annotations, image_shape, mask_format=self.mask_format
            )

            # # Create a tight bounding box from masks, useful when image is cropped
            # if self.crop_gen and instances.has("gt_masks"):
            #     instances.gt_boxes = instances.gt_masks.get_bounding_boxes()

            dataset_dict["instances"] = filter_empty_instances(instances)

        # convert to Instance type
        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,
        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.
        # Therefore it's important to use torch.Tensor.
        # h, w, c -> c, h, w
        dataset_dict["image"] = torch.as_tensor(
            np.ascontiguousarray(image.transpose(2, 0, 1)))

        return dataset_dict

    def __reset__(self):
        raise NotImplementedError

    def __len__(self):
        return len(self.dataset_dicts)

    def _load_annotations(self,
                          json_file,
                          image_root):
        """
        Load a json file in LVIS's annotation format.
        Args:
            json_file (str): full path to the LVIS json annotation file.
            image_root (str): the directory where the images in this json file exists.
        Returns:
            list[dict]: a list of dicts in cvpods standard format. (See
            `Using Custom Datasets </tutorials/datasets.html>`_ )
        Notes:
            1. This function does not read the image files.
            The results do not have the "image" field.
        """
        from lvis import LVIS

        json_file = PathManager.get_local_path(json_file)

        timer = Timer()
        lvis_api = LVIS(json_file)
        if timer.seconds() > 1:
            logger.info("Loading {} takes {:.2f} seconds.".format(json_file, timer.seconds()))

        # sort indices for reproducible results
        img_ids = sorted(lvis_api.imgs.keys())
        # imgs is a list of dicts, each looks something like:
        # {'license': 4,
        #  'url': 'http://farm6.staticflickr.com/5454/9413846304_881d5e5c3b_z.jpg',
        #  'file_name': 'COCO_val2014_000000001268.jpg',
        #  'height': 427,
        #  'width': 640,
        #  'date_captured': '2013-11-17 05:57:24',
        #  'id': 1268}
        imgs = lvis_api.load_imgs(img_ids)
        # anns is a list[list[dict]], where each dict is an annotation
        # record for an object. The inner list enumerates the objects in an image
        # and the outer list enumerates over images. Example of anns[0]:
        # [{'segmentation': [[192.81,
        #     247.09,
        #     ...
        #     219.03,
        #     249.06]],
        #   'area': 1035.749,
        #   'image_id': 1268,
        #   'bbox': [192.81, 224.8, 74.73, 33.43],
        #   'category_id': 16,
        #   'id': 42986},
        #  ...]
        anns = [lvis_api.img_ann_map[img_id] for img_id in img_ids]

        # Sanity check that each annotation has a unique id
        ann_ids = [ann["id"] for anns_per_image in anns for ann in anns_per_image]
        assert len(set(ann_ids)) == len(ann_ids), "Annotation ids in '{}' are not unique".format(
            json_file
        )

        imgs_anns = list(zip(imgs, anns))

        logger.info("Loaded {} images in the LVIS format from {}".format(len(imgs_anns), json_file))

        dataset_dicts = []
        for (img_dict, anno_dict_list) in imgs_anns:
            record = {}
            file_name = img_dict["file_name"]
            if img_dict["file_name"].startswith("COCO"):
                # Convert form the COCO 2014 file naming convention of
                # COCO_[train/val/test]2014_000000000000.jpg to the 2017 naming convention of
                # 000000000000.jpg (LVIS v1 will fix this naming issue)
                file_name = file_name[-16:]
            record["file_name"] = os.path.join(image_root, file_name)
            record["height"] = img_dict["height"]
            record["width"] = img_dict["width"]
            record["not_exhaustive_category_ids"] = img_dict.get("not_exhaustive_category_ids", [])
            record["neg_category_ids"] = img_dict.get("neg_category_ids", [])
            image_id = record["image_id"] = img_dict["id"]

            objs = []
            for anno in anno_dict_list:
                # Check that the image_id in this annotation is the same as
                # the image_id we're looking at.
                # This fails only when the data parsing logic or the annotation file is buggy.
                assert anno["image_id"] == image_id
                obj = {"bbox": anno["bbox"], "bbox_mode": BoxMode.XYWH_ABS}
                obj["category_id"] = anno["category_id"] - 1  # Convert 1-indexed to 0-indexed
                segm = anno["segmentation"]  # list[list[float]]
                # filter out invalid polygons (< 3 points)
                valid_segm = [poly for poly in segm if len(poly) % 2 == 0 and len(poly) >= 6]
                assert len(segm) == len(
                    valid_segm
                ), "Annotation contains an invalid polygon with < 3 points"
                assert len(segm) > 0
                obj["segmentation"] = segm
                objs.append(obj)
            record["annotations"] = objs
            dataset_dicts.append(record)

        return dataset_dicts

    def _get_metadata(self):
        if "lvis_v0.5" in self.name:
            assert len(LVIS_CATEGORIES) == 1230
            cat_ids = [k["id"] for k in LVIS_CATEGORIES]
            assert min(cat_ids) == 1 and max(cat_ids) == len(
                cat_ids
            ), "Category ids are not in [1, #categories], as expected"
            # Ensure that the category list is sorted by id
            lvis_categories = sorted(LVIS_CATEGORIES, key=lambda x: x["id"])
            thing_classes = [k["synonyms"][0] for k in lvis_categories]
            meta = {
                "thing_classes": thing_classes
            }
        # There will be a v1 in the future
        # elif "lvis_v1" in self.name:
        #   return _get_lvis_instances_meta_v1()
        else:
            raise ValueError("No built-in metadata for dataset {}.".format(self.name))
        meta["evaluator_type"] = _PREDEFINED_SPLITS_LVIS["evaluator_type"]["lvis_v0.5"]
        meta["image_root"] = self.image_root
        meta["json_file"] = self.json_file
        return meta

    def evaluate(self, predictions):
        """Dataset must provide a evaluation function to evaluate model."""
        raise NotImplementedError

    @property
    def ground_truth_annotations(self):
        return self.dataset_dicts
```

#### cvpods/data/datasets/torchvision_datasets.py

```python
import os.path as osp
from copy import deepcopy
from typing import Optional

import numpy as np
from PIL import Image

import torch
from torchvision.datasets import CIFAR10, STL10

import cvpods
from cvpods.data.base_dataset import BaseDataset
from cvpods.data.registry import DATASETS, PATH_ROUTES

_PREDEFINED_SPLITS_CIFAR10 = {
    "dataset_type": "CIFAR10Dataset",
    "evaluator_type": {"cifar10": "classification"},
    "cifar10": {
        "cifar10_train": ("cifar10", "train"),
        "cifar10_test": ("cifar10", "test"),
    },
}
PATH_ROUTES.register(_PREDEFINED_SPLITS_CIFAR10, "CIFAR10")


@DATASETS.register()
class CIFAR10Dataset(CIFAR10):

    def __init__(self, cfg, dataset_name, transforms, is_train=True, **kwargs):

        self.cfg = cfg
        self.misc = kwargs

        image_root, split = _PREDEFINED_SPLITS_CIFAR10["cifar10"][dataset_name]
        self.data_root = osp.join(osp.split(osp.split(cvpods.__file__)[0])[0], "datasets")

        if is_train:
            assert split == "train"
        else:
            assert split == "test"

        super(CIFAR10Dataset, self).__init__(
            root=osp.join(self.data_root, image_root),
            train=is_train,
            download=True,
            transform=transforms)
        self.aspect_ratios = np.zeros(len(self), dtype=np.uint8)
        self.transforms = self.transform
        self._apply_transforms = BaseDataset._apply_transforms
        self.meta["evaluator_type"] = "classification"

    def __getitem__(self, index):
        image, target = Image.fromarray(self.data[index]), self.targets[index]
        dataset_dict = {"image_id": index, "category_id": target}

        image = image.convert("RGB")
        image = np.asarray(image)
        # flip channels if needed for RGB to BGR
        image = image[:, :, ::-1]

        images, _ = self._apply_transforms(self, image, dataset_dict)

        def process(dd, img):
            if img.shape[0] == 3:  # CHW
                dd["image"] = torch.as_tensor(np.ascontiguousarray(img))
            elif len(img.shape) == 3 and img.shape[-1] == 3:
                dd["image"] = torch.as_tensor(
                    np.ascontiguousarray(img.transpose(2, 0, 1)))
            elif len(img.shape) == 4 and img.shape[-1] == 3:
                # NHWC -> NCHW
                dd["image"] = torch.as_tensor(
                    np.ascontiguousarray(img.transpose(0, 3, 1, 2)))

            return dd

        if isinstance(images, dict):
            ret = {}
            # multiple input pipelines
            for desc, item in images.items():
                img, anno = item
                ret[desc] = process(deepcopy(dataset_dict), img)
            return ret
        else:
            return process(dataset_dict, images)


_PREDEFINED_SPLITS_STL10 = {
    "dataset_type": "STL10Datasets",
    "evaluator_type": {"stl10": "classification"},
    "stl10": {
        "stl10_train": ("stl10", "train"),
        "stl10_unlabeled": ("stl10", "unlabeled"),
        "stl10_test": ("stl10", "test"),
    },
}
PATH_ROUTES.register(_PREDEFINED_SPLITS_STL10, "STL10")


@DATASETS.register()
class STL10Datasets(STL10):
    def __init__(self, cfg, dataset_name, transforms=[], is_train=True, **kwargs):
        self.cfg = cfg
        self.misc = kwargs

        image_root, split = _PREDEFINED_SPLITS_STL10["stl10"][dataset_name]
        self.data_root = osp.join(osp.split(osp.split(cvpods.__file__)[0])[0], "datasets")
        self.image_root = osp.join(self.data_root, image_root)
        super(STL10Datasets, self).__init__(
            self.image_root, split=split, download=True, transform=None)

        self.aspect_ratios = np.zeros(len(self), dtype=np.uint8)
        self.transforms = transforms
        self._apply_transforms = BaseDataset._apply_transforms

        self.is_train = is_train
        self.meta = {"evaluator_type": _PREDEFINED_SPLITS_STL10["evaluator_type"]["stl10"]}

    def __getitem__(self, index):

        target: Optional[int]
        if self.labels is not None:
            img, target = self.data[index], int(self.labels[index])
        else:
            img, target = self.data[index], None

        # doing this so that it is consistent with all other datasets
        # to return a PIL Image
        image = Image.fromarray(np.transpose(img, (1, 2, 0)))
        dataset_dict = {"image_id": index, "category_id": target}

        # format == BGR in cvpods
        image = image.convert("RGB")
        image = np.asarray(image)
        image = image[:, :, ::-1]  # flip channels for RGB -> BGR format

        images, _ = self._apply_transforms(self, image, dataset_dict)

        def process(dd, img):
            if img.shape[0] == 3:  # CHW
                dd["image"] = torch.as_tensor(np.ascontiguousarray(img))
            if len(img.shape) == 3 and img.shape[-1] == 3:
                dd["image"] = torch.as_tensor(np.ascontiguousarray(img.transpose(2, 0, 1)))
            elif len(img.shape) == 4 and img.shape[-1] == 3:
                # NHWC -> NCHW
                dd["image"] = torch.as_tensor(np.ascontiguousarray(img.transpose(0, 3, 1, 2)))

            return dd

        if isinstance(images, dict):
            ret = {}
            # multiple input pipelines
            for desc, item in images.items():
                img, anno = item
                ret[desc] = process(deepcopy(dataset_dict), img)
            return ret
        else:
            return process(dataset_dict, images)
```

#### cvpods/data/datasets/citypersons.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import contextlib
import copy
import io
import logging
import os
import os.path as osp

import numpy as np

import torch

from cvpods.structures import BoxMode
from cvpods.utils import PathManager, Timer

from ..base_dataset import BaseDataset
from ..detection_utils import (
    annotations_to_instances,
    check_image_size,
    create_keypoint_hflip_indices,
    filter_empty_instances,
    read_image
)
from ..registry import DATASETS
from .paths_route import _PREDEFINED_SPLITS_CITYPERSONS

"""
This file contains functions to parse COCO-format annotations into dicts in "cvpods format".
"""

logger = logging.getLogger(__name__)


@DATASETS.register()
class CityPersonsDataset(BaseDataset):
    def __init__(self, cfg, dataset_name, transforms=[], is_train=True):
        super(CityPersonsDataset, self).__init__(cfg, dataset_name, transforms, is_train)

        image_root, json_file = _PREDEFINED_SPLITS_CITYPERSONS["citypersons"][self.name]
        self.json_file = osp.join(self.data_root, json_file) \
            if "://" not in image_root else osp.join(image_root, json_file)
        self.image_root = osp.join(self.data_root, image_root) \
            if "://" not in image_root else image_root

        self.meta = self._get_metadata()

        self.dataset_dicts = self._load_annotations(
            self.json_file,
            self.image_root,
            self.name,
            extra_annotation_keys=None)

        # fmt: off
        self.data_format = cfg.INPUT.FORMAT
        self.mask_on = cfg.MODEL.MASK_ON
        self.mask_format = cfg.INPUT.MASK_FORMAT
        self.filter_empty = cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS
        self.keypoint_on = cfg.MODEL.KEYPOINT_ON
        self.load_proposals = cfg.MODEL.LOAD_PROPOSALS
        self.proposal_files = cfg.DATASETS.PROPOSAL_FILES_TRAIN
        # fmt: on

        if is_train:
            self.dataset_dicts = self._filter_annotations(
                filter_empty=self.filter_empty,
                min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE
                if self.keypoint_on else 0,
                proposal_files=self.proposal_files if self.load_proposals else None,
            )
            self._set_group_flag()

        self.eval_with_gt = cfg.TEST.get("WITH_GT", False)

        if self.keypoint_on:
            # Flip only makes sense in training
            self.keypoint_hflip_indices = create_keypoint_hflip_indices(
                cfg.DATASETS.TRAIN)
        else:
            self.keypoint_hflip_indices = None

    def __getitem__(self, index):
        """Load data, apply transforms, converto to Instances.
        """
        dataset_dict = copy.deepcopy(self.dataset_dicts[index])

        # read image
        image = read_image(dataset_dict["file_name"], format=self.data_format)
        check_image_size(dataset_dict, image)

        if "annotations" in dataset_dict:
            annotations = dataset_dict.pop("annotations")
            annotations = [
                ann for ann in annotations if ann.get("iscrowd", 0) == 0]
        else:
            annotations = None

        # apply transfrom
        image, annotations = self._apply_transforms(
            image, annotations)

        if annotations is not None:
            image_shape = image.shape[:2]  # h, w

            instances = annotations_to_instances(
                annotations, image_shape, mask_format=self.mask_format
            )

            # # Create a tight bounding box from masks, useful when image is cropped
            # if self.crop_gen and instances.has("gt_masks"):
            #     instances.gt_boxes = instances.gt_masks.get_bounding_boxes()

            dataset_dict["instances"] = filter_empty_instances(instances)

        # convert to Instance type
        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,
        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.
        # Therefore it's important to use torch.Tensor.
        # h, w, c -> c, h, w
        dataset_dict["image"] = torch.as_tensor(
            np.ascontiguousarray(image.transpose(2, 0, 1)))

        return dataset_dict

    def __reset__(self):
        raise NotImplementedError

    def __len__(self):
        return len(self.dataset_dicts)

    def _load_annotations(  # noqa
        self,
        json_file,
        image_root,
        dataset_name=None,
        extra_annotation_keys=None
    ):
        """
        Load a json file with COCO's instances annotation format.
        Currently supports instance detection, instance segmentation,
        and person keypoints annotations.

        Args:
            json_file (str): full path to the json file in COCO instances annotation format.
            image_root (str): the directory where the images in this json file exists.
            dataset_name (str): the name of the dataset (e.g., coco_2017_train).
                If provided, this function will also put "thing_classes" into
                the metadata associated with this dataset.
            extra_annotation_keys (list[str]): list of per-annotation keys that should also be
                loaded into the dataset dict (besides "iscrowd", "bbox", "keypoints",
                "category_id", "segmentation"). The values for these keys will be returned as-is.
                For example, the densepose annotations are loaded in this way.

        Returns:
            list[dict]: a list of dicts in cvpods standard format. (See
            `Using Custom Datasets </tutorials/datasets.html>`_ )

        Notes:
            1. This function does not read the image files.
            The results do not have the "image" field.
        """
        from pycocotools.coco import COCO

        timer = Timer()
        json_file = PathManager.get_local_path(json_file)
        with contextlib.redirect_stdout(io.StringIO()):
            coco_api = COCO(json_file)
        if timer.seconds() > 1:
            logger.info("Loading {} takes {:.2f} seconds.".format(
                json_file, timer.seconds()))

        id_map = None
        if dataset_name is not None:
            meta = self.meta
            cat_ids = sorted(coco_api.getCatIds())
            cats = coco_api.loadCats(cat_ids)
            # The categories in a custom json file may not be sorted.
            thing_classes = [
                c["name"] for c in sorted(cats, key=lambda x: x["id"])
            ]
            meta["thing_classes"] = thing_classes

            # In COCO, certain category ids are artificially removed,
            # and by convention they are always ignored.
            # We deal with COCO's id issue and translate
            # the category ids to contiguous ids in [0, 80).

            # It works by looking at the "categories" field in the json, therefore
            # if users' own json also have incontiguous ids, we'll
            # apply this mapping as well but print a warning.
            if not (min(cat_ids) == 1 and max(cat_ids) == len(cat_ids)):
                if "coco" not in dataset_name:
                    logger.warning("""
    Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.
    """)
            id_map = {v: i for i, v in enumerate(cat_ids)}
            meta["thing_dataset_id_to_contiguous_id"] = id_map

        # sort indices for reproducible results
        img_ids = sorted(coco_api.imgs.keys())
        # imgs is a list of dicts, each looks something like:
        # {'license': 4,
        #  'url': 'http://farm6.staticflickr.com/5454/9413846304_881d5e5c3b_z.jpg',
        #  'file_name': 'COCO_val2014_000000001268.jpg',
        #  'height': 427,
        #  'width': 640,
        #  'date_captured': '2013-11-17 05:57:24',
        #  'id': 1268}
        imgs = coco_api.loadImgs(img_ids)
        # anns is a list[list[dict]], where each dict is an annotation
        # record for an object. The inner list enumerates the objects in an image
        # and the outer list enumerates over images. Example of anns[0]:
        # [{'segmentation': [[192.81,
        #     247.09,
        #     ...
        #     219.03,
        #     249.06]],
        #   'area': 1035.749,
        #   'iscrowd': 0,
        #   'image_id': 1268,
        #   'bbox': [192.81, 224.8, 74.73, 33.43],
        #   'category_id': 16,
        #   'id': 42986},
        #  ...]
        anns = [coco_api.imgToAnns[img_id] for img_id in img_ids]

        if "minival" not in json_file:
            # The popular valminusminival & minival annotations for COCO2014 contain this bug.
            # However the ratio of buggy annotations there is tiny and does not affect accuracy.
            # Therefore we explicitly white-list them.
            ann_ids = [
                ann["id"] for anns_per_image in anns for ann in anns_per_image
            ]
            assert len(set(ann_ids)) == len(
                ann_ids), "Annotation ids in '{}' are not unique!".format(
                    json_file)

        imgs_anns = list(zip(imgs, anns))

        logger.info("Loaded {} images in COCO format from {}".format(
            len(imgs_anns), json_file))

        dataset_dicts = []

        ann_keys = ["iscrowd", "bbox", "keypoints", "category_id"
                    ] + (extra_annotation_keys or [])

        num_instances_without_valid_segmentation = 0
        num_instances_without_valid_bbox = 0

        for (img_dict, anno_dict_list) in imgs_anns:
            record = {}
            record["file_name"] = os.path.join(image_root,
                                               img_dict["file_name"])
            record["height"] = img_dict["height"]
            record["width"] = img_dict["width"]
            image_id = record["image_id"] = img_dict["id"]

            objs = []
            for anno in anno_dict_list:
                # Check that the image_id in this annotation is the same as
                # the image_id we're looking at.
                # This fails only when the data parsing logic or the annotation file is buggy.

                # The original COCO valminusminival2014 & minival2014 annotation files
                # actually contains bugs that, together with certain ways of using COCO API,
                # can trigger this assertion.
                assert anno["image_id"] == image_id

                # Some annotations have negative coordinates
                if (np.array(anno["bbox"]) < 0).sum() > 0:
                    num_instances_without_valid_bbox += 1
                    continue

                if anno.get("ignore", 0) != 0:
                    continue

                obj = {key: anno[key] for key in ann_keys if key in anno}

                segm = anno.get("segmentation", None)
                if segm:    # either list[list[float]] or dict(RLE)
                    if not isinstance(segm, dict):
                        # filter out invalid polygons (< 3 points)
                        segm = [
                            poly for poly in segm
                            if len(poly) % 2 == 0 and len(poly) >= 6
                        ]
                        if len(segm) == 0:
                            num_instances_without_valid_segmentation += 1
                            continue    # ignore this instance
                    obj["segmentation"] = segm

                keypts = anno.get("keypoints", None)
                if keypts:    # list[int]
                    for idx, v in enumerate(keypts):
                        if idx % 3 != 2:
                            # COCO's segmentation coordinates are floating points in [0, H or W],
                            # but keypoint coordinates are integers in [0, H-1 or W-1]
                            # Therefore we assume the coordinates are "pixel indices" and
                            # add 0.5 to convert to floating point coordinates.
                            keypts[idx] = v + 0.5
                    obj["keypoints"] = keypts

                obj["bbox_mode"] = BoxMode.XYWH_ABS
                if id_map:
                    obj["category_id"] = id_map[obj["category_id"]]
                objs.append(obj)
            record["annotations"] = objs
            dataset_dicts.append(record)

        if num_instances_without_valid_bbox > 0:
            logger.warning(
                "Filtered out {} instances without valid bbox. "
                "There might be issues in your dataset generation process.".
                format(num_instances_without_valid_bbox))

        if num_instances_without_valid_segmentation > 0:
            logger.warning(
                "Filtered out {} instances without valid segmentation. "
                "There might be issues in your dataset generation process.".
                format(num_instances_without_valid_segmentation))
        return dataset_dicts

    def _get_metadata(self):
        meta = {"thing_classes": ["person"]}
        meta["evaluator_type"] = _PREDEFINED_SPLITS_CITYPERSONS["evaluator_type"]["citypersons"]
        meta["image_root"] = self.image_root
        meta["json_file"] = self.json_file
        return meta

    def evaluate(self, predictions):
        """Dataset must provide a evaluation function to evaluate model."""
        raise NotImplementedError

    @property
    def ground_truth_annotations(self):
        return self.dataset_dicts
```

#### cvpods/data/datasets/crowdhuman.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import copy
import json
import logging
import os
import os.path as osp

import numpy as np

import torch

from cvpods.structures import BoxMode
from cvpods.utils import PathManager, Timer

from ..base_dataset import BaseDataset
from ..detection_utils import (
    annotations_to_instances,
    check_image_size,
    create_keypoint_hflip_indices,
    filter_empty_instances,
    read_image
)
from ..registry import DATASETS
from .paths_route import _PREDEFINED_SPLITS_CROWDHUMAN

"""
This file contains functions to parse COCO-format annotations into dicts in "cvpods format".
"""

logger = logging.getLogger(__name__)


@DATASETS.register()
class CrowdHumanDataset(BaseDataset):
    def __init__(self, cfg, dataset_name, transforms=[], is_train=True):
        super(CrowdHumanDataset, self).__init__(cfg, dataset_name, transforms, is_train)
        self.dataset_key = "_".join(self.name.split('_')[:-1])
        image_root, json_file = _PREDEFINED_SPLITS_CROWDHUMAN[self.dataset_key][self.name]
        self.json_file = osp.join(self.data_root, json_file) \
            if "://" not in image_root else osp.join(image_root, json_file)
        self.image_root = osp.join(self.data_root, image_root) \
            if "://" not in image_root else image_root

        self.meta = self._get_metadata()

        self.dataset_dicts = self._load_annotations(
            self.json_file,
            self.image_root)

        # fmt: off
        self.data_format = cfg.INPUT.FORMAT
        self.mask_on = cfg.MODEL.MASK_ON
        self.mask_format = cfg.INPUT.MASK_FORMAT
        self.filter_empty = cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS
        self.keypoint_on = cfg.MODEL.KEYPOINT_ON
        self.load_proposals = cfg.MODEL.LOAD_PROPOSALS
        self.proposal_files = cfg.DATASETS.PROPOSAL_FILES_TRAIN
        # fmt: on

        if is_train:
            self.dataset_dicts = self._filter_annotations(
                filter_empty=self.filter_empty,
                min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE
                if self.keypoint_on else 0,
                proposal_files=self.proposal_files if self.load_proposals else None,
            )
            self._set_group_flag()

        self.eval_with_gt = cfg.TEST.get("WITH_GT", False)

        if self.keypoint_on:
            # Flip only makes sense in training
            self.keypoint_hflip_indices = create_keypoint_hflip_indices(
                cfg.DATASETS.TRAIN)
        else:
            self.keypoint_hflip_indices = None

    def __getitem__(self, index):
        """Load data, apply transforms, converto to Instances.
        """
        dataset_dict = copy.deepcopy(self.dataset_dicts[index])

        # read image
        image = read_image(dataset_dict["file_name"], format=self.data_format)
        check_image_size(dataset_dict, image)

        if "annotations" in dataset_dict:
            annotations = dataset_dict.pop("annotations")
            annotations = [
                ann for ann in annotations if ann.get("iscrowd", 0) == 0]
        else:
            annotations = None

        # apply transfrom
        image, annotations = self._apply_transforms(
            image, annotations)

        if annotations is not None:
            image_shape = image.shape[:2]  # h, w

            instances = annotations_to_instances(
                annotations, image_shape, mask_format=self.mask_format
            )

            # # Create a tight bounding box from masks, useful when image is cropped
            # if self.crop_gen and instances.has("gt_masks"):
            #     instances.gt_boxes = instances.gt_masks.get_bounding_boxes()

            dataset_dict["instances"] = filter_empty_instances(instances)

        # convert to Instance type
        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,
        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.
        # Therefore it's important to use torch.Tensor.
        # h, w, c -> c, h, w
        dataset_dict["image"] = torch.as_tensor(
            np.ascontiguousarray(image.transpose(2, 0, 1)))

        return dataset_dict

    def __reset__(self):
        raise NotImplementedError

    def __len__(self):
        return len(self.dataset_dicts)

    def _load_annotations(self, json_file, image_root):
        """
        Load a json file with CrowdHuman's instances annotation format.
        Currently supports instance detection, instance segmentation,
        and person keypoints annotations.

        Args:
            json_file (str): full path to the json file in CrowdHuman instances annotation format.
            image_root (str): the directory where the images in this json file exists.

        Returns:
            list[dict]: a list of dicts in cvpods standard format. (See
            `Using Custom Datasets </tutorials/datasets.html>`_ )

        Notes:
            1. This function does not read the image files.
               The results do not have the "image" field.
        """
        timer = Timer()
        json_file = PathManager.get_local_path(json_file)
        with open(json_file, 'r') as file:
            gt_records = file.readlines()
        if timer.seconds() > 1:
            logger.info("Loading {} takes {:.2f} seconds.".format(
                json_file, timer.seconds()))

        logger.info("Loaded {} images in CrowdHuman format from {}".format(
            len(gt_records), json_file))

        dataset_dicts = []

        ann_keys = ["tag", "hbox", "vbox", "head_attr", "extra"]
        for anno_str in gt_records:
            anno_dict = json.loads(anno_str)

            record = {}
            record["file_name"] = os.path.join(image_root, "{}.jpg".format(anno_dict["ID"]))
            record["image_id"] = anno_dict["ID"]

            objs = []
            for anno in anno_dict['gtboxes']:
                # Check that the image_id in this annotation is the same as
                # the image_id we're looking at.
                # This fails only when the data parsing logic or the annotation file is buggy.

                # The original COCO valminusminival2014 & minival2014 annotation files
                # actually contains bugs that, together with certain ways of using COCO API,
                # can trigger this assertion.
                obj = {key: anno[key] for key in ann_keys if key in anno}
                obj["bbox"] = anno["fbox"]
                obj["category_id"] = 0

                if 'extra' in anno and 'ignore' in anno['extra'] and anno['extra']['ignore'] != 0:
                    obj["category_id"] = -1

                obj["bbox_mode"] = BoxMode.XYWH_ABS
                objs.append(obj)
            record["annotations"] = objs
            dataset_dicts.append(record)

        return dataset_dicts

    def _get_metadata(self):
        meta = {}
        meta["image_root"] = self.image_root
        meta["json_file"] = self.json_file
        meta["evaluator_type"] = _PREDEFINED_SPLITS_CROWDHUMAN["evaluator_type"][self.dataset_key]
        meta["thing_classes"] = ['person']

        return meta

    def evaluate(self, predictions):
        """Dataset must provide a evaluation function to evaluate model."""
        raise NotImplementedError

    @property
    def ground_truth_annotations(self):
        return self.dataset_dicts
```

#### cvpods/data/datasets/voc.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

import copy
import logging
import os
import os.path as osp
import xml.etree.ElementTree as ET

import numpy as np

import torch

from cvpods.structures import BoxMode
from cvpods.utils import PathManager

from ..base_dataset import BaseDataset
from ..detection_utils import (
    annotations_to_instances,
    check_image_size,
    create_keypoint_hflip_indices,
    filter_empty_instances,
    read_image
)
from ..registry import DATASETS
from .paths_route import _PREDEFINED_SPLITS_VOC

"""
This file contains functions to parse ImageNet-format annotations into dicts in "cvpods format".
"""

logger = logging.getLogger(__name__)


@DATASETS.register()
class VOCDataset(BaseDataset):
    def __init__(self, cfg, dataset_name, transforms=[], is_train=True):
        super(VOCDataset, self).__init__(cfg, dataset_name, transforms, is_train)

        image_root, split = _PREDEFINED_SPLITS_VOC["voc"][self.name]
        self.image_root = osp.join(self.data_root, image_root) \
            if "://" not in image_root else image_root
        self.split = split

        self.meta = self._get_metadata()
        self.dataset_dicts = self._load_annotations()

        # fmt: off
        self.data_format = cfg.INPUT.FORMAT
        self.mask_on = cfg.MODEL.MASK_ON
        self.mask_format = cfg.INPUT.MASK_FORMAT
        self.filter_empty = cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS
        self.keypoint_on = cfg.MODEL.KEYPOINT_ON
        self.load_proposals = cfg.MODEL.LOAD_PROPOSALS
        self.proposal_files = cfg.DATASETS.PROPOSAL_FILES_TRAIN
        # fmt: on

        if is_train:
            self.dataset_dicts = self._filter_annotations(
                filter_empty=self.filter_empty,
                min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE
                if self.keypoint_on else 0,
                proposal_files=self.proposal_files if self.load_proposals else None,
            )
            self._set_group_flag()

        self.eval_with_gt = cfg.TEST.get("WITH_GT", False)

        if self.keypoint_on:
            # Flip only makes sense in training
            self.keypoint_hflip_indices = create_keypoint_hflip_indices(
                cfg.DATASETS.TRAIN)
        else:
            self.keypoint_hflip_indices = None

    def __getitem__(self, index):
        """Load data, apply transforms, converto to Instances.
        """
        dataset_dict = copy.deepcopy(self.dataset_dicts[index])

        # read image
        image = read_image(dataset_dict["file_name"], format=self.data_format)
        check_image_size(dataset_dict, image)

        if "annotations" in dataset_dict:
            annotations = dataset_dict.pop("annotations")
            annotations = [
                ann for ann in annotations if ann.get("iscrowd", 0) == 0]
        else:
            annotations = None

        # apply transfrom
        image, annotations = self._apply_transforms(
            image, annotations)

        if annotations is not None:
            image_shape = image.shape[:2]  # h, w

            instances = annotations_to_instances(
                annotations, image_shape, mask_format=self.mask_format
            )

            # # Create a tight bounding box from masks, useful when image is cropped
            # if self.crop_gen and instances.has("gt_masks"):
            #     instances.gt_boxes = instances.gt_masks.get_bounding_boxes()

            dataset_dict["instances"] = filter_empty_instances(instances)

        # convert to Instance type
        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,
        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.
        # Therefore it's important to use torch.Tensor.
        # h, w, c -> c, h, w
        dataset_dict["image"] = torch.as_tensor(
            np.ascontiguousarray(image.transpose(2, 0, 1)))

        return dataset_dict

    def __len__(self):
        return len(self.dataset_dicts)

    def _get_metadata(self):
        # fmt: off
        thing_classes = [
            "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat",
            "chair", "cow", "diningtable", "dog", "horse", "motorbike", "person",
            "pottedplant", "sheep", "sofa", "train", "tvmonitor",
        ]
        meta = {
            "thing_classes": thing_classes,
            "evaluator_type": _PREDEFINED_SPLITS_VOC["evaluator_type"]["voc"],
            "dirname": self.image_root,
            "split": self.split,
            "year": 2007,
        }
        return meta

    def _load_annotations(self):
        """
        Load Pascal VOC detection annotations to cvpods format.

        Args:
            dirname: Contain "Annotations", "ImageSets", "JPEGImages"
            split (str): one of "train", "test", "val", "trainval"
        """

        dirname = self.image_root
        split = self.split

        with PathManager.open(os.path.join(dirname, "ImageSets", "Main", split + ".txt")) as f:
            fileids = np.loadtxt(f, dtype=np.str)

        dicts = []
        for fileid in fileids:
            anno_file = os.path.join(dirname, "Annotations", fileid + ".xml")
            jpeg_file = os.path.join(dirname, "JPEGImages", fileid + ".jpg")

            tree = ET.parse(anno_file)

            r = {
                "file_name": jpeg_file,
                "image_id": fileid,
                "height": int(tree.findall("./size/height")[0].text),
                "width": int(tree.findall("./size/width")[0].text),
            }
            instances = []

            for obj in tree.findall("object"):
                cls = obj.find("name").text
                # We include "difficult" samples in training.
                # Based on limited experiments, they don't hurt accuracy.
                # difficult = int(obj.find("difficult").text)
                # if difficult == 1:
                # continue
                bbox = obj.find("bndbox")
                bbox = [float(bbox.find(x).text) for x in ["xmin", "ymin", "xmax", "ymax"]]
                # Original annotations are integers in the range [1, W or H]
                # Assuming they mean 1-based pixel indices (inclusive),
                # a box with annotation (xmin=1, xmax=W) covers the whole image.
                # In coordinate space this is represented by (xmin=0, xmax=W)
                bbox[0] -= 1.0
                bbox[1] -= 1.0
                instances.append({
                    "category_id": CLASS_NAMES.index(cls),
                    "bbox": bbox,
                    "bbox_mode": BoxMode.XYXY_ABS
                })
            r["annotations"] = instances
            dicts.append(r)

        return dicts


# fmt: off
CLASS_NAMES = [
    "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat",
    "chair", "cow", "diningtable", "dog", "horse", "motorbike", "person",
    "pottedplant", "sheep", "sofa", "train", "tvmonitor",
]
```

#### cvpods/data/datasets/widerface.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import contextlib
import copy
import io
import logging
import os
import os.path as osp

import numpy as np

import torch

from cvpods.structures import BoxMode
from cvpods.utils import PathManager, Timer

from ..base_dataset import BaseDataset
from ..detection_utils import (
    annotations_to_instances,
    check_image_size,
    create_keypoint_hflip_indices,
    filter_empty_instances,
    read_image
)
from ..registry import DATASETS
from .paths_route import _PREDEFINED_SPLITS_WIDERFACE

"""
This file contains functions to parse COCO-format annotations into dicts in "cvpods format".
"""

logger = logging.getLogger(__name__)


@DATASETS.register()
class WiderFaceDataset(BaseDataset):
    def __init__(self, cfg, dataset_name, transforms=[], is_train=True):
        super(WiderFaceDataset, self).__init__(cfg, dataset_name, transforms, is_train)
        self.dataset_key = "_".join(self.name.split('_')[:-1])
        image_root, json_file = _PREDEFINED_SPLITS_WIDERFACE[self.dataset_key][self.name]
        self.json_file = osp.join(self.data_root, json_file) \
            if "://" not in image_root else osp.join(image_root, json_file)
        self.image_root = osp.join(self.data_root, image_root) \
            if "://" not in image_root else image_root

        self.meta = self._get_metadata()

        self.dataset_dicts = self._load_annotations(
            self.json_file,
            self.image_root,
            self.name,
            extra_annotation_keys=None)

        # fmt: off
        self.data_format = cfg.INPUT.FORMAT
        self.mask_on = cfg.MODEL.MASK_ON
        self.mask_format = cfg.INPUT.MASK_FORMAT
        self.filter_empty = cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS
        self.keypoint_on = cfg.MODEL.KEYPOINT_ON
        self.load_proposals = cfg.MODEL.LOAD_PROPOSALS
        self.proposal_files = cfg.DATASETS.PROPOSAL_FILES_TRAIN
        # fmt: on

        if is_train:
            self.dataset_dicts = self._filter_annotations(
                filter_empty=self.filter_empty,
                min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE
                if self.keypoint_on else 0,
                proposal_files=self.proposal_files if self.load_proposals else None,
            )
            self._set_group_flag()

        self.eval_with_gt = cfg.TEST.get("WITH_GT", False)

        if self.keypoint_on:
            # Flip only makes sense in training
            self.keypoint_hflip_indices = create_keypoint_hflip_indices(
                cfg.DATASETS.TRAIN)
        else:
            self.keypoint_hflip_indices = None

    def __getitem__(self, index):
        """Load data, apply transforms, converto to Instances.
        """
        dataset_dict = copy.deepcopy(self.dataset_dicts[index])

        # read image
        image = read_image(dataset_dict["file_name"], format=self.data_format)
        check_image_size(dataset_dict, image)

        if "annotations" in dataset_dict:
            annotations = dataset_dict.pop("annotations")
            annotations = [
                ann for ann in annotations if ann.get("iscrowd", 0) == 0]
        else:
            annotations = None

        # apply transfrom
        image, annotations = self._apply_transforms(
            image, annotations)

        if annotations is not None:
            image_shape = image.shape[:2]  # h, w

            instances = annotations_to_instances(
                annotations, image_shape, mask_format=self.mask_format
            )

            # # Create a tight bounding box from masks, useful when image is cropped
            # if self.crop_gen and instances.has("gt_masks"):
            #     instances.gt_boxes = instances.gt_masks.get_bounding_boxes()

            dataset_dict["instances"] = filter_empty_instances(instances)

        # convert to Instance type
        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,
        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.
        # Therefore it's important to use torch.Tensor.
        # h, w, c -> c, h, w
        dataset_dict["image"] = torch.as_tensor(
            np.ascontiguousarray(image.transpose(2, 0, 1)))

        return dataset_dict

    def __reset__(self):
        raise NotImplementedError

    def __len__(self):
        return len(self.dataset_dicts)

    def _load_annotations(
        self, json_file, image_root,
        dataset_name=None, extra_annotation_keys=None
    ):
        """
        Load a json file with WiderFace's instances annotation format.
        Currently supports instance detection, instance segmentation,
        and person keypoints annotations.

        Args:
            json_file (str): full path to the json file in WiderFace instances annotation format.
            image_root (str): the directory where the images in this json file exists.
            dataset_name (str): the name of the dataset (e.g., widerface_2019_train).
                If provided, this function will also put "thing_classes" into
                the metadata associated with this dataset.
            extra_annotation_keys (list[str]): list of per-annotation keys that should also be
                loaded into the dataset dict (besides "iscrowd", "bbox", "keypoints",
                "category_id", "segmentation"). The values for these keys will be returned as-is.
                For example, the densepose annotations are loaded in this way.

        Returns:
            list[dict]: a list of dicts in cvpods standard format. (See
            `Using Custom Datasets </tutorials/datasets.html>`_ )

        Notes:
            1. This function does not read the image files.
               The results do not have the "image" field.
        """
        from pycocotools.coco import COCO

        timer = Timer()
        json_file = PathManager.get_local_path(json_file)
        with contextlib.redirect_stdout(io.StringIO()):
            coco_api = COCO(json_file)
        if timer.seconds() > 1:
            logger.info("Loading {} takes {:.2f} seconds.".format(json_file, timer.seconds()))

        id_map = None
        if dataset_name is not None:
            meta = self.meta
            cat_ids = sorted(coco_api.getCatIds())

            id_map = {v: i for i, v in enumerate(cat_ids)}
            meta["thing_dataset_id_to_contiguous_id"] = id_map

        # sort indices for reproducible results
        img_ids = sorted(list(coco_api.imgs.keys()))
        # imgs is a list of dicts, each looks something like:
        # {'license': 4,
        #  'url': 'http://farm6.staticflickr.com/5454/9413846304_881d5e5c3b_z.jpg',
        #  'file_name': 'COCO_val2014_000000001268.jpg',
        #  'height': 427,
        #  'width': 640,
        #  'date_captured': '2013-11-17 05:57:24',
        #  'id': 1268}
        imgs = coco_api.loadImgs(img_ids)
        # anns is a list[list[dict]], where each dict is an annotation
        # record for an object. The inner list enumerates the objects in an image
        # and the outer list enumerates over images. Example of anns[0]:
        # [{'segmentation': [[192.81,
        #     247.09,
        #     ...
        #     219.03,
        #     249.06]],
        #   'area': 1035.749,
        #   'iscrowd': 0,
        #   'image_id': 1268,
        #   'bbox': [192.81, 224.8, 74.73, 33.43],
        #   'category_id': 16,
        #   'id': 42986},
        #  ...]
        anns = [coco_api.imgToAnns[img_id] for img_id in img_ids]

        if "minival" not in json_file:
            # The popular valminusminival & minival annotations for COCO2014 contain this bug.
            # However the ratio of buggy annotations there is tiny and does not affect accuracy.
            # Therefore we explicitly white-list them.
            ann_ids = [ann["id"] for anns_per_image in anns for ann in anns_per_image]
            assert len(set(ann_ids)) == len(ann_ids), \
                "Annotation ids in '{}' are not unique!".format(json_file)

        imgs_anns = list(zip(imgs, anns))

        logger.info("Loaded {} images in COCO format from {}".format(len(imgs_anns), json_file))

        dataset_dicts = []

        ann_keys = ["iscrowd", "bbox", "keypoints", "category_id"] + (extra_annotation_keys or [])

        for (img_dict, anno_dict_list) in imgs_anns:
            record = {}
            record["file_name"] = os.path.join(image_root, img_dict["file_name"])
            record["height"] = img_dict["height"]
            record["width"] = img_dict["width"]
            image_id = record["image_id"] = img_dict["id"]

            objs = []
            for anno in anno_dict_list:
                # Check that the image_id in this annotation is the same as
                # the image_id we're looking at.
                # This fails only when the data parsing logic or the annotation file is buggy.

                # The original COCO valminusminival2014 & minival2014 annotation files
                # actually contains bugs that, together with certain ways of using COCO API,
                # can trigger this assertion.
                assert anno["image_id"] == image_id

                # ensure the width and height of bbox are greater than 0
                if anno["bbox"][2] <= 0 or anno["bbox"][3] <= 0:
                    continue

                if anno.get("ignore", 0) != 0:
                    continue

                obj = {key: anno[key] for key in ann_keys if key in anno}

                obj["bbox_mode"] = BoxMode.XYWH_ABS
                if id_map:
                    obj["category_id"] = id_map[obj["category_id"]]
                objs.append(obj)
            record["annotations"] = objs
            dataset_dicts.append(record)
        return dataset_dicts

    def _get_metadata(self):
        meta = {}
        meta["image_root"] = self.image_root
        meta["json_file"] = self.json_file
        meta["evaluator_type"] = _PREDEFINED_SPLITS_WIDERFACE["evaluator_type"][self.dataset_key]
        meta["thing_classes"] = ['face']

        return meta

    def evaluate(self, predictions):
        """Dataset must provide a evaluation function to evaluate model."""
        raise NotImplementedError

    @property
    def ground_truth_annotations(self):
        return self.dataset_dicts
```

#### cvpods/data/datasets/lvis_categories.py

```python
# Autogen with
# import json
# import pprint
# def findall(string, sub):
#     idxes = []
#     start = 0
#     while True:
#         idx = string.find(sub, start)
#         if idx == -1:
#             break
#         idxes.append(idx)
#         start = idx + len(sub)
#     return idxes
# def convert_quotas(string):
#     idxes = findall(string, "'")
#     for left, right in zip(idxes[0::2], idxes[1::2]):
#         string = string.replace(string[left], "\"", 1)
#         string = string.replace(string[right], "\"", 1)
#     return string
# a = json.load(open("objects365_categories.json"))
# string = ""
# string = pprint.pformat(
#     a,
#     indent=4,
#     width=100,
#     compact=True).replace("{", "{\n" + " " * 5).replace("}", "\n" + " " * 4 + "}")
# string = string[:1] + "\n " + string[1:]
# string = "LVIS_CATEGORIES = " + string[:-1] + "\n]"
# flag = False
# with open("lvis_categories.py", mode="w+") as f:
#     for line in string.split("\n"):
#         if "[   " in line:
#             line = line.replace("[   ", "[")
#             flag = True
#         elif flag:
#             line = line[3:]  # align with last line
#             flag = False
#         print(convert_quotas(line), file=f)


LVIS_CATEGORIES = [
    {
        "def": "nut from an oak tree",
        "frequency": "r",
        "id": 1,
        "name": "acorn",
        "synonyms": ["acorn"],
        "synset": "acorn.n.01"
    },
    {
        "def": "a dispenser that holds a substance under pressure",
        "frequency": "c",
        "id": 2,
        "name": "aerosol_can",
        "synonyms": ["aerosol_can", "spray_can"],
        "synset": "aerosol.n.02"
    },
    {
        "def": "a machine that keeps air cool and dry",
        "frequency": "f",
        "id": 3,
        "name": "air_conditioner",
        "synonyms": ["air_conditioner"],
        "synset": "air_conditioner.n.01"
    },
    {
        "def": "an aircraft that has a fixed wing and is powered by propellers or jets",
        "frequency": "f",
        "id": 4,
        "name": "airplane",
        "synonyms": ["airplane", "aeroplane"],
        "synset": "airplane.n.01"
    },
    {
        "def": "a clock that wakes a sleeper at some preset time",
        "frequency": "c",
        "id": 5,
        "name": "alarm_clock",
        "synonyms": ["alarm_clock"],
        "synset": "alarm_clock.n.01"
    },
    {
        "def": "a liquor or brew containing alcohol as the active agent",
        "frequency": "c",
        "id": 6,
        "name": "alcohol",
        "synonyms": ["alcohol", "alcoholic_beverage"],
        "synset": "alcohol.n.01"
    },
    {
        "def": "amphibious reptiles related to crocodiles but with shorter broader snouts",
        "frequency": "r",
        "id": 7,
        "name": "alligator",
        "synonyms": ["alligator", "gator"],
        "synset": "alligator.n.02"
    },
    {
        "def": "oval-shaped edible seed of the almond tree",
        "frequency": "c",
        "id": 8,
        "name": "almond",
        "synonyms": ["almond"],
        "synset": "almond.n.02"
    },
    {
        "def": "a vehicle that takes people to and from hospitals",
        "frequency": "c",
        "id": 9,
        "name": "ambulance",
        "synonyms": ["ambulance"],
        "synset": "ambulance.n.01"
    },
    {
        "def": "electronic equipment that increases strength of signals",
        "frequency": "r",
        "id": 10,
        "name": "amplifier",
        "synonyms": ["amplifier"],
        "synset": "amplifier.n.01"
    },
    {
        "def": "an ornament worn around the ankle",
        "frequency": "c",
        "id": 11,
        "name": "anklet",
        "synonyms": ["anklet", "ankle_bracelet"],
        "synset": "anklet.n.03"
    },
    {
        "def": "an electrical device that sends or receives radio or television signals",
        "frequency": "f",
        "id": 12,
        "name": "antenna",
        "synonyms": ["antenna", "aerial", "transmitting_aerial"],
        "synset": "antenna.n.01"
    },
    {
        "def": "fruit with red or yellow or green skin and sweet to tart crisp whitish flesh",
        "frequency": "f",
        "id": 13,
        "name": "apple",
        "synonyms": ["apple"],
        "synset": "apple.n.01"
    },
    {
        "def": "the juice of apples",
        "frequency": "r",
        "id": 14,
        "name": "apple_juice",
        "synonyms": ["apple_juice"],
        "synset": "apple_juice.n.01"
    },
    {
        "def": "puree of stewed apples usually sweetened and spiced",
        "frequency": "r",
        "id": 15,
        "name": "applesauce",
        "synonyms": ["applesauce"],
        "synset": "applesauce.n.01"
    },
    {
        "def": "downy yellow to rosy-colored fruit resembling a small peach",
        "frequency": "r",
        "id": 16,
        "name": "apricot",
        "synonyms": ["apricot"],
        "synset": "apricot.n.02"
    },
    {
        "def": "a garment of cloth that is tied about the waist and worn to protect clothing",
        "frequency": "f",
        "id": 17,
        "name": "apron",
        "synonyms": ["apron"],
        "synset": "apron.n.01"
    },
    {
        "def": "a tank/pool/bowl filled with water for keeping live fish and underwater animals",
        "frequency": "c",
        "id": 18,
        "name": "aquarium",
        "synonyms": ["aquarium", "fish_tank"],
        "synset": "aquarium.n.01"
    },
    {
        "def": "a band worn around the upper arm",
        "frequency": "c",
        "id": 19,
        "name": "armband",
        "synonyms": ["armband"],
        "synset": "armband.n.02"
    },
    {
        "def": "chair with a support on each side for arms",
        "frequency": "f",
        "id": 20,
        "name": "armchair",
        "synonyms": ["armchair"],
        "synset": "armchair.n.01"
    },
    {
        "def": "a large wardrobe or cabinet",
        "frequency": "r",
        "id": 21,
        "name": "armoire",
        "synonyms": ["armoire"],
        "synset": "armoire.n.01"
    },
    {
        "def": "protective covering made of metal and used in combat",
        "frequency": "r",
        "id": 22,
        "name": "armor",
        "synonyms": ["armor", "armour"],
        "synset": "armor.n.01"
    },
    {
        "def": "a thistlelike flower head with edible fleshy leaves and heart",
        "frequency": "c",
        "id": 23,
        "name": "artichoke",
        "synonyms": ["artichoke"],
        "synset": "artichoke.n.02"
    },
    {
        "def": "a bin that holds rubbish until it is collected",
        "frequency": "f",
        "id": 24,
        "name": "trash_can",
        "synonyms": ["trash_can", "garbage_can", "wastebin", "dustbin", "trash_barrel",
                     "trash_bin"],
        "synset": "ashcan.n.01"
    },
    {
        "def": "a receptacle for the ash from smokers' cigars or cigarettes",
        "frequency": "c",
        "id": 25,
        "name": "ashtray",
        "synonyms": ["ashtray"],
        "synset": "ashtray.n.01"
    },
    {
        "def": "edible young shoots of the asparagus plant",
        "frequency": "c",
        "id": 26,
        "name": "asparagus",
        "synonyms": ["asparagus"],
        "synset": "asparagus.n.02"
    },
    {
        "def": "a dispenser that turns a liquid (such as perfume) into a fine mist",
        "frequency": "c",
        "id": 27,
        "name": "atomizer",
        "synonyms": ["atomizer", "atomiser", "spray", "sprayer", "nebulizer", "nebuliser"],
        "synset": "atomizer.n.01"
    },
    {
        "def": "a pear-shaped fruit with green or blackish skin and rich yellowish pulp enclosing "
               "a single large seed",
        "frequency": "c",
        "id": 28,
        "name": "avocado",
        "synonyms": ["avocado"],
        "synset": "avocado.n.01"
    },
    {
        "def": "a tangible symbol signifying approval or distinction",
        "frequency": "c",
        "id": 29,
        "name": "award",
        "synonyms": ["award", "accolade"],
        "synset": "award.n.02"
    },
    {
        "def": "a canopy made of canvas to shelter people or things from rain or sun",
        "frequency": "f",
        "id": 30,
        "name": "awning",
        "synonyms": ["awning"],
        "synset": "awning.n.01"
    },
    {
        "def": "an edge tool with a heavy bladed head mounted across a handle",
        "frequency": "r",
        "id": 31,
        "name": "ax",
        "synonyms": ["ax", "axe"],
        "synset": "ax.n.01"
    },
    {
        "def": "a small vehicle with four wheels in which a baby or child is pushed around",
        "frequency": "f",
        "id": 32,
        "name": "baby_buggy",
        "synonyms": ["baby_buggy", "baby_carriage", "perambulator", "pram", "stroller"],
        "synset": "baby_buggy.n.01"
    },
    {
        "def": "a raised vertical board with basket attached; used to play basketball",
        "frequency": "c",
        "id": 33,
        "name": "basketball_backboard",
        "synonyms": ["basketball_backboard"],
        "synset": "backboard.n.01"
    },
    {
        "def": "a bag carried by a strap on your back or shoulder",
        "frequency": "f",
        "id": 34,
        "name": "backpack",
        "synonyms": ["backpack", "knapsack", "packsack", "rucksack", "haversack"],
        "synset": "backpack.n.01"
    },
    {
        "def": "a container used for carrying money and small personal items or accessories",
        "frequency": "f",
        "id": 35,
        "name": "handbag",
        "synonyms": ["handbag", "purse", "pocketbook"],
        "synset": "bag.n.04"
    },
    {
        "def": "cases used to carry belongings when traveling",
        "frequency": "f",
        "id": 36,
        "name": "suitcase",
        "synonyms": ["suitcase", "baggage", "luggage"],
        "synset": "bag.n.06"
    },
    {
        "def": "glazed yeast-raised doughnut-shaped roll with hard crust",
        "frequency": "c",
        "id": 37,
        "name": "bagel",
        "synonyms": ["bagel", "beigel"],
        "synset": "bagel.n.01"
    },
    {
        "def": "a tubular wind instrument; the player blows air into a bag and squeezes it out",
        "frequency": "r",
        "id": 38,
        "name": "bagpipe",
        "synonyms": ["bagpipe"],
        "synset": "bagpipe.n.01"
    },
    {
        "def": "narrow French stick loaf",
        "frequency": "r",
        "id": 39,
        "name": "baguet",
        "synonyms": ["baguet", "baguette"],
        "synset": "baguet.n.01"
    },
    {
        "def": "something used to lure fish or other animals into danger so they can be trapped or "
               "killed",
        "frequency": "r",
        "id": 40,
        "name": "bait",
        "synonyms": ["bait", "lure"],
        "synset": "bait.n.02"
    },
    {
        "def": "a spherical object used as a plaything",
        "frequency": "f",
        "id": 41,
        "name": "ball",
        "synonyms": ["ball"],
        "synset": "ball.n.06"
    },
    {
        "def": "very short skirt worn by ballerinas",
        "frequency": "r",
        "id": 42,
        "name": "ballet_skirt",
        "synonyms": ["ballet_skirt", "tutu"],
        "synset": "ballet_skirt.n.01"
    },
    {
        "def": "large tough nonrigid bag filled with gas or heated air",
        "frequency": "f",
        "id": 43,
        "name": "balloon",
        "synonyms": ["balloon"],
        "synset": "balloon.n.01"
    },
    {
        "def": "woody tropical grass having hollow woody stems",
        "frequency": "c",
        "id": 44,
        "name": "bamboo",
        "synonyms": ["bamboo"],
        "synset": "bamboo.n.02"
    },
    {
        "def": "elongated crescent-shaped yellow fruit with soft sweet flesh",
        "frequency": "f",
        "id": 45,
        "name": "banana",
        "synonyms": ["banana"],
        "synset": "banana.n.02"
    },
    {
        "def": "trade name for an adhesive bandage to cover small cuts or blisters",
        "frequency": "r",
        "id": 46,
        "name": "Band_Aid",
        "synonyms": ["Band_Aid"],
        "synset": "band_aid.n.01"
    },
    {
        "def": "a piece of soft material that covers and protects an injured part of the body",
        "frequency": "c",
        "id": 47,
        "name": "bandage",
        "synonyms": ["bandage"],
        "synset": "bandage.n.01"
    },
    {
        "def": "large and brightly colored handkerchief; often used as a neckerchief",
        "frequency": "c",
        "id": 48,
        "name": "bandanna",
        "synonyms": ["bandanna", "bandana"],
        "synset": "bandanna.n.01"
    },
    {
        "def": "a stringed instrument of the guitar family with a long neck and circular body",
        "frequency": "r",
        "id": 49,
        "name": "banjo",
        "synonyms": ["banjo"],
        "synset": "banjo.n.01"
    },
    {
        "def": "long strip of cloth or paper used for decoration or advertising",
        "frequency": "f",
        "id": 50,
        "name": "banner",
        "synonyms": ["banner", "streamer"],
        "synset": "banner.n.01"
    },
    {
        "def": "a bar to which heavy discs are attached at each end; used in weightlifting",
        "frequency": "r",
        "id": 51,
        "name": "barbell",
        "synonyms": ["barbell"],
        "synset": "barbell.n.01"
    },
    {
        "def": "a flatbottom boat for carrying heavy loads (especially on canals)",
        "frequency": "r",
        "id": 52,
        "name": "barge",
        "synonyms": ["barge"],
        "synset": "barge.n.01"
    },
    {
        "def": "a cylindrical container that holds liquids",
        "frequency": "f",
        "id": 53,
        "name": "barrel",
        "synonyms": ["barrel", "cask"],
        "synset": "barrel.n.02"
    },
    {
        "def": "a pin for holding women's hair in place",
        "frequency": "c",
        "id": 54,
        "name": "barrette",
        "synonyms": ["barrette"],
        "synset": "barrette.n.01"
    },
    {
        "def": "a cart for carrying small loads; has handles and one or more wheels",
        "frequency": "c",
        "id": 55,
        "name": "barrow",
        "synonyms": ["barrow", "garden_cart", "lawn_cart", "wheelbarrow"],
        "synset": "barrow.n.03"
    },
    {
        "def": "a place that the runner must touch before scoring",
        "frequency": "f",
        "id": 56,
        "name": "baseball_base",
        "synonyms": ["baseball_base"],
        "synset": "base.n.03"
    },
    {
        "def": "a ball used in playing baseball",
        "frequency": "f",
        "id": 57,
        "name": "baseball",
        "synonyms": ["baseball"],
        "synset": "baseball.n.02"
    },
    {
        "def": "an implement used in baseball by the batter",
        "frequency": "f",
        "id": 58,
        "name": "baseball_bat",
        "synonyms": ["baseball_bat"],
        "synset": "baseball_bat.n.01"
    },
    {
        "def": "a cap with a bill",
        "frequency": "f",
        "id": 59,
        "name": "baseball_cap",
        "synonyms": ["baseball_cap", "jockey_cap", "golf_cap"],
        "synset": "baseball_cap.n.01"
    },
    {
        "def": "the handwear used by fielders in playing baseball",
        "frequency": "f",
        "id": 60,
        "name": "baseball_glove",
        "synonyms": ["baseball_glove", "baseball_mitt"],
        "synset": "baseball_glove.n.01"
    },
    {
        "def": "a container that is usually woven and has handles",
        "frequency": "f",
        "id": 61,
        "name": "basket",
        "synonyms": ["basket", "handbasket"],
        "synset": "basket.n.01"
    },
    {
        "def": "metal hoop supporting a net through which players try to throw the basketball",
        "frequency": "c",
        "id": 62,
        "name": "basketball_hoop",
        "synonyms": ["basketball_hoop"],
        "synset": "basket.n.03"
    },
    {
        "def": "an inflated ball used in playing basketball",
        "frequency": "c",
        "id": 63,
        "name": "basketball",
        "synonyms": ["basketball"],
        "synset": "basketball.n.02"
    },
    {
        "def": "the lowest brass wind instrument",
        "frequency": "r",
        "id": 64,
        "name": "bass_horn",
        "synonyms": ["bass_horn", "sousaphone", "tuba"],
        "synset": "bass_horn.n.01"
    },
    {
        "def": "nocturnal mouselike mammal with forelimbs modified to form membranous wings",
        "frequency": "r",
        "id": 65,
        "name": "bat_(animal)",
        "synonyms": ["bat_(animal)"],
        "synset": "bat.n.01"
    },
    {
        "def": "a heavy towel or mat to stand on while drying yourself after a bath",
        "frequency": "f",
        "id": 66,
        "name": "bath_mat",
        "synonyms": ["bath_mat"],
        "synset": "bath_mat.n.01"
    },
    {
        "def": "a large towel; to dry yourself after a bath",
        "frequency": "f",
        "id": 67,
        "name": "bath_towel",
        "synonyms": ["bath_towel"],
        "synset": "bath_towel.n.01"
    },
    {
        "def": "a loose-fitting robe of towelling; worn after a bath or swim",
        "frequency": "c",
        "id": 68,
        "name": "bathrobe",
        "synonyms": ["bathrobe"],
        "synset": "bathrobe.n.01"
    },
    {
        "def": "a large open container that you fill with water and use to wash the body",
        "frequency": "f",
        "id": 69,
        "name": "bathtub",
        "synonyms": ["bathtub", "bathing_tub"],
        "synset": "bathtub.n.01"
    },
    {
        "def": "a liquid or semiliquid mixture, as of flour, eggs, and milk, used in cooking",
        "frequency": "r",
        "id": 70,
        "name": "batter_(food)",
        "synonyms": ["batter_(food)"],
        "synset": "batter.n.02"
    },
    {
        "def": "a portable device that produces electricity",
        "frequency": "c",
        "id": 71,
        "name": "battery",
        "synonyms": ["battery"],
        "synset": "battery.n.02"
    },
    {
        "def": "large and light ball; for play at the seaside",
        "frequency": "r",
        "id": 72,
        "name": "beachball",
        "synonyms": ["beachball"],
        "synset": "beach_ball.n.01"
    },
    {
        "def": "a small ball with a hole through the middle used for ornamentation, jewellery, "
               "etc.",
        "frequency": "c",
        "id": 73,
        "name": "bead",
        "synonyms": ["bead"],
        "synset": "bead.n.01"
    },
    {
        "def": "a flatbottomed jar made of glass or plastic; used for chemistry",
        "frequency": "r",
        "id": 74,
        "name": "beaker",
        "synonyms": ["beaker"],
        "synset": "beaker.n.01"
    },
    {
        "def": "cheeselike food made of curdled soybean milk",
        "frequency": "c",
        "id": 75,
        "name": "bean_curd",
        "synonyms": ["bean_curd", "tofu"],
        "synset": "bean_curd.n.01"
    },
    {
        "def": "a bag filled with dried beans or similar items; used in games or to sit on",
        "frequency": "c",
        "id": 76,
        "name": "beanbag",
        "synonyms": ["beanbag"],
        "synset": "beanbag.n.01"
    },
    {
        "def": "a small skullcap; formerly worn by schoolboys and college freshmen",
        "frequency": "f",
        "id": 77,
        "name": "beanie",
        "synonyms": ["beanie", "beany"],
        "synset": "beanie.n.01"
    },
    {
        "def": "large carnivorous or omnivorous mammals with shaggy coats and claws",
        "frequency": "f",
        "id": 78,
        "name": "bear",
        "synonyms": ["bear"],
        "synset": "bear.n.01"
    },
    {
        "def": "a piece of furniture that provides a place to sleep",
        "frequency": "f",
        "id": 79,
        "name": "bed",
        "synonyms": ["bed"],
        "synset": "bed.n.01"
    },
    {
        "def": "decorative cover for a bed",
        "frequency": "c",
        "id": 80,
        "name": "bedspread",
        "synonyms": ["bedspread", "bedcover", "bed_covering", "counterpane", "spread"],
        "synset": "bedspread.n.01"
    },
    {
        "def": "cattle that are reared for their meat",
        "frequency": "f",
        "id": 81,
        "name": "cow",
        "synonyms": ["cow"],
        "synset": "beef.n.01"
    },
    {
        "def": "meat from an adult domestic bovine",
        "frequency": "c",
        "id": 82,
        "name": "beef_(food)",
        "synonyms": ["beef_(food)", "boeuf_(food)"],
        "synset": "beef.n.02"
    },
    {
        "def": "an device that beeps when the person carrying it is being paged",
        "frequency": "r",
        "id": 83,
        "name": "beeper",
        "synonyms": ["beeper", "pager"],
        "synset": "beeper.n.01"
    },
    {
        "def": "a bottle that holds beer",
        "frequency": "f",
        "id": 84,
        "name": "beer_bottle",
        "synonyms": ["beer_bottle"],
        "synset": "beer_bottle.n.01"
    },
    {
        "def": "a can that holds beer",
        "frequency": "c",
        "id": 85,
        "name": "beer_can",
        "synonyms": ["beer_can"],
        "synset": "beer_can.n.01"
    },
    {
        "def": "insect with hard wing covers",
        "frequency": "r",
        "id": 86,
        "name": "beetle",
        "synonyms": ["beetle"],
        "synset": "beetle.n.01"
    },
    {
        "def": "a hollow device made of metal that makes a ringing sound when struck",
        "frequency": "f",
        "id": 87,
        "name": "bell",
        "synonyms": ["bell"],
        "synset": "bell.n.01"
    },
    {
        "def": "large bell-shaped sweet pepper in green or red or yellow or orange or black "
               "varieties",
        "frequency": "f",
        "id": 88,
        "name": "bell_pepper",
        "synonyms": ["bell_pepper", "capsicum"],
        "synset": "bell_pepper.n.02"
    },
    {
        "def": "a band to tie or buckle around the body (usually at the waist)",
        "frequency": "f",
        "id": 89,
        "name": "belt",
        "synonyms": ["belt"],
        "synset": "belt.n.02"
    },
    {
        "def": "the buckle used to fasten a belt",
        "frequency": "f",
        "id": 90,
        "name": "belt_buckle",
        "synonyms": ["belt_buckle"],
        "synset": "belt_buckle.n.01"
    },
    {
        "def": "a long seat for more than one person",
        "frequency": "f",
        "id": 91,
        "name": "bench",
        "synonyms": ["bench"],
        "synset": "bench.n.01"
    },
    {
        "def": "a cap with no brim or bill; made of soft cloth",
        "frequency": "c",
        "id": 92,
        "name": "beret",
        "synonyms": ["beret"],
        "synset": "beret.n.01"
    },
    {
        "def": "a napkin tied under the chin of a child while eating",
        "frequency": "c",
        "id": 93,
        "name": "bib",
        "synonyms": ["bib"],
        "synset": "bib.n.02"
    },
    {
        "def": "the sacred writings of the Christian religions",
        "frequency": "r",
        "id": 94,
        "name": "Bible",
        "synonyms": ["Bible"],
        "synset": "bible.n.01"
    },
    {
        "def": "a wheeled vehicle that has two wheels and is moved by foot pedals",
        "frequency": "f",
        "id": 95,
        "name": "bicycle",
        "synonyms": ["bicycle", "bike_(bicycle)"],
        "synset": "bicycle.n.01"
    },
    {
        "def": "a brim that projects to the front to shade the eyes",
        "frequency": "f",
        "id": 96,
        "name": "visor",
        "synonyms": ["visor", "vizor"],
        "synset": "bill.n.09"
    },
    {
        "def": "holds loose papers or magazines",
        "frequency": "c",
        "id": 97,
        "name": "binder",
        "synonyms": ["binder", "ring-binder"],
        "synset": "binder.n.03"
    },
    {
        "def": "an optical instrument designed for simultaneous use by both eyes",
        "frequency": "c",
        "id": 98,
        "name": "binoculars",
        "synonyms": ["binoculars", "field_glasses", "opera_glasses"],
        "synset": "binoculars.n.01"
    },
    {
        "def": "animal characterized by feathers and wings",
        "frequency": "f",
        "id": 99,
        "name": "bird",
        "synonyms": ["bird"],
        "synset": "bird.n.01"
    },
    {
        "def": "an outdoor device that supplies food for wild birds",
        "frequency": "r",
        "id": 100,
        "name": "birdfeeder",
        "synonyms": ["birdfeeder"],
        "synset": "bird_feeder.n.01"
    },
    {
        "def": "an ornamental basin (usually in a garden) for birds to bathe in",
        "frequency": "r",
        "id": 101,
        "name": "birdbath",
        "synonyms": ["birdbath"],
        "synset": "birdbath.n.01"
    },
    {
        "def": "a cage in which a bird can be kept",
        "frequency": "c",
        "id": 102,
        "name": "birdcage",
        "synonyms": ["birdcage"],
        "synset": "birdcage.n.01"
    },
    {
        "def": "a shelter for birds",
        "frequency": "c",
        "id": 103,
        "name": "birdhouse",
        "synonyms": ["birdhouse"],
        "synset": "birdhouse.n.01"
    },
    {
        "def": "decorated cake served at a birthday party",
        "frequency": "f",
        "id": 104,
        "name": "birthday_cake",
        "synonyms": ["birthday_cake"],
        "synset": "birthday_cake.n.01"
    },
    {
        "def": "a card expressing a birthday greeting",
        "frequency": "r",
        "id": 105,
        "name": "birthday_card",
        "synonyms": ["birthday_card"],
        "synset": "birthday_card.n.01"
    },
    {
        "def": "small round bread leavened with baking-powder or soda",
        "frequency": "r",
        "id": 106,
        "name": "biscuit_(bread)",
        "synonyms": ["biscuit_(bread)"],
        "synset": "biscuit.n.01"
    },
    {
        "def": "a flag usually bearing a white skull and crossbones on a black background",
        "frequency": "r",
        "id": 107,
        "name": "pirate_flag",
        "synonyms": ["pirate_flag"],
        "synset": "black_flag.n.01"
    },
    {
        "def": "sheep with a black coat",
        "frequency": "c",
        "id": 108,
        "name": "black_sheep",
        "synonyms": ["black_sheep"],
        "synset": "black_sheep.n.02"
    },
    {
        "def": "sheet of slate; for writing with chalk",
        "frequency": "c",
        "id": 109,
        "name": "blackboard",
        "synonyms": ["blackboard", "chalkboard"],
        "synset": "blackboard.n.01"
    },
    {
        "def": "bedding that keeps a person warm in bed",
        "frequency": "f",
        "id": 110,
        "name": "blanket",
        "synonyms": ["blanket"],
        "synset": "blanket.n.01"
    },
    {
        "def": "lightweight jacket; often striped in the colors of a club or school",
        "frequency": "c",
        "id": 111,
        "name": "blazer",
        "synonyms": ["blazer", "sport_jacket", "sport_coat", "sports_jacket", "sports_coat"],
        "synset": "blazer.n.01"
    },
    {
        "def": "an electrically powered mixer that mix or chop or liquefy foods",
        "frequency": "f",
        "id": 112,
        "name": "blender",
        "synonyms": ["blender", "liquidizer", "liquidiser"],
        "synset": "blender.n.01"
    },
    {
        "def": "a small nonrigid airship used for observation or as a barrage balloon",
        "frequency": "r",
        "id": 113,
        "name": "blimp",
        "synonyms": ["blimp"],
        "synset": "blimp.n.02"
    },
    {
        "def": "a light that flashes on and off; used as a signal or to send messages",
        "frequency": "c",
        "id": 114,
        "name": "blinker",
        "synonyms": ["blinker", "flasher"],
        "synset": "blinker.n.01"
    },
    {
        "def": "sweet edible dark-blue berries of blueberry plants",
        "frequency": "c",
        "id": 115,
        "name": "blueberry",
        "synonyms": ["blueberry"],
        "synset": "blueberry.n.02"
    },
    {
        "def": "an uncastrated male hog",
        "frequency": "r",
        "id": 116,
        "name": "boar",
        "synonyms": ["boar"],
        "synset": "boar.n.02"
    },
    {
        "def": "a flat portable surface (usually rectangular) designed for board games",
        "frequency": "r",
        "id": 117,
        "name": "gameboard",
        "synonyms": ["gameboard"],
        "synset": "board.n.09"
    },
    {
        "def": "a vessel for travel on water",
        "frequency": "f",
        "id": 118,
        "name": "boat",
        "synonyms": ["boat", "ship_(boat)"],
        "synset": "boat.n.01"
    },
    {
        "def": "a thing around which thread/tape/film or other flexible materials can be wound",
        "frequency": "c",
        "id": 119,
        "name": "bobbin",
        "synonyms": ["bobbin", "spool", "reel"],
        "synset": "bobbin.n.01"
    },
    {
        "def": "a flat wire hairpin used to hold bobbed hair in place",
        "frequency": "r",
        "id": 120,
        "name": "bobby_pin",
        "synonyms": ["bobby_pin", "hairgrip"],
        "synset": "bobby_pin.n.01"
    },
    {
        "def": "egg cooked briefly in the shell in gently boiling water",
        "frequency": "c",
        "id": 121,
        "name": "boiled_egg",
        "synonyms": ["boiled_egg", "coddled_egg"],
        "synset": "boiled_egg.n.01"
    },
    {
        "def": "a cord fastened around the neck with an ornamental clasp and worn as a necktie",
        "frequency": "r",
        "id": 122,
        "name": "bolo_tie",
        "synonyms": ["bolo_tie", "bolo", "bola_tie", "bola"],
        "synset": "bolo_tie.n.01"
    },
    {
        "def": "the part of a lock that is engaged or withdrawn with a key",
        "frequency": "c",
        "id": 123,
        "name": "deadbolt",
        "synonyms": ["deadbolt"],
        "synset": "bolt.n.03"
    },
    {
        "def": "a screw that screws into a nut to form a fastener",
        "frequency": "f",
        "id": 124,
        "name": "bolt",
        "synonyms": ["bolt"],
        "synset": "bolt.n.06"
    },
    {
        "def": "a hat tied under the chin",
        "frequency": "r",
        "id": 125,
        "name": "bonnet",
        "synonyms": ["bonnet"],
        "synset": "bonnet.n.01"
    },
    {
        "def": "a written work or composition that has been published",
        "frequency": "f",
        "id": 126,
        "name": "book",
        "synonyms": ["book"],
        "synset": "book.n.01"
    },
    {
        "def": "a bag in which students carry their books",
        "frequency": "r",
        "id": 127,
        "name": "book_bag",
        "synonyms": ["book_bag"],
        "synset": "book_bag.n.01"
    },
    {
        "def": "a piece of furniture with shelves for storing books",
        "frequency": "c",
        "id": 128,
        "name": "bookcase",
        "synonyms": ["bookcase"],
        "synset": "bookcase.n.01"
    },
    {
        "def": "a small book usually having a paper cover",
        "frequency": "c",
        "id": 129,
        "name": "booklet",
        "synonyms": ["booklet", "brochure", "leaflet", "pamphlet"],
        "synset": "booklet.n.01"
    },
    {
        "def": "a marker (a piece of paper or ribbon) placed between the pages of a book",
        "frequency": "r",
        "id": 130,
        "name": "bookmark",
        "synonyms": ["bookmark", "bookmarker"],
        "synset": "bookmark.n.01"
    },
    {
        "def": "a pole carrying an overhead microphone projected over a film or tv set",
        "frequency": "r",
        "id": 131,
        "name": "boom_microphone",
        "synonyms": ["boom_microphone", "microphone_boom"],
        "synset": "boom.n.04"
    },
    {
        "def": "footwear that covers the whole foot and lower leg",
        "frequency": "f",
        "id": 132,
        "name": "boot",
        "synonyms": ["boot"],
        "synset": "boot.n.01"
    },
    {
        "def": "a glass or plastic vessel used for storing drinks or other liquids",
        "frequency": "f",
        "id": 133,
        "name": "bottle",
        "synonyms": ["bottle"],
        "synset": "bottle.n.01"
    },
    {
        "def": "an opener for removing caps or corks from bottles",
        "frequency": "c",
        "id": 134,
        "name": "bottle_opener",
        "synonyms": ["bottle_opener"],
        "synset": "bottle_opener.n.01"
    },
    {
        "def": "an arrangement of flowers that is usually given as a present",
        "frequency": "c",
        "id": 135,
        "name": "bouquet",
        "synonyms": ["bouquet"],
        "synset": "bouquet.n.01"
    },
    {
        "def": "a weapon for shooting arrows",
        "frequency": "r",
        "id": 136,
        "name": "bow_(weapon)",
        "synonyms": ["bow_(weapon)"],
        "synset": "bow.n.04"
    },
    {
        "def": "a decorative interlacing of ribbons",
        "frequency": "f",
        "id": 137,
        "name": "bow_(decorative_ribbons)",
        "synonyms": ["bow_(decorative_ribbons)"],
        "synset": "bow.n.08"
    },
    {
        "def": "a man's tie that ties in a bow",
        "frequency": "f",
        "id": 138,
        "name": "bow-tie",
        "synonyms": ["bow-tie", "bowtie"],
        "synset": "bow_tie.n.01"
    },
    {
        "def": "a dish that is round and open at the top for serving foods",
        "frequency": "f",
        "id": 139,
        "name": "bowl",
        "synonyms": ["bowl"],
        "synset": "bowl.n.03"
    },
    {
        "def": "a small round container that is open at the top for holding tobacco",
        "frequency": "r",
        "id": 140,
        "name": "pipe_bowl",
        "synonyms": ["pipe_bowl"],
        "synset": "bowl.n.08"
    },
    {
        "def": "a felt hat that is round and hard with a narrow brim",
        "frequency": "c",
        "id": 141,
        "name": "bowler_hat",
        "synonyms": ["bowler_hat", "bowler", "derby_hat", "derby", "plug_hat"],
        "synset": "bowler_hat.n.01"
    },
    {
        "def": "a large ball with finger holes used in the sport of bowling",
        "frequency": "r",
        "id": 142,
        "name": "bowling_ball",
        "synonyms": ["bowling_ball"],
        "synset": "bowling_ball.n.01"
    },
    {
        "def": "a club-shaped wooden object used in bowling",
        "frequency": "r",
        "id": 143,
        "name": "bowling_pin",
        "synonyms": ["bowling_pin"],
        "synset": "bowling_pin.n.01"
    },
    {
        "def": "large glove coverings the fists of a fighter worn for the sport of boxing",
        "frequency": "r",
        "id": 144,
        "name": "boxing_glove",
        "synonyms": ["boxing_glove"],
        "synset": "boxing_glove.n.01"
    },
    {
        "def": "elastic straps that hold trousers up (usually used in the plural)",
        "frequency": "c",
        "id": 145,
        "name": "suspenders",
        "synonyms": ["suspenders"],
        "synset": "brace.n.06"
    },
    {
        "def": "jewelry worn around the wrist for decoration",
        "frequency": "f",
        "id": 146,
        "name": "bracelet",
        "synonyms": ["bracelet", "bangle"],
        "synset": "bracelet.n.02"
    },
    {
        "def": "a memorial made of brass",
        "frequency": "r",
        "id": 147,
        "name": "brass_plaque",
        "synonyms": ["brass_plaque"],
        "synset": "brass.n.07"
    },
    {
        "def": "an undergarment worn by women to support their breasts",
        "frequency": "c",
        "id": 148,
        "name": "brassiere",
        "synonyms": ["brassiere", "bra", "bandeau"],
        "synset": "brassiere.n.01"
    },
    {
        "def": "a container used to keep bread or cake in",
        "frequency": "c",
        "id": 149,
        "name": "bread-bin",
        "synonyms": ["bread-bin", "breadbox"],
        "synset": "bread-bin.n.01"
    },
    {
        "def": "a garment that provides covering for the loins",
        "frequency": "r",
        "id": 150,
        "name": "breechcloth",
        "synonyms": ["breechcloth", "breechclout", "loincloth"],
        "synset": "breechcloth.n.01"
    },
    {
        "def": "a gown worn by the bride at a wedding",
        "frequency": "c",
        "id": 151,
        "name": "bridal_gown",
        "synonyms": ["bridal_gown", "wedding_gown", "wedding_dress"],
        "synset": "bridal_gown.n.01"
    },
    {
        "def": "a case with a handle; for carrying papers or files or books",
        "frequency": "c",
        "id": 152,
        "name": "briefcase",
        "synonyms": ["briefcase"],
        "synset": "briefcase.n.01"
    },
    {
        "def": "a brush that is made with the short stiff hairs of an animal or plant",
        "frequency": "c",
        "id": 153,
        "name": "bristle_brush",
        "synonyms": ["bristle_brush"],
        "synset": "bristle_brush.n.01"
    },
    {
        "def": "plant with dense clusters of tight green flower buds",
        "frequency": "f",
        "id": 154,
        "name": "broccoli",
        "synonyms": ["broccoli"],
        "synset": "broccoli.n.01"
    },
    {
        "def": "a decorative pin worn by women",
        "frequency": "r",
        "id": 155,
        "name": "broach",
        "synonyms": ["broach"],
        "synset": "brooch.n.01"
    },
    {
        "def": "bundle of straws or twigs attached to a long handle; used for cleaning",
        "frequency": "c",
        "id": 156,
        "name": "broom",
        "synonyms": ["broom"],
        "synset": "broom.n.01"
    },
    {
        "def": "square or bar of very rich chocolate cake usually with nuts",
        "frequency": "c",
        "id": 157,
        "name": "brownie",
        "synonyms": ["brownie"],
        "synset": "brownie.n.03"
    },
    {
        "def": "the small edible cabbage-like buds growing along a stalk",
        "frequency": "c",
        "id": 158,
        "name": "brussels_sprouts",
        "synonyms": ["brussels_sprouts"],
        "synset": "brussels_sprouts.n.01"
    },
    {
        "def": "a kind of chewing gum that can be blown into bubbles",
        "frequency": "r",
        "id": 159,
        "name": "bubble_gum",
        "synonyms": ["bubble_gum"],
        "synset": "bubble_gum.n.01"
    },
    {
        "def": "a roughly cylindrical vessel that is open at the top",
        "frequency": "f",
        "id": 160,
        "name": "bucket",
        "synonyms": ["bucket", "pail"],
        "synset": "bucket.n.01"
    },
    {
        "def": "a small lightweight carriage; drawn by a single horse",
        "frequency": "r",
        "id": 161,
        "name": "horse_buggy",
        "synonyms": ["horse_buggy"],
        "synset": "buggy.n.01"
    },
    {
        "def": "mature male cow",
        "frequency": "c",
        "id": 162,
        "name": "bull",
        "synonyms": ["bull"],
        "synset": "bull.n.11"
    },
    {
        "def": "a thickset short-haired dog with a large head and strong undershot lower jaw",
        "frequency": "r",
        "id": 163,
        "name": "bulldog",
        "synonyms": ["bulldog"],
        "synset": "bulldog.n.01"
    },
    {
        "def": "large powerful tractor; a large blade in front flattens areas of ground",
        "frequency": "r",
        "id": 164,
        "name": "bulldozer",
        "synonyms": ["bulldozer", "dozer"],
        "synset": "bulldozer.n.01"
    },
    {
        "def": "a high-speed passenger train",
        "frequency": "c",
        "id": 165,
        "name": "bullet_train",
        "synonyms": ["bullet_train"],
        "synset": "bullet_train.n.01"
    },
    {
        "def": "a board that hangs on a wall; displays announcements",
        "frequency": "c",
        "id": 166,
        "name": "bulletin_board",
        "synonyms": ["bulletin_board", "notice_board"],
        "synset": "bulletin_board.n.02"
    },
    {
        "def": "a vest capable of resisting the impact of a bullet",
        "frequency": "r",
        "id": 167,
        "name": "bulletproof_vest",
        "synonyms": ["bulletproof_vest"],
        "synset": "bulletproof_vest.n.01"
    },
    {
        "def": "a portable loudspeaker with built-in microphone and amplifier",
        "frequency": "c",
        "id": 168,
        "name": "bullhorn",
        "synonyms": ["bullhorn", "megaphone"],
        "synset": "bullhorn.n.01"
    },
    {
        "def": "beef cured or pickled in brine",
        "frequency": "r",
        "id": 169,
        "name": "corned_beef",
        "synonyms": ["corned_beef", "corn_beef"],
        "synset": "bully_beef.n.01"
    },
    {
        "def": "small rounded bread either plain or sweet",
        "frequency": "f",
        "id": 170,
        "name": "bun",
        "synonyms": ["bun", "roll"],
        "synset": "bun.n.01"
    },
    {
        "def": "beds built one above the other",
        "frequency": "c",
        "id": 171,
        "name": "bunk_bed",
        "synonyms": ["bunk_bed"],
        "synset": "bunk_bed.n.01"
    },
    {
        "def": "a float attached by rope to the seabed to mark channels in a harbor or underwater "
               "hazards",
        "frequency": "f",
        "id": 172,
        "name": "buoy",
        "synonyms": ["buoy"],
        "synset": "buoy.n.01"
    },
    {
        "def": "a flour tortilla folded around a filling",
        "frequency": "r",
        "id": 173,
        "name": "burrito",
        "synonyms": ["burrito"],
        "synset": "burrito.n.01"
    },
    {
        "def": "a vehicle carrying many passengers; used for public transport",
        "frequency": "f",
        "id": 174,
        "name": "bus_(vehicle)",
        "synonyms": ["bus_(vehicle)", "autobus", "charabanc", "double-decker", "motorbus",
                     "motorcoach"],
        "synset": "bus.n.01"
    },
    {
        "def": "a card on which are printed the person's name and business affiliation",
        "frequency": "c",
        "id": 175,
        "name": "business_card",
        "synonyms": ["business_card"],
        "synset": "business_card.n.01"
    },
    {
        "def": "a large sharp knife for cutting or trimming meat",
        "frequency": "c",
        "id": 176,
        "name": "butcher_knife",
        "synonyms": ["butcher_knife"],
        "synset": "butcher_knife.n.01"
    },
    {
        "def": "an edible emulsion of fat globules made by churning milk or cream; for cooking and "
               "table use",
        "frequency": "c",
        "id": 177,
        "name": "butter",
        "synonyms": ["butter"],
        "synset": "butter.n.01"
    },
    {
        "def": "insect typically having a slender body with knobbed antennae and broad colorful "
               "wings",
        "frequency": "c",
        "id": 178,
        "name": "butterfly",
        "synonyms": ["butterfly"],
        "synset": "butterfly.n.01"
    },
    {
        "def": "a round fastener sewn to shirts and coats etc to fit through buttonholes",
        "frequency": "f",
        "id": 179,
        "name": "button",
        "synonyms": ["button"],
        "synset": "button.n.01"
    },
    {
        "def": "a car that takes passengers where they want to go in exchange for money",
        "frequency": "f",
        "id": 180,
        "name": "cab_(taxi)",
        "synonyms": ["cab_(taxi)", "taxi", "taxicab"],
        "synset": "cab.n.03"
    },
    {
        "def": "a small tent used as a dressing room beside the sea or a swimming pool",
        "frequency": "r",
        "id": 181,
        "name": "cabana",
        "synonyms": ["cabana"],
        "synset": "cabana.n.01"
    },
    {
        "def": "a car on a freight train for use of the train crew; usually the last car on the "
               "train",
        "frequency": "r",
        "id": 182,
        "name": "cabin_car",
        "synonyms": ["cabin_car", "caboose"],
        "synset": "cabin_car.n.01"
    },
    {
        "def": "a piece of furniture resembling a cupboard with doors and shelves and drawers",
        "frequency": "f",
        "id": 183,
        "name": "cabinet",
        "synonyms": ["cabinet"],
        "synset": "cabinet.n.01"
    },
    {
        "def": "a storage compartment for clothes and valuables; usually it has a lock",
        "frequency": "r",
        "id": 184,
        "name": "locker",
        "synonyms": ["locker", "storage_locker"],
        "synset": "cabinet.n.03"
    },
    {
        "def": "baked goods made from or based on a mixture of flour, sugar, eggs, and fat",
        "frequency": "f",
        "id": 185,
        "name": "cake",
        "synonyms": ["cake"],
        "synset": "cake.n.03"
    },
    {
        "def": "a small machine that is used for mathematical calculations",
        "frequency": "c",
        "id": 186,
        "name": "calculator",
        "synonyms": ["calculator"],
        "synset": "calculator.n.02"
    },
    {
        "def": "a list or register of events (appointments/social events/court cases, etc)",
        "frequency": "f",
        "id": 187,
        "name": "calendar",
        "synonyms": ["calendar"],
        "synset": "calendar.n.02"
    },
    {
        "def": "young of domestic cattle",
        "frequency": "c",
        "id": 188,
        "name": "calf",
        "synonyms": ["calf"],
        "synset": "calf.n.01"
    },
    {
        "def": "a portable television camera and videocassette recorder",
        "frequency": "c",
        "id": 189,
        "name": "camcorder",
        "synonyms": ["camcorder"],
        "synset": "camcorder.n.01"
    },
    {
        "def": "cud-chewing mammal used as a draft or saddle animal in desert regions",
        "frequency": "c",
        "id": 190,
        "name": "camel",
        "synonyms": ["camel"],
        "synset": "camel.n.01"
    },
    {
        "def": "equipment for taking photographs",
        "frequency": "f",
        "id": 191,
        "name": "camera",
        "synonyms": ["camera"],
        "synset": "camera.n.01"
    },
    {
        "def": "a lens that focuses the image in a camera",
        "frequency": "c",
        "id": 192,
        "name": "camera_lens",
        "synonyms": ["camera_lens"],
        "synset": "camera_lens.n.01"
    },
    {
        "def": "a recreational vehicle equipped for camping out while traveling",
        "frequency": "c",
        "id": 193,
        "name": "camper_(vehicle)",
        "synonyms": ["camper_(vehicle)", "camping_bus", "motor_home"],
        "synset": "camper.n.02"
    },
    {
        "def": "airtight sealed metal container for food or drink or paint etc.",
        "frequency": "f",
        "id": 194,
        "name": "can",
        "synonyms": ["can", "tin_can"],
        "synset": "can.n.01"
    },
    {
        "def": "a device for cutting cans open",
        "frequency": "c",
        "id": 195,
        "name": "can_opener",
        "synonyms": ["can_opener", "tin_opener"],
        "synset": "can_opener.n.01"
    },
    {
        "def": "branched candlestick; ornamental; has several lights",
        "frequency": "r",
        "id": 196,
        "name": "candelabrum",
        "synonyms": ["candelabrum", "candelabra"],
        "synset": "candelabrum.n.01"
    },
    {
        "def": "stick of wax with a wick in the middle",
        "frequency": "f",
        "id": 197,
        "name": "candle",
        "synonyms": ["candle", "candlestick"],
        "synset": "candle.n.01"
    },
    {
        "def": "a holder with sockets for candles",
        "frequency": "f",
        "id": 198,
        "name": "candle_holder",
        "synonyms": ["candle_holder"],
        "synset": "candlestick.n.01"
    },
    {
        "def": "a candy shaped as a bar",
        "frequency": "r",
        "id": 199,
        "name": "candy_bar",
        "synonyms": ["candy_bar"],
        "synset": "candy_bar.n.01"
    },
    {
        "def": "a hard candy in the shape of a rod (usually with stripes)",
        "frequency": "c",
        "id": 200,
        "name": "candy_cane",
        "synonyms": ["candy_cane"],
        "synset": "candy_cane.n.01"
    },
    {
        "def": "a stick that people can lean on to help them walk",
        "frequency": "c",
        "id": 201,
        "name": "walking_cane",
        "synonyms": ["walking_cane"],
        "synset": "cane.n.01"
    },
    {
        "def": "metal container for storing dry foods such as tea or flour",
        "frequency": "c",
        "id": 202,
        "name": "canister",
        "synonyms": ["canister", "cannister"],
        "synset": "canister.n.02"
    },
    {
        "def": "heavy gun fired from a tank",
        "frequency": "r",
        "id": 203,
        "name": "cannon",
        "synonyms": ["cannon"],
        "synset": "cannon.n.02"
    },
    {
        "def": "small and light boat; pointed at both ends; propelled with a paddle",
        "frequency": "c",
        "id": 204,
        "name": "canoe",
        "synonyms": ["canoe"],
        "synset": "canoe.n.01"
    },
    {
        "def": "the fruit of a cantaloup vine; small to medium-sized melon with yellowish flesh",
        "frequency": "r",
        "id": 205,
        "name": "cantaloup",
        "synonyms": ["cantaloup", "cantaloupe"],
        "synset": "cantaloup.n.02"
    },
    {
        "def": "a flask for carrying water; used by soldiers or travelers",
        "frequency": "r",
        "id": 206,
        "name": "canteen",
        "synonyms": ["canteen"],
        "synset": "canteen.n.01"
    },
    {
        "def": "a tight-fitting headwear",
        "frequency": "c",
        "id": 207,
        "name": "cap_(headwear)",
        "synonyms": ["cap_(headwear)"],
        "synset": "cap.n.01"
    },
    {
        "def": "a top (as for a bottle)",
        "frequency": "f",
        "id": 208,
        "name": "bottle_cap",
        "synonyms": ["bottle_cap", "cap_(container_lid)"],
        "synset": "cap.n.02"
    },
    {
        "def": "a sleeveless garment like a cloak but shorter",
        "frequency": "r",
        "id": 209,
        "name": "cape",
        "synonyms": ["cape"],
        "synset": "cape.n.02"
    },
    {
        "def": "equal parts of espresso and steamed milk",
        "frequency": "c",
        "id": 210,
        "name": "cappuccino",
        "synonyms": ["cappuccino", "coffee_cappuccino"],
        "synset": "cappuccino.n.01"
    },
    {
        "def": "a motor vehicle with four wheels",
        "frequency": "f",
        "id": 211,
        "name": "car_(automobile)",
        "synonyms": ["car_(automobile)", "auto_(automobile)", "automobile"],
        "synset": "car.n.01"
    },
    {
        "def": "a wheeled vehicle adapted to the rails of railroad",
        "frequency": "f",
        "id": 212,
        "name": "railcar_(part_of_a_train)",
        "synonyms": ["railcar_(part_of_a_train)", "railway_car_(part_of_a_train)",
                     "railroad_car_(part_of_a_train)"],
        "synset": "car.n.02"
    },
    {
        "def": "where passengers ride up and down",
        "frequency": "r",
        "id": 213,
        "name": "elevator_car",
        "synonyms": ["elevator_car"],
        "synset": "car.n.04"
    },
    {
        "def": "a battery in a motor vehicle",
        "frequency": "r",
        "id": 214,
        "name": "car_battery",
        "synonyms": ["car_battery", "automobile_battery"],
        "synset": "car_battery.n.01"
    },
    {
        "def": "a card certifying the identity of the bearer",
        "frequency": "c",
        "id": 215,
        "name": "identity_card",
        "synonyms": ["identity_card"],
        "synset": "card.n.02"
    },
    {
        "def": "a rectangular piece of paper used to send messages (e.g. greetings or pictures)",
        "frequency": "c",
        "id": 216,
        "name": "card",
        "synonyms": ["card"],
        "synset": "card.n.03"
    },
    {
        "def": "knitted jacket that is fastened up the front with buttons or a zipper",
        "frequency": "r",
        "id": 217,
        "name": "cardigan",
        "synonyms": ["cardigan"],
        "synset": "cardigan.n.01"
    },
    {
        "def": "a ship designed to carry cargo",
        "frequency": "r",
        "id": 218,
        "name": "cargo_ship",
        "synonyms": ["cargo_ship", "cargo_vessel"],
        "synset": "cargo_ship.n.01"
    },
    {
        "def": "plant with pink to purple-red spice-scented usually double flowers",
        "frequency": "r",
        "id": 219,
        "name": "carnation",
        "synonyms": ["carnation"],
        "synset": "carnation.n.01"
    },
    {
        "def": "a vehicle with wheels drawn by one or more horses",
        "frequency": "c",
        "id": 220,
        "name": "horse_carriage",
        "synonyms": ["horse_carriage"],
        "synset": "carriage.n.02"
    },
    {
        "def": "deep orange edible root of the cultivated carrot plant",
        "frequency": "f",
        "id": 221,
        "name": "carrot",
        "synonyms": ["carrot"],
        "synset": "carrot.n.01"
    },
    {
        "def": "a capacious bag or basket",
        "frequency": "c",
        "id": 222,
        "name": "tote_bag",
        "synonyms": ["tote_bag"],
        "synset": "carryall.n.01"
    },
    {
        "def": "a heavy open wagon usually having two wheels and drawn by an animal",
        "frequency": "c",
        "id": 223,
        "name": "cart",
        "synonyms": ["cart"],
        "synset": "cart.n.01"
    },
    {
        "def": "a box made of cardboard; opens by flaps on top",
        "frequency": "c",
        "id": 224,
        "name": "carton",
        "synonyms": ["carton"],
        "synset": "carton.n.02"
    },
    {
        "def": "a cashbox with an adding machine to register transactions",
        "frequency": "c",
        "id": 225,
        "name": "cash_register",
        "synonyms": ["cash_register", "register_(for_cash_transactions)"],
        "synset": "cash_register.n.01"
    },
    {
        "def": "food cooked and served in a casserole",
        "frequency": "r",
        "id": 226,
        "name": "casserole",
        "synonyms": ["casserole"],
        "synset": "casserole.n.01"
    },
    {
        "def": "a container that holds a magnetic tape used for recording or playing sound or "
               "video",
        "frequency": "r",
        "id": 227,
        "name": "cassette",
        "synonyms": ["cassette"],
        "synset": "cassette.n.01"
    },
    {
        "def": "bandage consisting of a firm covering that immobilizes broken bones while they "
               "heal",
        "frequency": "c",
        "id": 228,
        "name": "cast",
        "synonyms": ["cast", "plaster_cast", "plaster_bandage"],
        "synset": "cast.n.05"
    },
    {
        "def": "a domestic house cat",
        "frequency": "f",
        "id": 229,
        "name": "cat",
        "synonyms": ["cat"],
        "synset": "cat.n.01"
    },
    {
        "def": "edible compact head of white undeveloped flowers",
        "frequency": "c",
        "id": 230,
        "name": "cauliflower",
        "synonyms": ["cauliflower"],
        "synset": "cauliflower.n.02"
    },
    {
        "def": "salted roe of sturgeon or other large fish; usually served as an hors d'oeuvre",
        "frequency": "r",
        "id": 231,
        "name": "caviar",
        "synonyms": ["caviar", "caviare"],
        "synset": "caviar.n.01"
    },
    {
        "def": "ground pods and seeds of pungent red peppers of the genus Capsicum",
        "frequency": "c",
        "id": 232,
        "name": "cayenne_(spice)",
        "synonyms": ["cayenne_(spice)", "cayenne_pepper_(spice)", "red_pepper_(spice)"],
        "synset": "cayenne.n.02"
    },
    {
        "def": "electronic equipment for playing compact discs (CDs)",
        "frequency": "c",
        "id": 233,
        "name": "CD_player",
        "synonyms": ["CD_player"],
        "synset": "cd_player.n.01"
    },
    {
        "def": "widely cultivated herb with aromatic leaf stalks that are eaten raw or cooked",
        "frequency": "c",
        "id": 234,
        "name": "celery",
        "synonyms": ["celery"],
        "synset": "celery.n.01"
    },
    {
        "def": "a hand-held mobile telephone",
        "frequency": "f",
        "id": 235,
        "name": "cellular_telephone",
        "synonyms": ["cellular_telephone", "cellular_phone", "cellphone", "mobile_phone",
                     "smart_phone"],
        "synset": "cellular_telephone.n.01"
    },
    {
        "def": "(Middle Ages) flexible armor made of interlinked metal rings",
        "frequency": "r",
        "id": 236,
        "name": "chain_mail",
        "synonyms": ["chain_mail", "ring_mail", "chain_armor", "chain_armour", "ring_armor",
                     "ring_armour"],
        "synset": "chain_mail.n.01"
    },
    {
        "def": "a seat for one person, with a support for the back",
        "frequency": "f",
        "id": 237,
        "name": "chair",
        "synonyms": ["chair"],
        "synset": "chair.n.01"
    },
    {
        "def": "a long chair; for reclining",
        "frequency": "r",
        "id": 238,
        "name": "chaise_longue",
        "synonyms": ["chaise_longue", "chaise", "daybed"],
        "synset": "chaise_longue.n.01"
    },
    {
        "def": "a white sparkling wine produced in Champagne or resembling that produced there",
        "frequency": "r",
        "id": 239,
        "name": "champagne",
        "synonyms": ["champagne"],
        "synset": "champagne.n.01"
    },
    {
        "def": "branched lighting fixture; often ornate; hangs from the ceiling",
        "frequency": "f",
        "id": 240,
        "name": "chandelier",
        "synonyms": ["chandelier"],
        "synset": "chandelier.n.01"
    },
    {
        "def": "leather leggings without a seat; worn over trousers by cowboys to protect their "
               "legs",
        "frequency": "r",
        "id": 241,
        "name": "chap",
        "synonyms": ["chap"],
        "synset": "chap.n.04"
    },
    {
        "def": "a book issued to holders of checking accounts",
        "frequency": "r",
        "id": 242,
        "name": "checkbook",
        "synonyms": ["checkbook", "chequebook"],
        "synset": "checkbook.n.01"
    },
    {
        "def": "a board having 64 squares of two alternating colors",
        "frequency": "r",
        "id": 243,
        "name": "checkerboard",
        "synonyms": ["checkerboard"],
        "synset": "checkerboard.n.01"
    },
    {
        "def": "a red fruit with a single hard stone",
        "frequency": "c",
        "id": 244,
        "name": "cherry",
        "synonyms": ["cherry"],
        "synset": "cherry.n.03"
    },
    {
        "def": "a checkerboard used to play chess",
        "frequency": "r",
        "id": 245,
        "name": "chessboard",
        "synonyms": ["chessboard"],
        "synset": "chessboard.n.01"
    },
    {
        "def": "furniture with drawers for keeping clothes",
        "frequency": "r",
        "id": 246,
        "name": "chest_of_drawers_(furniture)",
        "synonyms": ["chest_of_drawers_(furniture)", "bureau_(furniture)", "chest_(furniture)"],
        "synset": "chest_of_drawers.n.01"
    },
    {
        "def": "a domestic fowl bred for flesh or eggs",
        "frequency": "c",
        "id": 247,
        "name": "chicken_(animal)",
        "synonyms": ["chicken_(animal)"],
        "synset": "chicken.n.02"
    },
    {
        "def": "a galvanized wire network with a hexagonal mesh; used to build fences",
        "frequency": "c",
        "id": 248,
        "name": "chicken_wire",
        "synonyms": ["chicken_wire"],
        "synset": "chicken_wire.n.01"
    },
    {
        "def": "the seed of the chickpea plant; usually dried",
        "frequency": "r",
        "id": 249,
        "name": "chickpea",
        "synonyms": ["chickpea", "garbanzo"],
        "synset": "chickpea.n.01"
    },
    {
        "def": "an old breed of tiny short-haired dog with protruding eyes from Mexico",
        "frequency": "r",
        "id": 250,
        "name": "Chihuahua",
        "synonyms": ["Chihuahua"],
        "synset": "chihuahua.n.03"
    },
    {
        "def": "very hot and finely tapering pepper of special pungency",
        "frequency": "r",
        "id": 251,
        "name": "chili_(vegetable)",
        "synonyms": ["chili_(vegetable)", "chili_pepper_(vegetable)", "chilli_(vegetable)",
                     "chilly_(vegetable)", "chile_(vegetable)"],
        "synset": "chili.n.02"
    },
    {
        "def": "an instrument consisting of a set of bells that are struck with a hammer",
        "frequency": "r",
        "id": 252,
        "name": "chime",
        "synonyms": ["chime", "gong"],
        "synset": "chime.n.01"
    },
    {
        "def": "dishware made of high quality porcelain",
        "frequency": "r",
        "id": 253,
        "name": "chinaware",
        "synonyms": ["chinaware"],
        "synset": "chinaware.n.01"
    },
    {
        "def": "a thin crisp slice of potato fried in deep fat",
        "frequency": "c",
        "id": 254,
        "name": "crisp_(potato_chip)",
        "synonyms": ["crisp_(potato_chip)", "potato_chip"],
        "synset": "chip.n.04"
    },
    {
        "def": "a small disk-shaped counter used to represent money when gambling",
        "frequency": "r",
        "id": 255,
        "name": "poker_chip",
        "synonyms": ["poker_chip"],
        "synset": "chip.n.06"
    },
    {
        "def": "a bar of chocolate candy",
        "frequency": "c",
        "id": 256,
        "name": "chocolate_bar",
        "synonyms": ["chocolate_bar"],
        "synset": "chocolate_bar.n.01"
    },
    {
        "def": "cake containing chocolate",
        "frequency": "c",
        "id": 257,
        "name": "chocolate_cake",
        "synonyms": ["chocolate_cake"],
        "synset": "chocolate_cake.n.01"
    },
    {
        "def": "milk flavored with chocolate syrup",
        "frequency": "r",
        "id": 258,
        "name": "chocolate_milk",
        "synonyms": ["chocolate_milk"],
        "synset": "chocolate_milk.n.01"
    },
    {
        "def": "dessert mousse made with chocolate",
        "frequency": "r",
        "id": 259,
        "name": "chocolate_mousse",
        "synonyms": ["chocolate_mousse"],
        "synset": "chocolate_mousse.n.01"
    },
    {
        "def": "necklace that fits tightly around the neck",
        "frequency": "f",
        "id": 260,
        "name": "choker",
        "synonyms": ["choker", "collar", "neckband"],
        "synset": "choker.n.03"
    },
    {
        "def": "a wooden board where meats or vegetables can be cut",
        "frequency": "f",
        "id": 261,
        "name": "chopping_board",
        "synonyms": ["chopping_board", "cutting_board", "chopping_block"],
        "synset": "chopping_board.n.01"
    },
    {
        "def": "one of a pair of slender sticks used as oriental tableware to eat food with",
        "frequency": "c",
        "id": 262,
        "name": "chopstick",
        "synonyms": ["chopstick"],
        "synset": "chopstick.n.01"
    },
    {
        "def": "an ornamented evergreen used as a Christmas decoration",
        "frequency": "f",
        "id": 263,
        "name": "Christmas_tree",
        "synonyms": ["Christmas_tree"],
        "synset": "christmas_tree.n.05"
    },
    {
        "def": "sloping channel through which things can descend",
        "frequency": "c",
        "id": 264,
        "name": "slide",
        "synonyms": ["slide"],
        "synset": "chute.n.02"
    },
    {
        "def": "a beverage made from juice pressed from apples",
        "frequency": "r",
        "id": 265,
        "name": "cider",
        "synonyms": ["cider", "cyder"],
        "synset": "cider.n.01"
    },
    {
        "def": "a box for holding cigars",
        "frequency": "r",
        "id": 266,
        "name": "cigar_box",
        "synonyms": ["cigar_box"],
        "synset": "cigar_box.n.01"
    },
    {
        "def": "finely ground tobacco wrapped in paper; for smoking",
        "frequency": "c",
        "id": 267,
        "name": "cigarette",
        "synonyms": ["cigarette"],
        "synset": "cigarette.n.01"
    },
    {
        "def": "a small flat case for holding cigarettes",
        "frequency": "c",
        "id": 268,
        "name": "cigarette_case",
        "synonyms": ["cigarette_case", "cigarette_pack"],
        "synset": "cigarette_case.n.01"
    },
    {
        "def": "a tank that holds the water used to flush a toilet",
        "frequency": "f",
        "id": 269,
        "name": "cistern",
        "synonyms": ["cistern", "water_tank"],
        "synset": "cistern.n.02"
    },
    {
        "def": "a single-reed instrument with a straight tube",
        "frequency": "r",
        "id": 270,
        "name": "clarinet",
        "synonyms": ["clarinet"],
        "synset": "clarinet.n.01"
    },
    {
        "def": "a fastener (as a buckle or hook) that is used to hold two things together",
        "frequency": "r",
        "id": 271,
        "name": "clasp",
        "synonyms": ["clasp"],
        "synset": "clasp.n.01"
    },
    {
        "def": "a preparation used in cleaning something",
        "frequency": "c",
        "id": 272,
        "name": "cleansing_agent",
        "synonyms": ["cleansing_agent", "cleanser", "cleaner"],
        "synset": "cleansing_agent.n.01"
    },
    {
        "def": "a variety of mandarin orange",
        "frequency": "r",
        "id": 273,
        "name": "clementine",
        "synonyms": ["clementine"],
        "synset": "clementine.n.01"
    },
    {
        "def": "any of various small fasteners used to hold loose articles together",
        "frequency": "c",
        "id": 274,
        "name": "clip",
        "synonyms": ["clip"],
        "synset": "clip.n.03"
    },
    {
        "def": "a small writing board with a clip at the top for holding papers",
        "frequency": "c",
        "id": 275,
        "name": "clipboard",
        "synonyms": ["clipboard"],
        "synset": "clipboard.n.01"
    },
    {
        "def": "a timepiece that shows the time of day",
        "frequency": "f",
        "id": 276,
        "name": "clock",
        "synonyms": ["clock", "timepiece", "timekeeper"],
        "synset": "clock.n.01"
    },
    {
        "def": "a tower with a large clock visible high up on an outside face",
        "frequency": "f",
        "id": 277,
        "name": "clock_tower",
        "synonyms": ["clock_tower"],
        "synset": "clock_tower.n.01"
    },
    {
        "def": "a hamper that holds dirty clothes to be washed or wet clothes to be dried",
        "frequency": "c",
        "id": 278,
        "name": "clothes_hamper",
        "synonyms": ["clothes_hamper", "laundry_basket", "clothes_basket"],
        "synset": "clothes_hamper.n.01"
    },
    {
        "def": "wood or plastic fastener; for holding clothes on a clothesline",
        "frequency": "c",
        "id": 279,
        "name": "clothespin",
        "synonyms": ["clothespin", "clothes_peg"],
        "synset": "clothespin.n.01"
    },
    {
        "def": "a woman's strapless purse that is carried in the hand",
        "frequency": "r",
        "id": 280,
        "name": "clutch_bag",
        "synonyms": ["clutch_bag"],
        "synset": "clutch_bag.n.01"
    },
    {
        "def": "a covering (plate or mat) that protects the surface of a table",
        "frequency": "f",
        "id": 281,
        "name": "coaster",
        "synonyms": ["coaster"],
        "synset": "coaster.n.03"
    },
    {
        "def": "an outer garment that has sleeves and covers the body from shoulder down",
        "frequency": "f",
        "id": 282,
        "name": "coat",
        "synonyms": ["coat"],
        "synset": "coat.n.01"
    },
    {
        "def": "a hanger that is shaped like a person's shoulders",
        "frequency": "c",
        "id": 283,
        "name": "coat_hanger",
        "synonyms": ["coat_hanger", "clothes_hanger", "dress_hanger"],
        "synset": "coat_hanger.n.01"
    },
    {
        "def": "a rack with hooks for temporarily holding coats and hats",
        "frequency": "r",
        "id": 284,
        "name": "coatrack",
        "synonyms": ["coatrack", "hatrack"],
        "synset": "coatrack.n.01"
    },
    {
        "def": "adult male chicken",
        "frequency": "c",
        "id": 285,
        "name": "cock",
        "synonyms": ["cock", "rooster"],
        "synset": "cock.n.04"
    },
    {
        "def": "large hard-shelled brown oval nut with a fibrous husk",
        "frequency": "c",
        "id": 286,
        "name": "coconut",
        "synonyms": ["coconut", "cocoanut"],
        "synset": "coconut.n.02"
    },
    {
        "def": "filter (usually of paper) that passes the coffee and retains the coffee grounds",
        "frequency": "r",
        "id": 287,
        "name": "coffee_filter",
        "synonyms": ["coffee_filter"],
        "synset": "coffee_filter.n.01"
    },
    {
        "def": "a kitchen appliance for brewing coffee automatically",
        "frequency": "f",
        "id": 288,
        "name": "coffee_maker",
        "synonyms": ["coffee_maker", "coffee_machine"],
        "synset": "coffee_maker.n.01"
    },
    {
        "def": "low table where magazines can be placed and coffee or cocktails are served",
        "frequency": "f",
        "id": 289,
        "name": "coffee_table",
        "synonyms": ["coffee_table", "cocktail_table"],
        "synset": "coffee_table.n.01"
    },
    {
        "def": "tall pot in which coffee is brewed",
        "frequency": "c",
        "id": 290,
        "name": "coffeepot",
        "synonyms": ["coffeepot"],
        "synset": "coffeepot.n.01"
    },
    {
        "def": "tubing that is wound in a spiral",
        "frequency": "r",
        "id": 291,
        "name": "coil",
        "synonyms": ["coil"],
        "synset": "coil.n.05"
    },
    {
        "def": "a flat metal piece (usually a disc) used as money",
        "frequency": "c",
        "id": 292,
        "name": "coin",
        "synonyms": ["coin"],
        "synset": "coin.n.01"
    },
    {
        "def": "bowl-shaped strainer; used to wash or drain foods",
        "frequency": "r",
        "id": 293,
        "name": "colander",
        "synonyms": ["colander", "cullender"],
        "synset": "colander.n.01"
    },
    {
        "def": "basically shredded cabbage",
        "frequency": "c",
        "id": 294,
        "name": "coleslaw",
        "synonyms": ["coleslaw", "slaw"],
        "synset": "coleslaw.n.01"
    },
    {
        "def": "any material used for its color",
        "frequency": "r",
        "id": 295,
        "name": "coloring_material",
        "synonyms": ["coloring_material", "colouring_material"],
        "synset": "coloring_material.n.01"
    },
    {
        "def": "lock that can be opened only by turning dials in a special sequence",
        "frequency": "r",
        "id": 296,
        "name": "combination_lock",
        "synonyms": ["combination_lock"],
        "synset": "combination_lock.n.01"
    },
    {
        "def": "device used for an infant to suck or bite on",
        "frequency": "c",
        "id": 297,
        "name": "pacifier",
        "synonyms": ["pacifier", "teething_ring"],
        "synset": "comforter.n.04"
    },
    {
        "def": "a magazine devoted to comic strips",
        "frequency": "r",
        "id": 298,
        "name": "comic_book",
        "synonyms": ["comic_book"],
        "synset": "comic_book.n.01"
    },
    {
        "def": "a keyboard that is a data input device for computers",
        "frequency": "f",
        "id": 299,
        "name": "computer_keyboard",
        "synonyms": ["computer_keyboard", "keyboard_(computer)"],
        "synset": "computer_keyboard.n.01"
    },
    {
        "def": "a machine with a large revolving drum in which cement/concrete is mixed",
        "frequency": "r",
        "id": 300,
        "name": "concrete_mixer",
        "synonyms": ["concrete_mixer", "cement_mixer"],
        "synset": "concrete_mixer.n.01"
    },
    {
        "def": "a cone-shaped object used to direct traffic",
        "frequency": "f",
        "id": 301,
        "name": "cone",
        "synonyms": ["cone", "traffic_cone"],
        "synset": "cone.n.01"
    },
    {
        "def": "a mechanism that controls the operation of a machine",
        "frequency": "f",
        "id": 302,
        "name": "control",
        "synonyms": ["control", "controller"],
        "synset": "control.n.09"
    },
    {
        "def": "a car that has top that can be folded or removed",
        "frequency": "r",
        "id": 303,
        "name": "convertible_(automobile)",
        "synonyms": ["convertible_(automobile)"],
        "synset": "convertible.n.01"
    },
    {
        "def": "a sofa that can be converted into a bed",
        "frequency": "r",
        "id": 304,
        "name": "sofa_bed",
        "synonyms": ["sofa_bed"],
        "synset": "convertible.n.03"
    },
    {
        "def": "any of various small flat sweet cakes (`biscuit' is the British term)",
        "frequency": "c",
        "id": 305,
        "name": "cookie",
        "synonyms": ["cookie", "cooky", "biscuit_(cookie)"],
        "synset": "cookie.n.01"
    },
    {
        "def": "a jar in which cookies are kept (and sometimes money is hidden)",
        "frequency": "r",
        "id": 306,
        "name": "cookie_jar",
        "synonyms": ["cookie_jar", "cooky_jar"],
        "synset": "cookie_jar.n.01"
    },
    {
        "def": "a kitchen utensil made of material that does not melt easily; used for cooking",
        "frequency": "r",
        "id": 307,
        "name": "cooking_utensil",
        "synonyms": ["cooking_utensil"],
        "synset": "cooking_utensil.n.01"
    },
    {
        "def": "an insulated box for storing food often with ice",
        "frequency": "f",
        "id": 308,
        "name": "cooler_(for_food)",
        "synonyms": ["cooler_(for_food)", "ice_chest"],
        "synset": "cooler.n.01"
    },
    {
        "def": "the plug in the mouth of a bottle (especially a wine bottle)",
        "frequency": "c",
        "id": 309,
        "name": "cork_(bottle_plug)",
        "synonyms": ["cork_(bottle_plug)", "bottle_cork"],
        "synset": "cork.n.04"
    },
    {
        "def": "a sheet consisting of cork granules",
        "frequency": "r",
        "id": 310,
        "name": "corkboard",
        "synonyms": ["corkboard"],
        "synset": "corkboard.n.01"
    },
    {
        "def": "a bottle opener that pulls corks",
        "frequency": "r",
        "id": 311,
        "name": "corkscrew",
        "synonyms": ["corkscrew", "bottle_screw"],
        "synset": "corkscrew.n.01"
    },
    {
        "def": "ears of corn that can be prepared and served for human food",
        "frequency": "c",
        "id": 312,
        "name": "edible_corn",
        "synonyms": ["edible_corn", "corn", "maize"],
        "synset": "corn.n.03"
    },
    {
        "def": "bread made primarily of cornmeal",
        "frequency": "r",
        "id": 313,
        "name": "cornbread",
        "synonyms": ["cornbread"],
        "synset": "cornbread.n.01"
    },
    {
        "def": "a brass musical instrument with a narrow tube and a flared bell and many valves",
        "frequency": "c",
        "id": 314,
        "name": "cornet",
        "synonyms": ["cornet", "horn", "trumpet"],
        "synset": "cornet.n.01"
    },
    {
        "def": "a decorative framework to conceal curtain fixtures at the top of a window casing",
        "frequency": "c",
        "id": 315,
        "name": "cornice",
        "synonyms": ["cornice", "valance", "valance_board", "pelmet"],
        "synset": "cornice.n.01"
    },
    {
        "def": "coarsely ground corn",
        "frequency": "r",
        "id": 316,
        "name": "cornmeal",
        "synonyms": ["cornmeal"],
        "synset": "cornmeal.n.01"
    },
    {
        "def": "a woman's close-fitting foundation garment",
        "frequency": "r",
        "id": 317,
        "name": "corset",
        "synonyms": ["corset", "girdle"],
        "synset": "corset.n.01"
    },
    {
        "def": "lettuce with long dark-green leaves in a loosely packed elongated head",
        "frequency": "r",
        "id": 318,
        "name": "romaine_lettuce",
        "synonyms": ["romaine_lettuce"],
        "synset": "cos.n.02"
    },
    {
        "def": "the attire characteristic of a country or a time or a social class",
        "frequency": "c",
        "id": 319,
        "name": "costume",
        "synonyms": ["costume"],
        "synset": "costume.n.04"
    },
    {
        "def": "large American feline resembling a lion",
        "frequency": "r",
        "id": 320,
        "name": "cougar",
        "synonyms": ["cougar", "puma", "catamount", "mountain_lion", "panther"],
        "synset": "cougar.n.01"
    },
    {
        "def": "a loose-fitting protective garment that is worn over other clothing",
        "frequency": "r",
        "id": 321,
        "name": "coverall",
        "synonyms": ["coverall"],
        "synset": "coverall.n.01"
    },
    {
        "def": "a bell hung around the neck of cow so that the cow can be easily located",
        "frequency": "r",
        "id": 322,
        "name": "cowbell",
        "synonyms": ["cowbell"],
        "synset": "cowbell.n.01"
    },
    {
        "def": "a hat with a wide brim and a soft crown; worn by American ranch hands",
        "frequency": "f",
        "id": 323,
        "name": "cowboy_hat",
        "synonyms": ["cowboy_hat", "ten-gallon_hat"],
        "synset": "cowboy_hat.n.01"
    },
    {
        "def": "decapod having eyes on short stalks and a broad flattened shell and pincers",
        "frequency": "r",
        "id": 324,
        "name": "crab_(animal)",
        "synonyms": ["crab_(animal)"],
        "synset": "crab.n.01"
    },
    {
        "def": "a thin crisp wafer",
        "frequency": "c",
        "id": 325,
        "name": "cracker",
        "synonyms": ["cracker"],
        "synset": "cracker.n.01"
    },
    {
        "def": "small very thin pancake",
        "frequency": "r",
        "id": 326,
        "name": "crape",
        "synonyms": ["crape", "crepe", "French_pancake"],
        "synset": "crape.n.01"
    },
    {
        "def": "a rugged box (usually made of wood); used for shipping",
        "frequency": "f",
        "id": 327,
        "name": "crate",
        "synonyms": ["crate"],
        "synset": "crate.n.01"
    },
    {
        "def": "writing or drawing implement made of a colored stick of composition wax",
        "frequency": "r",
        "id": 328,
        "name": "crayon",
        "synonyms": ["crayon", "wax_crayon"],
        "synset": "crayon.n.01"
    },
    {
        "def": "a small pitcher for serving cream",
        "frequency": "r",
        "id": 329,
        "name": "cream_pitcher",
        "synonyms": ["cream_pitcher"],
        "synset": "cream_pitcher.n.01"
    },
    {
        "def": "a card, usually plastic, used to pay for goods and services",
        "frequency": "r",
        "id": 330,
        "name": "credit_card",
        "synonyms": ["credit_card", "charge_card", "debit_card"],
        "synset": "credit_card.n.01"
    },
    {
        "def": "very rich flaky crescent-shaped roll",
        "frequency": "c",
        "id": 331,
        "name": "crescent_roll",
        "synonyms": ["crescent_roll", "croissant"],
        "synset": "crescent_roll.n.01"
    },
    {
        "def": "baby bed with high sides made of slats",
        "frequency": "c",
        "id": 332,
        "name": "crib",
        "synonyms": ["crib", "cot"],
        "synset": "crib.n.01"
    },
    {
        "def": "an earthen jar (made of baked clay)",
        "frequency": "c",
        "id": 333,
        "name": "crock_pot",
        "synonyms": ["crock_pot", "earthenware_jar"],
        "synset": "crock.n.03"
    },
    {
        "def": "a horizontal bar that goes across something",
        "frequency": "f",
        "id": 334,
        "name": "crossbar",
        "synonyms": ["crossbar"],
        "synset": "crossbar.n.01"
    },
    {
        "def": "a small piece of toasted or fried bread; served in soup or salads",
        "frequency": "r",
        "id": 335,
        "name": "crouton",
        "synonyms": ["crouton"],
        "synset": "crouton.n.01"
    },
    {
        "def": "black birds having a raucous call",
        "frequency": "r",
        "id": 336,
        "name": "crow",
        "synonyms": ["crow"],
        "synset": "crow.n.01"
    },
    {
        "def": "an ornamental jeweled headdress signifying sovereignty",
        "frequency": "c",
        "id": 337,
        "name": "crown",
        "synonyms": ["crown"],
        "synset": "crown.n.04"
    },
    {
        "def": "representation of the cross on which Jesus died",
        "frequency": "c",
        "id": 338,
        "name": "crucifix",
        "synonyms": ["crucifix"],
        "synset": "crucifix.n.01"
    },
    {
        "def": "a passenger ship used commercially for pleasure cruises",
        "frequency": "c",
        "id": 339,
        "name": "cruise_ship",
        "synonyms": ["cruise_ship", "cruise_liner"],
        "synset": "cruise_ship.n.01"
    },
    {
        "def": "a car in which policemen cruise the streets",
        "frequency": "c",
        "id": 340,
        "name": "police_cruiser",
        "synonyms": ["police_cruiser", "patrol_car", "police_car", "squad_car"],
        "synset": "cruiser.n.01"
    },
    {
        "def": "small piece of e.g. bread or cake",
        "frequency": "c",
        "id": 341,
        "name": "crumb",
        "synonyms": ["crumb"],
        "synset": "crumb.n.03"
    },
    {
        "def": "a wooden or metal staff that fits under the armpit and reaches to the ground",
        "frequency": "r",
        "id": 342,
        "name": "crutch",
        "synonyms": ["crutch"],
        "synset": "crutch.n.01"
    },
    {
        "def": "the young of certain carnivorous mammals such as the bear or wolf or lion",
        "frequency": "c",
        "id": 343,
        "name": "cub_(animal)",
        "synonyms": ["cub_(animal)"],
        "synset": "cub.n.03"
    },
    {
        "def": "a block in the (approximate) shape of a cube",
        "frequency": "r",
        "id": 344,
        "name": "cube",
        "synonyms": ["cube", "square_block"],
        "synset": "cube.n.05"
    },
    {
        "def": "cylindrical green fruit with thin green rind and white flesh eaten as a vegetable",
        "frequency": "f",
        "id": 345,
        "name": "cucumber",
        "synonyms": ["cucumber", "cuke"],
        "synset": "cucumber.n.02"
    },
    {
        "def": "jewelry consisting of linked buttons used to fasten the cuffs of a shirt",
        "frequency": "c",
        "id": 346,
        "name": "cufflink",
        "synonyms": ["cufflink"],
        "synset": "cufflink.n.01"
    },
    {
        "def": "a small open container usually used for drinking; usually has a handle",
        "frequency": "f",
        "id": 347,
        "name": "cup",
        "synonyms": ["cup"],
        "synset": "cup.n.01"
    },
    {
        "def": "a metal vessel with handles that is awarded as a trophy to a competition winner",
        "frequency": "c",
        "id": 348,
        "name": "trophy_cup",
        "synonyms": ["trophy_cup"],
        "synset": "cup.n.08"
    },
    {
        "def": "small cake baked in a muffin tin",
        "frequency": "c",
        "id": 349,
        "name": "cupcake",
        "synonyms": ["cupcake"],
        "synset": "cupcake.n.01"
    },
    {
        "def": "a cylindrical tube around which the hair is wound to curl it",
        "frequency": "r",
        "id": 350,
        "name": "hair_curler",
        "synonyms": ["hair_curler", "hair_roller", "hair_crimper"],
        "synset": "curler.n.01"
    },
    {
        "def": "a cylindrical home appliance that heats hair that has been curled around it",
        "frequency": "r",
        "id": 351,
        "name": "curling_iron",
        "synonyms": ["curling_iron"],
        "synset": "curling_iron.n.01"
    },
    {
        "def": "hanging cloth used as a blind (especially for a window)",
        "frequency": "f",
        "id": 352,
        "name": "curtain",
        "synonyms": ["curtain", "drapery"],
        "synset": "curtain.n.01"
    },
    {
        "def": "a soft bag filled with air or padding such as feathers or foam rubber",
        "frequency": "f",
        "id": 353,
        "name": "cushion",
        "synonyms": ["cushion"],
        "synset": "cushion.n.03"
    },
    {
        "def": "sweetened mixture of milk and eggs baked or boiled or frozen",
        "frequency": "r",
        "id": 354,
        "name": "custard",
        "synonyms": ["custard"],
        "synset": "custard.n.01"
    },
    {
        "def": "a cutting implement; a tool for cutting",
        "frequency": "c",
        "id": 355,
        "name": "cutting_tool",
        "synonyms": ["cutting_tool"],
        "synset": "cutter.n.06"
    },
    {
        "def": "a cylindrical container",
        "frequency": "r",
        "id": 356,
        "name": "cylinder",
        "synonyms": ["cylinder"],
        "synset": "cylinder.n.04"
    },
    {
        "def": "a percussion instrument consisting of a concave brass disk",
        "frequency": "r",
        "id": 357,
        "name": "cymbal",
        "synonyms": ["cymbal"],
        "synset": "cymbal.n.01"
    },
    {
        "def": "small long-bodied short-legged breed of dog having a short sleek coat and long "
               "drooping ears",
        "frequency": "r",
        "id": 358,
        "name": "dachshund",
        "synonyms": ["dachshund", "dachsie", "badger_dog"],
        "synset": "dachshund.n.01"
    },
    {
        "def": "a short knife with a pointed blade used for piercing or stabbing",
        "frequency": "r",
        "id": 359,
        "name": "dagger",
        "synonyms": ["dagger"],
        "synset": "dagger.n.01"
    },
    {
        "def": "a circular board of wood or cork used as the target in the game of darts",
        "frequency": "r",
        "id": 360,
        "name": "dartboard",
        "synonyms": ["dartboard"],
        "synset": "dartboard.n.01"
    },
    {
        "def": "sweet edible fruit of the date palm with a single long woody seed",
        "frequency": "r",
        "id": 361,
        "name": "date_(fruit)",
        "synonyms": ["date_(fruit)"],
        "synset": "date.n.08"
    },
    {
        "def": "a folding chair for use outdoors; a wooden frame supports a length of canvas",
        "frequency": "f",
        "id": 362,
        "name": "deck_chair",
        "synonyms": ["deck_chair", "beach_chair"],
        "synset": "deck_chair.n.01"
    },
    {
        "def": "distinguished from Bovidae by the male's having solid deciduous antlers",
        "frequency": "c",
        "id": 363,
        "name": "deer",
        "synonyms": ["deer", "cervid"],
        "synset": "deer.n.01"
    },
    {
        "def": "a soft thread for cleaning the spaces between the teeth",
        "frequency": "c",
        "id": 364,
        "name": "dental_floss",
        "synonyms": ["dental_floss", "floss"],
        "synset": "dental_floss.n.01"
    },
    {
        "def": "a piece of furniture with a writing surface and usually drawers or other "
               "compartments",
        "frequency": "f",
        "id": 365,
        "name": "desk",
        "synonyms": ["desk"],
        "synset": "desk.n.01"
    },
    {
        "def": "a surface-active chemical widely used in industry and laundering",
        "frequency": "r",
        "id": 366,
        "name": "detergent",
        "synonyms": ["detergent"],
        "synset": "detergent.n.01"
    },
    {
        "def": "garment consisting of a folded cloth drawn up between the legs and fastened at the "
               "waist",
        "frequency": "c",
        "id": 367,
        "name": "diaper",
        "synonyms": ["diaper"],
        "synset": "diaper.n.01"
    },
    {
        "def": "a daily written record of (usually personal) experiences and observations",
        "frequency": "r",
        "id": 368,
        "name": "diary",
        "synonyms": ["diary", "journal"],
        "synset": "diary.n.01"
    },
    {
        "def": "a small cube with 1 to 6 spots on the six faces; used in gambling",
        "frequency": "r",
        "id": 369,
        "name": "die",
        "synonyms": ["die", "dice"],
        "synset": "die.n.01"
    },
    {
        "def": "a small boat of shallow draft with seats and oars with which it is propelled",
        "frequency": "r",
        "id": 370,
        "name": "dinghy",
        "synonyms": ["dinghy", "dory", "rowboat"],
        "synset": "dinghy.n.01"
    },
    {
        "def": "a table at which meals are served",
        "frequency": "f",
        "id": 371,
        "name": "dining_table",
        "synonyms": ["dining_table"],
        "synset": "dining_table.n.01"
    },
    {
        "def": "semiformal evening dress for men",
        "frequency": "r",
        "id": 372,
        "name": "tux",
        "synonyms": ["tux", "tuxedo"],
        "synset": "dinner_jacket.n.01"
    },
    {
        "def": "a piece of dishware normally used as a container for holding or serving food",
        "frequency": "c",
        "id": 373,
        "name": "dish",
        "synonyms": ["dish"],
        "synset": "dish.n.01"
    },
    {
        "def": "directional antenna consisting of a parabolic reflector",
        "frequency": "c",
        "id": 374,
        "name": "dish_antenna",
        "synonyms": ["dish_antenna"],
        "synset": "dish.n.05"
    },
    {
        "def": "a cloth for washing dishes",
        "frequency": "c",
        "id": 375,
        "name": "dishrag",
        "synonyms": ["dishrag", "dishcloth"],
        "synset": "dishrag.n.01"
    },
    {
        "def": "a towel for drying dishes",
        "frequency": "c",
        "id": 376,
        "name": "dishtowel",
        "synonyms": ["dishtowel", "tea_towel"],
        "synset": "dishtowel.n.01"
    },
    {
        "def": "a machine for washing dishes",
        "frequency": "f",
        "id": 377,
        "name": "dishwasher",
        "synonyms": ["dishwasher", "dishwashing_machine"],
        "synset": "dishwasher.n.01"
    },
    {
        "def": "a low-sudsing detergent designed for use in dishwashers",
        "frequency": "r",
        "id": 378,
        "name": "dishwasher_detergent",
        "synonyms": ["dishwasher_detergent", "dishwashing_detergent", "dishwashing_liquid"],
        "synset": "dishwasher_detergent.n.01"
    },
    {
        "def": "a small plastic magnetic disk enclosed in a stiff envelope used to store data",
        "frequency": "r",
        "id": 379,
        "name": "diskette",
        "synonyms": ["diskette", "floppy", "floppy_disk"],
        "synset": "diskette.n.01"
    },
    {
        "def": "a container so designed that the contents can be used in prescribed amounts",
        "frequency": "c",
        "id": 380,
        "name": "dispenser",
        "synonyms": ["dispenser"],
        "synset": "dispenser.n.01"
    },
    {
        "def": "a disposable cup made of paper; for holding drinks",
        "frequency": "c",
        "id": 381,
        "name": "Dixie_cup",
        "synonyms": ["Dixie_cup", "paper_cup"],
        "synset": "dixie_cup.n.01"
    },
    {
        "def": "a common domesticated dog",
        "frequency": "f",
        "id": 382,
        "name": "dog",
        "synonyms": ["dog"],
        "synset": "dog.n.01"
    },
    {
        "def": "a collar for a dog",
        "frequency": "f",
        "id": 383,
        "name": "dog_collar",
        "synonyms": ["dog_collar"],
        "synset": "dog_collar.n.01"
    },
    {
        "def": "a toy replica of a HUMAN (NOT AN ANIMAL)",
        "frequency": "c",
        "id": 384,
        "name": "doll",
        "synonyms": ["doll"],
        "synset": "doll.n.01"
    },
    {
        "def": "a piece of paper money worth one dollar",
        "frequency": "r",
        "id": 385,
        "name": "dollar",
        "synonyms": ["dollar", "dollar_bill", "one_dollar_bill"],
        "synset": "dollar.n.02"
    },
    {
        "def": "any of various small toothed whales with a beaklike snout; larger than porpoises",
        "frequency": "r",
        "id": 386,
        "name": "dolphin",
        "synonyms": ["dolphin"],
        "synset": "dolphin.n.02"
    },
    {
        "def": "domestic beast of burden descended from the African wild ass; patient but stubborn",
        "frequency": "c",
        "id": 387,
        "name": "domestic_ass",
        "synonyms": ["domestic_ass", "donkey"],
        "synset": "domestic_ass.n.01"
    },
    {
        "def": "a mask covering the upper part of the face but with holes for the eyes",
        "frequency": "r",
        "id": 388,
        "name": "eye_mask",
        "synonyms": ["eye_mask"],
        "synset": "domino.n.03"
    },
    {
        "def": "a button at an outer door that gives a ringing or buzzing signal when pushed",
        "frequency": "r",
        "id": 389,
        "name": "doorbell",
        "synonyms": ["doorbell", "buzzer"],
        "synset": "doorbell.n.01"
    },
    {
        "def": "a knob used to open a door (often called `doorhandle' in Great Britain)",
        "frequency": "f",
        "id": 390,
        "name": "doorknob",
        "synonyms": ["doorknob", "doorhandle"],
        "synset": "doorknob.n.01"
    },
    {
        "def": "a mat placed outside an exterior door for wiping the shoes before entering",
        "frequency": "c",
        "id": 391,
        "name": "doormat",
        "synonyms": ["doormat", "welcome_mat"],
        "synset": "doormat.n.02"
    },
    {
        "def": "a small ring-shaped friedcake",
        "frequency": "f",
        "id": 392,
        "name": "doughnut",
        "synonyms": ["doughnut", "donut"],
        "synset": "doughnut.n.02"
    },
    {
        "def": "any of numerous small pigeons",
        "frequency": "r",
        "id": 393,
        "name": "dove",
        "synonyms": ["dove"],
        "synset": "dove.n.01"
    },
    {
        "def": "slender-bodied non-stinging insect having iridescent wings that are outspread at "
               "rest",
        "frequency": "r",
        "id": 394,
        "name": "dragonfly",
        "synonyms": ["dragonfly"],
        "synset": "dragonfly.n.01"
    },
    {
        "def": "a boxlike container in a piece of furniture; made so as to slide in and out",
        "frequency": "f",
        "id": 395,
        "name": "drawer",
        "synonyms": ["drawer"],
        "synset": "drawer.n.01"
    },
    {
        "def": "underpants worn by men",
        "frequency": "c",
        "id": 396,
        "name": "underdrawers",
        "synonyms": ["underdrawers", "boxers", "boxershorts"],
        "synset": "drawers.n.01"
    },
    {
        "def": "a one-piece garment for a woman; has skirt and bodice",
        "frequency": "f",
        "id": 397,
        "name": "dress",
        "synonyms": ["dress", "frock"],
        "synset": "dress.n.01"
    },
    {
        "def": "a man's hat with a tall crown; usually covered with silk or with beaver fur",
        "frequency": "c",
        "id": 398,
        "name": "dress_hat",
        "synonyms": ["dress_hat", "high_hat", "opera_hat", "silk_hat", "top_hat"],
        "synset": "dress_hat.n.01"
    },
    {
        "def": "formalwear consisting of full evening dress for men",
        "frequency": "c",
        "id": 399,
        "name": "dress_suit",
        "synonyms": ["dress_suit"],
        "synset": "dress_suit.n.01"
    },
    {
        "def": "a cabinet with shelves",
        "frequency": "c",
        "id": 400,
        "name": "dresser",
        "synonyms": ["dresser"],
        "synset": "dresser.n.05"
    },
    {
        "def": "a tool with a sharp rotating point for making holes in hard materials",
        "frequency": "c",
        "id": 401,
        "name": "drill",
        "synonyms": ["drill"],
        "synset": "drill.n.01"
    },
    {
        "def": "a public fountain to provide a jet of drinking water",
        "frequency": "r",
        "id": 402,
        "name": "drinking_fountain",
        "synonyms": ["drinking_fountain"],
        "synset": "drinking_fountain.n.01"
    },
    {
        "def": "an aircraft without a pilot that is operated by remote control",
        "frequency": "r",
        "id": 403,
        "name": "drone",
        "synonyms": ["drone"],
        "synset": "drone.n.04"
    },
    {
        "def": "pipet consisting of a small tube with a vacuum bulb at one end for drawing liquid "
               "in and releasing it a drop at a time",
        "frequency": "r",
        "id": 404,
        "name": "dropper",
        "synonyms": ["dropper", "eye_dropper"],
        "synset": "dropper.n.01"
    },
    {
        "def": "a musical percussion instrument; usually consists of a hollow cylinder with a "
               "membrane stretched across each end",
        "frequency": "c",
        "id": 405,
        "name": "drum_(musical_instrument)",
        "synonyms": ["drum_(musical_instrument)"],
        "synset": "drum.n.01"
    },
    {
        "def": "a stick used for playing a drum",
        "frequency": "r",
        "id": 406,
        "name": "drumstick",
        "synonyms": ["drumstick"],
        "synset": "drumstick.n.02"
    },
    {
        "def": "small web-footed broad-billed swimming bird",
        "frequency": "f",
        "id": 407,
        "name": "duck",
        "synonyms": ["duck"],
        "synset": "duck.n.01"
    },
    {
        "def": "young duck",
        "frequency": "r",
        "id": 408,
        "name": "duckling",
        "synonyms": ["duckling"],
        "synset": "duckling.n.02"
    },
    {
        "def": "a wide silvery adhesive tape",
        "frequency": "c",
        "id": 409,
        "name": "duct_tape",
        "synonyms": ["duct_tape"],
        "synset": "duct_tape.n.01"
    },
    {
        "def": "a large cylindrical bag of heavy cloth",
        "frequency": "f",
        "id": 410,
        "name": "duffel_bag",
        "synonyms": ["duffel_bag", "duffle_bag", "duffel", "duffle"],
        "synset": "duffel_bag.n.01"
    },
    {
        "def": "an exercising weight with two ball-like ends connected by a short handle",
        "frequency": "r",
        "id": 411,
        "name": "dumbbell",
        "synonyms": ["dumbbell"],
        "synset": "dumbbell.n.01"
    },
    {
        "def": "a container designed to receive and transport and dump waste",
        "frequency": "c",
        "id": 412,
        "name": "dumpster",
        "synonyms": ["dumpster"],
        "synset": "dumpster.n.01"
    },
    {
        "def": "a short-handled receptacle into which dust can be swept",
        "frequency": "r",
        "id": 413,
        "name": "dustpan",
        "synonyms": ["dustpan"],
        "synset": "dustpan.n.02"
    },
    {
        "def": "iron or earthenware cooking pot; used for stews",
        "frequency": "r",
        "id": 414,
        "name": "Dutch_oven",
        "synonyms": ["Dutch_oven"],
        "synset": "dutch_oven.n.02"
    },
    {
        "def": "large birds of prey noted for their broad wings and strong soaring flight",
        "frequency": "c",
        "id": 415,
        "name": "eagle",
        "synonyms": ["eagle"],
        "synset": "eagle.n.01"
    },
    {
        "def": "device for listening to audio that is held over or inserted into the ear",
        "frequency": "f",
        "id": 416,
        "name": "earphone",
        "synonyms": ["earphone", "earpiece", "headphone"],
        "synset": "earphone.n.01"
    },
    {
        "def": "a soft plug that is inserted into the ear canal to block sound",
        "frequency": "r",
        "id": 417,
        "name": "earplug",
        "synonyms": ["earplug"],
        "synset": "earplug.n.01"
    },
    {
        "def": "jewelry to ornament the ear",
        "frequency": "f",
        "id": 418,
        "name": "earring",
        "synonyms": ["earring"],
        "synset": "earring.n.01"
    },
    {
        "def": "an upright tripod for displaying something (usually an artist's canvas)",
        "frequency": "c",
        "id": 419,
        "name": "easel",
        "synonyms": ["easel"],
        "synset": "easel.n.01"
    },
    {
        "def": "oblong cream puff",
        "frequency": "r",
        "id": 420,
        "name": "eclair",
        "synonyms": ["eclair"],
        "synset": "eclair.n.01"
    },
    {
        "def": "an elongate fish with fatty flesh",
        "frequency": "r",
        "id": 421,
        "name": "eel",
        "synonyms": ["eel"],
        "synset": "eel.n.01"
    },
    {
        "def": "oval reproductive body of a fowl (especially a hen) used as food",
        "frequency": "f",
        "id": 422,
        "name": "egg",
        "synonyms": ["egg", "eggs"],
        "synset": "egg.n.02"
    },
    {
        "def": "minced vegetables and meat wrapped in a pancake and fried",
        "frequency": "r",
        "id": 423,
        "name": "egg_roll",
        "synonyms": ["egg_roll", "spring_roll"],
        "synset": "egg_roll.n.01"
    },
    {
        "def": "the yellow spherical part of an egg",
        "frequency": "c",
        "id": 424,
        "name": "egg_yolk",
        "synonyms": ["egg_yolk", "yolk_(egg)"],
        "synset": "egg_yolk.n.01"
    },
    {
        "def": "a mixer for beating eggs or whipping cream",
        "frequency": "c",
        "id": 425,
        "name": "eggbeater",
        "synonyms": ["eggbeater", "eggwhisk"],
        "synset": "eggbeater.n.02"
    },
    {
        "def": "egg-shaped vegetable having a shiny skin typically dark purple",
        "frequency": "c",
        "id": 426,
        "name": "eggplant",
        "synonyms": ["eggplant", "aubergine"],
        "synset": "eggplant.n.01"
    },
    {
        "def": "a chair-shaped instrument of execution by electrocution",
        "frequency": "r",
        "id": 427,
        "name": "electric_chair",
        "synonyms": ["electric_chair"],
        "synset": "electric_chair.n.01"
    },
    {
        "def": "a refrigerator in which the coolant is pumped around by an electric motor",
        "frequency": "f",
        "id": 428,
        "name": "refrigerator",
        "synonyms": ["refrigerator"],
        "synset": "electric_refrigerator.n.01"
    },
    {
        "def": "a common elephant",
        "frequency": "f",
        "id": 429,
        "name": "elephant",
        "synonyms": ["elephant"],
        "synset": "elephant.n.01"
    },
    {
        "def": "large northern deer with enormous flattened antlers in the male",
        "frequency": "r",
        "id": 430,
        "name": "elk",
        "synonyms": ["elk", "moose"],
        "synset": "elk.n.01"
    },
    {
        "def": "a flat (usually rectangular) container for a letter, thin package, etc.",
        "frequency": "c",
        "id": 431,
        "name": "envelope",
        "synonyms": ["envelope"],
        "synset": "envelope.n.01"
    },
    {
        "def": "an implement used to erase something",
        "frequency": "c",
        "id": 432,
        "name": "eraser",
        "synonyms": ["eraser"],
        "synset": "eraser.n.01"
    },
    {
        "def": "edible snail usually served in the shell with a sauce of melted butter and garlic",
        "frequency": "r",
        "id": 433,
        "name": "escargot",
        "synonyms": ["escargot"],
        "synset": "escargot.n.01"
    },
    {
        "def": "a protective cloth covering for an injured eye",
        "frequency": "r",
        "id": 434,
        "name": "eyepatch",
        "synonyms": ["eyepatch"],
        "synset": "eyepatch.n.01"
    },
    {
        "def": "birds of prey having long pointed powerful wings adapted for swift flight",
        "frequency": "r",
        "id": 435,
        "name": "falcon",
        "synonyms": ["falcon"],
        "synset": "falcon.n.01"
    },
    {
        "def": "a device for creating a current of air by movement of a surface or surfaces",
        "frequency": "f",
        "id": 436,
        "name": "fan",
        "synonyms": ["fan"],
        "synset": "fan.n.01"
    },
    {
        "def": "a regulator for controlling the flow of a liquid from a reservoir",
        "frequency": "f",
        "id": 437,
        "name": "faucet",
        "synonyms": ["faucet", "spigot", "tap"],
        "synset": "faucet.n.01"
    },
    {
        "def": "a hat made of felt with a creased crown",
        "frequency": "r",
        "id": 438,
        "name": "fedora",
        "synonyms": ["fedora"],
        "synset": "fedora.n.01"
    },
    {
        "def": "domesticated albino variety of the European polecat bred for hunting rats and "
               "rabbits",
        "frequency": "r",
        "id": 439,
        "name": "ferret",
        "synonyms": ["ferret"],
        "synset": "ferret.n.02"
    },
    {
        "def": "a large wheel with suspended seats that remain upright as the wheel rotates",
        "frequency": "c",
        "id": 440,
        "name": "Ferris_wheel",
        "synonyms": ["Ferris_wheel"],
        "synset": "ferris_wheel.n.01"
    },
    {
        "def": "a boat that transports people or vehicles across a body of water and operates on a "
               "regular schedule",
        "frequency": "r",
        "id": 441,
        "name": "ferry",
        "synonyms": ["ferry", "ferryboat"],
        "synset": "ferry.n.01"
    },
    {
        "def": "fleshy sweet pear-shaped yellowish or purple fruit eaten fresh or preserved or "
               "dried",
        "frequency": "r",
        "id": 442,
        "name": "fig_(fruit)",
        "synonyms": ["fig_(fruit)"],
        "synset": "fig.n.04"
    },
    {
        "def": "a high-speed military or naval airplane designed to destroy enemy targets",
        "frequency": "c",
        "id": 443,
        "name": "fighter_jet",
        "synonyms": ["fighter_jet", "fighter_aircraft", "attack_aircraft"],
        "synset": "fighter.n.02"
    },
    {
        "def": "a small carved or molded figure",
        "frequency": "f",
        "id": 444,
        "name": "figurine",
        "synonyms": ["figurine"],
        "synset": "figurine.n.01"
    },
    {
        "def": "office furniture consisting of a container for keeping papers in order",
        "frequency": "c",
        "id": 445,
        "name": "file_cabinet",
        "synonyms": ["file_cabinet", "filing_cabinet"],
        "synset": "file.n.03"
    },
    {
        "def": "a steel hand tool with small sharp teeth on some or all of its surfaces; used for "
               "smoothing wood or metal",
        "frequency": "r",
        "id": 446,
        "name": "file_(tool)",
        "synonyms": ["file_(tool)"],
        "synset": "file.n.04"
    },
    {
        "def": "an alarm that is tripped off by fire or smoke",
        "frequency": "f",
        "id": 447,
        "name": "fire_alarm",
        "synonyms": ["fire_alarm", "smoke_alarm"],
        "synset": "fire_alarm.n.02"
    },
    {
        "def": "large trucks that carry firefighters and equipment to the site of a fire",
        "frequency": "c",
        "id": 448,
        "name": "fire_engine",
        "synonyms": ["fire_engine", "fire_truck"],
        "synset": "fire_engine.n.01"
    },
    {
        "def": "a manually operated device for extinguishing small fires",
        "frequency": "c",
        "id": 449,
        "name": "fire_extinguisher",
        "synonyms": ["fire_extinguisher", "extinguisher"],
        "synset": "fire_extinguisher.n.01"
    },
    {
        "def": "a large hose that carries water from a fire hydrant to the site of the fire",
        "frequency": "c",
        "id": 450,
        "name": "fire_hose",
        "synonyms": ["fire_hose"],
        "synset": "fire_hose.n.01"
    },
    {
        "def": "an open recess in a wall at the base of a chimney where a fire can be built",
        "frequency": "f",
        "id": 451,
        "name": "fireplace",
        "synonyms": ["fireplace"],
        "synset": "fireplace.n.01"
    },
    {
        "def": "an upright hydrant for drawing water to use in fighting a fire",
        "frequency": "f",
        "id": 452,
        "name": "fireplug",
        "synonyms": ["fireplug", "fire_hydrant", "hydrant"],
        "synset": "fireplug.n.01"
    },
    {
        "def": "any of various mostly cold-blooded aquatic vertebrates usually having scales and "
               "breathing through gills",
        "frequency": "c",
        "id": 453,
        "name": "fish",
        "synonyms": ["fish"],
        "synset": "fish.n.01"
    },
    {
        "def": "the flesh of fish used as food",
        "frequency": "r",
        "id": 454,
        "name": "fish_(food)",
        "synonyms": ["fish_(food)"],
        "synset": "fish.n.02"
    },
    {
        "def": "a transparent bowl in which small fish are kept",
        "frequency": "r",
        "id": 455,
        "name": "fishbowl",
        "synonyms": ["fishbowl", "goldfish_bowl"],
        "synset": "fishbowl.n.02"
    },
    {
        "def": "a vessel for fishing",
        "frequency": "r",
        "id": 456,
        "name": "fishing_boat",
        "synonyms": ["fishing_boat", "fishing_vessel"],
        "synset": "fishing_boat.n.01"
    },
    {
        "def": "a rod that is used in fishing to extend the fishing line",
        "frequency": "c",
        "id": 457,
        "name": "fishing_rod",
        "synonyms": ["fishing_rod", "fishing_pole"],
        "synset": "fishing_rod.n.01"
    },
    {
        "def": "emblem usually consisting of a rectangular piece of cloth of distinctive design "
               "(do not include pole)",
        "frequency": "f",
        "id": 458,
        "name": "flag",
        "synonyms": ["flag"],
        "synset": "flag.n.01"
    },
    {
        "def": "a tall staff or pole on which a flag is raised",
        "frequency": "f",
        "id": 459,
        "name": "flagpole",
        "synonyms": ["flagpole", "flagstaff"],
        "synset": "flagpole.n.02"
    },
    {
        "def": "large pink web-footed bird with down-bent bill",
        "frequency": "c",
        "id": 460,
        "name": "flamingo",
        "synonyms": ["flamingo"],
        "synset": "flamingo.n.01"
    },
    {
        "def": "a soft light woolen fabric; used for clothing",
        "frequency": "c",
        "id": 461,
        "name": "flannel",
        "synonyms": ["flannel"],
        "synset": "flannel.n.01"
    },
    {
        "def": "a lamp for providing momentary light to take a photograph",
        "frequency": "r",
        "id": 462,
        "name": "flash",
        "synonyms": ["flash", "flashbulb"],
        "synset": "flash.n.10"
    },
    {
        "def": "a small portable battery-powered electric lamp",
        "frequency": "c",
        "id": 463,
        "name": "flashlight",
        "synonyms": ["flashlight", "torch"],
        "synset": "flashlight.n.01"
    },
    {
        "def": "a soft bulky fabric with deep pile; used chiefly for clothing",
        "frequency": "r",
        "id": 464,
        "name": "fleece",
        "synonyms": ["fleece"],
        "synset": "fleece.n.03"
    },
    {
        "def": "a backless sandal held to the foot by a thong between two toes",
        "frequency": "f",
        "id": 465,
        "name": "flip-flop_(sandal)",
        "synonyms": ["flip-flop_(sandal)"],
        "synset": "flip-flop.n.02"
    },
    {
        "def": "a shoe to aid a person in swimming",
        "frequency": "c",
        "id": 466,
        "name": "flipper_(footwear)",
        "synonyms": ["flipper_(footwear)", "fin_(footwear)"],
        "synset": "flipper.n.01"
    },
    {
        "def": "a decorative arrangement of flowers",
        "frequency": "f",
        "id": 467,
        "name": "flower_arrangement",
        "synonyms": ["flower_arrangement", "floral_arrangement"],
        "synset": "flower_arrangement.n.01"
    },
    {
        "def": "a tall narrow wineglass",
        "frequency": "c",
        "id": 468,
        "name": "flute_glass",
        "synonyms": ["flute_glass", "champagne_flute"],
        "synset": "flute.n.02"
    },
    {
        "def": "a young horse",
        "frequency": "r",
        "id": 469,
        "name": "foal",
        "synonyms": ["foal"],
        "synset": "foal.n.01"
    },
    {
        "def": "a chair that can be folded flat for storage",
        "frequency": "c",
        "id": 470,
        "name": "folding_chair",
        "synonyms": ["folding_chair"],
        "synset": "folding_chair.n.01"
    },
    {
        "def": "a kitchen appliance for shredding, blending, chopping, or slicing food",
        "frequency": "c",
        "id": 471,
        "name": "food_processor",
        "synonyms": ["food_processor"],
        "synset": "food_processor.n.01"
    },
    {
        "def": "the inflated oblong ball used in playing American football",
        "frequency": "c",
        "id": 472,
        "name": "football_(American)",
        "synonyms": ["football_(American)"],
        "synset": "football.n.02"
    },
    {
        "def": "a padded helmet with a face mask to protect the head of football players",
        "frequency": "r",
        "id": 473,
        "name": "football_helmet",
        "synonyms": ["football_helmet"],
        "synset": "football_helmet.n.01"
    },
    {
        "def": "a low seat or a stool to rest the feet of a seated person",
        "frequency": "c",
        "id": 474,
        "name": "footstool",
        "synonyms": ["footstool", "footrest"],
        "synset": "footstool.n.01"
    },
    {
        "def": "cutlery used for serving and eating food",
        "frequency": "f",
        "id": 475,
        "name": "fork",
        "synonyms": ["fork"],
        "synset": "fork.n.01"
    },
    {
        "def": "an industrial vehicle with a power operated fork in front that can be inserted "
               "under loads to lift and move them",
        "frequency": "r",
        "id": 476,
        "name": "forklift",
        "synonyms": ["forklift"],
        "synset": "forklift.n.01"
    },
    {
        "def": "a railway car that carries freight",
        "frequency": "r",
        "id": 477,
        "name": "freight_car",
        "synonyms": ["freight_car"],
        "synset": "freight_car.n.01"
    },
    {
        "def": "bread slice dipped in egg and milk and fried",
        "frequency": "r",
        "id": 478,
        "name": "French_toast",
        "synonyms": ["French_toast"],
        "synset": "french_toast.n.01"
    },
    {
        "def": "anything that freshens",
        "frequency": "c",
        "id": 479,
        "name": "freshener",
        "synonyms": ["freshener", "air_freshener"],
        "synset": "freshener.n.01"
    },
    {
        "def": "a light, plastic disk propelled with a flip of the wrist for recreation or "
               "competition",
        "frequency": "f",
        "id": 480,
        "name": "frisbee",
        "synonyms": ["frisbee"],
        "synset": "frisbee.n.01"
    },
    {
        "def": "a tailless stout-bodied amphibians with long hind limbs for leaping",
        "frequency": "c",
        "id": 481,
        "name": "frog",
        "synonyms": ["frog", "toad", "toad_frog"],
        "synset": "frog.n.01"
    },
    {
        "def": "drink produced by squeezing or crushing fruit",
        "frequency": "c",
        "id": 482,
        "name": "fruit_juice",
        "synonyms": ["fruit_juice"],
        "synset": "fruit_juice.n.01"
    },
    {
        "def": "salad composed of fruits",
        "frequency": "r",
        "id": 483,
        "name": "fruit_salad",
        "synonyms": ["fruit_salad"],
        "synset": "fruit_salad.n.01"
    },
    {
        "def": "a pan used for frying foods",
        "frequency": "c",
        "id": 484,
        "name": "frying_pan",
        "synonyms": ["frying_pan", "frypan", "skillet"],
        "synset": "frying_pan.n.01"
    },
    {
        "def": "soft creamy candy",
        "frequency": "r",
        "id": 485,
        "name": "fudge",
        "synonyms": ["fudge"],
        "synset": "fudge.n.01"
    },
    {
        "def": "a cone-shaped utensil used to channel a substance into a container with a small "
               "mouth",
        "frequency": "r",
        "id": 486,
        "name": "funnel",
        "synonyms": ["funnel"],
        "synset": "funnel.n.02"
    },
    {
        "def": "a pad that is used for sleeping on the floor or on a raised frame",
        "frequency": "c",
        "id": 487,
        "name": "futon",
        "synonyms": ["futon"],
        "synset": "futon.n.01"
    },
    {
        "def": "restraint put into a person's mouth to prevent speaking or shouting",
        "frequency": "r",
        "id": 488,
        "name": "gag",
        "synonyms": ["gag", "muzzle"],
        "synset": "gag.n.02"
    },
    {
        "def": "a receptacle where waste can be discarded",
        "frequency": "r",
        "id": 489,
        "name": "garbage",
        "synonyms": ["garbage"],
        "synset": "garbage.n.03"
    },
    {
        "def": "a truck for collecting domestic refuse",
        "frequency": "c",
        "id": 490,
        "name": "garbage_truck",
        "synonyms": ["garbage_truck"],
        "synset": "garbage_truck.n.01"
    },
    {
        "def": "a hose used for watering a lawn or garden",
        "frequency": "c",
        "id": 491,
        "name": "garden_hose",
        "synonyms": ["garden_hose"],
        "synset": "garden_hose.n.01"
    },
    {
        "def": "a medicated solution used for gargling and rinsing the mouth",
        "frequency": "c",
        "id": 492,
        "name": "gargle",
        "synonyms": ["gargle", "mouthwash"],
        "synset": "gargle.n.01"
    },
    {
        "def": "an ornament consisting of a grotesquely carved figure of a person or animal",
        "frequency": "r",
        "id": 493,
        "name": "gargoyle",
        "synonyms": ["gargoyle"],
        "synset": "gargoyle.n.02"
    },
    {
        "def": "aromatic bulb used as seasoning",
        "frequency": "c",
        "id": 494,
        "name": "garlic",
        "synonyms": ["garlic", "ail"],
        "synset": "garlic.n.02"
    },
    {
        "def": "a protective face mask with a filter",
        "frequency": "r",
        "id": 495,
        "name": "gasmask",
        "synonyms": ["gasmask", "respirator", "gas_helmet"],
        "synset": "gasmask.n.01"
    },
    {
        "def": "small swift graceful antelope of Africa and Asia having lustrous eyes",
        "frequency": "r",
        "id": 496,
        "name": "gazelle",
        "synonyms": ["gazelle"],
        "synset": "gazelle.n.01"
    },
    {
        "def": "an edible jelly made with gelatin and used as a dessert or salad base or a coating "
               "for foods",
        "frequency": "c",
        "id": 497,
        "name": "gelatin",
        "synonyms": ["gelatin", "jelly"],
        "synset": "gelatin.n.02"
    },
    {
        "def": "a crystalline rock that can be cut and polished for jewelry",
        "frequency": "r",
        "id": 498,
        "name": "gemstone",
        "synonyms": ["gemstone"],
        "synset": "gem.n.02"
    },
    {
        "def": "large black-and-white herbivorous mammal of bamboo forests of China and Tibet",
        "frequency": "c",
        "id": 499,
        "name": "giant_panda",
        "synonyms": ["giant_panda", "panda", "panda_bear"],
        "synset": "giant_panda.n.01"
    },
    {
        "def": "attractive wrapping paper suitable for wrapping gifts",
        "frequency": "c",
        "id": 500,
        "name": "gift_wrap",
        "synonyms": ["gift_wrap"],
        "synset": "gift_wrap.n.01"
    },
    {
        "def": "the root of the common ginger plant; used fresh as a seasoning",
        "frequency": "c",
        "id": 501,
        "name": "ginger",
        "synonyms": ["ginger", "gingerroot"],
        "synset": "ginger.n.03"
    },
    {
        "def": "tall animal having a spotted coat and small horns and very long neck and legs",
        "frequency": "f",
        "id": 502,
        "name": "giraffe",
        "synonyms": ["giraffe"],
        "synset": "giraffe.n.01"
    },
    {
        "def": "a band of material around the waist that strengthens a skirt or trousers",
        "frequency": "c",
        "id": 503,
        "name": "cincture",
        "synonyms": ["cincture", "sash", "waistband", "waistcloth"],
        "synset": "girdle.n.02"
    },
    {
        "def": "a container for holding liquids while drinking",
        "frequency": "f",
        "id": 504,
        "name": "glass_(drink_container)",
        "synonyms": ["glass_(drink_container)", "drinking_glass"],
        "synset": "glass.n.02"
    },
    {
        "def": "a sphere on which a map (especially of the earth) is represented",
        "frequency": "c",
        "id": 505,
        "name": "globe",
        "synonyms": ["globe"],
        "synset": "globe.n.03"
    },
    {
        "def": "handwear covering the hand",
        "frequency": "f",
        "id": 506,
        "name": "glove",
        "synonyms": ["glove"],
        "synset": "glove.n.02"
    },
    {
        "def": "a common goat",
        "frequency": "c",
        "id": 507,
        "name": "goat",
        "synonyms": ["goat"],
        "synset": "goat.n.01"
    },
    {
        "def": "tight-fitting spectacles worn to protect the eyes",
        "frequency": "f",
        "id": 508,
        "name": "goggles",
        "synonyms": ["goggles"],
        "synset": "goggles.n.01"
    },
    {
        "def": "small golden or orange-red freshwater fishes used as pond or aquarium pets",
        "frequency": "r",
        "id": 509,
        "name": "goldfish",
        "synonyms": ["goldfish"],
        "synset": "goldfish.n.01"
    },
    {
        "def": "golf equipment used by a golfer to hit a golf ball",
        "frequency": "r",
        "id": 510,
        "name": "golf_club",
        "synonyms": ["golf_club", "golf-club"],
        "synset": "golf_club.n.02"
    },
    {
        "def": "a small motor vehicle in which golfers can ride between shots",
        "frequency": "c",
        "id": 511,
        "name": "golfcart",
        "synonyms": ["golfcart"],
        "synset": "golfcart.n.01"
    },
    {
        "def": "long narrow flat-bottomed boat propelled by sculling; traditionally used on canals "
               "of Venice",
        "frequency": "r",
        "id": 512,
        "name": "gondola_(boat)",
        "synonyms": ["gondola_(boat)"],
        "synset": "gondola.n.02"
    },
    {
        "def": "loud, web-footed long-necked aquatic birds usually larger than ducks",
        "frequency": "c",
        "id": 513,
        "name": "goose",
        "synonyms": ["goose"],
        "synset": "goose.n.01"
    },
    {
        "def": "largest ape",
        "frequency": "r",
        "id": 514,
        "name": "gorilla",
        "synonyms": ["gorilla"],
        "synset": "gorilla.n.01"
    },
    {
        "def": "any of numerous inedible fruits with hard rinds",
        "frequency": "r",
        "id": 515,
        "name": "gourd",
        "synonyms": ["gourd"],
        "synset": "gourd.n.02"
    },
    {
        "def": "protective garment worn by surgeons during operations",
        "frequency": "r",
        "id": 516,
        "name": "surgical_gown",
        "synonyms": ["surgical_gown", "scrubs_(surgical_clothing)"],
        "synset": "gown.n.04"
    },
    {
        "def": "any of various juicy fruit with green or purple skins; grow in clusters",
        "frequency": "f",
        "id": 517,
        "name": "grape",
        "synonyms": ["grape"],
        "synset": "grape.n.01"
    },
    {
        "def": "plant-eating insect with hind legs adapted for leaping",
        "frequency": "r",
        "id": 518,
        "name": "grasshopper",
        "synonyms": ["grasshopper"],
        "synset": "grasshopper.n.01"
    },
    {
        "def": "utensil with sharp perforations for shredding foods (as vegetables or cheese)",
        "frequency": "c",
        "id": 519,
        "name": "grater",
        "synonyms": ["grater"],
        "synset": "grater.n.01"
    },
    {
        "def": "a stone that is used to mark a grave",
        "frequency": "c",
        "id": 520,
        "name": "gravestone",
        "synonyms": ["gravestone", "headstone", "tombstone"],
        "synset": "gravestone.n.01"
    },
    {
        "def": "a dish (often boat-shaped) for serving gravy or sauce",
        "frequency": "r",
        "id": 521,
        "name": "gravy_boat",
        "synonyms": ["gravy_boat", "gravy_holder"],
        "synset": "gravy_boat.n.01"
    },
    {
        "def": "a common bean plant cultivated for its slender green edible pods",
        "frequency": "c",
        "id": 522,
        "name": "green_bean",
        "synonyms": ["green_bean"],
        "synset": "green_bean.n.02"
    },
    {
        "def": "a young onion before the bulb has enlarged",
        "frequency": "c",
        "id": 523,
        "name": "green_onion",
        "synonyms": ["green_onion", "spring_onion", "scallion"],
        "synset": "green_onion.n.01"
    },
    {
        "def": "cooking utensil consisting of a flat heated surface on which food is cooked",
        "frequency": "r",
        "id": 524,
        "name": "griddle",
        "synonyms": ["griddle"],
        "synset": "griddle.n.01"
    },
    {
        "def": "a restaurant where food is cooked on a grill",
        "frequency": "r",
        "id": 525,
        "name": "grillroom",
        "synonyms": ["grillroom", "grill_(restaurant)"],
        "synset": "grillroom.n.01"
    },
    {
        "def": "a machine tool that polishes metal",
        "frequency": "r",
        "id": 526,
        "name": "grinder_(tool)",
        "synonyms": ["grinder_(tool)"],
        "synset": "grinder.n.04"
    },
    {
        "def": "coarsely ground corn boiled as a breakfast dish",
        "frequency": "r",
        "id": 527,
        "name": "grits",
        "synonyms": ["grits", "hominy_grits"],
        "synset": "grits.n.01"
    },
    {
        "def": "powerful brownish-yellow bear of the uplands of western North America",
        "frequency": "c",
        "id": 528,
        "name": "grizzly",
        "synonyms": ["grizzly", "grizzly_bear"],
        "synset": "grizzly.n.01"
    },
    {
        "def": "a sack for holding customer's groceries",
        "frequency": "c",
        "id": 529,
        "name": "grocery_bag",
        "synonyms": ["grocery_bag"],
        "synset": "grocery_bag.n.01"
    },
    {
        "def": "a dip made of mashed avocado mixed with chopped onions and other seasonings",
        "frequency": "r",
        "id": 530,
        "name": "guacamole",
        "synonyms": ["guacamole"],
        "synset": "guacamole.n.01"
    },
    {
        "def": "a stringed instrument usually having six strings; played by strumming or plucking",
        "frequency": "f",
        "id": 531,
        "name": "guitar",
        "synonyms": ["guitar"],
        "synset": "guitar.n.01"
    },
    {
        "def": "mostly white aquatic bird having long pointed wings and short legs",
        "frequency": "c",
        "id": 532,
        "name": "gull",
        "synonyms": ["gull", "seagull"],
        "synset": "gull.n.02"
    },
    {
        "def": "a weapon that discharges a bullet at high velocity from a metal tube",
        "frequency": "c",
        "id": 533,
        "name": "gun",
        "synonyms": ["gun"],
        "synset": "gun.n.01"
    },
    {
        "def": "substance sprayed on the hair to hold it in place",
        "frequency": "r",
        "id": 534,
        "name": "hair_spray",
        "synonyms": ["hair_spray"],
        "synset": "hair_spray.n.01"
    },
    {
        "def": "a brush used to groom a person's hair",
        "frequency": "c",
        "id": 535,
        "name": "hairbrush",
        "synonyms": ["hairbrush"],
        "synset": "hairbrush.n.01"
    },
    {
        "def": "a small net that someone wears over their hair to keep it in place",
        "frequency": "c",
        "id": 536,
        "name": "hairnet",
        "synonyms": ["hairnet"],
        "synset": "hairnet.n.01"
    },
    {
        "def": "a double pronged pin used to hold women's hair in place",
        "frequency": "c",
        "id": 537,
        "name": "hairpin",
        "synonyms": ["hairpin"],
        "synset": "hairpin.n.01"
    },
    {
        "def": "meat cut from the thigh of a hog (usually smoked)",
        "frequency": "f",
        "id": 538,
        "name": "ham",
        "synonyms": ["ham", "jambon", "gammon"],
        "synset": "ham.n.01"
    },
    {
        "def": "a sandwich consisting of a patty of minced beef served on a bun",
        "frequency": "c",
        "id": 539,
        "name": "hamburger",
        "synonyms": ["hamburger", "beefburger", "burger"],
        "synset": "hamburger.n.01"
    },
    {
        "def": "a hand tool with a heavy head and a handle; used to deliver an impulsive force by "
               "striking",
        "frequency": "c",
        "id": 540,
        "name": "hammer",
        "synonyms": ["hammer"],
        "synset": "hammer.n.02"
    },
    {
        "def": "a hanging bed of canvas or rope netting (usually suspended between two trees)",
        "frequency": "r",
        "id": 541,
        "name": "hammock",
        "synonyms": ["hammock"],
        "synset": "hammock.n.02"
    },
    {
        "def": "a basket usually with a cover",
        "frequency": "r",
        "id": 542,
        "name": "hamper",
        "synonyms": ["hamper"],
        "synset": "hamper.n.02"
    },
    {
        "def": "short-tailed burrowing rodent with large cheek pouches",
        "frequency": "r",
        "id": 543,
        "name": "hamster",
        "synonyms": ["hamster"],
        "synset": "hamster.n.01"
    },
    {
        "def": "a hand-held electric blower that can blow warm air onto the hair",
        "frequency": "c",
        "id": 544,
        "name": "hair_dryer",
        "synonyms": ["hair_dryer"],
        "synset": "hand_blower.n.01"
    },
    {
        "def": "a mirror intended to be held in the hand",
        "frequency": "r",
        "id": 545,
        "name": "hand_glass",
        "synonyms": ["hand_glass", "hand_mirror"],
        "synset": "hand_glass.n.01"
    },
    {
        "def": "a small towel used to dry the hands or face",
        "frequency": "f",
        "id": 546,
        "name": "hand_towel",
        "synonyms": ["hand_towel", "face_towel"],
        "synset": "hand_towel.n.01"
    },
    {
        "def": "wheeled vehicle that can be pushed by a person",
        "frequency": "c",
        "id": 547,
        "name": "handcart",
        "synonyms": ["handcart", "pushcart", "hand_truck"],
        "synset": "handcart.n.01"
    },
    {
        "def": "shackle that consists of a metal loop that can be locked around the wrist",
        "frequency": "r",
        "id": 548,
        "name": "handcuff",
        "synonyms": ["handcuff"],
        "synset": "handcuff.n.01"
    },
    {
        "def": "a square piece of cloth used for wiping the eyes or nose or as a costume accessory",
        "frequency": "c",
        "id": 549,
        "name": "handkerchief",
        "synonyms": ["handkerchief"],
        "synset": "handkerchief.n.01"
    },
    {
        "def": "the appendage to an object that is designed to be held in order to use or move it",
        "frequency": "f",
        "id": 550,
        "name": "handle",
        "synonyms": ["handle", "grip", "handgrip"],
        "synset": "handle.n.01"
    },
    {
        "def": "a saw used with one hand for cutting wood",
        "frequency": "r",
        "id": 551,
        "name": "handsaw",
        "synonyms": ["handsaw", "carpenter's_saw"],
        "synset": "handsaw.n.01"
    },
    {
        "def": "a book with cardboard or cloth or leather covers",
        "frequency": "r",
        "id": 552,
        "name": "hardback_book",
        "synonyms": ["hardback_book", "hardcover_book"],
        "synset": "hardback.n.01"
    },
    {
        "def": "a free-reed instrument in which air is forced through the reeds by bellows",
        "frequency": "r",
        "id": 553,
        "name": "harmonium",
        "synonyms": ["harmonium", "organ_(musical_instrument)", "reed_organ_(musical_instrument)"],
        "synset": "harmonium.n.01"
    },
    {
        "def": "headwear that protects the head from bad weather, sun, or worn for fashion",
        "frequency": "f",
        "id": 554,
        "name": "hat",
        "synonyms": ["hat"],
        "synset": "hat.n.01"
    },
    {
        "def": "a round piece of luggage for carrying hats",
        "frequency": "r",
        "id": 555,
        "name": "hatbox",
        "synonyms": ["hatbox"],
        "synset": "hatbox.n.01"
    },
    {
        "def": "a movable barrier covering a hatchway",
        "frequency": "r",
        "id": 556,
        "name": "hatch",
        "synonyms": ["hatch"],
        "synset": "hatch.n.03"
    },
    {
        "def": "a garment that covers the head and face",
        "frequency": "c",
        "id": 557,
        "name": "veil",
        "synonyms": ["veil"],
        "synset": "head_covering.n.01"
    },
    {
        "def": "a band worn around or over the head",
        "frequency": "f",
        "id": 558,
        "name": "headband",
        "synonyms": ["headband"],
        "synset": "headband.n.01"
    },
    {
        "def": "a vertical board or panel forming the head of a bedstead",
        "frequency": "f",
        "id": 559,
        "name": "headboard",
        "synonyms": ["headboard"],
        "synset": "headboard.n.01"
    },
    {
        "def": "a powerful light with reflector; attached to the front of an automobile or "
               "locomotive",
        "frequency": "f",
        "id": 560,
        "name": "headlight",
        "synonyms": ["headlight", "headlamp"],
        "synset": "headlight.n.01"
    },
    {
        "def": "a kerchief worn over the head and tied under the chin",
        "frequency": "c",
        "id": 561,
        "name": "headscarf",
        "synonyms": ["headscarf"],
        "synset": "headscarf.n.01"
    },
    {
        "def": "receiver consisting of a pair of headphones",
        "frequency": "r",
        "id": 562,
        "name": "headset",
        "synonyms": ["headset"],
        "synset": "headset.n.01"
    },
    {
        "def": "the band that is the part of a bridle that fits around a horse's head",
        "frequency": "c",
        "id": 563,
        "name": "headstall_(for_horses)",
        "synonyms": ["headstall_(for_horses)", "headpiece_(for_horses)"],
        "synset": "headstall.n.01"
    },
    {
        "def": "an acoustic device used to direct sound to the ear of a hearing-impaired person",
        "frequency": "r",
        "id": 564,
        "name": "hearing_aid",
        "synonyms": ["hearing_aid"],
        "synset": "hearing_aid.n.02"
    },
    {
        "def": "a muscular organ; its contractions move the blood through the body",
        "frequency": "c",
        "id": 565,
        "name": "heart",
        "synonyms": ["heart"],
        "synset": "heart.n.02"
    },
    {
        "def": "device that heats water or supplies warmth to a room",
        "frequency": "c",
        "id": 566,
        "name": "heater",
        "synonyms": ["heater", "warmer"],
        "synset": "heater.n.01"
    },
    {
        "def": "an aircraft without wings that obtains its lift from the rotation of overhead "
               "blades",
        "frequency": "c",
        "id": 567,
        "name": "helicopter",
        "synonyms": ["helicopter"],
        "synset": "helicopter.n.01"
    },
    {
        "def": "a protective headgear made of hard material to resist blows",
        "frequency": "f",
        "id": 568,
        "name": "helmet",
        "synonyms": ["helmet"],
        "synset": "helmet.n.02"
    },
    {
        "def": "grey or white wading bird with long neck and long legs and (usually) long bill",
        "frequency": "r",
        "id": 569,
        "name": "heron",
        "synonyms": ["heron"],
        "synset": "heron.n.02"
    },
    {
        "def": "a chair for feeding a very young child",
        "frequency": "c",
        "id": 570,
        "name": "highchair",
        "synonyms": ["highchair", "feeding_chair"],
        "synset": "highchair.n.01"
    },
    {
        "def": "a joint that holds two parts together so that one can swing relative to the other",
        "frequency": "f",
        "id": 571,
        "name": "hinge",
        "synonyms": ["hinge"],
        "synset": "hinge.n.01"
    },
    {
        "def": "massive thick-skinned animal living in or around rivers of tropical Africa",
        "frequency": "r",
        "id": 572,
        "name": "hippopotamus",
        "synonyms": ["hippopotamus"],
        "synset": "hippopotamus.n.01"
    },
    {
        "def": "sports implement consisting of a stick used by hockey players to move the puck",
        "frequency": "r",
        "id": 573,
        "name": "hockey_stick",
        "synonyms": ["hockey_stick"],
        "synset": "hockey_stick.n.01"
    },
    {
        "def": "domestic swine",
        "frequency": "c",
        "id": 574,
        "name": "hog",
        "synonyms": ["hog", "pig"],
        "synset": "hog.n.03"
    },
    {
        "def": "(baseball) a rubber slab where the batter stands; it must be touched by a base "
               "runner in order to score",
        "frequency": "f",
        "id": 575,
        "name": "home_plate_(baseball)",
        "synonyms": ["home_plate_(baseball)", "home_base_(baseball)"],
        "synset": "home_plate.n.01"
    },
    {
        "def": "a sweet yellow liquid produced by bees",
        "frequency": "c",
        "id": 576,
        "name": "honey",
        "synonyms": ["honey"],
        "synset": "honey.n.01"
    },
    {
        "def": "metal covering leading to a vent that exhausts smoke or fumes",
        "frequency": "f",
        "id": 577,
        "name": "fume_hood",
        "synonyms": ["fume_hood", "exhaust_hood"],
        "synset": "hood.n.06"
    },
    {
        "def": "a curved or bent implement for suspending or pulling something",
        "frequency": "f",
        "id": 578,
        "name": "hook",
        "synonyms": ["hook"],
        "synset": "hook.n.05"
    },
    {
        "def": "a common horse",
        "frequency": "f",
        "id": 579,
        "name": "horse",
        "synonyms": ["horse"],
        "synset": "horse.n.01"
    },
    {
        "def": "a flexible pipe for conveying a liquid or gas",
        "frequency": "f",
        "id": 580,
        "name": "hose",
        "synonyms": ["hose", "hosepipe"],
        "synset": "hose.n.03"
    },
    {
        "def": "balloon for travel through the air in a basket suspended below a large bag of "
               "heated air",
        "frequency": "r",
        "id": 581,
        "name": "hot-air_balloon",
        "synonyms": ["hot-air_balloon"],
        "synset": "hot-air_balloon.n.01"
    },
    {
        "def": "a portable electric appliance for heating or cooking or keeping food warm",
        "frequency": "r",
        "id": 582,
        "name": "hotplate",
        "synonyms": ["hotplate"],
        "synset": "hot_plate.n.01"
    },
    {
        "def": "a pungent peppery sauce",
        "frequency": "c",
        "id": 583,
        "name": "hot_sauce",
        "synonyms": ["hot_sauce"],
        "synset": "hot_sauce.n.01"
    },
    {
        "def": "a sandglass timer that runs for sixty minutes",
        "frequency": "r",
        "id": 584,
        "name": "hourglass",
        "synonyms": ["hourglass"],
        "synset": "hourglass.n.01"
    },
    {
        "def": "a barge that is designed and equipped for use as a dwelling",
        "frequency": "r",
        "id": 585,
        "name": "houseboat",
        "synonyms": ["houseboat"],
        "synset": "houseboat.n.01"
    },
    {
        "def": "tiny American bird having brilliant iridescent plumage and long slender bills",
        "frequency": "r",
        "id": 586,
        "name": "hummingbird",
        "synonyms": ["hummingbird"],
        "synset": "hummingbird.n.01"
    },
    {
        "def": "a thick spread made from mashed chickpeas",
        "frequency": "r",
        "id": 587,
        "name": "hummus",
        "synonyms": ["hummus", "humus", "hommos", "hoummos", "humous"],
        "synset": "hummus.n.01"
    },
    {
        "def": "white bear of Arctic regions",
        "frequency": "c",
        "id": 588,
        "name": "polar_bear",
        "synonyms": ["polar_bear"],
        "synset": "ice_bear.n.01"
    },
    {
        "def": "frozen dessert containing cream and sugar and flavoring",
        "frequency": "c",
        "id": 589,
        "name": "icecream",
        "synonyms": ["icecream"],
        "synset": "ice_cream.n.01"
    },
    {
        "def": "ice cream or water ice on a small wooden stick",
        "frequency": "r",
        "id": 590,
        "name": "popsicle",
        "synonyms": ["popsicle"],
        "synset": "ice_lolly.n.01"
    },
    {
        "def": "an appliance included in some electric refrigerators for making ice cubes",
        "frequency": "c",
        "id": 591,
        "name": "ice_maker",
        "synonyms": ["ice_maker"],
        "synset": "ice_maker.n.01"
    },
    {
        "def": "a waterproof bag filled with ice: applied to the body (especially the head) to "
               "cool or reduce swelling",
        "frequency": "r",
        "id": 592,
        "name": "ice_pack",
        "synonyms": ["ice_pack", "ice_bag"],
        "synset": "ice_pack.n.01"
    },
    {
        "def": "skate consisting of a boot with a steel blade fitted to the sole",
        "frequency": "r",
        "id": 593,
        "name": "ice_skate",
        "synonyms": ["ice_skate"],
        "synset": "ice_skate.n.01"
    },
    {
        "def": "strong tea served over ice",
        "frequency": "r",
        "id": 594,
        "name": "ice_tea",
        "synonyms": ["ice_tea", "iced_tea"],
        "synset": "ice_tea.n.01"
    },
    {
        "def": "a substance or device used to start a fire",
        "frequency": "c",
        "id": 595,
        "name": "igniter",
        "synonyms": ["igniter", "ignitor", "lighter"],
        "synset": "igniter.n.01"
    },
    {
        "def": "a substance that produces a fragrant odor when burned",
        "frequency": "r",
        "id": 596,
        "name": "incense",
        "synonyms": ["incense"],
        "synset": "incense.n.01"
    },
    {
        "def": "a dispenser that produces a chemical vapor to be inhaled through mouth or nose",
        "frequency": "r",
        "id": 597,
        "name": "inhaler",
        "synonyms": ["inhaler", "inhalator"],
        "synset": "inhaler.n.01"
    },
    {
        "def": "a pocket-sized device used to play music files",
        "frequency": "c",
        "id": 598,
        "name": "iPod",
        "synonyms": ["iPod"],
        "synset": "ipod.n.01"
    },
    {
        "def": "home appliance consisting of a flat metal base that is heated and used to smooth "
               "cloth",
        "frequency": "c",
        "id": 599,
        "name": "iron_(for_clothing)",
        "synonyms": ["iron_(for_clothing)", "smoothing_iron_(for_clothing)"],
        "synset": "iron.n.04"
    },
    {
        "def": "narrow padded board on collapsible supports; used for ironing clothes",
        "frequency": "r",
        "id": 600,
        "name": "ironing_board",
        "synonyms": ["ironing_board"],
        "synset": "ironing_board.n.01"
    },
    {
        "def": "a waist-length coat",
        "frequency": "f",
        "id": 601,
        "name": "jacket",
        "synonyms": ["jacket"],
        "synset": "jacket.n.01"
    },
    {
        "def": "preserve of crushed fruit",
        "frequency": "r",
        "id": 602,
        "name": "jam",
        "synonyms": ["jam"],
        "synset": "jam.n.01"
    },
    {
        "def": "(usually plural) close-fitting trousers of heavy denim for manual work or casual "
               "wear",
        "frequency": "f",
        "id": 603,
        "name": "jean",
        "synonyms": ["jean", "blue_jean", "denim"],
        "synset": "jean.n.01"
    },
    {
        "def": "a car suitable for traveling over rough terrain",
        "frequency": "c",
        "id": 604,
        "name": "jeep",
        "synonyms": ["jeep", "landrover"],
        "synset": "jeep.n.01"
    },
    {
        "def": "sugar-glazed jellied candy",
        "frequency": "r",
        "id": 605,
        "name": "jelly_bean",
        "synonyms": ["jelly_bean", "jelly_egg"],
        "synset": "jelly_bean.n.01"
    },
    {
        "def": "a close-fitting pullover shirt",
        "frequency": "f",
        "id": 606,
        "name": "jersey",
        "synonyms": ["jersey", "T-shirt", "tee_shirt"],
        "synset": "jersey.n.03"
    },
    {
        "def": "an airplane powered by one or more jet engines",
        "frequency": "c",
        "id": 607,
        "name": "jet_plane",
        "synonyms": ["jet_plane", "jet-propelled_plane"],
        "synset": "jet.n.01"
    },
    {
        "def": "an adornment (as a bracelet or ring or necklace) made of precious metals and set "
               "with gems (or imitation gems)",
        "frequency": "c",
        "id": 608,
        "name": "jewelry",
        "synonyms": ["jewelry", "jewellery"],
        "synset": "jewelry.n.01"
    },
    {
        "def": "a control device for computers consisting of a vertical handle that can move "
               "freely in two directions",
        "frequency": "r",
        "id": 609,
        "name": "joystick",
        "synonyms": ["joystick"],
        "synset": "joystick.n.02"
    },
    {
        "def": "one-piece garment fashioned after a parachutist's uniform",
        "frequency": "r",
        "id": 610,
        "name": "jumpsuit",
        "synonyms": ["jumpsuit"],
        "synset": "jump_suit.n.01"
    },
    {
        "def": "a small canoe consisting of a light frame made watertight with animal skins",
        "frequency": "c",
        "id": 611,
        "name": "kayak",
        "synonyms": ["kayak"],
        "synset": "kayak.n.01"
    },
    {
        "def": "small cask or barrel",
        "frequency": "r",
        "id": 612,
        "name": "keg",
        "synonyms": ["keg"],
        "synset": "keg.n.02"
    },
    {
        "def": "outbuilding that serves as a shelter for a dog",
        "frequency": "r",
        "id": 613,
        "name": "kennel",
        "synonyms": ["kennel", "doghouse"],
        "synset": "kennel.n.01"
    },
    {
        "def": "a metal pot for stewing or boiling; usually has a lid",
        "frequency": "c",
        "id": 614,
        "name": "kettle",
        "synonyms": ["kettle", "boiler"],
        "synset": "kettle.n.01"
    },
    {
        "def": "metal instrument used to unlock a lock",
        "frequency": "f",
        "id": 615,
        "name": "key",
        "synonyms": ["key"],
        "synset": "key.n.01"
    },
    {
        "def": "a plastic card used to gain access typically to a door",
        "frequency": "r",
        "id": 616,
        "name": "keycard",
        "synonyms": ["keycard"],
        "synset": "keycard.n.01"
    },
    {
        "def": "a knee-length pleated tartan skirt worn by men as part of the traditional dress in "
               "the Highlands of northern Scotland",
        "frequency": "r",
        "id": 617,
        "name": "kilt",
        "synonyms": ["kilt"],
        "synset": "kilt.n.01"
    },
    {
        "def": "a loose robe; imitated from robes originally worn by Japanese",
        "frequency": "c",
        "id": 618,
        "name": "kimono",
        "synonyms": ["kimono"],
        "synset": "kimono.n.01"
    },
    {
        "def": "a sink in a kitchen",
        "frequency": "f",
        "id": 619,
        "name": "kitchen_sink",
        "synonyms": ["kitchen_sink"],
        "synset": "kitchen_sink.n.01"
    },
    {
        "def": "a table in the kitchen",
        "frequency": "c",
        "id": 620,
        "name": "kitchen_table",
        "synonyms": ["kitchen_table"],
        "synset": "kitchen_table.n.01"
    },
    {
        "def": "plaything consisting of a light frame covered with tissue paper; flown in wind at "
               "end of a string",
        "frequency": "f",
        "id": 621,
        "name": "kite",
        "synonyms": ["kite"],
        "synset": "kite.n.03"
    },
    {
        "def": "young domestic cat",
        "frequency": "c",
        "id": 622,
        "name": "kitten",
        "synonyms": ["kitten", "kitty"],
        "synset": "kitten.n.01"
    },
    {
        "def": "fuzzy brown egg-shaped fruit with slightly tart green flesh",
        "frequency": "c",
        "id": 623,
        "name": "kiwi_fruit",
        "synonyms": ["kiwi_fruit"],
        "synset": "kiwi.n.03"
    },
    {
        "def": "protective garment consisting of a pad worn by football or baseball or hockey "
               "players",
        "frequency": "f",
        "id": 624,
        "name": "knee_pad",
        "synonyms": ["knee_pad"],
        "synset": "knee_pad.n.01"
    },
    {
        "def": "tool with a blade and point used as a cutting instrument",
        "frequency": "f",
        "id": 625,
        "name": "knife",
        "synonyms": ["knife"],
        "synset": "knife.n.01"
    },
    {
        "def": "a chess game piece shaped to resemble the head of a horse",
        "frequency": "r",
        "id": 626,
        "name": "knight_(chess_piece)",
        "synonyms": ["knight_(chess_piece)", "horse_(chess_piece)"],
        "synset": "knight.n.02"
    },
    {
        "def": "needle consisting of a slender rod with pointed ends; usually used in pairs",
        "frequency": "r",
        "id": 627,
        "name": "knitting_needle",
        "synonyms": ["knitting_needle"],
        "synset": "knitting_needle.n.01"
    },
    {
        "def": "a round handle often found on a door",
        "frequency": "f",
        "id": 628,
        "name": "knob",
        "synonyms": ["knob"],
        "synset": "knob.n.02"
    },
    {
        "def": "a device (usually metal and ornamental) attached by a hinge to a door",
        "frequency": "r",
        "id": 629,
        "name": "knocker_(on_a_door)",
        "synonyms": ["knocker_(on_a_door)", "doorknocker"],
        "synset": "knocker.n.05"
    },
    {
        "def": "sluggish tailless Australian marsupial with grey furry ears and coat",
        "frequency": "r",
        "id": 630,
        "name": "koala",
        "synonyms": ["koala", "koala_bear"],
        "synset": "koala.n.01"
    },
    {
        "def": "a light coat worn to protect clothing from substances used while working in a "
               "laboratory",
        "frequency": "r",
        "id": 631,
        "name": "lab_coat",
        "synonyms": ["lab_coat", "laboratory_coat"],
        "synset": "lab_coat.n.01"
    },
    {
        "def": "steps consisting of two parallel members connected by rungs",
        "frequency": "f",
        "id": 632,
        "name": "ladder",
        "synonyms": ["ladder"],
        "synset": "ladder.n.01"
    },
    {
        "def": "a spoon-shaped vessel with a long handle frequently used to transfer liquids",
        "frequency": "c",
        "id": 633,
        "name": "ladle",
        "synonyms": ["ladle"],
        "synset": "ladle.n.01"
    },
    {
        "def": "small round bright-colored and spotted beetle, typically red and black",
        "frequency": "r",
        "id": 634,
        "name": "ladybug",
        "synonyms": ["ladybug", "ladybeetle", "ladybird_beetle"],
        "synset": "ladybug.n.01"
    },
    {
        "def": "young sheep",
        "frequency": "c",
        "id": 635,
        "name": "lamb_(animal)",
        "synonyms": ["lamb_(animal)"],
        "synset": "lamb.n.01"
    },
    {
        "def": "chop cut from a lamb",
        "frequency": "r",
        "id": 636,
        "name": "lamb-chop",
        "synonyms": ["lamb-chop", "lambchop"],
        "synset": "lamb_chop.n.01"
    },
    {
        "def": "a piece of furniture holding one or more electric light bulbs",
        "frequency": "f",
        "id": 637,
        "name": "lamp",
        "synonyms": ["lamp"],
        "synset": "lamp.n.02"
    },
    {
        "def": "a metal post supporting an outdoor lamp (such as a streetlight)",
        "frequency": "f",
        "id": 638,
        "name": "lamppost",
        "synonyms": ["lamppost"],
        "synset": "lamppost.n.01"
    },
    {
        "def": "a protective ornamental shade used to screen a light bulb from direct view",
        "frequency": "f",
        "id": 639,
        "name": "lampshade",
        "synonyms": ["lampshade"],
        "synset": "lampshade.n.01"
    },
    {
        "def": "light in a transparent protective case",
        "frequency": "c",
        "id": 640,
        "name": "lantern",
        "synonyms": ["lantern"],
        "synset": "lantern.n.01"
    },
    {
        "def": "a cord worn around the neck to hold a knife or whistle, etc.",
        "frequency": "f",
        "id": 641,
        "name": "lanyard",
        "synonyms": ["lanyard", "laniard"],
        "synset": "lanyard.n.02"
    },
    {
        "def": "a portable computer small enough to use in your lap",
        "frequency": "f",
        "id": 642,
        "name": "laptop_computer",
        "synonyms": ["laptop_computer", "notebook_computer"],
        "synset": "laptop.n.01"
    },
    {
        "def": "baked dish of layers of lasagna pasta with sauce and cheese and meat or vegetables",
        "frequency": "r",
        "id": 643,
        "name": "lasagna",
        "synonyms": ["lasagna", "lasagne"],
        "synset": "lasagna.n.01"
    },
    {
        "def": "a bar that can be lowered or slid into a groove to fasten a door or gate",
        "frequency": "c",
        "id": 644,
        "name": "latch",
        "synonyms": ["latch"],
        "synset": "latch.n.02"
    },
    {
        "def": "garden tool for mowing grass on lawns",
        "frequency": "r",
        "id": 645,
        "name": "lawn_mower",
        "synonyms": ["lawn_mower"],
        "synset": "lawn_mower.n.01"
    },
    {
        "def": "an animal skin made smooth and flexible by removing the hair and then tanning",
        "frequency": "r",
        "id": 646,
        "name": "leather",
        "synonyms": ["leather"],
        "synset": "leather.n.01"
    },
    {
        "def": "a garment covering the leg (usually extending from the knee to the ankle)",
        "frequency": "c",
        "id": 647,
        "name": "legging_(clothing)",
        "synonyms": ["legging_(clothing)", "leging_(clothing)", "leg_covering"],
        "synset": "legging.n.01"
    },
    {
        "def": "a child's plastic construction set for making models from blocks",
        "frequency": "c",
        "id": 648,
        "name": "Lego",
        "synonyms": ["Lego", "Lego_set"],
        "synset": "lego.n.01"
    },
    {
        "def": "yellow oval fruit with juicy acidic flesh",
        "frequency": "f",
        "id": 649,
        "name": "lemon",
        "synonyms": ["lemon"],
        "synset": "lemon.n.01"
    },
    {
        "def": "sweetened beverage of diluted lemon juice",
        "frequency": "r",
        "id": 650,
        "name": "lemonade",
        "synonyms": ["lemonade"],
        "synset": "lemonade.n.01"
    },
    {
        "def": "leafy plant commonly eaten in salad or on sandwiches",
        "frequency": "f",
        "id": 651,
        "name": "lettuce",
        "synonyms": ["lettuce"],
        "synset": "lettuce.n.02"
    },
    {
        "def": "a plate mounted on the front and back of car and bearing the car's registration "
               "number",
        "frequency": "f",
        "id": 652,
        "name": "license_plate",
        "synonyms": ["license_plate", "numberplate"],
        "synset": "license_plate.n.01"
    },
    {
        "def": "a ring-shaped life preserver used to prevent drowning (NOT a life-jacket or vest)",
        "frequency": "f",
        "id": 653,
        "name": "life_buoy",
        "synonyms": ["life_buoy", "lifesaver", "life_belt", "life_ring"],
        "synset": "life_buoy.n.01"
    },
    {
        "def": "life preserver consisting of a sleeveless jacket of buoyant or inflatable design",
        "frequency": "f",
        "id": 654,
        "name": "life_jacket",
        "synonyms": ["life_jacket", "life_vest"],
        "synset": "life_jacket.n.01"
    },
    {
        "def": "glass bulb or tube shaped electric device that emits light (DO NOT MARK LAMPS AS A "
               "WHOLE)",
        "frequency": "f",
        "id": 655,
        "name": "lightbulb",
        "synonyms": ["lightbulb"],
        "synset": "light_bulb.n.01"
    },
    {
        "def": "a metallic conductor that is attached to a high point and leads to the ground",
        "frequency": "r",
        "id": 656,
        "name": "lightning_rod",
        "synonyms": ["lightning_rod", "lightning_conductor"],
        "synset": "lightning_rod.n.02"
    },
    {
        "def": "the green acidic fruit of any of various lime trees",
        "frequency": "c",
        "id": 657,
        "name": "lime",
        "synonyms": ["lime"],
        "synset": "lime.n.06"
    },
    {
        "def": "long luxurious car; usually driven by a chauffeur",
        "frequency": "r",
        "id": 658,
        "name": "limousine",
        "synonyms": ["limousine"],
        "synset": "limousine.n.01"
    },
    {
        "def": "a high-quality paper made of linen fibers or with a linen finish",
        "frequency": "r",
        "id": 659,
        "name": "linen_paper",
        "synonyms": ["linen_paper"],
        "synset": "linen.n.02"
    },
    {
        "def": "large gregarious predatory cat of Africa and India",
        "frequency": "c",
        "id": 660,
        "name": "lion",
        "synonyms": ["lion"],
        "synset": "lion.n.01"
    },
    {
        "def": "a balm applied to the lips",
        "frequency": "c",
        "id": 661,
        "name": "lip_balm",
        "synonyms": ["lip_balm"],
        "synset": "lip_balm.n.01"
    },
    {
        "def": "makeup that is used to color the lips",
        "frequency": "c",
        "id": 662,
        "name": "lipstick",
        "synonyms": ["lipstick", "lip_rouge"],
        "synset": "lipstick.n.01"
    },
    {
        "def": "an alcoholic beverage that is distilled rather than fermented",
        "frequency": "r",
        "id": 663,
        "name": "liquor",
        "synonyms": ["liquor", "spirits", "hard_liquor", "liqueur", "cordial"],
        "synset": "liquor.n.01"
    },
    {
        "def": "a reptile with usually two pairs of legs and a tapering tail",
        "frequency": "r",
        "id": 664,
        "name": "lizard",
        "synonyms": ["lizard"],
        "synset": "lizard.n.01"
    },
    {
        "def": "a low leather step-in shoe",
        "frequency": "r",
        "id": 665,
        "name": "Loafer_(type_of_shoe)",
        "synonyms": ["Loafer_(type_of_shoe)"],
        "synset": "loafer.n.02"
    },
    {
        "def": "a segment of the trunk of a tree when stripped of branches",
        "frequency": "f",
        "id": 666,
        "name": "log",
        "synonyms": ["log"],
        "synset": "log.n.01"
    },
    {
        "def": "hard candy on a stick",
        "frequency": "c",
        "id": 667,
        "name": "lollipop",
        "synonyms": ["lollipop"],
        "synset": "lollipop.n.02"
    },
    {
        "def": "any of various cosmetic preparations that are applied to the skin",
        "frequency": "c",
        "id": 668,
        "name": "lotion",
        "synonyms": ["lotion"],
        "synset": "lotion.n.01"
    },
    {
        "def": "electronic device that produces sound often as part of a stereo system",
        "frequency": "f",
        "id": 669,
        "name": "speaker_(stero_equipment)",
        "synonyms": ["speaker_(stero_equipment)"],
        "synset": "loudspeaker.n.01"
    },
    {
        "def": "small sofa that seats two people",
        "frequency": "c",
        "id": 670,
        "name": "loveseat",
        "synonyms": ["loveseat"],
        "synset": "love_seat.n.01"
    },
    {
        "def": "a rapidly firing automatic gun",
        "frequency": "r",
        "id": 671,
        "name": "machine_gun",
        "synonyms": ["machine_gun"],
        "synset": "machine_gun.n.01"
    },
    {
        "def": "a paperback periodic publication",
        "frequency": "f",
        "id": 672,
        "name": "magazine",
        "synonyms": ["magazine"],
        "synset": "magazine.n.02"
    },
    {
        "def": "a device that attracts iron and produces a magnetic field",
        "frequency": "f",
        "id": 673,
        "name": "magnet",
        "synonyms": ["magnet"],
        "synset": "magnet.n.01"
    },
    {
        "def": "a slot (usually in a door) through which mail can be delivered",
        "frequency": "r",
        "id": 674,
        "name": "mail_slot",
        "synonyms": ["mail_slot"],
        "synset": "mail_slot.n.01"
    },
    {
        "def": "a private box for delivery of mail",
        "frequency": "c",
        "id": 675,
        "name": "mailbox_(at_home)",
        "synonyms": ["mailbox_(at_home)", "letter_box_(at_home)"],
        "synset": "mailbox.n.01"
    },
    {
        "def": "a sports implement with a long handle and a hammer-like head used to hit a ball",
        "frequency": "r",
        "id": 676,
        "name": "mallet",
        "synonyms": ["mallet"],
        "synset": "mallet.n.01"
    },
    {
        "def": "any of numerous extinct elephants widely distributed in the Pleistocene",
        "frequency": "r",
        "id": 677,
        "name": "mammoth",
        "synonyms": ["mammoth"],
        "synset": "mammoth.n.01"
    },
    {
        "def": "a somewhat flat reddish-orange loose skinned citrus of China",
        "frequency": "c",
        "id": 678,
        "name": "mandarin_orange",
        "synonyms": ["mandarin_orange"],
        "synset": "mandarin.n.05"
    },
    {
        "def": "a container (usually in a barn or stable) from which cattle or horses feed",
        "frequency": "c",
        "id": 679,
        "name": "manger",
        "synonyms": ["manger", "trough"],
        "synset": "manger.n.01"
    },
    {
        "def": "a hole (usually with a flush cover) through which a person can gain access to an "
               "underground structure",
        "frequency": "f",
        "id": 680,
        "name": "manhole",
        "synonyms": ["manhole"],
        "synset": "manhole.n.01"
    },
    {
        "def": "a diagrammatic representation of the earth's surface (or part of it)",
        "frequency": "c",
        "id": 681,
        "name": "map",
        "synonyms": ["map"],
        "synset": "map.n.01"
    },
    {
        "def": "a writing implement for making a mark",
        "frequency": "c",
        "id": 682,
        "name": "marker",
        "synonyms": ["marker"],
        "synset": "marker.n.03"
    },
    {
        "def": "a cocktail made of gin (or vodka) with dry vermouth",
        "frequency": "r",
        "id": 683,
        "name": "martini",
        "synonyms": ["martini"],
        "synset": "martini.n.01"
    },
    {
        "def": "a person or animal that is adopted by a team or other group as a symbolic figure",
        "frequency": "r",
        "id": 684,
        "name": "mascot",
        "synonyms": ["mascot"],
        "synset": "mascot.n.01"
    },
    {
        "def": "potato that has been peeled and boiled and then mashed",
        "frequency": "c",
        "id": 685,
        "name": "mashed_potato",
        "synonyms": ["mashed_potato"],
        "synset": "mashed_potato.n.01"
    },
    {
        "def": "a kitchen utensil used for mashing (e.g. potatoes)",
        "frequency": "r",
        "id": 686,
        "name": "masher",
        "synonyms": ["masher"],
        "synset": "masher.n.02"
    },
    {
        "def": "a protective covering worn over the face",
        "frequency": "f",
        "id": 687,
        "name": "mask",
        "synonyms": ["mask", "facemask"],
        "synset": "mask.n.04"
    },
    {
        "def": "a vertical spar for supporting sails",
        "frequency": "f",
        "id": 688,
        "name": "mast",
        "synonyms": ["mast"],
        "synset": "mast.n.01"
    },
    {
        "def": "sports equipment consisting of a piece of thick padding on the floor for "
               "gymnastics",
        "frequency": "c",
        "id": 689,
        "name": "mat_(gym_equipment)",
        "synonyms": ["mat_(gym_equipment)", "gym_mat"],
        "synset": "mat.n.03"
    },
    {
        "def": "a box for holding matches",
        "frequency": "r",
        "id": 690,
        "name": "matchbox",
        "synonyms": ["matchbox"],
        "synset": "matchbox.n.01"
    },
    {
        "def": "a thick pad filled with resilient material used as a bed or part of a bed",
        "frequency": "f",
        "id": 691,
        "name": "mattress",
        "synonyms": ["mattress"],
        "synset": "mattress.n.01"
    },
    {
        "def": "graduated cup used to measure liquid or granular ingredients",
        "frequency": "c",
        "id": 692,
        "name": "measuring_cup",
        "synonyms": ["measuring_cup"],
        "synset": "measuring_cup.n.01"
    },
    {
        "def": "measuring instrument having a sequence of marks at regular intervals",
        "frequency": "c",
        "id": 693,
        "name": "measuring_stick",
        "synonyms": ["measuring_stick", "ruler_(measuring_stick)", "measuring_rod"],
        "synset": "measuring_stick.n.01"
    },
    {
        "def": "ground meat formed into a ball and fried or simmered in broth",
        "frequency": "c",
        "id": 694,
        "name": "meatball",
        "synonyms": ["meatball"],
        "synset": "meatball.n.01"
    },
    {
        "def": "something that treats or prevents or alleviates the symptoms of disease",
        "frequency": "c",
        "id": 695,
        "name": "medicine",
        "synonyms": ["medicine"],
        "synset": "medicine.n.02"
    },
    {
        "def": "fruit of the gourd family having a hard rind and sweet juicy flesh",
        "frequency": "r",
        "id": 696,
        "name": "melon",
        "synonyms": ["melon"],
        "synset": "melon.n.01"
    },
    {
        "def": "device for converting sound waves into electrical energy",
        "frequency": "f",
        "id": 697,
        "name": "microphone",
        "synonyms": ["microphone"],
        "synset": "microphone.n.01"
    },
    {
        "def": "magnifier of the image of small objects",
        "frequency": "r",
        "id": 698,
        "name": "microscope",
        "synonyms": ["microscope"],
        "synset": "microscope.n.01"
    },
    {
        "def": "kitchen appliance that cooks food by passing an electromagnetic wave through it",
        "frequency": "f",
        "id": 699,
        "name": "microwave_oven",
        "synonyms": ["microwave_oven"],
        "synset": "microwave.n.02"
    },
    {
        "def": "stone post at side of a road to show distances",
        "frequency": "r",
        "id": 700,
        "name": "milestone",
        "synonyms": ["milestone", "milepost"],
        "synset": "milestone.n.01"
    },
    {
        "def": "a white nutritious liquid secreted by mammals and used as food by human beings",
        "frequency": "c",
        "id": 701,
        "name": "milk",
        "synonyms": ["milk"],
        "synset": "milk.n.01"
    },
    {
        "def": "a small box-shaped passenger van",
        "frequency": "f",
        "id": 702,
        "name": "minivan",
        "synonyms": ["minivan"],
        "synset": "minivan.n.01"
    },
    {
        "def": "a candy that is flavored with a mint oil",
        "frequency": "r",
        "id": 703,
        "name": "mint_candy",
        "synonyms": ["mint_candy"],
        "synset": "mint.n.05"
    },
    {
        "def": "polished surface that forms images by reflecting light",
        "frequency": "f",
        "id": 704,
        "name": "mirror",
        "synonyms": ["mirror"],
        "synset": "mirror.n.01"
    },
    {
        "def": "glove that encases the thumb separately and the other four fingers together",
        "frequency": "c",
        "id": 705,
        "name": "mitten",
        "synonyms": ["mitten"],
        "synset": "mitten.n.01"
    },
    {
        "def": "a kitchen utensil that is used for mixing foods",
        "frequency": "c",
        "id": 706,
        "name": "mixer_(kitchen_tool)",
        "synonyms": ["mixer_(kitchen_tool)", "stand_mixer"],
        "synset": "mixer.n.04"
    },
    {
        "def": "the official currency issued by a government or national bank",
        "frequency": "c",
        "id": 707,
        "name": "money",
        "synonyms": ["money"],
        "synset": "money.n.03"
    },
    {
        "def": "a computer monitor",
        "frequency": "f",
        "id": 708,
        "name": "monitor_(computer_equipment) computer_monitor",
        "synonyms": ["monitor_(computer_equipment) computer_monitor"],
        "synset": "monitor.n.04"
    },
    {
        "def": "any of various long-tailed primates",
        "frequency": "c",
        "id": 709,
        "name": "monkey",
        "synonyms": ["monkey"],
        "synset": "monkey.n.01"
    },
    {
        "def": "machine that converts other forms of energy into mechanical energy and so imparts "
               "motion",
        "frequency": "f",
        "id": 710,
        "name": "motor",
        "synonyms": ["motor"],
        "synset": "motor.n.01"
    },
    {
        "def": "a wheeled vehicle with small wheels and a low-powered engine",
        "frequency": "f",
        "id": 711,
        "name": "motor_scooter",
        "synonyms": ["motor_scooter", "scooter"],
        "synset": "motor_scooter.n.01"
    },
    {
        "def": "a self-propelled wheeled vehicle that does not run on rails",
        "frequency": "r",
        "id": 712,
        "name": "motor_vehicle",
        "synonyms": ["motor_vehicle", "automotive_vehicle"],
        "synset": "motor_vehicle.n.01"
    },
    {
        "def": "a boat propelled by an internal-combustion engine",
        "frequency": "r",
        "id": 713,
        "name": "motorboat",
        "synonyms": ["motorboat", "powerboat"],
        "synset": "motorboat.n.01"
    },
    {
        "def": "a motor vehicle with two wheels and a strong frame",
        "frequency": "f",
        "id": 714,
        "name": "motorcycle",
        "synonyms": ["motorcycle"],
        "synset": "motorcycle.n.01"
    },
    {
        "def": "(baseball) the slight elevation on which the pitcher stands",
        "frequency": "f",
        "id": 715,
        "name": "mound_(baseball)",
        "synonyms": ["mound_(baseball)", "pitcher's_mound"],
        "synset": "mound.n.01"
    },
    {
        "def": "a small rodent with pointed snouts and small ears on elongated bodies with slender "
               "usually hairless tails",
        "frequency": "r",
        "id": 716,
        "name": "mouse_(animal_rodent)",
        "synonyms": ["mouse_(animal_rodent)"],
        "synset": "mouse.n.01"
    },
    {
        "def": "a computer input device that controls an on-screen pointer",
        "frequency": "f",
        "id": 717,
        "name": "mouse_(computer_equipment)",
        "synonyms": ["mouse_(computer_equipment)", "computer_mouse"],
        "synset": "mouse.n.04"
    },
    {
        "def": "a small portable pad that provides an operating surface for a computer mouse",
        "frequency": "f",
        "id": 718,
        "name": "mousepad",
        "synonyms": ["mousepad"],
        "synset": "mousepad.n.01"
    },
    {
        "def": "a sweet quick bread baked in a cup-shaped pan",
        "frequency": "c",
        "id": 719,
        "name": "muffin",
        "synonyms": ["muffin"],
        "synset": "muffin.n.01"
    },
    {
        "def": "with handle and usually cylindrical",
        "frequency": "f",
        "id": 720,
        "name": "mug",
        "synonyms": ["mug"],
        "synset": "mug.n.04"
    },
    {
        "def": "a common mushroom",
        "frequency": "f",
        "id": 721,
        "name": "mushroom",
        "synonyms": ["mushroom"],
        "synset": "mushroom.n.02"
    },
    {
        "def": "a stool for piano players; usually adjustable in height",
        "frequency": "r",
        "id": 722,
        "name": "music_stool",
        "synonyms": ["music_stool", "piano_stool"],
        "synset": "music_stool.n.01"
    },
    {
        "def": "any of various devices or contrivances that can be used to produce musical tones "
               "or sounds",
        "frequency": "r",
        "id": 723,
        "name": "musical_instrument",
        "synonyms": ["musical_instrument", "instrument_(musical)"],
        "synset": "musical_instrument.n.01"
    },
    {
        "def": "a small flat file for shaping the nails",
        "frequency": "r",
        "id": 724,
        "name": "nailfile",
        "synonyms": ["nailfile"],
        "synset": "nailfile.n.01"
    },
    {
        "def": "a plate bearing a name",
        "frequency": "r",
        "id": 725,
        "name": "nameplate",
        "synonyms": ["nameplate"],
        "synset": "nameplate.n.01"
    },
    {
        "def": "a small piece of table linen or paper that is used to wipe the mouth and to cover "
               "the lap in order to protect clothing",
        "frequency": "f",
        "id": 726,
        "name": "napkin",
        "synonyms": ["napkin", "table_napkin", "serviette"],
        "synset": "napkin.n.01"
    },
    {
        "def": "a kerchief worn around the neck",
        "frequency": "r",
        "id": 727,
        "name": "neckerchief",
        "synonyms": ["neckerchief"],
        "synset": "neckerchief.n.01"
    },
    {
        "def": "jewelry consisting of a cord or chain (often bearing gems) worn about the neck as "
               "an ornament",
        "frequency": "f",
        "id": 728,
        "name": "necklace",
        "synonyms": ["necklace"],
        "synset": "necklace.n.01"
    },
    {
        "def": "neckwear consisting of a long narrow piece of material worn under a collar and "
               "tied in knot at the front",
        "frequency": "f",
        "id": 729,
        "name": "necktie",
        "synonyms": ["necktie", "tie_(necktie)"],
        "synset": "necktie.n.01"
    },
    {
        "def": "a sharp pointed implement (usually metal)",
        "frequency": "r",
        "id": 730,
        "name": "needle",
        "synonyms": ["needle"],
        "synset": "needle.n.03"
    },
    {
        "def": "a structure in which animals lay eggs or give birth to their young",
        "frequency": "c",
        "id": 731,
        "name": "nest",
        "synonyms": ["nest"],
        "synset": "nest.n.01"
    },
    {
        "def": "a stall where newspapers and other periodicals are sold",
        "frequency": "r",
        "id": 732,
        "name": "newsstand",
        "synonyms": ["newsstand"],
        "synset": "newsstand.n.01"
    },
    {
        "def": "garments designed to be worn in bed",
        "frequency": "c",
        "id": 733,
        "name": "nightshirt",
        "synonyms": ["nightshirt", "nightwear", "sleepwear", "nightclothes"],
        "synset": "nightwear.n.01"
    },
    {
        "def": "a canvas bag that is used to feed an animal (such as a horse); covers the muzzle "
               "and fastens at the top of the head",
        "frequency": "r",
        "id": 734,
        "name": "nosebag_(for_animals)",
        "synonyms": ["nosebag_(for_animals)", "feedbag"],
        "synset": "nosebag.n.01"
    },
    {
        "def": "a strap that is the part of a bridle that goes over the animal's nose",
        "frequency": "r",
        "id": 735,
        "name": "noseband_(for_animals)",
        "synonyms": ["noseband_(for_animals)", "nosepiece_(for_animals)"],
        "synset": "noseband.n.01"
    },
    {
        "def": "a book with blank pages for recording notes or memoranda",
        "frequency": "f",
        "id": 736,
        "name": "notebook",
        "synonyms": ["notebook"],
        "synset": "notebook.n.01"
    },
    {
        "def": "a pad of paper for keeping notes",
        "frequency": "c",
        "id": 737,
        "name": "notepad",
        "synonyms": ["notepad"],
        "synset": "notepad.n.01"
    },
    {
        "def": "a small metal block (usually square or hexagonal) with internal screw thread to be "
               "fitted onto a bolt",
        "frequency": "c",
        "id": 738,
        "name": "nut",
        "synonyms": ["nut"],
        "synset": "nut.n.03"
    },
    {
        "def": "a hand tool used to crack nuts open",
        "frequency": "r",
        "id": 739,
        "name": "nutcracker",
        "synonyms": ["nutcracker"],
        "synset": "nutcracker.n.01"
    },
    {
        "def": "an implement used to propel or steer a boat",
        "frequency": "c",
        "id": 740,
        "name": "oar",
        "synonyms": ["oar"],
        "synset": "oar.n.01"
    },
    {
        "def": "tentacles of octopus prepared as food",
        "frequency": "r",
        "id": 741,
        "name": "octopus_(food)",
        "synonyms": ["octopus_(food)"],
        "synset": "octopus.n.01"
    },
    {
        "def": "bottom-living cephalopod having a soft oval body with eight long tentacles",
        "frequency": "r",
        "id": 742,
        "name": "octopus_(animal)",
        "synonyms": ["octopus_(animal)"],
        "synset": "octopus.n.02"
    },
    {
        "def": "a lamp that burns oil (as kerosine) for light",
        "frequency": "c",
        "id": 743,
        "name": "oil_lamp",
        "synonyms": ["oil_lamp", "kerosene_lamp", "kerosine_lamp"],
        "synset": "oil_lamp.n.01"
    },
    {
        "def": "oil from olives",
        "frequency": "c",
        "id": 744,
        "name": "olive_oil",
        "synonyms": ["olive_oil"],
        "synset": "olive_oil.n.01"
    },
    {
        "def": "beaten eggs cooked until just set; may be folded around e.g. ham or cheese or "
               "jelly",
        "frequency": "r",
        "id": 745,
        "name": "omelet",
        "synonyms": ["omelet", "omelette"],
        "synset": "omelet.n.01"
    },
    {
        "def": "the bulb of an onion plant",
        "frequency": "f",
        "id": 746,
        "name": "onion",
        "synonyms": ["onion"],
        "synset": "onion.n.01"
    },
    {
        "def": "orange (FRUIT of an orange tree)",
        "frequency": "f",
        "id": 747,
        "name": "orange_(fruit)",
        "synonyms": ["orange_(fruit)"],
        "synset": "orange.n.01"
    },
    {
        "def": "bottled or freshly squeezed juice of oranges",
        "frequency": "c",
        "id": 748,
        "name": "orange_juice",
        "synonyms": ["orange_juice"],
        "synset": "orange_juice.n.01"
    },
    {
        "def": "aromatic Eurasian perennial herb used in cooking and baking",
        "frequency": "r",
        "id": 749,
        "name": "oregano",
        "synonyms": ["oregano", "marjoram"],
        "synset": "oregano.n.01"
    },
    {
        "def": "fast-running African flightless bird with two-toed feet; largest living bird",
        "frequency": "c",
        "id": 750,
        "name": "ostrich",
        "synonyms": ["ostrich"],
        "synset": "ostrich.n.02"
    },
    {
        "def": "thick cushion used as a seat",
        "frequency": "c",
        "id": 751,
        "name": "ottoman",
        "synonyms": ["ottoman", "pouf", "pouffe", "hassock"],
        "synset": "ottoman.n.03"
    },
    {
        "def": "work clothing consisting of denim trousers usually with a bib and shoulder straps",
        "frequency": "c",
        "id": 752,
        "name": "overalls_(clothing)",
        "synonyms": ["overalls_(clothing)"],
        "synset": "overall.n.01"
    },
    {
        "def": "nocturnal bird of prey with hawk-like beak and claws and large head with "
               "front-facing eyes",
        "frequency": "c",
        "id": 753,
        "name": "owl",
        "synonyms": ["owl"],
        "synset": "owl.n.01"
    },
    {
        "def": "a small package or bundle",
        "frequency": "c",
        "id": 754,
        "name": "packet",
        "synonyms": ["packet"],
        "synset": "packet.n.03"
    },
    {
        "def": "absorbent material saturated with ink used to transfer ink evenly to a rubber "
               "stamp",
        "frequency": "r",
        "id": 755,
        "name": "inkpad",
        "synonyms": ["inkpad", "inking_pad", "stamp_pad"],
        "synset": "pad.n.03"
    },
    {
        "def": "a flat mass of soft material used for protection, stuffing, or comfort",
        "frequency": "c",
        "id": 756,
        "name": "pad",
        "synonyms": ["pad"],
        "synset": "pad.n.04"
    },
    {
        "def": "a short light oar used without an oarlock to propel a canoe or small boat",
        "frequency": "c",
        "id": 757,
        "name": "paddle",
        "synonyms": ["paddle", "boat_paddle"],
        "synset": "paddle.n.04"
    },
    {
        "def": "a detachable, portable lock",
        "frequency": "c",
        "id": 758,
        "name": "padlock",
        "synonyms": ["padlock"],
        "synset": "padlock.n.01"
    },
    {
        "def": "a box containing a collection of cubes or tubes of artists' paint",
        "frequency": "r",
        "id": 759,
        "name": "paintbox",
        "synonyms": ["paintbox"],
        "synset": "paintbox.n.01"
    },
    {
        "def": "a brush used as an applicator to apply paint",
        "frequency": "c",
        "id": 760,
        "name": "paintbrush",
        "synonyms": ["paintbrush"],
        "synset": "paintbrush.n.01"
    },
    {
        "def": "graphic art consisting of an artistic composition made by applying paints to a "
               "surface",
        "frequency": "f",
        "id": 761,
        "name": "painting",
        "synonyms": ["painting"],
        "synset": "painting.n.01"
    },
    {
        "def": "loose-fitting nightclothes worn for sleeping or lounging",
        "frequency": "c",
        "id": 762,
        "name": "pajamas",
        "synonyms": ["pajamas", "pyjamas"],
        "synset": "pajama.n.02"
    },
    {
        "def": "board that provides a flat surface on which artists mix paints and the range of "
               "colors used",
        "frequency": "c",
        "id": 763,
        "name": "palette",
        "synonyms": ["palette", "pallet"],
        "synset": "palette.n.02"
    },
    {
        "def": "cooking utensil consisting of a wide metal vessel",
        "frequency": "f",
        "id": 764,
        "name": "pan_(for_cooking)",
        "synonyms": ["pan_(for_cooking)", "cooking_pan"],
        "synset": "pan.n.01"
    },
    {
        "def": "shallow container made of metal",
        "frequency": "r",
        "id": 765,
        "name": "pan_(metal_container)",
        "synonyms": ["pan_(metal_container)"],
        "synset": "pan.n.03"
    },
    {
        "def": "a flat cake of thin batter fried on both sides on a griddle",
        "frequency": "c",
        "id": 766,
        "name": "pancake",
        "synonyms": ["pancake"],
        "synset": "pancake.n.01"
    },
    {
        "def": "a woman's tights consisting of underpants and stockings",
        "frequency": "r",
        "id": 767,
        "name": "pantyhose",
        "synonyms": ["pantyhose"],
        "synset": "pantyhose.n.01"
    },
    {
        "def": "large oval melon-like tropical fruit with yellowish flesh",
        "frequency": "r",
        "id": 768,
        "name": "papaya",
        "synonyms": ["papaya"],
        "synset": "papaya.n.02"
    },
    {
        "def": "a wire or plastic clip for holding sheets of paper together",
        "frequency": "r",
        "id": 769,
        "name": "paperclip",
        "synonyms": ["paperclip"],
        "synset": "paper_clip.n.01"
    },
    {
        "def": "a disposable plate made of cardboard",
        "frequency": "f",
        "id": 770,
        "name": "paper_plate",
        "synonyms": ["paper_plate"],
        "synset": "paper_plate.n.01"
    },
    {
        "def": "a disposable towel made of absorbent paper",
        "frequency": "f",
        "id": 771,
        "name": "paper_towel",
        "synonyms": ["paper_towel"],
        "synset": "paper_towel.n.01"
    },
    {
        "def": "a book with paper covers",
        "frequency": "r",
        "id": 772,
        "name": "paperback_book",
        "synonyms": ["paperback_book", "paper-back_book", "softback_book", "soft-cover_book"],
        "synset": "paperback_book.n.01"
    },
    {
        "def": "a weight used to hold down a stack of papers",
        "frequency": "r",
        "id": 773,
        "name": "paperweight",
        "synonyms": ["paperweight"],
        "synset": "paperweight.n.01"
    },
    {
        "def": "rescue equipment consisting of a device that fills with air and retards your fall",
        "frequency": "c",
        "id": 774,
        "name": "parachute",
        "synonyms": ["parachute"],
        "synset": "parachute.n.01"
    },
    {
        "def": "any of numerous small slender long-tailed parrots",
        "frequency": "r",
        "id": 775,
        "name": "parakeet",
        "synonyms": ["parakeet", "parrakeet", "parroket", "paraquet", "paroquet", "parroquet"],
        "synset": "parakeet.n.01"
    },
    {
        "def": "parachute that will lift a person up into the air when it is towed by a motorboat "
               "or a car",
        "frequency": "c",
        "id": 776,
        "name": "parasail_(sports)",
        "synonyms": ["parasail_(sports)"],
        "synset": "parasail.n.01"
    },
    {
        "def": "a superior paper resembling sheepskin",
        "frequency": "r",
        "id": 777,
        "name": "parchment",
        "synonyms": ["parchment"],
        "synset": "parchment.n.01"
    },
    {
        "def": "a kind of heavy jacket (`windcheater' is a British term)",
        "frequency": "r",
        "id": 778,
        "name": "parka",
        "synonyms": ["parka", "anorak"],
        "synset": "parka.n.01"
    },
    {
        "def": "a coin-operated timer located next to a parking space",
        "frequency": "f",
        "id": 779,
        "name": "parking_meter",
        "synonyms": ["parking_meter"],
        "synset": "parking_meter.n.01"
    },
    {
        "def": "usually brightly colored tropical birds with short hooked beaks and the ability to "
               "mimic sounds",
        "frequency": "c",
        "id": 780,
        "name": "parrot",
        "synonyms": ["parrot"],
        "synset": "parrot.n.01"
    },
    {
        "def": "a railcar where passengers ride",
        "frequency": "c",
        "id": 781,
        "name": "passenger_car_(part_of_a_train)",
        "synonyms": ["passenger_car_(part_of_a_train)", "coach_(part_of_a_train)"],
        "synset": "passenger_car.n.01"
    },
    {
        "def": "a ship built to carry passengers",
        "frequency": "r",
        "id": 782,
        "name": "passenger_ship",
        "synonyms": ["passenger_ship"],
        "synset": "passenger_ship.n.01"
    },
    {
        "def": "a document issued by a country to a citizen allowing that person to travel abroad "
               "and re-enter the home country",
        "frequency": "r",
        "id": 783,
        "name": "passport",
        "synonyms": ["passport"],
        "synset": "passport.n.02"
    },
    {
        "def": "any of various baked foods made of dough or batter",
        "frequency": "f",
        "id": 784,
        "name": "pastry",
        "synonyms": ["pastry"],
        "synset": "pastry.n.02"
    },
    {
        "def": "small flat mass of chopped food",
        "frequency": "r",
        "id": 785,
        "name": "patty_(food)",
        "synonyms": ["patty_(food)"],
        "synset": "patty.n.01"
    },
    {
        "def": "seed of a pea plant used for food",
        "frequency": "c",
        "id": 786,
        "name": "pea_(food)",
        "synonyms": ["pea_(food)"],
        "synset": "pea.n.01"
    },
    {
        "def": "downy juicy fruit with sweet yellowish or whitish flesh",
        "frequency": "c",
        "id": 787,
        "name": "peach",
        "synonyms": ["peach"],
        "synset": "peach.n.03"
    },
    {
        "def": "a spread made from ground peanuts",
        "frequency": "c",
        "id": 788,
        "name": "peanut_butter",
        "synonyms": ["peanut_butter"],
        "synset": "peanut_butter.n.01"
    },
    {
        "def": "sweet juicy gritty-textured fruit available in many varieties",
        "frequency": "c",
        "id": 789,
        "name": "pear",
        "synonyms": ["pear"],
        "synset": "pear.n.01"
    },
    {
        "def": "a device for peeling vegetables or fruits",
        "frequency": "r",
        "id": 790,
        "name": "peeler_(tool_for_fruit_and_vegetables)",
        "synonyms": ["peeler_(tool_for_fruit_and_vegetables)"],
        "synset": "peeler.n.03"
    },
    {
        "def": "a board perforated with regularly spaced holes into which pegs can be fitted",
        "frequency": "r",
        "id": 791,
        "name": "pegboard",
        "synonyms": ["pegboard"],
        "synset": "pegboard.n.01"
    },
    {
        "def": "large long-winged warm-water seabird having a large bill with a distensible pouch "
               "for fish",
        "frequency": "c",
        "id": 792,
        "name": "pelican",
        "synonyms": ["pelican"],
        "synset": "pelican.n.01"
    },
    {
        "def": "a writing implement with a point from which ink flows",
        "frequency": "f",
        "id": 793,
        "name": "pen",
        "synonyms": ["pen"],
        "synset": "pen.n.01"
    },
    {
        "def": "a thin cylindrical pointed writing implement made of wood and graphite",
        "frequency": "c",
        "id": 794,
        "name": "pencil",
        "synonyms": ["pencil"],
        "synset": "pencil.n.01"
    },
    {
        "def": "a box for holding pencils",
        "frequency": "r",
        "id": 795,
        "name": "pencil_box",
        "synonyms": ["pencil_box", "pencil_case"],
        "synset": "pencil_box.n.01"
    },
    {
        "def": "a rotary implement for sharpening the point on pencils",
        "frequency": "r",
        "id": 796,
        "name": "pencil_sharpener",
        "synonyms": ["pencil_sharpener"],
        "synset": "pencil_sharpener.n.01"
    },
    {
        "def": "an apparatus consisting of an object mounted so that it swings freely under the "
               "influence of gravity",
        "frequency": "r",
        "id": 797,
        "name": "pendulum",
        "synonyms": ["pendulum"],
        "synset": "pendulum.n.01"
    },
    {
        "def": "short-legged flightless birds of cold southern regions having webbed feet and "
               "wings modified as flippers",
        "frequency": "c",
        "id": 798,
        "name": "penguin",
        "synonyms": ["penguin"],
        "synset": "penguin.n.01"
    },
    {
        "def": "a flag longer than it is wide (and often tapering)",
        "frequency": "r",
        "id": 799,
        "name": "pennant",
        "synonyms": ["pennant"],
        "synset": "pennant.n.02"
    },
    {
        "def": "a coin worth one-hundredth of the value of the basic unit",
        "frequency": "r",
        "id": 800,
        "name": "penny_(coin)",
        "synonyms": ["penny_(coin)"],
        "synset": "penny.n.02"
    },
    {
        "def": "pungent seasoning from the berry of the common pepper plant; whole or ground",
        "frequency": "c",
        "id": 801,
        "name": "pepper",
        "synonyms": ["pepper", "peppercorn"],
        "synset": "pepper.n.03"
    },
    {
        "def": "a mill for grinding pepper",
        "frequency": "c",
        "id": 802,
        "name": "pepper_mill",
        "synonyms": ["pepper_mill", "pepper_grinder"],
        "synset": "pepper_mill.n.01"
    },
    {
        "def": "a toiletry that emits and diffuses a fragrant odor",
        "frequency": "c",
        "id": 803,
        "name": "perfume",
        "synonyms": ["perfume"],
        "synset": "perfume.n.02"
    },
    {
        "def": "orange fruit resembling a plum; edible when fully ripe",
        "frequency": "r",
        "id": 804,
        "name": "persimmon",
        "synonyms": ["persimmon"],
        "synset": "persimmon.n.02"
    },
    {
        "def": "a human being",
        "frequency": "f",
        "id": 805,
        "name": "baby",
        "synonyms": ["baby", "child", "boy", "girl", "man", "woman", "person", "human"],
        "synset": "person.n.01"
    },
    {
        "def": "a domesticated animal kept for companionship or amusement",
        "frequency": "r",
        "id": 806,
        "name": "pet",
        "synonyms": ["pet"],
        "synset": "pet.n.01"
    },
    {
        "def": "food prepared for animal pets",
        "frequency": "r",
        "id": 807,
        "name": "petfood",
        "synonyms": ["petfood", "pet-food"],
        "synset": "petfood.n.01"
    },
    {
        "def": "long bench with backs; used in church by the congregation",
        "frequency": "r",
        "id": 808,
        "name": "pew_(church_bench)",
        "synonyms": ["pew_(church_bench)", "church_bench"],
        "synset": "pew.n.01"
    },
    {
        "def": "a directory containing an alphabetical list of telephone subscribers and their "
               "telephone numbers",
        "frequency": "r",
        "id": 809,
        "name": "phonebook",
        "synonyms": ["phonebook", "telephone_book", "telephone_directory"],
        "synset": "phonebook.n.01"
    },
    {
        "def": "sound recording consisting of a typically black disk with a continuous groove",
        "frequency": "c",
        "id": 810,
        "name": "phonograph_record",
        "synonyms": ["phonograph_record", "phonograph_recording", "record_(phonograph_recording)"],
        "synset": "phonograph_record.n.01"
    },
    {
        "def": "a keyboard instrument that is played by depressing keys that cause hammers to "
               "strike tuned strings and produce sounds",
        "frequency": "c",
        "id": 811,
        "name": "piano",
        "synonyms": ["piano"],
        "synset": "piano.n.01"
    },
    {
        "def": "vegetables (especially cucumbers) preserved in brine or vinegar",
        "frequency": "f",
        "id": 812,
        "name": "pickle",
        "synonyms": ["pickle"],
        "synset": "pickle.n.01"
    },
    {
        "def": "a light truck with an open body and low sides and a tailboard",
        "frequency": "f",
        "id": 813,
        "name": "pickup_truck",
        "synonyms": ["pickup_truck"],
        "synset": "pickup.n.01"
    },
    {
        "def": "dish baked in pastry-lined pan often with a pastry top",
        "frequency": "c",
        "id": 814,
        "name": "pie",
        "synonyms": ["pie"],
        "synset": "pie.n.01"
    },
    {
        "def": "wild and domesticated birds having a heavy body and short legs",
        "frequency": "c",
        "id": 815,
        "name": "pigeon",
        "synonyms": ["pigeon"],
        "synset": "pigeon.n.01"
    },
    {
        "def": "a child's coin bank (often shaped like a pig)",
        "frequency": "r",
        "id": 816,
        "name": "piggy_bank",
        "synonyms": ["piggy_bank", "penny_bank"],
        "synset": "piggy_bank.n.01"
    },
    {
        "def": "a cushion to support the head of a sleeping person",
        "frequency": "f",
        "id": 817,
        "name": "pillow",
        "synonyms": ["pillow"],
        "synset": "pillow.n.01"
    },
    {
        "def": "a small slender (often pointed) piece of wood or metal used to support or fasten "
               "or attach things",
        "frequency": "r",
        "id": 818,
        "name": "pin_(non_jewelry)",
        "synonyms": ["pin_(non_jewelry)"],
        "synset": "pin.n.09"
    },
    {
        "def": "large sweet fleshy tropical fruit with a tuft of stiff leaves",
        "frequency": "f",
        "id": 819,
        "name": "pineapple",
        "synonyms": ["pineapple"],
        "synset": "pineapple.n.02"
    },
    {
        "def": "the seed-producing cone of a pine tree",
        "frequency": "c",
        "id": 820,
        "name": "pinecone",
        "synonyms": ["pinecone"],
        "synset": "pinecone.n.01"
    },
    {
        "def": "light hollow ball used in playing table tennis",
        "frequency": "r",
        "id": 821,
        "name": "ping-pong_ball",
        "synonyms": ["ping-pong_ball"],
        "synset": "ping-pong_ball.n.01"
    },
    {
        "def": "a toy consisting of vanes of colored paper or plastic that is pinned to a stick "
               "and spins when it is pointed into the wind",
        "frequency": "r",
        "id": 822,
        "name": "pinwheel",
        "synonyms": ["pinwheel"],
        "synset": "pinwheel.n.03"
    },
    {
        "def": "a tube with a small bowl at one end; used for smoking tobacco",
        "frequency": "r",
        "id": 823,
        "name": "tobacco_pipe",
        "synonyms": ["tobacco_pipe"],
        "synset": "pipe.n.01"
    },
    {
        "def": "a long tube made of metal or plastic that is used to carry water or oil or gas "
               "etc.",
        "frequency": "f",
        "id": 824,
        "name": "pipe",
        "synonyms": ["pipe", "piping"],
        "synset": "pipe.n.02"
    },
    {
        "def": "a firearm that is held and fired with one hand",
        "frequency": "r",
        "id": 825,
        "name": "pistol",
        "synonyms": ["pistol", "handgun"],
        "synset": "pistol.n.01"
    },
    {
        "def": "usually small round bread that can open into a pocket for filling",
        "frequency": "r",
        "id": 826,
        "name": "pita_(bread)",
        "synonyms": ["pita_(bread)", "pocket_bread"],
        "synset": "pita.n.01"
    },
    {
        "def": "an open vessel with a handle and a spout for pouring",
        "frequency": "f",
        "id": 827,
        "name": "pitcher_(vessel_for_liquid)",
        "synonyms": ["pitcher_(vessel_for_liquid)", "ewer"],
        "synset": "pitcher.n.02"
    },
    {
        "def": "a long-handled hand tool with sharp widely spaced prongs for lifting and pitching "
               "hay",
        "frequency": "r",
        "id": 828,
        "name": "pitchfork",
        "synonyms": ["pitchfork"],
        "synset": "pitchfork.n.01"
    },
    {
        "def": "Italian open pie made of thin bread dough spread with a spiced mixture of e.g. "
               "tomato sauce and cheese",
        "frequency": "f",
        "id": 829,
        "name": "pizza",
        "synonyms": ["pizza"],
        "synset": "pizza.n.01"
    },
    {
        "def": "a mat placed on a table for an individual place setting",
        "frequency": "f",
        "id": 830,
        "name": "place_mat",
        "synonyms": ["place_mat"],
        "synset": "place_mat.n.01"
    },
    {
        "def": "dish on which food is served or from which food is eaten",
        "frequency": "f",
        "id": 831,
        "name": "plate",
        "synonyms": ["plate"],
        "synset": "plate.n.04"
    },
    {
        "def": "a large shallow dish used for serving food",
        "frequency": "c",
        "id": 832,
        "name": "platter",
        "synonyms": ["platter"],
        "synset": "platter.n.01"
    },
    {
        "def": "one of a pack of cards that are used to play card games",
        "frequency": "r",
        "id": 833,
        "name": "playing_card",
        "synonyms": ["playing_card"],
        "synset": "playing_card.n.01"
    },
    {
        "def": "a portable enclosure in which babies may be left to play",
        "frequency": "r",
        "id": 834,
        "name": "playpen",
        "synonyms": ["playpen"],
        "synset": "playpen.n.01"
    },
    {
        "def": "a gripping hand tool with two hinged arms and (usually) serrated jaws",
        "frequency": "c",
        "id": 835,
        "name": "pliers",
        "synonyms": ["pliers", "plyers"],
        "synset": "pliers.n.01"
    },
    {
        "def": "a farm tool having one or more heavy blades to break the soil and cut a furrow "
               "prior to sowing",
        "frequency": "r",
        "id": 836,
        "name": "plow_(farm_equipment)",
        "synonyms": ["plow_(farm_equipment)", "plough_(farm_equipment)"],
        "synset": "plow.n.01"
    },
    {
        "def": "a watch that is carried in a small watch pocket",
        "frequency": "r",
        "id": 837,
        "name": "pocket_watch",
        "synonyms": ["pocket_watch"],
        "synset": "pocket_watch.n.01"
    },
    {
        "def": "a knife with a blade that folds into the handle; suitable for carrying in the "
               "pocket",
        "frequency": "c",
        "id": 838,
        "name": "pocketknife",
        "synonyms": ["pocketknife"],
        "synset": "pocketknife.n.01"
    },
    {
        "def": "fire iron consisting of a metal rod with a handle; used to stir a fire",
        "frequency": "c",
        "id": 839,
        "name": "poker_(fire_stirring_tool)",
        "synonyms": ["poker_(fire_stirring_tool)", "stove_poker", "fire_hook"],
        "synset": "poker.n.01"
    },
    {
        "def": "a long (usually round) rod of wood or metal or plastic",
        "frequency": "f",
        "id": 840,
        "name": "pole",
        "synonyms": ["pole", "post"],
        "synset": "pole.n.01"
    },
    {
        "def": "van used by police to transport prisoners",
        "frequency": "r",
        "id": 841,
        "name": "police_van",
        "synonyms": ["police_van", "police_wagon", "paddy_wagon", "patrol_wagon"],
        "synset": "police_van.n.01"
    },
    {
        "def": "a shirt with short sleeves designed for comfort and casual wear",
        "frequency": "f",
        "id": 842,
        "name": "polo_shirt",
        "synonyms": ["polo_shirt", "sport_shirt"],
        "synset": "polo_shirt.n.01"
    },
    {
        "def": "a blanket-like cloak with a hole in the center for the head",
        "frequency": "r",
        "id": 843,
        "name": "poncho",
        "synonyms": ["poncho"],
        "synset": "poncho.n.01"
    },
    {
        "def": "any of various breeds of small gentle horses usually less than five feet high at "
               "the shoulder",
        "frequency": "c",
        "id": 844,
        "name": "pony",
        "synonyms": ["pony"],
        "synset": "pony.n.05"
    },
    {
        "def": "game equipment consisting of a heavy table on which pool is played",
        "frequency": "r",
        "id": 845,
        "name": "pool_table",
        "synonyms": ["pool_table", "billiard_table", "snooker_table"],
        "synset": "pool_table.n.01"
    },
    {
        "def": "a sweet drink containing carbonated water and flavoring",
        "frequency": "f",
        "id": 846,
        "name": "pop_(soda)",
        "synonyms": ["pop_(soda)", "soda_(pop)", "tonic", "soft_drink"],
        "synset": "pop.n.02"
    },
    {
        "def": "any likeness of a person, in any medium",
        "frequency": "r",
        "id": 847,
        "name": "portrait",
        "synonyms": ["portrait", "portrayal"],
        "synset": "portrait.n.02"
    },
    {
        "def": "public box for deposit of mail",
        "frequency": "c",
        "id": 848,
        "name": "postbox_(public)",
        "synonyms": ["postbox_(public)", "mailbox_(public)"],
        "synset": "postbox.n.01"
    },
    {
        "def": "a card for sending messages by post without an envelope",
        "frequency": "c",
        "id": 849,
        "name": "postcard",
        "synonyms": ["postcard", "postal_card", "mailing-card"],
        "synset": "postcard.n.01"
    },
    {
        "def": "a sign posted in a public place as an advertisement",
        "frequency": "f",
        "id": 850,
        "name": "poster",
        "synonyms": ["poster", "placard"],
        "synset": "poster.n.01"
    },
    {
        "def": "metal or earthenware cooking vessel that is usually round and deep; often has a "
               "handle and lid",
        "frequency": "f",
        "id": 851,
        "name": "pot",
        "synonyms": ["pot"],
        "synset": "pot.n.01"
    },
    {
        "def": "a container in which plants are cultivated",
        "frequency": "f",
        "id": 852,
        "name": "flowerpot",
        "synonyms": ["flowerpot"],
        "synset": "pot.n.04"
    },
    {
        "def": "an edible tuber native to South America",
        "frequency": "f",
        "id": 853,
        "name": "potato",
        "synonyms": ["potato"],
        "synset": "potato.n.01"
    },
    {
        "def": "an insulated pad for holding hot pots",
        "frequency": "c",
        "id": 854,
        "name": "potholder",
        "synonyms": ["potholder"],
        "synset": "potholder.n.01"
    },
    {
        "def": "ceramic ware made from clay and baked in a kiln",
        "frequency": "c",
        "id": 855,
        "name": "pottery",
        "synonyms": ["pottery", "clayware"],
        "synset": "pottery.n.01"
    },
    {
        "def": "a small or medium size container for holding or carrying things",
        "frequency": "c",
        "id": 856,
        "name": "pouch",
        "synonyms": ["pouch"],
        "synset": "pouch.n.01"
    },
    {
        "def": "a machine for excavating",
        "frequency": "r",
        "id": 857,
        "name": "power_shovel",
        "synonyms": ["power_shovel", "excavator", "digger"],
        "synset": "power_shovel.n.01"
    },
    {
        "def": "any of various edible decapod crustaceans",
        "frequency": "c",
        "id": 858,
        "name": "prawn",
        "synonyms": ["prawn", "shrimp"],
        "synset": "prawn.n.01"
    },
    {
        "def": "a machine that prints",
        "frequency": "f",
        "id": 859,
        "name": "printer",
        "synonyms": ["printer", "printing_machine"],
        "synset": "printer.n.03"
    },
    {
        "def": "a weapon that is forcibly thrown or projected at a targets",
        "frequency": "c",
        "id": 860,
        "name": "projectile_(weapon)",
        "synonyms": ["projectile_(weapon)", "missile"],
        "synset": "projectile.n.01"
    },
    {
        "def": "an optical instrument that projects an enlarged image onto a screen",
        "frequency": "c",
        "id": 861,
        "name": "projector",
        "synonyms": ["projector"],
        "synset": "projector.n.02"
    },
    {
        "def": "a mechanical device that rotates to push against air or water",
        "frequency": "f",
        "id": 862,
        "name": "propeller",
        "synonyms": ["propeller", "propellor"],
        "synset": "propeller.n.01"
    },
    {
        "def": "dried plum",
        "frequency": "r",
        "id": 863,
        "name": "prune",
        "synonyms": ["prune"],
        "synset": "prune.n.01"
    },
    {
        "def": "any of various soft thick unsweetened baked dishes",
        "frequency": "r",
        "id": 864,
        "name": "pudding",
        "synonyms": ["pudding"],
        "synset": "pudding.n.01"
    },
    {
        "def": "fishes whose elongated spiny body can inflate itself with water or air to form a "
               "globe",
        "frequency": "r",
        "id": 865,
        "name": "puffer_(fish)",
        "synonyms": ["puffer_(fish)", "pufferfish", "blowfish", "globefish"],
        "synset": "puffer.n.02"
    },
    {
        "def": "seabirds having short necks and brightly colored compressed bills",
        "frequency": "r",
        "id": 866,
        "name": "puffin",
        "synonyms": ["puffin"],
        "synset": "puffin.n.01"
    },
    {
        "def": "small compact smooth-coated breed of Asiatic origin having a tightly curled tail "
               "and broad flat wrinkled muzzle",
        "frequency": "r",
        "id": 867,
        "name": "pug-dog",
        "synonyms": ["pug-dog"],
        "synset": "pug.n.01"
    },
    {
        "def": "usually large pulpy deep-yellow round fruit of the squash family maturing in late "
               "summer or early autumn",
        "frequency": "c",
        "id": 868,
        "name": "pumpkin",
        "synonyms": ["pumpkin"],
        "synset": "pumpkin.n.02"
    },
    {
        "def": "a tool for making holes or indentations",
        "frequency": "r",
        "id": 869,
        "name": "puncher",
        "synonyms": ["puncher"],
        "synset": "punch.n.03"
    },
    {
        "def": "a small figure of a person operated from above with strings by a puppeteer",
        "frequency": "r",
        "id": 870,
        "name": "puppet",
        "synonyms": ["puppet", "marionette"],
        "synset": "puppet.n.01"
    },
    {
        "def": "a young dog",
        "frequency": "r",
        "id": 871,
        "name": "puppy",
        "synonyms": ["puppy"],
        "synset": "puppy.n.01"
    },
    {
        "def": "a tortilla that is filled with cheese and heated",
        "frequency": "r",
        "id": 872,
        "name": "quesadilla",
        "synonyms": ["quesadilla"],
        "synset": "quesadilla.n.01"
    },
    {
        "def": "a tart filled with rich unsweetened custard; often contains other ingredients (as "
               "cheese or ham or seafood or vegetables)",
        "frequency": "r",
        "id": 873,
        "name": "quiche",
        "synonyms": ["quiche"],
        "synset": "quiche.n.02"
    },
    {
        "def": "bedding made of two layers of cloth filled with stuffing and stitched together",
        "frequency": "f",
        "id": 874,
        "name": "quilt",
        "synonyms": ["quilt", "comforter"],
        "synset": "quilt.n.01"
    },
    {
        "def": "any of various burrowing animals of the family Leporidae having long ears and "
               "short tails",
        "frequency": "c",
        "id": 875,
        "name": "rabbit",
        "synonyms": ["rabbit"],
        "synset": "rabbit.n.01"
    },
    {
        "def": "a fast car that competes in races",
        "frequency": "r",
        "id": 876,
        "name": "race_car",
        "synonyms": ["race_car", "racing_car"],
        "synset": "racer.n.02"
    },
    {
        "def": "a sports implement used to strike a ball in various games",
        "frequency": "c",
        "id": 877,
        "name": "racket",
        "synonyms": ["racket", "racquet"],
        "synset": "racket.n.04"
    },
    {
        "def": "measuring instrument in which the echo of a pulse of microwave radiation is used "
               "to detect and locate distant objects",
        "frequency": "r",
        "id": 878,
        "name": "radar",
        "synonyms": ["radar"],
        "synset": "radar.n.01"
    },
    {
        "def": "a mechanism consisting of a metal honeycomb through which hot fluids circulate",
        "frequency": "c",
        "id": 879,
        "name": "radiator",
        "synonyms": ["radiator"],
        "synset": "radiator.n.03"
    },
    {
        "def": "an electronic receiver that detects and demodulates and amplifies transmitted "
               "radio signals",
        "frequency": "c",
        "id": 880,
        "name": "radio_receiver",
        "synonyms": ["radio_receiver", "radio_set", "radio", "tuner_(radio)"],
        "synset": "radio_receiver.n.01"
    },
    {
        "def": "pungent edible root of any of various cultivated radish plants",
        "frequency": "c",
        "id": 881,
        "name": "radish",
        "synonyms": ["radish", "daikon"],
        "synset": "radish.n.03"
    },
    {
        "def": "a flat float (usually made of logs or planks) that can be used for transport or as "
               "a platform for swimmers",
        "frequency": "c",
        "id": 882,
        "name": "raft",
        "synonyms": ["raft"],
        "synset": "raft.n.01"
    },
    {
        "def": "a cloth doll that is stuffed and (usually) painted",
        "frequency": "r",
        "id": 883,
        "name": "rag_doll",
        "synonyms": ["rag_doll"],
        "synset": "rag_doll.n.01"
    },
    {
        "def": "a water-resistant coat",
        "frequency": "c",
        "id": 884,
        "name": "raincoat",
        "synonyms": ["raincoat", "waterproof_jacket"],
        "synset": "raincoat.n.01"
    },
    {
        "def": "uncastrated adult male sheep",
        "frequency": "c",
        "id": 885,
        "name": "ram_(animal)",
        "synonyms": ["ram_(animal)"],
        "synset": "ram.n.05"
    },
    {
        "def": "red or black edible aggregate berries usually smaller than the related "
               "blackberries",
        "frequency": "c",
        "id": 886,
        "name": "raspberry",
        "synonyms": ["raspberry"],
        "synset": "raspberry.n.02"
    },
    {
        "def": "any of various long-tailed rodents similar to but larger than a mouse",
        "frequency": "r",
        "id": 887,
        "name": "rat",
        "synonyms": ["rat"],
        "synset": "rat.n.01"
    },
    {
        "def": "a blade that has very sharp edge",
        "frequency": "c",
        "id": 888,
        "name": "razorblade",
        "synonyms": ["razorblade"],
        "synset": "razorblade.n.01"
    },
    {
        "def": "a squeezer with a conical ridged center that is used for squeezing juice from "
               "citrus fruit",
        "frequency": "c",
        "id": 889,
        "name": "reamer_(juicer)",
        "synonyms": ["reamer_(juicer)", "juicer", "juice_reamer"],
        "synset": "reamer.n.01"
    },
    {
        "def": "car mirror that reflects the view out of the rear window",
        "frequency": "f",
        "id": 890,
        "name": "rearview_mirror",
        "synonyms": ["rearview_mirror"],
        "synset": "rearview_mirror.n.01"
    },
    {
        "def": "an acknowledgment (usually tangible) that payment has been made",
        "frequency": "c",
        "id": 891,
        "name": "receipt",
        "synonyms": ["receipt"],
        "synset": "receipt.n.02"
    },
    {
        "def": "an armchair whose back can be lowered and foot can be raised to allow the sitter "
               "to recline in it",
        "frequency": "c",
        "id": 892,
        "name": "recliner",
        "synonyms": ["recliner", "reclining_chair", "lounger_(chair)"],
        "synset": "recliner.n.01"
    },
    {
        "def": "machine in which rotating records cause a stylus to vibrate and the vibrations are "
               "amplified acoustically or electronically",
        "frequency": "r",
        "id": 893,
        "name": "record_player",
        "synonyms": ["record_player", "phonograph_(record_player)", "turntable"],
        "synset": "record_player.n.01"
    },
    {
        "def": "compact head of purplish-red leaves",
        "frequency": "r",
        "id": 894,
        "name": "red_cabbage",
        "synonyms": ["red_cabbage"],
        "synset": "red_cabbage.n.02"
    },
    {
        "def": "device that reflects light, radiation, etc.",
        "frequency": "f",
        "id": 895,
        "name": "reflector",
        "synonyms": ["reflector"],
        "synset": "reflector.n.01"
    },
    {
        "def": "a device that can be used to control a machine or apparatus from a distance",
        "frequency": "f",
        "id": 896,
        "name": "remote_control",
        "synonyms": ["remote_control"],
        "synset": "remote_control.n.01"
    },
    {
        "def": "massive powerful herbivorous odd-toed ungulate of southeast Asia and Africa having "
               "very thick skin and one or two horns on the snout",
        "frequency": "c",
        "id": 897,
        "name": "rhinoceros",
        "synonyms": ["rhinoceros"],
        "synset": "rhinoceros.n.01"
    },
    {
        "def": "cut of meat including one or more ribs",
        "frequency": "r",
        "id": 898,
        "name": "rib_(food)",
        "synonyms": ["rib_(food)"],
        "synset": "rib.n.03"
    },
    {
        "def": "a shoulder firearm with a long barrel",
        "frequency": "r",
        "id": 899,
        "name": "rifle",
        "synonyms": ["rifle"],
        "synset": "rifle.n.01"
    },
    {
        "def": "jewelry consisting of a circlet of precious metal (often set with jewels) worn on "
               "the finger",
        "frequency": "f",
        "id": 900,
        "name": "ring",
        "synonyms": ["ring"],
        "synset": "ring.n.08"
    },
    {
        "def": "a boat used on rivers or to ply a river",
        "frequency": "r",
        "id": 901,
        "name": "river_boat",
        "synonyms": ["river_boat"],
        "synset": "river_boat.n.01"
    },
    {
        "def": "(NOT A ROAD) a MAP showing roads (for automobile travel)",
        "frequency": "r",
        "id": 902,
        "name": "road_map",
        "synonyms": ["road_map"],
        "synset": "road_map.n.02"
    },
    {
        "def": "any loose flowing garment",
        "frequency": "c",
        "id": 903,
        "name": "robe",
        "synonyms": ["robe"],
        "synset": "robe.n.01"
    },
    {
        "def": "a chair mounted on rockers",
        "frequency": "c",
        "id": 904,
        "name": "rocking_chair",
        "synonyms": ["rocking_chair"],
        "synset": "rocking_chair.n.01"
    },
    {
        "def": "a shoe with pairs of rollers (small hard wheels) fixed to the sole",
        "frequency": "r",
        "id": 905,
        "name": "roller_skate",
        "synonyms": ["roller_skate"],
        "synset": "roller_skate.n.01"
    },
    {
        "def": "an in-line variant of a roller skate",
        "frequency": "r",
        "id": 906,
        "name": "Rollerblade",
        "synonyms": ["Rollerblade"],
        "synset": "rollerblade.n.01"
    },
    {
        "def": "utensil consisting of a cylinder (usually of wood) with a handle at each end; used "
               "to roll out dough",
        "frequency": "c",
        "id": 907,
        "name": "rolling_pin",
        "synonyms": ["rolling_pin"],
        "synset": "rolling_pin.n.01"
    },
    {
        "def": "carbonated drink containing extracts of roots and herbs",
        "frequency": "r",
        "id": 908,
        "name": "root_beer",
        "synonyms": ["root_beer"],
        "synset": "root_beer.n.01"
    },
    {
        "def": "a device that forwards data packets between computer networks",
        "frequency": "c",
        "id": 909,
        "name": "router_(computer_equipment)",
        "synonyms": ["router_(computer_equipment)"],
        "synset": "router.n.02"
    },
    {
        "def": "a narrow band of elastic rubber used to hold things (such as papers) together",
        "frequency": "f",
        "id": 910,
        "name": "rubber_band",
        "synonyms": ["rubber_band", "elastic_band"],
        "synset": "rubber_band.n.01"
    },
    {
        "def": "a long narrow carpet",
        "frequency": "c",
        "id": 911,
        "name": "runner_(carpet)",
        "synonyms": ["runner_(carpet)"],
        "synset": "runner.n.08"
    },
    {
        "def": "a bag made of paper or plastic for holding customer's purchases",
        "frequency": "f",
        "id": 912,
        "name": "plastic_bag",
        "synonyms": ["plastic_bag", "paper_bag"],
        "synset": "sack.n.01"
    },
    {
        "def": "a seat for the rider of a horse or camel",
        "frequency": "f",
        "id": 913,
        "name": "saddle_(on_an_animal)",
        "synonyms": ["saddle_(on_an_animal)"],
        "synset": "saddle.n.01"
    },
    {
        "def": "stable gear consisting of a blanket placed under the saddle",
        "frequency": "f",
        "id": 914,
        "name": "saddle_blanket",
        "synonyms": ["saddle_blanket", "saddlecloth", "horse_blanket"],
        "synset": "saddle_blanket.n.01"
    },
    {
        "def": "a large bag (or pair of bags) hung over a saddle",
        "frequency": "c",
        "id": 915,
        "name": "saddlebag",
        "synonyms": ["saddlebag"],
        "synset": "saddlebag.n.01"
    },
    {
        "def": "a pin in the form of a clasp; has a guard so the point of the pin will not stick "
               "the user",
        "frequency": "r",
        "id": 916,
        "name": "safety_pin",
        "synonyms": ["safety_pin"],
        "synset": "safety_pin.n.01"
    },
    {
        "def": "a large piece of fabric by means of which wind is used to propel a sailing vessel",
        "frequency": "c",
        "id": 917,
        "name": "sail",
        "synonyms": ["sail"],
        "synset": "sail.n.01"
    },
    {
        "def": "food mixtures either arranged on a plate or tossed and served with a moist "
               "dressing; usually consisting of or including greens",
        "frequency": "c",
        "id": 918,
        "name": "salad",
        "synonyms": ["salad"],
        "synset": "salad.n.01"
    },
    {
        "def": "a plate or bowl for individual servings of salad",
        "frequency": "r",
        "id": 919,
        "name": "salad_plate",
        "synonyms": ["salad_plate", "salad_bowl"],
        "synset": "salad_plate.n.01"
    },
    {
        "def": "highly seasoned fatty sausage of pork and beef usually dried",
        "frequency": "r",
        "id": 920,
        "name": "salami",
        "synonyms": ["salami"],
        "synset": "salami.n.01"
    },
    {
        "def": "any of various large food and game fishes of northern waters",
        "frequency": "r",
        "id": 921,
        "name": "salmon_(fish)",
        "synonyms": ["salmon_(fish)"],
        "synset": "salmon.n.01"
    },
    {
        "def": "flesh of any of various marine or freshwater fish of the family Salmonidae",
        "frequency": "r",
        "id": 922,
        "name": "salmon_(food)",
        "synonyms": ["salmon_(food)"],
        "synset": "salmon.n.03"
    },
    {
        "def": "spicy sauce of tomatoes and onions and chili peppers to accompany Mexican foods",
        "frequency": "r",
        "id": 923,
        "name": "salsa",
        "synonyms": ["salsa"],
        "synset": "salsa.n.01"
    },
    {
        "def": "a shaker with a perforated top for sprinkling salt",
        "frequency": "f",
        "id": 924,
        "name": "saltshaker",
        "synonyms": ["saltshaker"],
        "synset": "saltshaker.n.01"
    },
    {
        "def": "a shoe consisting of a sole fastened by straps to the foot",
        "frequency": "f",
        "id": 925,
        "name": "sandal_(type_of_shoe)",
        "synonyms": ["sandal_(type_of_shoe)"],
        "synset": "sandal.n.01"
    },
    {
        "def": "two (or more) slices of bread with a filling between them",
        "frequency": "f",
        "id": 926,
        "name": "sandwich",
        "synonyms": ["sandwich"],
        "synset": "sandwich.n.01"
    },
    {
        "def": "luggage consisting of a small case with a flat bottom and (usually) a shoulder "
               "strap",
        "frequency": "r",
        "id": 927,
        "name": "satchel",
        "synonyms": ["satchel"],
        "synset": "satchel.n.01"
    },
    {
        "def": "a deep pan with a handle; used for stewing or boiling",
        "frequency": "r",
        "id": 928,
        "name": "saucepan",
        "synonyms": ["saucepan"],
        "synset": "saucepan.n.01"
    },
    {
        "def": "a small shallow dish for holding a cup at the table",
        "frequency": "f",
        "id": 929,
        "name": "saucer",
        "synonyms": ["saucer"],
        "synset": "saucer.n.02"
    },
    {
        "def": "highly seasoned minced meat stuffed in casings",
        "frequency": "f",
        "id": 930,
        "name": "sausage",
        "synonyms": ["sausage"],
        "synset": "sausage.n.01"
    },
    {
        "def": "a framework for holding wood that is being sawed",
        "frequency": "r",
        "id": 931,
        "name": "sawhorse",
        "synonyms": ["sawhorse", "sawbuck"],
        "synset": "sawhorse.n.01"
    },
    {
        "def": "a wind instrument with a `J'-shaped form typically made of brass",
        "frequency": "r",
        "id": 932,
        "name": "saxophone",
        "synonyms": ["saxophone"],
        "synset": "sax.n.02"
    },
    {
        "def": "a measuring instrument for weighing; shows amount of mass",
        "frequency": "f",
        "id": 933,
        "name": "scale_(measuring_instrument)",
        "synonyms": ["scale_(measuring_instrument)"],
        "synset": "scale.n.07"
    },
    {
        "def": "an effigy in the shape of a man to frighten birds away from seeds",
        "frequency": "r",
        "id": 934,
        "name": "scarecrow",
        "synonyms": ["scarecrow", "strawman"],
        "synset": "scarecrow.n.01"
    },
    {
        "def": "a garment worn around the head or neck or shoulders for warmth or decoration",
        "frequency": "f",
        "id": 935,
        "name": "scarf",
        "synonyms": ["scarf"],
        "synset": "scarf.n.01"
    },
    {
        "def": "a bus used to transport children to or from school",
        "frequency": "c",
        "id": 936,
        "name": "school_bus",
        "synonyms": ["school_bus"],
        "synset": "school_bus.n.01"
    },
    {
        "def": "a tool having two crossed pivoting blades with looped handles",
        "frequency": "f",
        "id": 937,
        "name": "scissors",
        "synonyms": ["scissors"],
        "synset": "scissors.n.01"
    },
    {
        "def": "a large board for displaying the score of a contest (and some other information)",
        "frequency": "c",
        "id": 938,
        "name": "scoreboard",
        "synonyms": ["scoreboard"],
        "synset": "scoreboard.n.01"
    },
    {
        "def": "eggs beaten and cooked to a soft firm consistency while stirring",
        "frequency": "c",
        "id": 939,
        "name": "scrambled_eggs",
        "synonyms": ["scrambled_eggs"],
        "synset": "scrambled_eggs.n.01"
    },
    {
        "def": "any of various hand tools for scraping",
        "frequency": "r",
        "id": 940,
        "name": "scraper",
        "synonyms": ["scraper"],
        "synset": "scraper.n.01"
    },
    {
        "def": "a device used for scratching",
        "frequency": "r",
        "id": 941,
        "name": "scratcher",
        "synonyms": ["scratcher"],
        "synset": "scratcher.n.03"
    },
    {
        "def": "a hand tool for driving screws; has a tip that fits into the head of a screw",
        "frequency": "c",
        "id": 942,
        "name": "screwdriver",
        "synonyms": ["screwdriver"],
        "synset": "screwdriver.n.01"
    },
    {
        "def": "a brush with short stiff bristles for heavy cleaning",
        "frequency": "c",
        "id": 943,
        "name": "scrubbing_brush",
        "synonyms": ["scrubbing_brush"],
        "synset": "scrub_brush.n.01"
    },
    {
        "def": "a three-dimensional work of art",
        "frequency": "c",
        "id": 944,
        "name": "sculpture",
        "synonyms": ["sculpture"],
        "synset": "sculpture.n.01"
    },
    {
        "def": "a bird that frequents coastal waters and the open ocean: gulls; pelicans; gannets; "
               "cormorants; albatrosses; petrels; etc.",
        "frequency": "r",
        "id": 945,
        "name": "seabird",
        "synonyms": ["seabird", "seafowl"],
        "synset": "seabird.n.01"
    },
    {
        "def": "small fish with horse-like heads bent sharply downward and curled tails",
        "frequency": "r",
        "id": 946,
        "name": "seahorse",
        "synonyms": ["seahorse"],
        "synset": "seahorse.n.02"
    },
    {
        "def": "an airplane that can land on or take off from water",
        "frequency": "r",
        "id": 947,
        "name": "seaplane",
        "synonyms": ["seaplane", "hydroplane"],
        "synset": "seaplane.n.01"
    },
    {
        "def": "the shell of a marine organism",
        "frequency": "c",
        "id": 948,
        "name": "seashell",
        "synonyms": ["seashell"],
        "synset": "seashell.n.01"
    },
    {
        "def": "young plant or tree grown from a seed",
        "frequency": "r",
        "id": 949,
        "name": "seedling",
        "synonyms": ["seedling"],
        "synset": "seedling.n.01"
    },
    {
        "def": "a dish used for serving food",
        "frequency": "c",
        "id": 950,
        "name": "serving_dish",
        "synonyms": ["serving_dish"],
        "synset": "serving_dish.n.01"
    },
    {
        "def": "a textile machine used as a home appliance for sewing",
        "frequency": "r",
        "id": 951,
        "name": "sewing_machine",
        "synonyms": ["sewing_machine"],
        "synset": "sewing_machine.n.01"
    },
    {
        "def": "a container in which something can be shaken",
        "frequency": "r",
        "id": 952,
        "name": "shaker",
        "synonyms": ["shaker"],
        "synset": "shaker.n.03"
    },
    {
        "def": "cleansing agent consisting of soaps or detergents used for washing the hair",
        "frequency": "c",
        "id": 953,
        "name": "shampoo",
        "synonyms": ["shampoo"],
        "synset": "shampoo.n.01"
    },
    {
        "def": "typically large carnivorous fishes with sharpe teeth",
        "frequency": "r",
        "id": 954,
        "name": "shark",
        "synonyms": ["shark"],
        "synset": "shark.n.01"
    },
    {
        "def": "any implement that is used to make something (an edge or a point) sharper",
        "frequency": "r",
        "id": 955,
        "name": "sharpener",
        "synonyms": ["sharpener"],
        "synset": "sharpener.n.01"
    },
    {
        "def": "a pen with indelible ink that will write on any surface",
        "frequency": "r",
        "id": 956,
        "name": "Sharpie",
        "synonyms": ["Sharpie"],
        "synset": "sharpie.n.03"
    },
    {
        "def": "a razor powered by an electric motor",
        "frequency": "r",
        "id": 957,
        "name": "shaver_(electric)",
        "synonyms": ["shaver_(electric)", "electric_shaver", "electric_razor"],
        "synset": "shaver.n.03"
    },
    {
        "def": "toiletry consisting that forms a rich lather for softening the beard before "
               "shaving",
        "frequency": "c",
        "id": 958,
        "name": "shaving_cream",
        "synonyms": ["shaving_cream", "shaving_soap"],
        "synset": "shaving_cream.n.01"
    },
    {
        "def": "cloak consisting of an oblong piece of cloth used to cover the head and shoulders",
        "frequency": "r",
        "id": 959,
        "name": "shawl",
        "synonyms": ["shawl"],
        "synset": "shawl.n.01"
    },
    {
        "def": "large scissors with strong blades",
        "frequency": "r",
        "id": 960,
        "name": "shears",
        "synonyms": ["shears"],
        "synset": "shears.n.01"
    },
    {
        "def": "woolly usually horned ruminant mammal related to the goat",
        "frequency": "f",
        "id": 961,
        "name": "sheep",
        "synonyms": ["sheep"],
        "synset": "sheep.n.01"
    },
    {
        "def": "any of various usually long-haired breeds of dog reared to herd and guard sheep",
        "frequency": "r",
        "id": 962,
        "name": "shepherd_dog",
        "synonyms": ["shepherd_dog", "sheepdog"],
        "synset": "shepherd_dog.n.01"
    },
    {
        "def": "a frozen dessert made primarily of fruit juice and sugar",
        "frequency": "r",
        "id": 963,
        "name": "sherbert",
        "synonyms": ["sherbert", "sherbet"],
        "synset": "sherbert.n.01"
    },
    {
        "def": "armor carried on the arm to intercept blows",
        "frequency": "r",
        "id": 964,
        "name": "shield",
        "synonyms": ["shield"],
        "synset": "shield.n.02"
    },
    {
        "def": "a garment worn on the upper half of the body",
        "frequency": "f",
        "id": 965,
        "name": "shirt",
        "synonyms": ["shirt"],
        "synset": "shirt.n.01"
    },
    {
        "def": "common footwear covering the foot",
        "frequency": "f",
        "id": 966,
        "name": "shoe",
        "synonyms": ["shoe", "sneaker_(type_of_shoe)", "tennis_shoe"],
        "synset": "shoe.n.01"
    },
    {
        "def": "a bag made of plastic or strong paper (often with handles); used to transport "
               "goods after shopping",
        "frequency": "c",
        "id": 967,
        "name": "shopping_bag",
        "synonyms": ["shopping_bag"],
        "synset": "shopping_bag.n.01"
    },
    {
        "def": "a handcart that holds groceries or other goods while shopping",
        "frequency": "c",
        "id": 968,
        "name": "shopping_cart",
        "synonyms": ["shopping_cart"],
        "synset": "shopping_cart.n.01"
    },
    {
        "def": "trousers that end at or above the knee",
        "frequency": "f",
        "id": 969,
        "name": "short_pants",
        "synonyms": ["short_pants", "shorts_(clothing)", "trunks_(clothing)"],
        "synset": "short_pants.n.01"
    },
    {
        "def": "a small glass adequate to hold a single swallow of whiskey",
        "frequency": "r",
        "id": 970,
        "name": "shot_glass",
        "synonyms": ["shot_glass"],
        "synset": "shot_glass.n.01"
    },
    {
        "def": "a large handbag that can be carried by a strap looped over the shoulder",
        "frequency": "c",
        "id": 971,
        "name": "shoulder_bag",
        "synonyms": ["shoulder_bag"],
        "synset": "shoulder_bag.n.01"
    },
    {
        "def": "a hand tool for lifting loose material such as snow, dirt, etc.",
        "frequency": "c",
        "id": 972,
        "name": "shovel",
        "synonyms": ["shovel"],
        "synset": "shovel.n.01"
    },
    {
        "def": "a plumbing fixture that sprays water over you",
        "frequency": "f",
        "id": 973,
        "name": "shower_head",
        "synonyms": ["shower_head"],
        "synset": "shower.n.01"
    },
    {
        "def": "a curtain that keeps water from splashing out of the shower area",
        "frequency": "f",
        "id": 974,
        "name": "shower_curtain",
        "synonyms": ["shower_curtain"],
        "synset": "shower_curtain.n.01"
    },
    {
        "def": "a device that shreds documents",
        "frequency": "r",
        "id": 975,
        "name": "shredder_(for_paper)",
        "synonyms": ["shredder_(for_paper)"],
        "synset": "shredder.n.01"
    },
    {
        "def": "a strainer for separating lumps from powdered material or grading particles",
        "frequency": "r",
        "id": 976,
        "name": "sieve",
        "synonyms": ["sieve", "screen_(sieve)"],
        "synset": "sieve.n.01"
    },
    {
        "def": "structure displaying a board on which advertisements can be posted",
        "frequency": "f",
        "id": 977,
        "name": "signboard",
        "synonyms": ["signboard"],
        "synset": "signboard.n.01"
    },
    {
        "def": "a cylindrical tower used for storing goods",
        "frequency": "c",
        "id": 978,
        "name": "silo",
        "synonyms": ["silo"],
        "synset": "silo.n.01"
    },
    {
        "def": "plumbing fixture consisting of a water basin fixed to a wall or floor and having a "
               "drainpipe",
        "frequency": "f",
        "id": 979,
        "name": "sink",
        "synonyms": ["sink"],
        "synset": "sink.n.01"
    },
    {
        "def": "a board with wheels that is ridden in a standing or crouching position and "
               "propelled by foot",
        "frequency": "f",
        "id": 980,
        "name": "skateboard",
        "synonyms": ["skateboard"],
        "synset": "skateboard.n.01"
    },
    {
        "def": "a long pin for holding meat in position while it is being roasted",
        "frequency": "c",
        "id": 981,
        "name": "skewer",
        "synonyms": ["skewer"],
        "synset": "skewer.n.01"
    },
    {
        "def": "sports equipment for skiing on snow",
        "frequency": "f",
        "id": 982,
        "name": "ski",
        "synonyms": ["ski"],
        "synset": "ski.n.01"
    },
    {
        "def": "a stiff boot that is fastened to a ski with a ski binding",
        "frequency": "f",
        "id": 983,
        "name": "ski_boot",
        "synonyms": ["ski_boot"],
        "synset": "ski_boot.n.01"
    },
    {
        "def": "a parka to be worn while skiing",
        "frequency": "f",
        "id": 984,
        "name": "ski_parka",
        "synonyms": ["ski_parka", "ski_jacket"],
        "synset": "ski_parka.n.01"
    },
    {
        "def": "a pole with metal points used as an aid in skiing",
        "frequency": "f",
        "id": 985,
        "name": "ski_pole",
        "synonyms": ["ski_pole"],
        "synset": "ski_pole.n.01"
    },
    {
        "def": "a garment hanging from the waist; worn mainly by girls and women",
        "frequency": "f",
        "id": 986,
        "name": "skirt",
        "synonyms": ["skirt"],
        "synset": "skirt.n.02"
    },
    {
        "def": "a vehicle or flat object for transportation over snow by sliding or pulled by "
               "dogs, etc.",
        "frequency": "c",
        "id": 987,
        "name": "sled",
        "synonyms": ["sled", "sledge", "sleigh"],
        "synset": "sled.n.01"
    },
    {
        "def": "large padded bag designed to be slept in outdoors",
        "frequency": "c",
        "id": 988,
        "name": "sleeping_bag",
        "synonyms": ["sleeping_bag"],
        "synset": "sleeping_bag.n.01"
    },
    {
        "def": "bandage to support an injured forearm; slung over the shoulder or neck",
        "frequency": "r",
        "id": 989,
        "name": "sling_(bandage)",
        "synonyms": ["sling_(bandage)", "triangular_bandage"],
        "synset": "sling.n.05"
    },
    {
        "def": "low footwear that can be slipped on and off easily; usually worn indoors",
        "frequency": "c",
        "id": 990,
        "name": "slipper_(footwear)",
        "synonyms": ["slipper_(footwear)", "carpet_slipper_(footwear)"],
        "synset": "slipper.n.01"
    },
    {
        "def": "a thick smooth drink consisting of fresh fruit pureed with ice cream or yoghurt or "
               "milk",
        "frequency": "r",
        "id": 991,
        "name": "smoothie",
        "synonyms": ["smoothie"],
        "synset": "smoothie.n.02"
    },
    {
        "def": "limbless scaly elongate reptile; some are venomous",
        "frequency": "r",
        "id": 992,
        "name": "snake",
        "synonyms": ["snake", "serpent"],
        "synset": "snake.n.01"
    },
    {
        "def": "a board that resembles a broad ski or a small surfboard; used in a standing "
               "position to slide down snow-covered slopes",
        "frequency": "f",
        "id": 993,
        "name": "snowboard",
        "synonyms": ["snowboard"],
        "synset": "snowboard.n.01"
    },
    {
        "def": "a figure of a person made of packed snow",
        "frequency": "c",
        "id": 994,
        "name": "snowman",
        "synonyms": ["snowman"],
        "synset": "snowman.n.01"
    },
    {
        "def": "tracked vehicle for travel on snow having skis in front",
        "frequency": "c",
        "id": 995,
        "name": "snowmobile",
        "synonyms": ["snowmobile"],
        "synset": "snowmobile.n.01"
    },
    {
        "def": "a cleansing agent made from the salts of vegetable or animal fats",
        "frequency": "f",
        "id": 996,
        "name": "soap",
        "synonyms": ["soap"],
        "synset": "soap.n.01"
    },
    {
        "def": "an inflated ball used in playing soccer (called `football' outside of the United "
               "States)",
        "frequency": "f",
        "id": 997,
        "name": "soccer_ball",
        "synonyms": ["soccer_ball"],
        "synset": "soccer_ball.n.01"
    },
    {
        "def": "cloth covering for the foot; worn inside the shoe; reaches to between the ankle "
               "and the knee",
        "frequency": "f",
        "id": 998,
        "name": "sock",
        "synonyms": ["sock"],
        "synset": "sock.n.01"
    },
    {
        "def": "an apparatus for dispensing soda water",
        "frequency": "r",
        "id": 999,
        "name": "soda_fountain",
        "synonyms": ["soda_fountain"],
        "synset": "soda_fountain.n.02"
    },
    {
        "def": "effervescent beverage artificially charged with carbon dioxide",
        "frequency": "r",
        "id": 1000,
        "name": "carbonated_water",
        "synonyms": ["carbonated_water", "club_soda", "seltzer", "sparkling_water"],
        "synset": "soda_water.n.01"
    },
    {
        "def": "an upholstered seat for more than one person",
        "frequency": "f",
        "id": 1001,
        "name": "sofa",
        "synonyms": ["sofa", "couch", "lounge"],
        "synset": "sofa.n.01"
    },
    {
        "def": "ball used in playing softball",
        "frequency": "r",
        "id": 1002,
        "name": "softball",
        "synonyms": ["softball"],
        "synset": "softball.n.01"
    },
    {
        "def": "electrical device consisting of a large array of connected solar cells",
        "frequency": "c",
        "id": 1003,
        "name": "solar_array",
        "synonyms": ["solar_array", "solar_battery", "solar_panel"],
        "synset": "solar_array.n.01"
    },
    {
        "def": "a straw hat with a tall crown and broad brim; worn in American southwest and in "
               "Mexico",
        "frequency": "r",
        "id": 1004,
        "name": "sombrero",
        "synonyms": ["sombrero"],
        "synset": "sombrero.n.02"
    },
    {
        "def": "liquid food especially of meat or fish or vegetable stock often containing pieces "
               "of solid food",
        "frequency": "c",
        "id": 1005,
        "name": "soup",
        "synonyms": ["soup"],
        "synset": "soup.n.01"
    },
    {
        "def": "a bowl for serving soup",
        "frequency": "r",
        "id": 1006,
        "name": "soup_bowl",
        "synonyms": ["soup_bowl"],
        "synset": "soup_bowl.n.01"
    },
    {
        "def": "a spoon with a rounded bowl for eating soup",
        "frequency": "c",
        "id": 1007,
        "name": "soupspoon",
        "synonyms": ["soupspoon"],
        "synset": "soupspoon.n.01"
    },
    {
        "def": "soured light cream",
        "frequency": "c",
        "id": 1008,
        "name": "sour_cream",
        "synonyms": ["sour_cream", "soured_cream"],
        "synset": "sour_cream.n.01"
    },
    {
        "def": "a milk substitute containing soybean flour and water; used in some infant formulas "
               "and in making tofu",
        "frequency": "r",
        "id": 1009,
        "name": "soya_milk",
        "synonyms": ["soya_milk", "soybean_milk", "soymilk"],
        "synset": "soya_milk.n.01"
    },
    {
        "def": "a reusable spacecraft with wings for a controlled descent through the Earth's "
               "atmosphere",
        "frequency": "r",
        "id": 1010,
        "name": "space_shuttle",
        "synonyms": ["space_shuttle"],
        "synset": "space_shuttle.n.01"
    },
    {
        "def": "a firework that burns slowly and throws out a shower of sparks",
        "frequency": "r",
        "id": 1011,
        "name": "sparkler_(fireworks)",
        "synonyms": ["sparkler_(fireworks)"],
        "synset": "sparkler.n.02"
    },
    {
        "def": "a hand tool with a thin flexible blade used to mix or spread soft substances",
        "frequency": "f",
        "id": 1012,
        "name": "spatula",
        "synonyms": ["spatula"],
        "synset": "spatula.n.02"
    },
    {
        "def": "a long pointed rod used as a tool or weapon",
        "frequency": "r",
        "id": 1013,
        "name": "spear",
        "synonyms": ["spear", "lance"],
        "synset": "spear.n.01"
    },
    {
        "def": "optical instrument consisting of a frame that holds a pair of lenses for "
               "correcting defective vision",
        "frequency": "f",
        "id": 1014,
        "name": "spectacles",
        "synonyms": ["spectacles", "specs", "eyeglasses", "glasses"],
        "synset": "spectacles.n.01"
    },
    {
        "def": "a rack for displaying containers filled with spices",
        "frequency": "c",
        "id": 1015,
        "name": "spice_rack",
        "synonyms": ["spice_rack"],
        "synset": "spice_rack.n.01"
    },
    {
        "def": "predatory arachnid with eight legs, two poison fangs, two feelers, and usually two "
               "silk-spinning organs at the back end of the body",
        "frequency": "r",
        "id": 1016,
        "name": "spider",
        "synonyms": ["spider"],
        "synset": "spider.n.01"
    },
    {
        "def": "a porous mass usable to absorb water typically used for cleaning",
        "frequency": "c",
        "id": 1017,
        "name": "sponge",
        "synonyms": ["sponge"],
        "synset": "sponge.n.01"
    },
    {
        "def": "a piece of cutlery with a shallow bowl-shaped container and a handle",
        "frequency": "f",
        "id": 1018,
        "name": "spoon",
        "synonyms": ["spoon"],
        "synset": "spoon.n.01"
    },
    {
        "def": "attire worn for sport or for casual wear",
        "frequency": "c",
        "id": 1019,
        "name": "sportswear",
        "synonyms": ["sportswear", "athletic_wear", "activewear"],
        "synset": "sportswear.n.01"
    },
    {
        "def": "a lamp that produces a strong beam of light to illuminate a restricted area; used "
               "to focus attention of a stage performer",
        "frequency": "c",
        "id": 1020,
        "name": "spotlight",
        "synonyms": ["spotlight"],
        "synset": "spotlight.n.02"
    },
    {
        "def": "a kind of arboreal rodent having a long bushy tail",
        "frequency": "r",
        "id": 1021,
        "name": "squirrel",
        "synonyms": ["squirrel"],
        "synset": "squirrel.n.01"
    },
    {
        "def": "a machine that inserts staples into sheets of paper in order to fasten them "
               "together",
        "frequency": "c",
        "id": 1022,
        "name": "stapler_(stapling_machine)",
        "synonyms": ["stapler_(stapling_machine)"],
        "synset": "stapler.n.01"
    },
    {
        "def": "echinoderms characterized by five arms extending from a central disk",
        "frequency": "r",
        "id": 1023,
        "name": "starfish",
        "synonyms": ["starfish", "sea_star"],
        "synset": "starfish.n.01"
    },
    {
        "def": "a sculpture representing a human or animal",
        "frequency": "f",
        "id": 1024,
        "name": "statue_(sculpture)",
        "synonyms": ["statue_(sculpture)"],
        "synset": "statue.n.01"
    },
    {
        "def": "a slice of meat cut from the fleshy part of an animal or large fish",
        "frequency": "c",
        "id": 1025,
        "name": "steak_(food)",
        "synonyms": ["steak_(food)"],
        "synset": "steak.n.01"
    },
    {
        "def": "a sharp table knife used in eating steak",
        "frequency": "r",
        "id": 1026,
        "name": "steak_knife",
        "synonyms": ["steak_knife"],
        "synset": "steak_knife.n.01"
    },
    {
        "def": "a cooking utensil that can be used to cook food by steaming it",
        "frequency": "r",
        "id": 1027,
        "name": "steamer_(kitchen_appliance)",
        "synonyms": ["steamer_(kitchen_appliance)"],
        "synset": "steamer.n.02"
    },
    {
        "def": "a handwheel that is used for steering",
        "frequency": "f",
        "id": 1028,
        "name": "steering_wheel",
        "synonyms": ["steering_wheel"],
        "synset": "steering_wheel.n.01"
    },
    {
        "def": "a sheet of material (metal, plastic, etc.) that has been perforated with a "
               "pattern; ink or paint can pass through the perforations to create the printed "
               "pattern on the surface below",
        "frequency": "r",
        "id": 1029,
        "name": "stencil",
        "synonyms": ["stencil"],
        "synset": "stencil.n.01"
    },
    {
        "def": "a folding portable ladder hinged at the top",
        "frequency": "r",
        "id": 1030,
        "name": "stepladder",
        "synonyms": ["stepladder"],
        "synset": "step_ladder.n.01"
    },
    {
        "def": "a stool that has one or two steps that fold under the seat",
        "frequency": "c",
        "id": 1031,
        "name": "step_stool",
        "synonyms": ["step_stool"],
        "synset": "step_stool.n.01"
    },
    {
        "def": "electronic device for playing audio",
        "frequency": "c",
        "id": 1032,
        "name": "stereo_(sound_system)",
        "synonyms": ["stereo_(sound_system)"],
        "synset": "stereo.n.01"
    },
    {
        "def": "food prepared by stewing especially meat or fish with vegetables",
        "frequency": "r",
        "id": 1033,
        "name": "stew",
        "synonyms": ["stew"],
        "synset": "stew.n.02"
    },
    {
        "def": "an implement used for stirring",
        "frequency": "r",
        "id": 1034,
        "name": "stirrer",
        "synonyms": ["stirrer"],
        "synset": "stirrer.n.02"
    },
    {
        "def": "support consisting of metal loops into which rider's feet go",
        "frequency": "f",
        "id": 1035,
        "name": "stirrup",
        "synonyms": ["stirrup"],
        "synset": "stirrup.n.01"
    },
    {
        "def": "close-fitting hosiery to cover the foot and leg; come in matched pairs",
        "frequency": "c",
        "id": 1036,
        "name": "stockings_(leg_wear)",
        "synonyms": ["stockings_(leg_wear)"],
        "synset": "stocking.n.01"
    },
    {
        "def": "a simple seat without a back or arms",
        "frequency": "f",
        "id": 1037,
        "name": "stool",
        "synonyms": ["stool"],
        "synset": "stool.n.01"
    },
    {
        "def": "a traffic sign to notify drivers that they must come to a complete stop",
        "frequency": "f",
        "id": 1038,
        "name": "stop_sign",
        "synonyms": ["stop_sign"],
        "synset": "stop_sign.n.01"
    },
    {
        "def": "a red light on the rear of a motor vehicle that signals when the brakes are "
               "applied",
        "frequency": "f",
        "id": 1039,
        "name": "brake_light",
        "synonyms": ["brake_light"],
        "synset": "stoplight.n.01"
    },
    {
        "def": "a kitchen appliance used for cooking food",
        "frequency": "f",
        "id": 1040,
        "name": "stove",
        "synonyms": ["stove", "kitchen_stove", "range_(kitchen_appliance)", "kitchen_range",
                     "cooking_stove"],
        "synset": "stove.n.01"
    },
    {
        "def": "a filter to retain larger pieces while smaller pieces and liquids pass through",
        "frequency": "c",
        "id": 1041,
        "name": "strainer",
        "synonyms": ["strainer"],
        "synset": "strainer.n.01"
    },
    {
        "def": "an elongated strip of material for binding things together or holding",
        "frequency": "f",
        "id": 1042,
        "name": "strap",
        "synonyms": ["strap"],
        "synset": "strap.n.01"
    },
    {
        "def": "a thin paper or plastic tube used to suck liquids into the mouth",
        "frequency": "f",
        "id": 1043,
        "name": "straw_(for_drinking)",
        "synonyms": ["straw_(for_drinking)", "drinking_straw"],
        "synset": "straw.n.04"
    },
    {
        "def": "sweet fleshy red fruit",
        "frequency": "f",
        "id": 1044,
        "name": "strawberry",
        "synonyms": ["strawberry"],
        "synset": "strawberry.n.01"
    },
    {
        "def": "a sign visible from the street",
        "frequency": "f",
        "id": 1045,
        "name": "street_sign",
        "synonyms": ["street_sign"],
        "synset": "street_sign.n.01"
    },
    {
        "def": "a lamp supported on a lamppost; for illuminating a street",
        "frequency": "f",
        "id": 1046,
        "name": "streetlight",
        "synonyms": ["streetlight", "street_lamp"],
        "synset": "streetlight.n.01"
    },
    {
        "def": "cheese formed in long strings twisted together",
        "frequency": "r",
        "id": 1047,
        "name": "string_cheese",
        "synonyms": ["string_cheese"],
        "synset": "string_cheese.n.01"
    },
    {
        "def": "a pointed tool for writing or drawing or engraving",
        "frequency": "r",
        "id": 1048,
        "name": "stylus",
        "synonyms": ["stylus"],
        "synset": "stylus.n.02"
    },
    {
        "def": "a loudspeaker that is designed to reproduce very low bass frequencies",
        "frequency": "r",
        "id": 1049,
        "name": "subwoofer",
        "synonyms": ["subwoofer"],
        "synset": "subwoofer.n.01"
    },
    {
        "def": "a dish in which sugar is served",
        "frequency": "r",
        "id": 1050,
        "name": "sugar_bowl",
        "synonyms": ["sugar_bowl"],
        "synset": "sugar_bowl.n.01"
    },
    {
        "def": "juicy canes whose sap is a source of molasses and commercial sugar; fresh canes "
               "are sometimes chewed for the juice",
        "frequency": "r",
        "id": 1051,
        "name": "sugarcane_(plant)",
        "synonyms": ["sugarcane_(plant)"],
        "synset": "sugarcane.n.01"
    },
    {
        "def": "a set of garments (usually including a jacket and trousers or skirt) for outerwear "
               "all of the same fabric and color",
        "frequency": "c",
        "id": 1052,
        "name": "suit_(clothing)",
        "synonyms": ["suit_(clothing)"],
        "synset": "suit.n.01"
    },
    {
        "def": "any plant of the genus Helianthus having large flower heads with dark disk florets "
               "and showy yellow rays",
        "frequency": "c",
        "id": 1053,
        "name": "sunflower",
        "synonyms": ["sunflower"],
        "synset": "sunflower.n.01"
    },
    {
        "def": "spectacles that are darkened or polarized to protect the eyes from the glare of "
               "the sun",
        "frequency": "f",
        "id": 1054,
        "name": "sunglasses",
        "synonyms": ["sunglasses"],
        "synset": "sunglasses.n.01"
    },
    {
        "def": "a hat with a broad brim that protects the face from direct exposure to the sun",
        "frequency": "c",
        "id": 1055,
        "name": "sunhat",
        "synonyms": ["sunhat"],
        "synset": "sunhat.n.01"
    },
    {
        "def": "a cream spread on the skin; contains a chemical to filter out ultraviolet light "
               "and so protect from sunburn",
        "frequency": "r",
        "id": 1056,
        "name": "sunscreen",
        "synonyms": ["sunscreen", "sunblock"],
        "synset": "sunscreen.n.01"
    },
    {
        "def": "a narrow buoyant board for riding surf",
        "frequency": "f",
        "id": 1057,
        "name": "surfboard",
        "synonyms": ["surfboard"],
        "synset": "surfboard.n.01"
    },
    {
        "def": "rice (with raw fish) wrapped in seaweed",
        "frequency": "c",
        "id": 1058,
        "name": "sushi",
        "synonyms": ["sushi"],
        "synset": "sushi.n.01"
    },
    {
        "def": "cleaning implement consisting of absorbent material fastened to a handle; for "
               "cleaning floors",
        "frequency": "c",
        "id": 1059,
        "name": "mop",
        "synonyms": ["mop"],
        "synset": "swab.n.02"
    },
    {
        "def": "loose-fitting trousers with elastic cuffs; worn by athletes",
        "frequency": "c",
        "id": 1060,
        "name": "sweat_pants",
        "synonyms": ["sweat_pants"],
        "synset": "sweat_pants.n.01"
    },
    {
        "def": "a band of material tied around the forehead or wrist to absorb sweat",
        "frequency": "c",
        "id": 1061,
        "name": "sweatband",
        "synonyms": ["sweatband"],
        "synset": "sweatband.n.02"
    },
    {
        "def": "a crocheted or knitted garment covering the upper part of the body",
        "frequency": "f",
        "id": 1062,
        "name": "sweater",
        "synonyms": ["sweater"],
        "synset": "sweater.n.01"
    },
    {
        "def": "cotton knit pullover with long sleeves worn during athletic activity",
        "frequency": "f",
        "id": 1063,
        "name": "sweatshirt",
        "synonyms": ["sweatshirt"],
        "synset": "sweatshirt.n.01"
    },
    {
        "def": "the edible tuberous root of the sweet potato vine",
        "frequency": "c",
        "id": 1064,
        "name": "sweet_potato",
        "synonyms": ["sweet_potato"],
        "synset": "sweet_potato.n.02"
    },
    {
        "def": "garment worn for swimming",
        "frequency": "f",
        "id": 1065,
        "name": "swimsuit",
        "synonyms": ["swimsuit", "swimwear", "bathing_suit", "swimming_costume",
                     "bathing_costume", "swimming_trunks", "bathing_trunks"],
        "synset": "swimsuit.n.01"
    },
    {
        "def": "a cutting or thrusting weapon that has a long metal blade",
        "frequency": "c",
        "id": 1066,
        "name": "sword",
        "synonyms": ["sword"],
        "synset": "sword.n.01"
    },
    {
        "def": "a medical instrument used to inject or withdraw fluids",
        "frequency": "r",
        "id": 1067,
        "name": "syringe",
        "synonyms": ["syringe"],
        "synset": "syringe.n.01"
    },
    {
        "def": "very spicy sauce (trade name Tabasco) made from fully-aged red peppers",
        "frequency": "r",
        "id": 1068,
        "name": "Tabasco_sauce",
        "synonyms": ["Tabasco_sauce"],
        "synset": "tabasco.n.02"
    },
    {
        "def": "a table used for playing table tennis",
        "frequency": "r",
        "id": 1069,
        "name": "table-tennis_table",
        "synonyms": ["table-tennis_table", "ping-pong_table"],
        "synset": "table-tennis_table.n.01"
    },
    {
        "def": "a piece of furniture having a smooth flat top that is usually supported by one or "
               "more vertical legs",
        "frequency": "f",
        "id": 1070,
        "name": "table",
        "synonyms": ["table"],
        "synset": "table.n.02"
    },
    {
        "def": "a lamp that sits on a table",
        "frequency": "c",
        "id": 1071,
        "name": "table_lamp",
        "synonyms": ["table_lamp"],
        "synset": "table_lamp.n.01"
    },
    {
        "def": "a covering spread over a dining table",
        "frequency": "f",
        "id": 1072,
        "name": "tablecloth",
        "synonyms": ["tablecloth"],
        "synset": "tablecloth.n.01"
    },
    {
        "def": "measuring instrument for indicating speed of rotation",
        "frequency": "r",
        "id": 1073,
        "name": "tachometer",
        "synonyms": ["tachometer"],
        "synset": "tachometer.n.01"
    },
    {
        "def": "a small tortilla cupped around a filling",
        "frequency": "r",
        "id": 1074,
        "name": "taco",
        "synonyms": ["taco"],
        "synset": "taco.n.02"
    },
    {
        "def": "a label associated with something for the purpose of identification or information",
        "frequency": "f",
        "id": 1075,
        "name": "tag",
        "synonyms": ["tag"],
        "synset": "tag.n.02"
    },
    {
        "def": "lamp (usually red) mounted at the rear of a motor vehicle",
        "frequency": "f",
        "id": 1076,
        "name": "taillight",
        "synonyms": ["taillight", "rear_light"],
        "synset": "taillight.n.01"
    },
    {
        "def": "a shallow drum with a single drumhead and with metallic disks in the sides",
        "frequency": "r",
        "id": 1077,
        "name": "tambourine",
        "synonyms": ["tambourine"],
        "synset": "tambourine.n.01"
    },
    {
        "def": "an enclosed armored military vehicle; has a cannon and moves on caterpillar treads",
        "frequency": "r",
        "id": 1078,
        "name": "army_tank",
        "synonyms": ["army_tank", "armored_combat_vehicle", "armoured_combat_vehicle"],
        "synset": "tank.n.01"
    },
    {
        "def": "a large (usually metallic) vessel for holding gases or liquids",
        "frequency": "c",
        "id": 1079,
        "name": "tank_(storage_vessel)",
        "synonyms": ["tank_(storage_vessel)", "storage_tank"],
        "synset": "tank.n.02"
    },
    {
        "def": "a tight-fitting sleeveless shirt with wide shoulder straps and low neck and no "
               "front opening",
        "frequency": "f",
        "id": 1080,
        "name": "tank_top_(clothing)",
        "synonyms": ["tank_top_(clothing)"],
        "synset": "tank_top.n.01"
    },
    {
        "def": "a long thin piece of cloth or paper as used for binding or fastening",
        "frequency": "c",
        "id": 1081,
        "name": "tape_(sticky_cloth_or_paper)",
        "synonyms": ["tape_(sticky_cloth_or_paper)"],
        "synset": "tape.n.01"
    },
    {
        "def": "measuring instrument consisting of a narrow strip (cloth or metal) marked in "
               "inches or centimeters and used for measuring lengths",
        "frequency": "c",
        "id": 1082,
        "name": "tape_measure",
        "synonyms": ["tape_measure", "measuring_tape"],
        "synset": "tape.n.04"
    },
    {
        "def": "a heavy textile with a woven design; used for curtains and upholstery",
        "frequency": "c",
        "id": 1083,
        "name": "tapestry",
        "synonyms": ["tapestry"],
        "synset": "tapestry.n.02"
    },
    {
        "def": "waterproofed canvas",
        "frequency": "f",
        "id": 1084,
        "name": "tarp",
        "synonyms": ["tarp"],
        "synset": "tarpaulin.n.01"
    },
    {
        "def": "a cloth having a crisscross design",
        "frequency": "c",
        "id": 1085,
        "name": "tartan",
        "synonyms": ["tartan", "plaid"],
        "synset": "tartan.n.01"
    },
    {
        "def": "adornment consisting of a bunch of cords fastened at one end",
        "frequency": "c",
        "id": 1086,
        "name": "tassel",
        "synonyms": ["tassel"],
        "synset": "tassel.n.01"
    },
    {
        "def": "a measured amount of tea in a bag for an individual serving of tea",
        "frequency": "r",
        "id": 1087,
        "name": "tea_bag",
        "synonyms": ["tea_bag"],
        "synset": "tea_bag.n.01"
    },
    {
        "def": "a cup from which tea is drunk",
        "frequency": "c",
        "id": 1088,
        "name": "teacup",
        "synonyms": ["teacup"],
        "synset": "teacup.n.02"
    },
    {
        "def": "kettle for boiling water to make tea",
        "frequency": "c",
        "id": 1089,
        "name": "teakettle",
        "synonyms": ["teakettle"],
        "synset": "teakettle.n.01"
    },
    {
        "def": "pot for brewing tea; usually has a spout and handle",
        "frequency": "c",
        "id": 1090,
        "name": "teapot",
        "synonyms": ["teapot"],
        "synset": "teapot.n.01"
    },
    {
        "def": "plaything consisting of a child's toy bear (usually plush and stuffed with soft "
               "materials)",
        "frequency": "f",
        "id": 1091,
        "name": "teddy_bear",
        "synonyms": ["teddy_bear"],
        "synset": "teddy.n.01"
    },
    {
        "def": "electronic device for communicating by voice over long distances",
        "frequency": "f",
        "id": 1092,
        "name": "telephone",
        "synonyms": ["telephone", "phone", "telephone_set"],
        "synset": "telephone.n.01"
    },
    {
        "def": "booth for using a telephone",
        "frequency": "c",
        "id": 1093,
        "name": "telephone_booth",
        "synonyms": ["telephone_booth", "phone_booth", "call_box", "telephone_box",
                     "telephone_kiosk"],
        "synset": "telephone_booth.n.01"
    },
    {
        "def": "tall pole supporting telephone wires",
        "frequency": "f",
        "id": 1094,
        "name": "telephone_pole",
        "synonyms": ["telephone_pole", "telegraph_pole", "telegraph_post"],
        "synset": "telephone_pole.n.01"
    },
    {
        "def": "a camera lens that magnifies the image",
        "frequency": "r",
        "id": 1095,
        "name": "telephoto_lens",
        "synonyms": ["telephoto_lens", "zoom_lens"],
        "synset": "telephoto_lens.n.01"
    },
    {
        "def": "television equipment for capturing and recording video",
        "frequency": "c",
        "id": 1096,
        "name": "television_camera",
        "synonyms": ["television_camera", "tv_camera"],
        "synset": "television_camera.n.01"
    },
    {
        "def": "an electronic device that receives television signals and displays them on a "
               "screen",
        "frequency": "f",
        "id": 1097,
        "name": "television_set",
        "synonyms": ["television_set", "tv", "tv_set"],
        "synset": "television_receiver.n.01"
    },
    {
        "def": "ball about the size of a fist used in playing tennis",
        "frequency": "f",
        "id": 1098,
        "name": "tennis_ball",
        "synonyms": ["tennis_ball"],
        "synset": "tennis_ball.n.01"
    },
    {
        "def": "a racket used to play tennis",
        "frequency": "f",
        "id": 1099,
        "name": "tennis_racket",
        "synonyms": ["tennis_racket"],
        "synset": "tennis_racket.n.01"
    },
    {
        "def": "Mexican liquor made from fermented juices of an agave plant",
        "frequency": "r",
        "id": 1100,
        "name": "tequila",
        "synonyms": ["tequila"],
        "synset": "tequila.n.01"
    },
    {
        "def": "measuring instrument for measuring temperature",
        "frequency": "c",
        "id": 1101,
        "name": "thermometer",
        "synonyms": ["thermometer"],
        "synset": "thermometer.n.01"
    },
    {
        "def": "vacuum flask that preserves temperature of hot or cold drinks",
        "frequency": "c",
        "id": 1102,
        "name": "thermos_bottle",
        "synonyms": ["thermos_bottle"],
        "synset": "thermos.n.01"
    },
    {
        "def": "a regulator for automatically regulating temperature by starting or stopping the "
               "supply of heat",
        "frequency": "c",
        "id": 1103,
        "name": "thermostat",
        "synonyms": ["thermostat"],
        "synset": "thermostat.n.01"
    },
    {
        "def": "a small metal cap to protect the finger while sewing; can be used as a small "
               "container",
        "frequency": "r",
        "id": 1104,
        "name": "thimble",
        "synonyms": ["thimble"],
        "synset": "thimble.n.02"
    },
    {
        "def": "a fine cord of twisted fibers (of cotton or silk or wool or nylon etc.) used in "
               "sewing and weaving",
        "frequency": "c",
        "id": 1105,
        "name": "thread",
        "synonyms": ["thread", "yarn"],
        "synset": "thread.n.01"
    },
    {
        "def": "a tack for attaching papers to a bulletin board or drawing board",
        "frequency": "c",
        "id": 1106,
        "name": "thumbtack",
        "synonyms": ["thumbtack", "drawing_pin", "pushpin"],
        "synset": "thumbtack.n.01"
    },
    {
        "def": "a jeweled headdress worn by women on formal occasions",
        "frequency": "c",
        "id": 1107,
        "name": "tiara",
        "synonyms": ["tiara"],
        "synset": "tiara.n.01"
    },
    {
        "def": "large feline of forests in most of Asia having a tawny coat with black stripes",
        "frequency": "c",
        "id": 1108,
        "name": "tiger",
        "synonyms": ["tiger"],
        "synset": "tiger.n.02"
    },
    {
        "def": "skintight knit hose covering the body from the waist to the feet worn by acrobats "
               "and dancers and as stockings by women and girls",
        "frequency": "c",
        "id": 1109,
        "name": "tights_(clothing)",
        "synonyms": ["tights_(clothing)", "leotards"],
        "synset": "tights.n.01"
    },
    {
        "def": "a timepiece that measures a time interval and signals its end",
        "frequency": "c",
        "id": 1110,
        "name": "timer",
        "synonyms": ["timer", "stopwatch"],
        "synset": "timer.n.01"
    },
    {
        "def": "foil made of tin or an alloy of tin and lead",
        "frequency": "f",
        "id": 1111,
        "name": "tinfoil",
        "synonyms": ["tinfoil"],
        "synset": "tinfoil.n.01"
    },
    {
        "def": "a showy decoration that is basically valueless",
        "frequency": "r",
        "id": 1112,
        "name": "tinsel",
        "synonyms": ["tinsel"],
        "synset": "tinsel.n.01"
    },
    {
        "def": "a soft thin (usually translucent) paper",
        "frequency": "f",
        "id": 1113,
        "name": "tissue_paper",
        "synonyms": ["tissue_paper"],
        "synset": "tissue.n.02"
    },
    {
        "def": "slice of bread that has been toasted",
        "frequency": "c",
        "id": 1114,
        "name": "toast_(food)",
        "synonyms": ["toast_(food)"],
        "synset": "toast.n.01"
    },
    {
        "def": "a kitchen appliance (usually electric) for toasting bread",
        "frequency": "f",
        "id": 1115,
        "name": "toaster",
        "synonyms": ["toaster"],
        "synset": "toaster.n.02"
    },
    {
        "def": "kitchen appliance consisting of a small electric oven for toasting or warming food",
        "frequency": "c",
        "id": 1116,
        "name": "toaster_oven",
        "synonyms": ["toaster_oven"],
        "synset": "toaster_oven.n.01"
    },
    {
        "def": "a plumbing fixture for defecation and urination",
        "frequency": "f",
        "id": 1117,
        "name": "toilet",
        "synonyms": ["toilet"],
        "synset": "toilet.n.02"
    },
    {
        "def": "a soft thin absorbent paper for use in toilets",
        "frequency": "f",
        "id": 1118,
        "name": "toilet_tissue",
        "synonyms": ["toilet_tissue", "toilet_paper", "bathroom_tissue"],
        "synset": "toilet_tissue.n.01"
    },
    {
        "def": "mildly acid red or yellow pulpy fruit eaten as a vegetable",
        "frequency": "f",
        "id": 1119,
        "name": "tomato",
        "synonyms": ["tomato"],
        "synset": "tomato.n.01"
    },
    {
        "def": "any of various devices for taking hold of objects; usually have two hinged legs "
               "with handles above and pointed hooks below",
        "frequency": "c",
        "id": 1120,
        "name": "tongs",
        "synonyms": ["tongs"],
        "synset": "tongs.n.01"
    },
    {
        "def": "a box or chest or cabinet for holding hand tools",
        "frequency": "c",
        "id": 1121,
        "name": "toolbox",
        "synonyms": ["toolbox"],
        "synset": "toolbox.n.01"
    },
    {
        "def": "small brush; has long handle; used to clean teeth",
        "frequency": "f",
        "id": 1122,
        "name": "toothbrush",
        "synonyms": ["toothbrush"],
        "synset": "toothbrush.n.01"
    },
    {
        "def": "a dentifrice in the form of a paste",
        "frequency": "f",
        "id": 1123,
        "name": "toothpaste",
        "synonyms": ["toothpaste"],
        "synset": "toothpaste.n.01"
    },
    {
        "def": "pick consisting of a small strip of wood or plastic; used to pick food from "
               "between the teeth",
        "frequency": "c",
        "id": 1124,
        "name": "toothpick",
        "synonyms": ["toothpick"],
        "synset": "toothpick.n.01"
    },
    {
        "def": "covering for a hole (especially a hole in the top of a container)",
        "frequency": "c",
        "id": 1125,
        "name": "cover",
        "synonyms": ["cover"],
        "synset": "top.n.09"
    },
    {
        "def": "thin unleavened pancake made from cornmeal or wheat flour",
        "frequency": "c",
        "id": 1126,
        "name": "tortilla",
        "synonyms": ["tortilla"],
        "synset": "tortilla.n.01"
    },
    {
        "def": "a truck equipped to hoist and pull wrecked cars (or to remove cars from no-parking "
               "zones)",
        "frequency": "c",
        "id": 1127,
        "name": "tow_truck",
        "synonyms": ["tow_truck"],
        "synset": "tow_truck.n.01"
    },
    {
        "def": "a rectangular piece of absorbent cloth (or paper) for drying or wiping",
        "frequency": "f",
        "id": 1128,
        "name": "towel",
        "synonyms": ["towel"],
        "synset": "towel.n.01"
    },
    {
        "def": "a rack consisting of one or more bars on which towels can be hung",
        "frequency": "f",
        "id": 1129,
        "name": "towel_rack",
        "synonyms": ["towel_rack", "towel_rail", "towel_bar"],
        "synset": "towel_rack.n.01"
    },
    {
        "def": "a device regarded as providing amusement",
        "frequency": "f",
        "id": 1130,
        "name": "toy",
        "synonyms": ["toy"],
        "synset": "toy.n.03"
    },
    {
        "def": "a wheeled vehicle with large wheels; used in farming and other applications",
        "frequency": "c",
        "id": 1131,
        "name": "tractor_(farm_equipment)",
        "synonyms": ["tractor_(farm_equipment)"],
        "synset": "tractor.n.01"
    },
    {
        "def": "a device to control vehicle traffic often consisting of three or more lights",
        "frequency": "f",
        "id": 1132,
        "name": "traffic_light",
        "synonyms": ["traffic_light"],
        "synset": "traffic_light.n.01"
    },
    {
        "def": "a lightweight motorcycle equipped with rugged tires and suspension for off-road "
               "use",
        "frequency": "r",
        "id": 1133,
        "name": "dirt_bike",
        "synonyms": ["dirt_bike"],
        "synset": "trail_bike.n.01"
    },
    {
        "def": "a truck consisting of a tractor and trailer together",
        "frequency": "c",
        "id": 1134,
        "name": "trailer_truck",
        "synonyms": ["trailer_truck", "tractor_trailer", "trucking_rig", "articulated_lorry",
                     "semi_truck"],
        "synset": "trailer_truck.n.01"
    },
    {
        "def": "public or private transport provided by a line of railway cars coupled together "
               "and drawn by a locomotive",
        "frequency": "f",
        "id": 1135,
        "name": "train_(railroad_vehicle)",
        "synonyms": ["train_(railroad_vehicle)", "railroad_train"],
        "synset": "train.n.01"
    },
    {
        "def": "gymnastic apparatus consisting of a strong canvas sheet attached with springs to a "
               "metal frame",
        "frequency": "r",
        "id": 1136,
        "name": "trampoline",
        "synonyms": ["trampoline"],
        "synset": "trampoline.n.01"
    },
    {
        "def": "an open receptacle for holding or displaying or serving articles or food",
        "frequency": "f",
        "id": 1137,
        "name": "tray",
        "synonyms": ["tray"],
        "synset": "tray.n.01"
    },
    {
        "def": "(NOT A TREE) a PLAYHOUSE built in the branches of a tree",
        "frequency": "r",
        "id": 1138,
        "name": "tree_house",
        "synonyms": ["tree_house"],
        "synset": "tree_house.n.01"
    },
    {
        "def": "a military style raincoat; belted with deep pockets",
        "frequency": "r",
        "id": 1139,
        "name": "trench_coat",
        "synonyms": ["trench_coat"],
        "synset": "trench_coat.n.01"
    },
    {
        "def": "a percussion instrument consisting of a metal bar bent in the shape of an open "
               "triangle",
        "frequency": "r",
        "id": 1140,
        "name": "triangle_(musical_instrument)",
        "synonyms": ["triangle_(musical_instrument)"],
        "synset": "triangle.n.05"
    },
    {
        "def": "a vehicle with three wheels that is moved by foot pedals",
        "frequency": "r",
        "id": 1141,
        "name": "tricycle",
        "synonyms": ["tricycle"],
        "synset": "tricycle.n.01"
    },
    {
        "def": "a three-legged rack used for support",
        "frequency": "c",
        "id": 1142,
        "name": "tripod",
        "synonyms": ["tripod"],
        "synset": "tripod.n.01"
    },
    {
        "def": "a garment extending from the waist to the knee or ankle, covering each leg "
               "separately",
        "frequency": "f",
        "id": 1143,
        "name": "trousers",
        "synonyms": ["trousers", "pants_(clothing)"],
        "synset": "trouser.n.01"
    },
    {
        "def": "an automotive vehicle suitable for hauling",
        "frequency": "f",
        "id": 1144,
        "name": "truck",
        "synonyms": ["truck"],
        "synset": "truck.n.01"
    },
    {
        "def": "creamy chocolate candy",
        "frequency": "r",
        "id": 1145,
        "name": "truffle_(chocolate)",
        "synonyms": ["truffle_(chocolate)", "chocolate_truffle"],
        "synset": "truffle.n.03"
    },
    {
        "def": "luggage consisting of a large strong case used when traveling or for storage",
        "frequency": "c",
        "id": 1146,
        "name": "trunk",
        "synonyms": ["trunk"],
        "synset": "trunk.n.02"
    },
    {
        "def": "a large open vessel for holding or storing liquids",
        "frequency": "r",
        "id": 1147,
        "name": "vat",
        "synonyms": ["vat"],
        "synset": "tub.n.02"
    },
    {
        "def": "a traditional headdress consisting of a long scarf wrapped around the head",
        "frequency": "c",
        "id": 1148,
        "name": "turban",
        "synonyms": ["turban"],
        "synset": "turban.n.01"
    },
    {
        "def": "large gallinaceous bird with fan-shaped tail; widely domesticated for food",
        "frequency": "r",
        "id": 1149,
        "name": "turkey_(bird)",
        "synonyms": ["turkey_(bird)"],
        "synset": "turkey.n.01"
    },
    {
        "def": "flesh of large domesticated fowl usually roasted",
        "frequency": "c",
        "id": 1150,
        "name": "turkey_(food)",
        "synonyms": ["turkey_(food)"],
        "synset": "turkey.n.04"
    },
    {
        "def": "widely cultivated plant having a large fleshy edible white or yellow root",
        "frequency": "r",
        "id": 1151,
        "name": "turnip",
        "synonyms": ["turnip"],
        "synset": "turnip.n.01"
    },
    {
        "def": "any of various aquatic and land reptiles having a bony shell and flipper-like "
               "limbs for swimming",
        "frequency": "c",
        "id": 1152,
        "name": "turtle",
        "synonyms": ["turtle"],
        "synset": "turtle.n.02"
    },
    {
        "def": "a sweater or jersey with a high close-fitting collar",
        "frequency": "r",
        "id": 1153,
        "name": "turtleneck_(clothing)",
        "synonyms": ["turtleneck_(clothing)", "polo-neck"],
        "synset": "turtleneck.n.01"
    },
    {
        "def": "hand-operated character printer for printing written messages one character at a "
               "time",
        "frequency": "r",
        "id": 1154,
        "name": "typewriter",
        "synonyms": ["typewriter"],
        "synset": "typewriter.n.01"
    },
    {
        "def": "a lightweight handheld collapsible canopy",
        "frequency": "f",
        "id": 1155,
        "name": "umbrella",
        "synonyms": ["umbrella"],
        "synset": "umbrella.n.01"
    },
    {
        "def": "undergarment worn next to the skin and under the outer garments",
        "frequency": "c",
        "id": 1156,
        "name": "underwear",
        "synonyms": ["underwear", "underclothes", "underclothing", "underpants"],
        "synset": "underwear.n.01"
    },
    {
        "def": "a vehicle with a single wheel that is driven by pedals",
        "frequency": "r",
        "id": 1157,
        "name": "unicycle",
        "synonyms": ["unicycle"],
        "synset": "unicycle.n.01"
    },
    {
        "def": "a plumbing fixture (usually attached to the wall) used by men to urinate",
        "frequency": "c",
        "id": 1158,
        "name": "urinal",
        "synonyms": ["urinal"],
        "synset": "urinal.n.01"
    },
    {
        "def": "a large vase that usually has a pedestal or feet",
        "frequency": "r",
        "id": 1159,
        "name": "urn",
        "synonyms": ["urn"],
        "synset": "urn.n.01"
    },
    {
        "def": "an electrical home appliance that cleans by suction",
        "frequency": "c",
        "id": 1160,
        "name": "vacuum_cleaner",
        "synonyms": ["vacuum_cleaner"],
        "synset": "vacuum.n.04"
    },
    {
        "def": "control consisting of a mechanical device for controlling the flow of a fluid",
        "frequency": "c",
        "id": 1161,
        "name": "valve",
        "synonyms": ["valve"],
        "synset": "valve.n.03"
    },
    {
        "def": "an open jar of glass or porcelain used as an ornament or to hold flowers",
        "frequency": "f",
        "id": 1162,
        "name": "vase",
        "synonyms": ["vase"],
        "synset": "vase.n.01"
    },
    {
        "def": "a slot machine for selling goods",
        "frequency": "c",
        "id": 1163,
        "name": "vending_machine",
        "synonyms": ["vending_machine"],
        "synset": "vending_machine.n.01"
    },
    {
        "def": "a hole for the escape of gas or air",
        "frequency": "f",
        "id": 1164,
        "name": "vent",
        "synonyms": ["vent", "blowhole", "air_vent"],
        "synset": "vent.n.01"
    },
    {
        "def": "a video recording made on magnetic tape",
        "frequency": "c",
        "id": 1165,
        "name": "videotape",
        "synonyms": ["videotape"],
        "synset": "videotape.n.01"
    },
    {
        "def": "sour-tasting liquid produced usually by oxidation of the alcohol in wine or cider "
               "and used as a condiment or food preservative",
        "frequency": "r",
        "id": 1166,
        "name": "vinegar",
        "synonyms": ["vinegar"],
        "synset": "vinegar.n.01"
    },
    {
        "def": "bowed stringed instrument that is the highest member of the violin family",
        "frequency": "r",
        "id": 1167,
        "name": "violin",
        "synonyms": ["violin", "fiddle"],
        "synset": "violin.n.01"
    },
    {
        "def": "unaged colorless liquor originating in Russia",
        "frequency": "r",
        "id": 1168,
        "name": "vodka",
        "synonyms": ["vodka"],
        "synset": "vodka.n.01"
    },
    {
        "def": "an inflated ball used in playing volleyball",
        "frequency": "r",
        "id": 1169,
        "name": "volleyball",
        "synonyms": ["volleyball"],
        "synset": "volleyball.n.02"
    },
    {
        "def": "any of various large birds of prey having naked heads and weak claws and feeding "
               "chiefly on carrion",
        "frequency": "r",
        "id": 1170,
        "name": "vulture",
        "synonyms": ["vulture"],
        "synset": "vulture.n.01"
    },
    {
        "def": "pancake batter baked in a waffle iron",
        "frequency": "c",
        "id": 1171,
        "name": "waffle",
        "synonyms": ["waffle"],
        "synset": "waffle.n.01"
    },
    {
        "def": "a kitchen appliance for baking waffles",
        "frequency": "r",
        "id": 1172,
        "name": "waffle_iron",
        "synonyms": ["waffle_iron"],
        "synset": "waffle_iron.n.01"
    },
    {
        "def": "any of various kinds of wheeled vehicles drawn by an animal or a tractor",
        "frequency": "c",
        "id": 1173,
        "name": "wagon",
        "synonyms": ["wagon"],
        "synset": "wagon.n.01"
    },
    {
        "def": "a wheel of a wagon",
        "frequency": "c",
        "id": 1174,
        "name": "wagon_wheel",
        "synonyms": ["wagon_wheel"],
        "synset": "wagon_wheel.n.01"
    },
    {
        "def": "a stick carried in the hand for support in walking",
        "frequency": "c",
        "id": 1175,
        "name": "walking_stick",
        "synonyms": ["walking_stick"],
        "synset": "walking_stick.n.01"
    },
    {
        "def": "a clock mounted on a wall",
        "frequency": "c",
        "id": 1176,
        "name": "wall_clock",
        "synonyms": ["wall_clock"],
        "synset": "wall_clock.n.01"
    },
    {
        "def": "receptacle providing a place in a wiring system where current can be taken to run "
               "electrical devices",
        "frequency": "f",
        "id": 1177,
        "name": "wall_socket",
        "synonyms": ["wall_socket", "wall_plug", "electric_outlet", "electrical_outlet",
                     "outlet", "electric_receptacle"],
        "synset": "wall_socket.n.01"
    },
    {
        "def": "a pocket-size case for holding papers and paper money",
        "frequency": "c",
        "id": 1178,
        "name": "wallet",
        "synonyms": ["wallet", "billfold"],
        "synset": "wallet.n.01"
    },
    {
        "def": "either of two large northern marine mammals having ivory tusks and tough hide over "
               "thick blubber",
        "frequency": "r",
        "id": 1179,
        "name": "walrus",
        "synonyms": ["walrus"],
        "synset": "walrus.n.01"
    },
    {
        "def": "a tall piece of furniture that provides storage space for clothes; has a door and "
               "rails or hooks for hanging clothes",
        "frequency": "r",
        "id": 1180,
        "name": "wardrobe",
        "synonyms": ["wardrobe"],
        "synset": "wardrobe.n.01"
    },
    {
        "def": "the thick green root of the wasabi plant that the Japanese use in cooking and that "
               "tastes like strong horseradish",
        "frequency": "r",
        "id": 1181,
        "name": "wasabi",
        "synonyms": ["wasabi"],
        "synset": "wasabi.n.02"
    },
    {
        "def": "a home appliance for washing clothes and linens automatically",
        "frequency": "c",
        "id": 1182,
        "name": "automatic_washer",
        "synonyms": ["automatic_washer", "washing_machine"],
        "synset": "washer.n.03"
    },
    {
        "def": "a small, portable timepiece",
        "frequency": "f",
        "id": 1183,
        "name": "watch",
        "synonyms": ["watch", "wristwatch"],
        "synset": "watch.n.01"
    },
    {
        "def": "a bottle for holding water",
        "frequency": "f",
        "id": 1184,
        "name": "water_bottle",
        "synonyms": ["water_bottle"],
        "synset": "water_bottle.n.01"
    },
    {
        "def": "a device for cooling and dispensing drinking water",
        "frequency": "c",
        "id": 1185,
        "name": "water_cooler",
        "synonyms": ["water_cooler"],
        "synset": "water_cooler.n.01"
    },
    {
        "def": "a faucet for drawing water from a pipe or cask",
        "frequency": "c",
        "id": 1186,
        "name": "water_faucet",
        "synonyms": ["water_faucet", "water_tap", "tap_(water_faucet)"],
        "synset": "water_faucet.n.01"
    },
    {
        "def": "a filter to remove impurities from the water supply",
        "frequency": "r",
        "id": 1187,
        "name": "water_filter",
        "synonyms": ["water_filter"],
        "synset": "water_filter.n.01"
    },
    {
        "def": "a heater and storage tank to supply heated water",
        "frequency": "r",
        "id": 1188,
        "name": "water_heater",
        "synonyms": ["water_heater", "hot-water_heater"],
        "synset": "water_heater.n.01"
    },
    {
        "def": "a jug that holds water",
        "frequency": "r",
        "id": 1189,
        "name": "water_jug",
        "synonyms": ["water_jug"],
        "synset": "water_jug.n.01"
    },
    {
        "def": "plaything consisting of a toy pistol that squirts water",
        "frequency": "r",
        "id": 1190,
        "name": "water_gun",
        "synonyms": ["water_gun", "squirt_gun"],
        "synset": "water_pistol.n.01"
    },
    {
        "def": "a motorboat resembling a motor scooter (NOT A SURFBOARD OR WATER SKI)",
        "frequency": "c",
        "id": 1191,
        "name": "water_scooter",
        "synonyms": ["water_scooter", "sea_scooter", "jet_ski"],
        "synset": "water_scooter.n.01"
    },
    {
        "def": "broad ski for skimming over water towed by a speedboat (DO NOT MARK WATER)",
        "frequency": "c",
        "id": 1192,
        "name": "water_ski",
        "synonyms": ["water_ski"],
        "synset": "water_ski.n.01"
    },
    {
        "def": "a large reservoir for water",
        "frequency": "c",
        "id": 1193,
        "name": "water_tower",
        "synonyms": ["water_tower"],
        "synset": "water_tower.n.01"
    },
    {
        "def": "a container with a handle and a spout with a perforated nozzle; used to sprinkle "
               "water over plants",
        "frequency": "c",
        "id": 1194,
        "name": "watering_can",
        "synonyms": ["watering_can"],
        "synset": "watering_can.n.01"
    },
    {
        "def": "large oblong or roundish melon with a hard green rind and sweet watery red or "
               "occasionally yellowish pulp",
        "frequency": "c",
        "id": 1195,
        "name": "watermelon",
        "synonyms": ["watermelon"],
        "synset": "watermelon.n.02"
    },
    {
        "def": "mechanical device attached to an elevated structure; rotates freely to show the "
               "direction of the wind",
        "frequency": "f",
        "id": 1196,
        "name": "weathervane",
        "synonyms": ["weathervane", "vane_(weathervane)", "wind_vane"],
        "synset": "weathervane.n.01"
    },
    {
        "def": "a digital camera designed to take digital photographs and transmit them over the "
               "internet",
        "frequency": "c",
        "id": 1197,
        "name": "webcam",
        "synonyms": ["webcam"],
        "synset": "webcam.n.01"
    },
    {
        "def": "a rich cake with two or more tiers and covered with frosting and decorations; "
               "served at a wedding reception",
        "frequency": "c",
        "id": 1198,
        "name": "wedding_cake",
        "synonyms": ["wedding_cake", "bridecake"],
        "synset": "wedding_cake.n.01"
    },
    {
        "def": "a ring given to the bride and/or groom at the wedding",
        "frequency": "c",
        "id": 1199,
        "name": "wedding_ring",
        "synonyms": ["wedding_ring", "wedding_band"],
        "synset": "wedding_ring.n.01"
    },
    {
        "def": "a close-fitting garment made of a permeable material; worn in cold water to retain "
               "body heat",
        "frequency": "f",
        "id": 1200,
        "name": "wet_suit",
        "synonyms": ["wet_suit"],
        "synset": "wet_suit.n.01"
    },
    {
        "def": "a circular frame with spokes (or a solid disc) that can rotate on a shaft or axle",
        "frequency": "f",
        "id": 1201,
        "name": "wheel",
        "synonyms": ["wheel"],
        "synset": "wheel.n.01"
    },
    {
        "def": "a movable chair mounted on large wheels",
        "frequency": "c",
        "id": 1202,
        "name": "wheelchair",
        "synonyms": ["wheelchair"],
        "synset": "wheelchair.n.01"
    },
    {
        "def": "cream that has been beaten until light and fluffy",
        "frequency": "c",
        "id": 1203,
        "name": "whipped_cream",
        "synonyms": ["whipped_cream"],
        "synset": "whipped_cream.n.01"
    },
    {
        "def": "a liquor made from fermented mash of grain",
        "frequency": "r",
        "id": 1204,
        "name": "whiskey",
        "synonyms": ["whiskey"],
        "synset": "whiskey.n.01"
    },
    {
        "def": "a small wind instrument that produces a whistling sound by blowing into it",
        "frequency": "r",
        "id": 1205,
        "name": "whistle",
        "synonyms": ["whistle"],
        "synset": "whistle.n.03"
    },
    {
        "def": "a loosely woven cord in a candle or oil lamp that is lit on fire",
        "frequency": "r",
        "id": 1206,
        "name": "wick",
        "synonyms": ["wick"],
        "synset": "wick.n.02"
    },
    {
        "def": "hairpiece covering the head and made of real or synthetic hair",
        "frequency": "c",
        "id": 1207,
        "name": "wig",
        "synonyms": ["wig"],
        "synset": "wig.n.01"
    },
    {
        "def": "a decorative arrangement of pieces of metal or glass or pottery that hang together "
               "loosely so the wind can cause them to tinkle",
        "frequency": "c",
        "id": 1208,
        "name": "wind_chime",
        "synonyms": ["wind_chime"],
        "synset": "wind_chime.n.01"
    },
    {
        "def": "a mill that is powered by the wind",
        "frequency": "c",
        "id": 1209,
        "name": "windmill",
        "synonyms": ["windmill"],
        "synset": "windmill.n.01"
    },
    {
        "def": "a container for growing plants on a windowsill",
        "frequency": "c",
        "id": 1210,
        "name": "window_box_(for_plants)",
        "synonyms": ["window_box_(for_plants)"],
        "synset": "window_box.n.01"
    },
    {
        "def": "a mechanical device that cleans the windshield",
        "frequency": "f",
        "id": 1211,
        "name": "windshield_wiper",
        "synonyms": ["windshield_wiper", "windscreen_wiper", "wiper_(for_windshield/screen)"],
        "synset": "windshield_wiper.n.01"
    },
    {
        "def": "a truncated cloth cone mounted on a mast/pole; shows wind direction",
        "frequency": "c",
        "id": 1212,
        "name": "windsock",
        "synonyms": ["windsock", "air_sock", "air-sleeve", "wind_sleeve", "wind_cone"],
        "synset": "windsock.n.01"
    },
    {
        "def": "a bottle for holding wine",
        "frequency": "f",
        "id": 1213,
        "name": "wine_bottle",
        "synonyms": ["wine_bottle"],
        "synset": "wine_bottle.n.01"
    },
    {
        "def": "a bucket of ice used to chill a bottle of wine",
        "frequency": "r",
        "id": 1214,
        "name": "wine_bucket",
        "synonyms": ["wine_bucket", "wine_cooler"],
        "synset": "wine_bucket.n.01"
    },
    {
        "def": "a glass that has a stem and in which wine is served",
        "frequency": "f",
        "id": 1215,
        "name": "wineglass",
        "synonyms": ["wineglass"],
        "synset": "wineglass.n.01"
    },
    {
        "def": "easy chair having wings on each side of a high back",
        "frequency": "r",
        "id": 1216,
        "name": "wing_chair",
        "synonyms": ["wing_chair"],
        "synset": "wing_chair.n.01"
    },
    {
        "def": "blinds that prevent a horse from seeing something on either side",
        "frequency": "c",
        "id": 1217,
        "name": "blinder_(for_horses)",
        "synonyms": ["blinder_(for_horses)"],
        "synset": "winker.n.02"
    },
    {
        "def": "pan with a convex bottom; used for frying in Chinese cooking",
        "frequency": "c",
        "id": 1218,
        "name": "wok",
        "synonyms": ["wok"],
        "synset": "wok.n.01"
    },
    {
        "def": "a wild carnivorous mammal of the dog family, living and hunting in packs",
        "frequency": "r",
        "id": 1219,
        "name": "wolf",
        "synonyms": ["wolf"],
        "synset": "wolf.n.01"
    },
    {
        "def": "a spoon made of wood",
        "frequency": "c",
        "id": 1220,
        "name": "wooden_spoon",
        "synonyms": ["wooden_spoon"],
        "synset": "wooden_spoon.n.02"
    },
    {
        "def": "an arrangement of flowers, leaves, or stems fastened in a ring",
        "frequency": "c",
        "id": 1221,
        "name": "wreath",
        "synonyms": ["wreath"],
        "synset": "wreath.n.01"
    },
    {
        "def": "a hand tool that is used to hold or twist a nut or bolt",
        "frequency": "c",
        "id": 1222,
        "name": "wrench",
        "synonyms": ["wrench", "spanner"],
        "synset": "wrench.n.03"
    },
    {
        "def": "band consisting of a part of a sleeve that covers the wrist",
        "frequency": "c",
        "id": 1223,
        "name": "wristband",
        "synonyms": ["wristband"],
        "synset": "wristband.n.01"
    },
    {
        "def": "a band or bracelet worn around the wrist",
        "frequency": "f",
        "id": 1224,
        "name": "wristlet",
        "synonyms": ["wristlet", "wrist_band"],
        "synset": "wristlet.n.01"
    },
    {
        "def": "an expensive vessel propelled by sail or power and used for cruising or racing",
        "frequency": "r",
        "id": 1225,
        "name": "yacht",
        "synonyms": ["yacht"],
        "synset": "yacht.n.01"
    },
    {
        "def": "large long-haired wild ox of Tibet often domesticated",
        "frequency": "r",
        "id": 1226,
        "name": "yak",
        "synonyms": ["yak"],
        "synset": "yak.n.02"
    },
    {
        "def": "a custard-like food made from curdled milk",
        "frequency": "c",
        "id": 1227,
        "name": "yogurt",
        "synonyms": ["yogurt", "yoghurt", "yoghourt"],
        "synset": "yogurt.n.01"
    },
    {
        "def": "gear joining two animals at the neck; NOT egg yolk",
        "frequency": "r",
        "id": 1228,
        "name": "yoke_(animal_equipment)",
        "synonyms": ["yoke_(animal_equipment)"],
        "synset": "yoke.n.07"
    },
    {
        "def": "any of several fleet black-and-white striped African equines",
        "frequency": "f",
        "id": 1229,
        "name": "zebra",
        "synonyms": ["zebra"],
        "synset": "zebra.n.01"
    },
    {
        "def": "small cucumber-shaped vegetable marrow; typically dark green",
        "frequency": "c",
        "id": 1230,
        "name": "zucchini",
        "synonyms": ["zucchini", "courgette"],
        "synset": "zucchini.n.02"
    }
]
```

#### cvpods/data/datasets/objects365_categories.py

```python
# Autogen with
# import json
# import pprint
# a = json.load(open('objects365_categories.json'))
# string = pprint.pformat(a, indent=4).replace("'", "\"")
# string = string[:1] + "\n " + string[1:]
# string = string[:-1] + "\n]\n"
# OBJECTS365_CATEGORIES= string

OBJECTS365_CATEGORIES = [
    {"id": 1, "name": "person"},
    {"id": 2, "name": "sneakers"},
    {"id": 3, "name": "chair"},
    {"id": 4, "name": "hat"},
    {"id": 5, "name": "lamp"},
    {"id": 6, "name": "bottle"},
    {"id": 7, "name": "cabinet/shelf"},
    {"id": 8, "name": "cup"},
    {"id": 9, "name": "car"},
    {"id": 10, "name": "glasses"},
    {"id": 11, "name": "picture/frame"},
    {"id": 12, "name": "desk"},
    {"id": 13, "name": "handbag"},
    {"id": 14, "name": "street lights"},
    {"id": 15, "name": "book"},
    {"id": 16, "name": "plate"},
    {"id": 17, "name": "helmet"},
    {"id": 18, "name": "leather shoes"},
    {"id": 19, "name": "pillow"},
    {"id": 20, "name": "glove"},
    {"id": 21, "name": "potted plant"},
    {"id": 22, "name": "bracelet"},
    {"id": 23, "name": "flower"},
    {"id": 24, "name": "tv"},
    {"id": 25, "name": "storage box"},
    {"id": 26, "name": "vase"},
    {"id": 27, "name": "bench"},
    {"id": 28, "name": "wine glass"},
    {"id": 29, "name": "boots"},
    {"id": 30, "name": "bowl"},
    {"id": 31, "name": "dining table"},
    {"id": 32, "name": "umbrella"},
    {"id": 33, "name": "boat"},
    {"id": 34, "name": "flag"},
    {"id": 35, "name": "speaker"},
    {"id": 36, "name": "trash bin/can"},
    {"id": 37, "name": "stool"},
    {"id": 38, "name": "backpack"},
    {"id": 39, "name": "couch"},
    {"id": 40, "name": "belt"},
    {"id": 41, "name": "carpet"},
    {"id": 42, "name": "basket"},
    {"id": 43, "name": "towel/napkin"},
    {"id": 44, "name": "slippers"},
    {"id": 45, "name": "barrel/bucket"},
    {"id": 46, "name": "coffee table"},
    {"id": 47, "name": "suv"},
    {"id": 48, "name": "toy"},
    {"id": 49, "name": "tie"},
    {"id": 50, "name": "bed"},
    {"id": 51, "name": "traffic light"},
    {"id": 52, "name": "pen/pencil"},
    {"id": 53, "name": "microphone"},
    {"id": 54, "name": "sandals"},
    {"id": 55, "name": "canned"},
    {"id": 56, "name": "necklace"},
    {"id": 57, "name": "mirror"},
    {"id": 58, "name": "faucet"},
    {"id": 59, "name": "bicycle"},
    {"id": 60, "name": "bread"},
    {"id": 61, "name": "high heels"},
    {"id": 62, "name": "ring"},
    {"id": 63, "name": "van"},
    {"id": 64, "name": "watch"},
    {"id": 65, "name": "sink"},
    {"id": 66, "name": "horse"},
    {"id": 67, "name": "fish"},
    {"id": 68, "name": "apple"},
    {"id": 69, "name": "camera"},
    {"id": 70, "name": "candle"},
    {"id": 71, "name": "teddy bear"},
    {"id": 72, "name": "cake"},
    {"id": 73, "name": "motorcycle"},
    {"id": 74, "name": "wild bird"},
    {"id": 75, "name": "laptop"},
    {"id": 76, "name": "knife"},
    {"id": 77, "name": "traffic sign"},
    {"id": 78, "name": "cell phone"},
    {"id": 79, "name": "paddle"},
    {"id": 80, "name": "truck"},
    {"id": 81, "name": "cow"},
    {"id": 82, "name": "power outlet"},
    {"id": 83, "name": "clock"},
    {"id": 84, "name": "drum"},
    {"id": 85, "name": "fork"},
    {"id": 86, "name": "bus"},
    {"id": 87, "name": "hanger"},
    {"id": 88, "name": "nightstand"},
    {"id": 89, "name": "pot/pan"},
    {"id": 90, "name": "sheep"},
    {"id": 91, "name": "guitar"},
    {"id": 92, "name": "traffic cone"},
    {"id": 93, "name": "tea pot"},
    {"id": 94, "name": "keyboard"},
    {"id": 95, "name": "tripod"},
    {"id": 96, "name": "hockey"},
    {"id": 97, "name": "fan"},
    {"id": 98, "name": "dog"},
    {"id": 99, "name": "spoon"},
    {"id": 100, "name": "blackboard/whiteboard"},
    {"id": 101, "name": "balloon"},
    {"id": 102, "name": "air conditioner"},
    {"id": 103, "name": "cymbal"},
    {"id": 104, "name": "mouse"},
    {"id": 105, "name": "telephone"},
    {"id": 106, "name": "pickup truck"},
    {"id": 107, "name": "orange"},
    {"id": 108, "name": "banana"},
    {"id": 109, "name": "airplane"},
    {"id": 110, "name": "luggage"},
    {"id": 111, "name": "skis"},
    {"id": 112, "name": "soccer"},
    {"id": 113, "name": "trolley"},
    {"id": 114, "name": "oven"},
    {"id": 115, "name": "remote"},
    {"id": 116, "name": "baseball glove"},
    {"id": 117, "name": "paper towel"},
    {"id": 118, "name": "refrigerator"},
    {"id": 119, "name": "train"},
    {"id": 120, "name": "tomato"},
    {"id": 121, "name": "machinery vehicle"},
    {"id": 122, "name": "tent"},
    {"id": 123, "name": "shampoo/shower gel"},
    {"id": 124, "name": "head phone"},
    {"id": 125, "name": "lantern"},
    {"id": 126, "name": "donut"},
    {"id": 127, "name": "cleaning products"},
    {"id": 128, "name": "sailboat"},
    {"id": 129, "name": "tangerine"},
    {"id": 130, "name": "pizza"},
    {"id": 131, "name": "kite"},
    {"id": 132, "name": "computer box"},
    {"id": 133, "name": "elephant"},
    {"id": 134, "name": "toiletries"},
    {"id": 135, "name": "gas stove"},
    {"id": 136, "name": "broccoli"},
    {"id": 137, "name": "toilet"},
    {"id": 138, "name": "stroller"},
    {"id": 139, "name": "shovel"},
    {"id": 140, "name": "baseball bat"},
    {"id": 141, "name": "microwave"},
    {"id": 142, "name": "skateboard"},
    {"id": 143, "name": "surfboard"},
    {"id": 144, "name": "surveillance camera"},
    {"id": 145, "name": "gun"},
    {"id": 146, "name": "life saver"},
    {"id": 147, "name": "cat"},
    {"id": 148, "name": "lemon"},
    {"id": 149, "name": "liquid soap"},
    {"id": 150, "name": "zebra"},
    {"id": 151, "name": "duck"},
    {"id": 152, "name": "sports car"},
    {"id": 153, "name": "giraffe"},
    {"id": 154, "name": "pumpkin"},
    {"id": 155, "name": "piano"},
    {"id": 156, "name": "stop sign"},
    {"id": 157, "name": "radiator"},
    {"id": 158, "name": "converter"},
    {"id": 159, "name": "tissue "},
    {"id": 160, "name": "carrot"},
    {"id": 161, "name": "washing machine"},
    {"id": 162, "name": "vent"},
    {"id": 163, "name": "cookies"},
    {"id": 164, "name": "cutting/chopping board"},
    {"id": 165, "name": "tennis racket"},
    {"id": 166, "name": "candy"},
    {"id": 167, "name": "skating and skiing shoes"},
    {"id": 168, "name": "scissors"},
    {"id": 169, "name": "folder"},
    {"id": 170, "name": "baseball"},
    {"id": 171, "name": "strawberry"},
    {"id": 172, "name": "bow tie"},
    {"id": 173, "name": "pigeon"},
    {"id": 174, "name": "pepper"},
    {"id": 175, "name": "coffee machine"},
    {"id": 176, "name": "bathtub"},
    {"id": 177, "name": "snowboard"},
    {"id": 178, "name": "suitcase"},
    {"id": 179, "name": "grapes"},
    {"id": 180, "name": "ladder"},
    {"id": 181, "name": "pear"},
    {"id": 182, "name": "american football"},
    {"id": 183, "name": "basketball"},
    {"id": 184, "name": "potato"},
    {"id": 185, "name": "paint brush"},
    {"id": 186, "name": "printer"},
    {"id": 187, "name": "billiards"},
    {"id": 188, "name": "fire hydrant"},
    {"id": 189, "name": "goose"},
    {"id": 190, "name": "projector"},
    {"id": 191, "name": "sausage"},
    {"id": 192, "name": "fire extinguisher"},
    {"id": 193, "name": "extension cord"},
    {"id": 194, "name": "facial mask"},
    {"id": 195, "name": "tennis ball"},
    {"id": 196, "name": "chopsticks"},
    {"id": 197, "name": "electronic stove and gas stove"},
    {"id": 198, "name": "pie"},
    {"id": 199, "name": "frisbee"},
    {"id": 200, "name": "kettle"},
    {"id": 201, "name": "hamburger"},
    {"id": 202, "name": "golf club"},
    {"id": 203, "name": "cucumber"},
    {"id": 204, "name": "clutch"},
    {"id": 205, "name": "blender"},
    {"id": 206, "name": "tong"},
    {"id": 207, "name": "slide"},
    {"id": 208, "name": "hot dog"},
    {"id": 209, "name": "toothbrush"},
    {"id": 210, "name": "facial cleanser"},
    {"id": 211, "name": "mango"},
    {"id": 212, "name": "deer"},
    {"id": 213, "name": "egg"},
    {"id": 214, "name": "violin"},
    {"id": 215, "name": "marker"},
    {"id": 216, "name": "ship"},
    {"id": 217, "name": "chicken"},
    {"id": 218, "name": "onion"},
    {"id": 219, "name": "ice cream"},
    {"id": 220, "name": "tape"},
    {"id": 221, "name": "wheelchair"},
    {"id": 222, "name": "plum"},
    {"id": 223, "name": "bar soap"},
    {"id": 224, "name": "scale"},
    {"id": 225, "name": "watermelon"},
    {"id": 226, "name": "cabbage"},
    {"id": 227, "name": "router/modem"},
    {"id": 228, "name": "golf ball"},
    {"id": 229, "name": "pine apple"},
    {"id": 230, "name": "crane"},
    {"id": 231, "name": "fire truck"},
    {"id": 232, "name": "peach"},
    {"id": 233, "name": "cello"},
    {"id": 234, "name": "notepaper"},
    {"id": 235, "name": "tricycle"},
    {"id": 236, "name": "toaster"},
    {"id": 237, "name": "helicopter"},
    {"id": 238, "name": "green beans"},
    {"id": 239, "name": "brush"},
    {"id": 240, "name": "carriage"},
    {"id": 241, "name": "cigar"},
    {"id": 242, "name": "earphone"},
    {"id": 243, "name": "penguin"},
    {"id": 244, "name": "hurdle"},
    {"id": 245, "name": "swing"},
    {"id": 246, "name": "radio"},
    {"id": 247, "name": "CD"},
    {"id": 248, "name": "parking meter"},
    {"id": 249, "name": "swan"},
    {"id": 250, "name": "garlic"},
    {"id": 251, "name": "french fries"},
    {"id": 252, "name": "horn"},
    {"id": 253, "name": "avocado"},
    {"id": 254, "name": "saxophone"},
    {"id": 255, "name": "trumpet"},
    {"id": 256, "name": "sandwich"},
    {"id": 257, "name": "cue"},
    {"id": 258, "name": "kiwi fruit"},
    {"id": 259, "name": "bear"},
    {"id": 260, "name": "fishing rod"},
    {"id": 261, "name": "cherry"},
    {"id": 262, "name": "tablet"},
    {"id": 263, "name": "green vegetables"},
    {"id": 264, "name": "nuts"},
    {"id": 265, "name": "corn"},
    {"id": 266, "name": "key"},
    {"id": 267, "name": "screwdriver"},
    {"id": 268, "name": "globe"},
    {"id": 269, "name": "broom"},
    {"id": 270, "name": "pliers"},
    {"id": 271, "name": "volleyball"},
    {"id": 272, "name": "hammer"},
    {"id": 273, "name": "eggplant"},
    {"id": 274, "name": "trophy"},
    {"id": 275, "name": "dates"},
    {"id": 276, "name": "board eraser"},
    {"id": 277, "name": "rice"},
    {"id": 278, "name": "tape measure/ruler"},
    {"id": 279, "name": "dumbbell"},
    {"id": 280, "name": "hamimelon"},
    {"id": 281, "name": "stapler"},
    {"id": 282, "name": "camel"},
    {"id": 283, "name": "lettuce"},
    {"id": 284, "name": "goldfish"},
    {"id": 285, "name": "meat balls"},
    {"id": 286, "name": "medal"},
    {"id": 287, "name": "toothpaste"},
    {"id": 288, "name": "antelope"},
    {"id": 289, "name": "shrimp"},
    {"id": 290, "name": "rickshaw"},
    {"id": 291, "name": "trombone"},
    {"id": 292, "name": "pomegranate"},
    {"id": 293, "name": "coconut"},
    {"id": 294, "name": "jellyfish"},
    {"id": 295, "name": "mushroom"},
    {"id": 296, "name": "calculator"},
    {"id": 297, "name": "treadmill"},
    {"id": 298, "name": "butterfly"},
    {"id": 299, "name": "egg tart"},
    {"id": 300, "name": "cheese"},
    {"id": 301, "name": "pig"},
    {"id": 302, "name": "pomelo"},
    {"id": 303, "name": "race car"},
    {"id": 304, "name": "rice cooker"},
    {"id": 305, "name": "tuba"},
    {"id": 306, "name": "crosswalk sign"},
    {"id": 307, "name": "papaya"},
    {"id": 308, "name": "hair drier"},
    {"id": 309, "name": "green onion"},
    {"id": 310, "name": "chips"},
    {"id": 311, "name": "dolphin"},
    {"id": 312, "name": "sushi"},
    {"id": 313, "name": "urinal"},
    {"id": 314, "name": "donkey"},
    {"id": 315, "name": "electric drill"},
    {"id": 316, "name": "spring rolls"},
    {"id": 317, "name": "tortoise/turtle"},
    {"id": 318, "name": "parrot"},
    {"id": 319, "name": "flute"},
    {"id": 320, "name": "measuring cup"},
    {"id": 321, "name": "shark"},
    {"id": 322, "name": "steak"},
    {"id": 323, "name": "poker card"},
    {"id": 324, "name": "binoculars"},
    {"id": 325, "name": "llama"},
    {"id": 326, "name": "radish"},
    {"id": 327, "name": "noodles"},
    {"id": 328, "name": "yak"},
    {"id": 329, "name": "mop"},
    {"id": 330, "name": "crab"},
    {"id": 331, "name": "microscope"},
    {"id": 332, "name": "barbell"},
    {"id": 333, "name": "bread/bun"},
    {"id": 334, "name": "baozi"},
    {"id": 335, "name": "lion"},
    {"id": 336, "name": "red cabbage"},
    {"id": 337, "name": "polar bear"},
    {"id": 338, "name": "lighter"},
    {"id": 339, "name": "seal"},
    {"id": 340, "name": "mangosteen"},
    {"id": 341, "name": "comb"},
    {"id": 342, "name": "eraser"},
    {"id": 343, "name": "pitaya"},
    {"id": 344, "name": "scallop"},
    {"id": 345, "name": "pencil case"},
    {"id": 346, "name": "saw"},
    {"id": 347, "name": "table tennis paddle"},
    {"id": 348, "name": "okra"},
    {"id": 349, "name": "starfish"},
    {"id": 350, "name": "eagle"},
    {"id": 351, "name": "monkey"},
    {"id": 352, "name": "durian"},
    {"id": 353, "name": "game board"},
    {"id": 354, "name": "rabbit"},
    {"id": 355, "name": "french horn"},
    {"id": 356, "name": "ambulance"},
    {"id": 357, "name": "asparagus"},
    {"id": 358, "name": "hoverboard"},
    {"id": 359, "name": "pasta"},
    {"id": 360, "name": "target"},
    {"id": 361, "name": "hotair balloon"},
    {"id": 362, "name": "chainsaw"},
    {"id": 363, "name": "lobster"},
    {"id": 364, "name": "iron"},
    {"id": 365, "name": "flashlight"}
]
```

#### cvpods/data/datasets/builtin_meta.py

```python
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved


# All coco categories, together with their nice-looking visualization colors
# It's from https://github.com/cocodataset/panopticapi/blob/master/panoptic_coco_categories.json
COCO_CATEGORIES = [
    {"color": [220, 20, 60], "isthing": 1, "id": 1, "name": "person"},
    {"color": [119, 11, 32], "isthing": 1, "id": 2, "name": "bicycle"},
    {"color": [0, 0, 142], "isthing": 1, "id": 3, "name": "car"},
    {"color": [0, 0, 230], "isthing": 1, "id": 4, "name": "motorcycle"},
    {"color": [106, 0, 228], "isthing": 1, "id": 5, "name": "airplane"},
    {"color": [0, 60, 100], "isthing": 1, "id": 6, "name": "bus"},
    {"color": [0, 80, 100], "isthing": 1, "id": 7, "name": "train"},
    {"color": [0, 0, 70], "isthing": 1, "id": 8, "name": "truck"},
    {"color": [0, 0, 192], "isthing": 1, "id": 9, "name": "boat"},
    {"color": [250, 170, 30], "isthing": 1, "id": 10, "name": "traffic light"},
    {"color": [100, 170, 30], "isthing": 1, "id": 11, "name": "fire hydrant"},
    {"color": [220, 220, 0], "isthing": 1, "id": 13, "name": "stop sign"},
    {"color": [175, 116, 175], "isthing": 1, "id": 14, "name": "parking meter"},
    {"color": [250, 0, 30], "isthing": 1, "id": 15, "name": "bench"},
    {"color": [165, 42, 42], "isthing": 1, "id": 16, "name": "bird"},
    {"color": [255, 77, 255], "isthing": 1, "id": 17, "name": "cat"},
    {"color": [0, 226, 252], "isthing": 1, "id": 18, "name": "dog"},
    {"color": [182, 182, 255], "isthing": 1, "id": 19, "name": "horse"},
    {"color": [0, 82, 0], "isthing": 1, "id": 20, "name": "sheep"},
    {"color": [120, 166, 157], "isthing": 1, "id": 21, "name": "cow"},
    {"color": [110, 76, 0], "isthing": 1, "id": 22, "name": "elephant"},
    {"color": [174, 57, 255], "isthing": 1, "id": 23, "name": "bear"},
    {"color": [199, 100, 0], "isthing": 1, "id": 24, "name": "zebra"},
    {"color": [72, 0, 118], "isthing": 1, "id": 25, "name": "giraffe"},
    {"color": [255, 179, 240], "isthing": 1, "id": 27, "name": "backpack"},
    {"color": [0, 125, 92], "isthing": 1, "id": 28, "name": "umbrella"},
    {"color": [209, 0, 151], "isthing": 1, "id": 31, "name": "handbag"},
    {"color": [188, 208, 182], "isthing": 1, "id": 32, "name": "tie"},
    {"color": [0, 220, 176], "isthing": 1, "id": 33, "name": "suitcase"},
    {"color": [255, 99, 164], "isthing": 1, "id": 34, "name": "frisbee"},
    {"color": [92, 0, 73], "isthing": 1, "id": 35, "name": "skis"},
    {"color": [133, 129, 255], "isthing": 1, "id": 36, "name": "snowboard"},
    {"color": [78, 180, 255], "isthing": 1, "id": 37, "name": "sports ball"},
    {"color": [0, 228, 0], "isthing": 1, "id": 38, "name": "kite"},
    {"color": [174, 255, 243], "isthing": 1, "id": 39, "name": "baseball bat"},
    {"color": [45, 89, 255], "isthing": 1, "id": 40, "name": "baseball glove"},
    {"color": [134, 134, 103], "isthing": 1, "id": 41, "name": "skateboard"},
    {"color": [145, 148, 174], "isthing": 1, "id": 42, "name": "surfboard"},
    {"color": [255, 208, 186], "isthing": 1, "id": 43, "name": "tennis racket"},
    {"color": [197, 226, 255], "isthing": 1, "id": 44, "name": "bottle"},
    {"color": [171, 134, 1], "isthing": 1, "id": 46, "name": "wine glass"},
    {"color": [109, 63, 54], "isthing": 1, "id": 47, "name": "cup"},
    {"color": [207, 138, 255], "isthing": 1, "id": 48, "name": "fork"},
    {"color": [151, 0, 95], "isthing": 1, "id": 49, "name": "knife"},
    {"color": [9, 80, 61], "isthing": 1, "id": 50, "name": "spoon"},
    {"color": [84, 105, 51], "isthing": 1, "id": 51, "name": "bowl"},
    {"color": [74, 65, 105], "isthing": 1, "id": 52, "name": "banana"},
    {"color": [166, 196, 102], "isthing": 1, "id": 53, "name": "apple"},
    {"color": [208, 195, 210], "isthing": 1, "id": 54, "name": "sandwich"},
    {"color": [255, 109, 65], "isthing": 1, "id": 55, "name": "orange"},
    {"color": [0, 143, 149], "isthing": 1, "id": 56, "name": "broccoli"},
    {"color": [179, 0, 194], "isthing": 1, "id": 57, "name": "carrot"},
    {"color": [209, 99, 106], "isthing": 1, "id": 58, "name": "hot dog"},
    {"color": [5, 121, 0], "isthing": 1, "id": 59, "name": "pizza"},
    {"color": [227, 255, 205], "isthing": 1, "id": 60, "name": "donut"},
    {"color": [147, 186, 208], "isthing": 1, "id": 61, "name": "cake"},
    {"color": [153, 69, 1], "isthing": 1, "id": 62, "name": "chair"},
    {"color": [3, 95, 161], "isthing": 1, "id": 63, "name": "couch"},
    {"color": [163, 255, 0], "isthing": 1, "id": 64, "name": "potted plant"},
    {"color": [119, 0, 170], "isthing": 1, "id": 65, "name": "bed"},
    {"color": [0, 182, 199], "isthing": 1, "id": 67, "name": "dining table"},
    {"color": [0, 165, 120], "isthing": 1, "id": 70, "name": "toilet"},
    {"color": [183, 130, 88], "isthing": 1, "id": 72, "name": "tv"},
    {"color": [95, 32, 0], "isthing": 1, "id": 73, "name": "laptop"},
    {"color": [130, 114, 135], "isthing": 1, "id": 74, "name": "mouse"},
    {"color": [110, 129, 133], "isthing": 1, "id": 75, "name": "remote"},
    {"color": [166, 74, 118], "isthing": 1, "id": 76, "name": "keyboard"},
    {"color": [219, 142, 185], "isthing": 1, "id": 77, "name": "cell phone"},
    {"color": [79, 210, 114], "isthing": 1, "id": 78, "name": "microwave"},
    {"color": [178, 90, 62], "isthing": 1, "id": 79, "name": "oven"},
    {"color": [65, 70, 15], "isthing": 1, "id": 80, "name": "toaster"},
    {"color": [127, 167, 115], "isthing": 1, "id": 81, "name": "sink"},
    {"color": [59, 105, 106], "isthing": 1, "id": 82, "name": "refrigerator"},
    {"color": [142, 108, 45], "isthing": 1, "id": 84, "name": "book"},
    {"color": [196, 172, 0], "isthing": 1, "id": 85, "name": "clock"},
    {"color": [95, 54, 80], "isthing": 1, "id": 86, "name": "vase"},
    {"color": [128, 76, 255], "isthing": 1, "id": 87, "name": "scissors"},
    {"color": [201, 57, 1], "isthing": 1, "id": 88, "name": "teddy bear"},
    {"color": [246, 0, 122], "isthing": 1, "id": 89, "name": "hair drier"},
    {"color": [191, 162, 208], "isthing": 1, "id": 90, "name": "toothbrush"},
    {"color": [255, 255, 128], "isthing": 0, "id": 92, "name": "banner"},
    {"color": [147, 211, 203], "isthing": 0, "id": 93, "name": "blanket"},
    {"color": [150, 100, 100], "isthing": 0, "id": 95, "name": "bridge"},
    {"color": [168, 171, 172], "isthing": 0, "id": 100, "name": "cardboard"},
    {"color": [146, 112, 198], "isthing": 0, "id": 107, "name": "counter"},
    {"color": [210, 170, 100], "isthing": 0, "id": 109, "name": "curtain"},
    {"color": [92, 136, 89], "isthing": 0, "id": 112, "name": "door-stuff"},
    {"color": [218, 88, 184], "isthing": 0, "id": 118, "name": "floor-wood"},
    {"color": [241, 129, 0], "isthing": 0, "id": 119, "name": "flower"},
    {"color": [217, 17, 255], "isthing": 0, "id": 122, "name": "fruit"},
    {"color": [124, 74, 181], "isthing": 0, "id": 125, "name": "gravel"},
    {"color": [70, 70, 70], "isthing": 0, "id": 128, "name": "house"},
    {"color": [255, 228, 255], "isthing": 0, "id": 130, "name": "light"},
    {"color": [154, 208, 0], "isthing": 0, "id": 133, "name": "mirror-stuff"},
    {"color": [193, 0, 92], "isthing": 0, "id": 138, "name": "net"},
    {"color": [76, 91, 113], "isthing": 0, "id": 141, "name": "pillow"},
    {"color": [255, 180, 195], "isthing": 0, "id": 144, "name": "platform"},
    {"color": [106, 154, 176], "isthing": 0, "id": 145, "name": "playingfield"},
    {"color": [230, 150, 140], "isthing": 0, "id": 147, "name": "railroad"},
    {"color": [60, 143, 255], "isthing": 0, "id": 148, "name": "river"},
    {"color": [128, 64, 128], "isthing": 0, "id": 149, "name": "road"},
    {"color": [92, 82, 55], "isthing": 0, "id": 151, "name": "roof"},
    {"color": [254, 212, 124], "isthing": 0, "id": 154, "name": "sand"},
    {"color": [73, 77, 174], "isthing": 0, "id": 155, "name": "sea"},
    {"color": [255, 160, 98], "isthing": 0, "id": 156, "name": "shelf"},
    {"color": [255, 255, 255], "isthing": 0, "id": 159, "name": "snow"},
    {"color": [104, 84, 109], "isthing": 0, "id": 161, "name": "stairs"},
    {"color": [169, 164, 131], "isthing": 0, "id": 166, "name": "tent"},
    {"color": [225, 199, 255], "isthing": 0, "id": 168, "name": "towel"},
    {"color": [137, 54, 74], "isthing": 0, "id": 171, "name": "wall-brick"},
    {"color": [135, 158, 223], "isthing": 0, "id": 175, "name": "wall-stone"},
    {"color": [7, 246, 231], "isthing": 0, "id": 176, "name": "wall-tile"},
    {"color": [107, 255, 200], "isthing": 0, "id": 177, "name": "wall-wood"},
    {"color": [58, 41, 149], "isthing": 0, "id": 178, "name": "water-other"},
    {"color": [183, 121, 142], "isthing": 0, "id": 180, "name": "window-blind"},
    {"color": [255, 73, 97], "isthing": 0, "id": 181, "name": "window-other"},
    {"color": [107, 142, 35], "isthing": 0, "id": 184, "name": "tree-merged"},
    {"color": [190, 153, 153], "isthing": 0, "id": 185, "name": "fence-merged"},
    {"color": [146, 139, 141], "isthing": 0, "id": 186, "name": "ceiling-merged"},
    {"color": [70, 130, 180], "isthing": 0, "id": 187, "name": "sky-other-merged"},
    {"color": [134, 199, 156], "isthing": 0, "id": 188, "name": "cabinet-merged"},
    {"color": [209, 226, 140], "isthing": 0, "id": 189, "name": "table-merged"},
    {"color": [96, 36, 108], "isthing": 0, "id": 190, "name": "floor-other-merged"},
    {"color": [96, 96, 96], "isthing": 0, "id": 191, "name": "pavement-merged"},
    {"color": [64, 170, 64], "isthing": 0, "id": 192, "name": "mountain-merged"},
    {"color": [152, 251, 152], "isthing": 0, "id": 193, "name": "grass-merged"},
    {"color": [208, 229, 228], "isthing": 0, "id": 194, "name": "dirt-merged"},
    {"color": [206, 186, 171], "isthing": 0, "id": 195, "name": "paper-merged"},
    {"color": [152, 161, 64], "isthing": 0, "id": 196, "name": "food-other-merged"},
    {"color": [116, 112, 0], "isthing": 0, "id": 197, "name": "building-other-merged"},
    {"color": [0, 114, 143], "isthing": 0, "id": 198, "name": "rock-merged"},
    {"color": [102, 102, 156], "isthing": 0, "id": 199, "name": "wall-other-merged"},
    {"color": [250, 141, 255], "isthing": 0, "id": 200, "name": "rug-merged"},
]

# fmt: off
COCO_PERSON_KEYPOINT_NAMES = (
    "nose",
    "left_eye", "right_eye",
    "left_ear", "right_ear",
    "left_shoulder", "right_shoulder",
    "left_elbow", "right_elbow",
    "left_wrist", "right_wrist",
    "left_hip", "right_hip",
    "left_knee", "right_knee",
    "left_ankle", "right_ankle",
)
# fmt: on

# Pairs of keypoints that should be exchanged under horizontal flipping
COCO_PERSON_KEYPOINT_FLIP_MAP = (
    ("left_eye", "right_eye"),
    ("left_ear", "right_ear"),
    ("left_shoulder", "right_shoulder"),
    ("left_elbow", "right_elbow"),
    ("left_wrist", "right_wrist"),
    ("left_hip", "right_hip"),
    ("left_knee", "right_knee"),
    ("left_ankle", "right_ankle"),
)

# rules for pairs of keypoints to draw a line between, and the line color to use.
KEYPOINT_CONNECTION_RULES = [
    # face
    ("left_ear", "left_eye", (102, 204, 255)),
    ("right_ear", "right_eye", (51, 153, 255)),
    ("left_eye", "nose", (102, 0, 204)),
    ("nose", "right_eye", (51, 102, 255)),
    # upper-body
    ("left_shoulder", "right_shoulder", (255, 128, 0)),
    ("left_shoulder", "left_elbow", (153, 255, 204)),
    ("right_shoulder", "right_elbow", (128, 229, 255)),
    ("left_elbow", "left_wrist", (153, 255, 153)),
    ("right_elbow", "right_wrist", (102, 255, 224)),
    # lower-body
    ("left_hip", "right_hip", (255, 102, 0)),
    ("left_hip", "left_knee", (255, 255, 77)),
    ("right_hip", "right_knee", (153, 255, 204)),
    ("left_knee", "left_ankle", (191, 255, 128)),
    ("right_knee", "right_ankle", (255, 195, 77)),
]


def _get_coco_instances_meta():
    thing_ids = [k["id"] for k in COCO_CATEGORIES if k["isthing"] == 1]
    thing_colors = [k["color"] for k in COCO_CATEGORIES if k["isthing"] == 1]
    assert len(thing_ids) == 80, len(thing_ids)
    # Mapping from the incontiguous COCO category id to an id in [0, 79]
    thing_dataset_id_to_contiguous_id = {k: i for i, k in enumerate(thing_ids)}
    thing_classes = [k["name"] for k in COCO_CATEGORIES if k["isthing"] == 1]
    ret = {
        "thing_dataset_id_to_contiguous_id": thing_dataset_id_to_contiguous_id,
        "thing_classes": thing_classes,
        "thing_colors": thing_colors,
    }
    return ret


def _get_coco_panoptic_separated_meta():
    """
    Returns metadata for "separated" version of the panoptic segmentation dataset.
    """
    stuff_ids = [k["id"] for k in COCO_CATEGORIES if k["isthing"] == 0]
    assert len(stuff_ids) == 53, len(stuff_ids)

    # For semantic segmentation, this mapping maps from contiguous stuff id
    # (in [0, 53], used in models) to ids in the dataset (used for processing results)
    # The id 0 is mapped to an extra category "thing".
    stuff_dataset_id_to_contiguous_id = {k: i + 1 for i, k in enumerate(stuff_ids)}
    # When converting COCO panoptic annotations to semantic annotations
    # We label the "thing" category to 0
    stuff_dataset_id_to_contiguous_id[0] = 0

    # 54 names for COCO stuff categories (including "things")
    stuff_classes = ["things"] + [
        k["name"].replace("-other", "").replace("-merged", "")
        for k in COCO_CATEGORIES
        if k["isthing"] == 0
    ]

    # NOTE: I randomly picked a color for things
    stuff_colors = [[82, 18, 128]] + [k["color"] for k in COCO_CATEGORIES if k["isthing"] == 0]
    ret = {
        "stuff_dataset_id_to_contiguous_id": stuff_dataset_id_to_contiguous_id,
        "stuff_classes": stuff_classes,
        "stuff_colors": stuff_colors,
    }
    ret.update(_get_coco_instances_meta())
    return ret


def _get_builtin_metadata(dataset_name):
    if dataset_name == "coco":
        return _get_coco_instances_meta()
    if dataset_name == "coco_panoptic_separated":
        return _get_coco_panoptic_separated_meta()
    elif dataset_name == "coco_person":
        return {
            "thing_classes": ["person"],
            "keypoint_names": COCO_PERSON_KEYPOINT_NAMES,
            "keypoint_flip_map": COCO_PERSON_KEYPOINT_FLIP_MAP,
            "keypoint_connection_rules": KEYPOINT_CONNECTION_RULES,
        }
    elif dataset_name == "cityscapes":
        # fmt: off
        CITYSCAPES_THING_CLASSES = [
            "person", "rider", "car", "truck",
            "bus", "train", "motorcycle", "bicycle",
        ]
        CITYSCAPES_STUFF_CLASSES = [
            "road", "sidewalk", "building", "wall", "fence", "pole", "traffic light",
            "traffic sign", "vegetation", "terrain", "sky", "person", "rider", "car",
            "truck", "bus", "train", "motorcycle", "bicycle", "license plate",
        ]
        # fmt: on
        return {
            "thing_classes": CITYSCAPES_THING_CLASSES,
            "stuff_classes": CITYSCAPES_STUFF_CLASSES,
        }
    raise KeyError("No built-in metadata for dataset {}".format(dataset_name))
```

#### cvpods/data/transforms/transform_util.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import numpy as np

import torch


# pyre-ignore-all-errors
def to_float_tensor(numpy_array: np.ndarray) -> torch.Tensor:
    """
    Convert the numpy array to torch float tensor with dimension of NxCxHxW.
    Pytorch is not fully supporting uint8, so convert tensor to float if the
    numpy_array is uint8.
    Args:
        numpy_array (ndarray): of shape NxHxWxC, or HxWxC or HxW to
            represent an image. The array can be of type uint8 in range
            [0, 255], or floating point in range [0, 1] or [0, 255].
    Returns:
        float_tensor (tensor): converted float tensor.
    """
    assert isinstance(numpy_array, np.ndarray)
    assert len(numpy_array.shape) in (2, 3, 4)

    # Some of the input numpy array has negative strides. Pytorch currently
    # does not support negative strides, perform ascontiguousarray to
    # resolve the issue.
    float_tensor = torch.from_numpy(np.ascontiguousarray(numpy_array))
    if numpy_array.dtype == np.uint8:
        float_tensor = float_tensor.float()

    if len(numpy_array.shape) == 2:
        # HxW -> 1x1xHxW.
        float_tensor = float_tensor[None, None, :, :]
    elif len(numpy_array.shape) == 3:
        # HxWxC -> 1xCxHxW.
        float_tensor = float_tensor.permute(2, 0, 1)
        float_tensor = float_tensor[None, :, :, :]
    elif len(numpy_array.shape) == 4:
        # NxHxWxC -> NxCxHxW
        float_tensor = float_tensor.permute(0, 3, 1, 2)
    else:
        raise NotImplementedError(
            "Unknow numpy_array dimension of {}".format(float_tensor.shape)
        )
    return float_tensor


def to_numpy(
    float_tensor: torch.Tensor, target_shape: list, target_dtype: np.dtype
) -> np.ndarray:
    """
    Convert float tensor with dimension of NxCxHxW back to numpy array.
    Args:
        float_tensor (tensor): a float pytorch tensor with shape of NxCxHxW.
        target_shape (list): the target shape of the numpy array to represent
            the image as output. options include NxHxWxC, or HxWxC or HxW.
        target_dtype (dtype): the target dtype of the numpy array to represent
            the image as output. The array can be of type uint8 in range
            [0, 255], or floating point in range [0, 1] or [0, 255].
    Returns:
        (ndarray): converted numpy array.
    """
    assert len(target_shape) in (2, 3, 4)

    if len(target_shape) == 2:
        # 1x1xHxW -> HxW.
        assert float_tensor.shape[0] == 1
        assert float_tensor.shape[1] == 1
        float_tensor = float_tensor[0, 0, :, :]
    elif len(target_shape) == 3:
        assert float_tensor.shape[0] == 1
        # 1xCxHxW -> HxWxC.
        float_tensor = float_tensor[0].permute(1, 2, 0)
    elif len(target_shape) == 4:
        # NxCxHxW -> NxHxWxC
        float_tensor = float_tensor.permute(0, 2, 3, 1)
    else:
        raise NotImplementedError(
            "Unknow target shape dimension of {}".format(target_shape)
        )
    if target_dtype == np.uint8:
        # Need to specifically call round here, notice in pytroch the round
        # is half to even.
        # https://github.com/pytorch/pytorch/issues/16498
        float_tensor = float_tensor.round().byte()
    return float_tensor.numpy()
```

#### cvpods/data/transforms/auto_aug.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import math
import random

import numpy as np
import PIL
from PIL import Image, ImageEnhance, ImageOps

from cvpods.data.transforms import Transform

_PIL_VER = tuple([int(x) for x in PIL.__version__.split('.')[:2]])

_FILL = (128, 128, 128)

# This signifies the max integer that the controller RNN could predict for the
# augmentation scheme.
_MAX_LEVEL = 10.

_HPARAMS_DEFAULT = dict(
    translate_const=250,
    img_mean=_FILL,
)

_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)


def _interpolation(kwargs):
    interpolation = kwargs.pop('resample', Image.BILINEAR)
    if isinstance(interpolation, (list, tuple)):
        return random.choice(interpolation)
    else:
        return interpolation


def _check_args_tf(kwargs):
    if 'fillcolor' in kwargs and _PIL_VER < (5, 0):
        kwargs.pop('fillcolor')
    kwargs['resample'] = _interpolation(kwargs)


def shear_x(img, factor, **kwargs):
    _check_args_tf(kwargs)
    return img.transform(img.size, Image.AFFINE, (1, factor, 0, 0, 1, 0), **kwargs)


def shear_y(img, factor, **kwargs):
    _check_args_tf(kwargs)
    return img.transform(img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0), **kwargs)


def translate_x_rel(img, pct, **kwargs):
    pixels = pct * img.size[0]
    _check_args_tf(kwargs)
    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)


def translate_y_rel(img, pct, **kwargs):
    pixels = pct * img.size[1]
    _check_args_tf(kwargs)
    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)


def translate_x_abs(img, pixels, **kwargs):
    _check_args_tf(kwargs)
    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)


def translate_y_abs(img, pixels, **kwargs):
    _check_args_tf(kwargs)
    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)


def rotate(img, degrees, **kwargs):
    _check_args_tf(kwargs)
    if _PIL_VER >= (5, 2):
        return img.rotate(degrees, **kwargs)
    elif _PIL_VER >= (5, 0):
        w, h = img.size
        post_trans = (0, 0)
        rotn_center = (w / 2.0, h / 2.0)
        angle = -math.radians(degrees)
        matrix = [
            round(math.cos(angle), 15),
            round(math.sin(angle), 15),
            0.0,
            round(-math.sin(angle), 15),
            round(math.cos(angle), 15),
            0.0,
        ]

        def transform(x, y, matrix):
            (a, b, c, d, e, f) = matrix
            return a * x + b * y + c, d * x + e * y + f

        matrix[2], matrix[5] = transform(
            -rotn_center[0] - post_trans[0],
            - rotn_center[1] - post_trans[1], matrix
        )
        matrix[2] += rotn_center[0]
        matrix[5] += rotn_center[1]
        return img.transform(img.size, Image.AFFINE, matrix, **kwargs)
    else:
        return img.rotate(degrees, resample=kwargs['resample'])


def auto_contrast(img, **__):
    return ImageOps.autocontrast(img)


def invert(img, **__):
    return ImageOps.invert(img)


def identity(img, **__):
    return img


def equalize(img, **__):
    return ImageOps.equalize(img)


def solarize(img, thresh, **__):
    return ImageOps.solarize(img, thresh)


def solarize_add(img, add, thresh=128, **__):
    lut = []
    for i in range(256):
        if i < thresh:
            lut.append(min(255, i + add))
        else:
            lut.append(i)
    if img.mode in ("L", "RGB"):
        if img.mode == "RGB" and len(lut) == 256:
            lut = lut + lut + lut
        return img.point(lut)
    else:
        return img


def posterize(img, bits_to_keep, **__):
    if bits_to_keep >= 8:
        return img
    return ImageOps.posterize(img, bits_to_keep)


def contrast(img, factor, **__):
    return ImageEnhance.Contrast(img).enhance(factor)


def color(img, factor, **__):
    return ImageEnhance.Color(img).enhance(factor)


def brightness(img, factor, **__):
    return ImageEnhance.Brightness(img).enhance(factor)


def sharpness(img, factor, **__):
    return ImageEnhance.Sharpness(img).enhance(factor)


def _randomly_negate(v):
    """With 50% prob, negate the value"""
    return -v if random.random() > 0.5 else v


def _rotate_level_to_arg(level, _hparams):
    # range [-30, 30]
    level = (level / _MAX_LEVEL) * 30.
    level = _randomly_negate(level)
    return level,


def _enhance_level_to_arg(level, _hparams):
    # range [0.1, 1.9]
    return (level / _MAX_LEVEL) * 1.8 + 0.1,


def _shear_level_to_arg(level, _hparams):
    # range [-0.3, 0.3]
    level = (level / _MAX_LEVEL) * 0.3
    level = _randomly_negate(level)
    return level,


def _translate_abs_level_to_arg(level, hparams):
    translate_const = hparams['translate_const']
    level = (level / _MAX_LEVEL) * float(translate_const)
    level = _randomly_negate(level)
    return level,


def _translate_rel_level_to_arg(level, _hparams):
    # range [-0.45, 0.45]
    level = (level / _MAX_LEVEL) * 0.45
    level = _randomly_negate(level)
    return level,


def _posterize_original_level_to_arg(level, _hparams):
    # As per original AutoAugment paper description
    # range [4, 8], 'keep 4 up to 8 MSB of image'
    return int((level / _MAX_LEVEL) * 4) + 4,


def _posterize_research_level_to_arg(level, _hparams):
    # As per Tensorflow models research and UDA impl
    # range [4, 0], 'keep 4 down to 0 MSB of original image'
    return 4 - int((level / _MAX_LEVEL) * 4),


def _posterize_tpu_level_to_arg(level, _hparams):
    # As per Tensorflow TPU EfficientNet impl
    # range [0, 4], 'keep 0 up to 4 MSB of original image'
    return int((level / _MAX_LEVEL) * 4),


def _solarize_level_to_arg(level, _hparams):
    # range [0, 256]
    return int((level / _MAX_LEVEL) * 256),


def _solarize_add_level_to_arg(level, _hparams):
    # range [0, 110]
    return int((level / _MAX_LEVEL) * 110),


LEVEL_TO_ARG = {
    'AutoContrast': None,
    'Equalize': None,
    'Invert': None,
    'Identity': None,
    'Rotate': _rotate_level_to_arg,
    'PosterizeOriginal': _posterize_original_level_to_arg,
    'PosterizeResearch': _posterize_research_level_to_arg,
    'PosterizeTpu': _posterize_tpu_level_to_arg,
    'Solarize': _solarize_level_to_arg,
    'SolarizeAdd': _solarize_add_level_to_arg,
    'Color': _enhance_level_to_arg,
    'Contrast': _enhance_level_to_arg,
    'Brightness': _enhance_level_to_arg,
    'Sharpness': _enhance_level_to_arg,
    'ShearX': _shear_level_to_arg,
    'ShearY': _shear_level_to_arg,
    'TranslateX': _translate_abs_level_to_arg,
    'TranslateY': _translate_abs_level_to_arg,
    'TranslateXRel': _translate_rel_level_to_arg,
    'TranslateYRel': _translate_rel_level_to_arg,
}


NAME_TO_OP = {
    'AutoContrast': auto_contrast,
    'Equalize': equalize,
    'Invert': invert,
    'Identity': identity,
    'Rotate': rotate,
    'PosterizeOriginal': posterize,
    'PosterizeResearch': posterize,
    'PosterizeTpu': posterize,
    'Solarize': solarize,
    'SolarizeAdd': solarize_add,
    'Color': color,
    'Contrast': contrast,
    'Brightness': brightness,
    'Sharpness': sharpness,
    'ShearX': shear_x,
    'ShearY': shear_y,
    'TranslateX': translate_x_abs,
    'TranslateY': translate_y_abs,
    'TranslateXRel': translate_x_rel,
    'TranslateYRel': translate_y_rel,
}


class AutoAugmentTransform(Transform):
    """
    AutoAugment from Google.
    Implementation adapted from:
        https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py
    """

    def __init__(self, name, prob=0.5, magnitude=10, hparams=None):
        """
        Args:
            name (str): any type of transforms list in _RAND_TRANSFORMS.
            prob (float): probability of perform current augmentation.
            magnitude (int): intensity / magnitude of each augmentation.
            hparams (dict): hyper-parameters required by each augmentation.
        """
        hparams = hparams or _HPARAMS_DEFAULT
        self.aug_fn = NAME_TO_OP[name]
        self.level_fn = LEVEL_TO_ARG[name]
        self.prob = prob
        self.magnitude = magnitude
        self.hparams = hparams.copy()
        self.kwargs = dict(
            fillcolor=hparams['img_mean'] if 'img_mean' in hparams else _FILL,
            resample=hparams['interpolation'] if 'interpolation' in hparams
            else _RANDOM_INTERPOLATION,
        )

        # If magnitude_std is > 0, we introduce some randomness
        # in the usually fixed policy and sample magnitude from a normal distribution
        # with mean `magnitude` and std-dev of `magnitude_std`.
        # NOTE This is my own hack, being tested, not in papers or reference impls.
        self.magnitude_std = self.hparams.get('magnitude_std', 0)

    def apply_image(self, img: np.ndarray) -> np.ndarray:
        if random.random() > self.prob:
            return img
        magnitude = self.magnitude
        if self.magnitude_std and self.magnitude_std > 0:
            magnitude = random.gauss(magnitude, self.magnitude_std)
        magnitude = min(_MAX_LEVEL, max(0, magnitude))  # clip to valid range
        level_args = self.level_fn(
            magnitude, self.hparams) if self.level_fn is not None else tuple()
        return np.array(self.aug_fn(Image.fromarray(img), *level_args, **self.kwargs))

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        return coords


_RAND_TRANSFORMS = [
    'AutoContrast',
    'Equalize',
    'Invert',
    'Rotate',
    'PosterizeTpu',
    'Solarize',
    'SolarizeAdd',
    'Color',
    'Contrast',
    'Brightness',
    'Sharpness',
    'ShearX',
    'ShearY',
    'TranslateXRel',
    'TranslateYRel',
    # 'Cutout'  # FIXME I implement this as random erasing separately
]

_RAND_TRANSFORMS_CMC = [
    'AutoContrast',
    'Identity',
    'Rotate',
    'Sharpness',
    'ShearX',
    'ShearY',
    'TranslateXRel',
    'TranslateYRel',
    # 'Cutout'  # FIXME I implement this as random erasing separately
]


# These experimental weights are based loosely on the relative improvements mentioned in paper.
# They may not result in increased performance, but could likely be tuned to so.
_RAND_CHOICE_WEIGHTS_0 = {
    'Rotate': 0.3,
    'ShearX': 0.2,
    'ShearY': 0.2,
    'TranslateXRel': 0.1,
    'TranslateYRel': 0.1,
    'Color': .025,
    'Sharpness': 0.025,
    'AutoContrast': 0.025,
    'Solarize': .005,
    'SolarizeAdd': .005,
    'Contrast': .005,
    'Brightness': .005,
    'Equalize': .005,
    'PosterizeTpu': 0,
    'Invert': 0,
}
```

#### cvpods/data/transforms/__init__.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from .transform import *
from .transform_gen import *

__all__ = [k for k in globals().keys() if not k.startswith("_")]
```

#### cvpods/data/transforms/transform_gen.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import inspect
import pprint
import sys
from abc import ABCMeta, abstractmethod

import numpy as np
from PIL import Image

import torch

from cvpods.structures import Boxes, BoxMode, pairwise_iou

from ..registry import TRANSFORMS
from .auto_aug import AutoAugmentTransform

from .transform import (  # isort:skip
    ScaleTransform,
    AffineTransform,
    BlendTransform,
    IoUCropTransform,
    CropTransform,
    CropPadTransform,
    JitterCropTransform,
    HFlipTransform,
    NoOpTransform,
    VFlipTransform,
    DistortTransform,
    DistortTransform2,
    ShiftTransform,
    RandomSwapChannelsTransform,
    ExpandTransform,
    ExtentTransform,
    ResizeTransform,
    # Transforms used in ssl
    GaussianBlurTransform,
    GaussianBlurConvTransform,
    SolarizationTransform,
    ComposeTransform,
    # LabSpaceTransform,
    PadTransform,
)

__all__ = [
    "Pad",
    "RandomScale",
    "Expand",
    "MinIoURandomCrop",
    "RandomSwapChannels",
    "CenterAffine",
    "RandomBrightness",
    "RandomContrast",
    "RandomCrop",
    "RandomCropWithInstance",
    "RandomCropWithMaxAreaLimit",
    "RandomCropPad",
    "RandomExtent",
    "RandomFlip",
    "RandomShift",
    "JitterCrop",
    "RandomSaturation",
    "RandomLighting",
    "RandomDistortion",
    "RandomDistortion2",
    "Resize",
    "ResizeShortestEdge",
    "ResizeLongestEdge",
    "ShuffleList",
    "RandomList",
    "RepeatList",
    "TransformGen",
    "TorchTransformGen",
    # transforms used in ssl
    "GaussianBlur",
    "GaussianBlurConv",
    "Solarization",
    "AutoAugment",
]


def check_dtype(img):
    """
    Check the image data type and dimensions to ensure that transforms can be applied on it.

    Args:
        img (np.array): image to be checked.
    """
    assert isinstance(
        img, np.ndarray
    ), "[TransformGen] Needs an numpy array, but got a {}!".format(type(img))
    assert not isinstance(img.dtype, np.integer) or (
            img.dtype == np.uint8
    ), "[TransformGen] Got image of type {}, use uint8 or floating points instead!".format(
        img.dtype
    )
    assert img.ndim in [2, 3], img.ndim


@TRANSFORMS.register()
class TransformGen(metaclass=ABCMeta):
    """
    TransformGen takes an image of type uint8 in range [0, 255], or
    floating point in range [0, 1] or [0, 255] as input.

    It creates a :class:`Transform` based on the given image, sometimes with randomness.
    The transform can then be used to transform images
    or other data (boxes, points, annotations, etc.) associated with it.

    The assumption made in this class
    is that the image itself is sufficient to instantiate a transform.
    When this assumption is not true, you need to create the transforms by your own.

    A list of `TransformGen` can be applied with :func:`apply_transform_gens`.
    """

    def _init(self, params=None):
        if params:
            for k, v in params.items():
                if k != "self" and not k.startswith("_"):
                    setattr(self, k, v)

    @abstractmethod
    def get_transform(self, img, annotations=None):
        raise NotImplementedError

    def __call__(self, img, annotations=None, **kwargs):
        return self.get_transform(img, annotations)(img, annotations, **kwargs)

    def _rand_range(self, low=1.0, high=None, size=None):
        """
        Uniform float random number between low and high.
        """
        if high is None:
            low, high = 0, low
        if size is None:
            size = []
        return np.random.uniform(low, high, size)

    def __repr__(self):
        """
        Produce something like:
        "MyTransformGen(field1={self.field1}, field2={self.field2})"
        """
        try:
            sig = inspect.signature(self.__init__)
            classname = type(self).__name__
            argstr = []
            for name, param in sig.parameters.items():
                assert (
                        param.kind != param.VAR_POSITIONAL
                        and param.kind != param.VAR_KEYWORD
                ), "The default __repr__ doesn't support *args or **kwargs"
                assert hasattr(self, name), (
                    "Attribute {} not found! "
                    "Default __repr__ only works if attributes match the constructor.".format(
                        name
                    )
                )
                attr = getattr(self, name)
                default = param.default
                if default is attr:
                    continue
                argstr.append("{}={}".format(name, pprint.pformat(attr)))
            return "{}({})".format(classname, ", ".join(argstr))
        except AssertionError:
            return super().__repr__()

    __str__ = __repr__


@TRANSFORMS.register()
class RandomShift(TransformGen):
    """
    Shift the image and box given shift pixels and probability.
    """

    def __init__(self, prob=0.5, max_shifts=8):
        """
        Args:
            prob (float): probability of shifts.
            max_shifts (int): the max pixels for shifting.
        """
        super().__init__()
        self._init(locals())

    def get_transform(self, img, annotations=None):
        do = self._rand_range() < self.prob
        if do:
            shift_x = np.random.randint(low=-self.max_shifts,
                                        high=self.max_shifts)
            shift_y = np.random.randint(low=-self.max_shifts,
                                        high=self.max_shifts)
            return ShiftTransform(shift_x, shift_y)
        else:
            return NoOpTransform()


@TRANSFORMS.register()
class JitterCrop(TransformGen):
    """Jitter and crop the image and box."""

    def __init__(self, jitter_ratio):
        super().__init__()
        self._init(locals())

    def __call__(self, img, annotations=None, **kwargs):
        for annotation in annotations:
            annotation["meta_infos"] = dict()
        return self.get_transform(img, annotations)(img, annotations)

    def get_transform(self, img, annotations=None):
        oh, ow = img.shape[:2]
        dw = int(ow * self.jitter_ratio)
        dh = int(oh * self.jitter_ratio)
        pleft = np.random.randint(-dw, dw)
        pright = np.random.randint(-dw, dw)
        ptop = np.random.randint(-dh, dh)
        pbot = np.random.randint(-dh, dh)

        swidth = ow - pleft - pright
        sheight = oh - ptop - pbot
        return JitterCropTransform(
            pleft=pleft, pright=pright, ptop=ptop, pbot=pbot,
            output_size=(swidth, sheight))


@TRANSFORMS.register()
class RandomDistortion2(TransformGen):
    """
    Random distort image's hue, saturation and exposure.
    """

    def __init__(self, hue, saturation, exposure):
        """
        RandomDistortion Initialization.
        Args:
            hue (float): value of hue
            saturation (float): value of saturation
            exposure (float): value of exposure
        """
        super().__init__()
        self._init(locals())

    def get_transform(self, img, annotations=None):
        return DistortTransform2(self.hue, self.saturation, self.exposure)


@TRANSFORMS.register()
class RandomFlip(TransformGen):
    """
    Flip the image horizontally or vertically with the given probability.
    """

    def __init__(self, prob=0.5, *, horizontal=True, vertical=False):
        """
        Args:
            prob (float): probability of flip.
            horizontal (boolean): whether to apply horizontal flipping
            vertical (boolean): whether to apply vertical flipping
        """
        super().__init__()

        if horizontal and vertical:
            raise ValueError(
                "Cannot do both horiz and vert. Please use two Flip instead."
            )
        if not horizontal and not vertical:
            raise ValueError("At least one of horiz or vert has to be True!")
        self._init(locals())

    def get_transform(self, img, annotations=None):
        h, w = img.shape[:2]
        do = self._rand_range() < self.prob
        if do:
            if self.horizontal:
                return HFlipTransform(w)
            elif self.vertical:
                return VFlipTransform(h)
        else:
            return NoOpTransform()


@TRANSFORMS.register()
class TorchTransformGen:
    """
    Wrapper transfrom of transforms in torchvision.
    It convert img (np.ndarray) to PIL image, and convert back to np.ndarray after transform.
    """

    def __init__(self, tfm):
        self.tfm = tfm

    def __call__(self, img: np.ndarray, annotations: None, **kwargs):
        pil_image = Image.fromarray(img)
        return np.array(self.tfm(pil_image)), annotations


@TRANSFORMS.register()
class RandomDistortion(TransformGen):
    """
    Random distort image's hue, saturation and exposure.
    """

    def __init__(self, hue, saturation, exposure, image_format="BGR"):
        """
        RandomDistortion Initialization.
        Args:
            hue (float): value of hue
            saturation (float): value of saturation
            exposure (float): value of exposure
        """
        assert image_format in ["RGB", "BGR"]
        super().__init__()
        self._init(locals())

    def get_transform(self, img, annotations=None):
        return DistortTransform(self.hue, self.saturation, self.exposure,
                                self.image_format)


@TRANSFORMS.register()
class CenterAffine(TransformGen):
    """
    Affine Transform for CenterNet
    """

    def __init__(self, boarder, output_size, pad_value=[0, 0, 0],
                 random_aug=True):
        """
        output_size (w, h) shape
        """
        super().__init__()
        self._init(locals())

    def get_transform(self, img, annotations=None):
        img_shape = img.shape[:2]
        center, scale = self.generate_center_and_scale(img_shape)
        src, dst = self.generate_src_and_dst(center, scale, self.output_size)
        return AffineTransform(src, dst, self.output_size, self.pad_value)

    @staticmethod
    def _get_boarder(boarder, size):
        """
        This func may be rewirite someday
        """
        i = 1
        size //= 2
        while size <= boarder // i:
            i *= 2
        return boarder // i

    def generate_center_and_scale(self, img_shape):
        """
        generate center
        shpae : (h, w)
        """
        height, width = img_shape
        center = np.array([width / 2, height / 2], dtype=np.float32)
        scale = float(max(img_shape))
        if self.random_aug:
            scale = scale * np.random.choice(np.arange(0.6, 1.4, 0.1))
            h_boarder = self._get_boarder(self.boarder, height)
            w_boarder = self._get_boarder(self.boarder, width)
            center[0] = np.random.randint(low=w_boarder,
                                          high=width - w_boarder)
            center[1] = np.random.randint(low=h_boarder,
                                          high=height - h_boarder)
        else:
            pass

        return center, scale

    @staticmethod
    def generate_src_and_dst(center, scale, output_size):
        if not isinstance(scale, np.ndarray) and not isinstance(scale, list):
            scale = np.array([scale, scale], dtype=np.float32)
        src = np.zeros((3, 2), dtype=np.float32)
        src_w = scale[0]
        src_dir = [0, src_w * -0.5]
        src[0, :] = center
        src[1, :] = src[0, :] + src_dir
        src[2, :] = src[1, :] + (src_dir[1], -src_dir[0])

        dst = np.zeros((3, 2), dtype=np.float32)
        dst_w, dst_h = output_size
        dst_dir = [0, dst_w * -0.5]
        dst[0, :] = [dst_w * 0.5, dst_h * 0.5]
        dst[1, :] = dst[0, :] + dst_dir
        dst[2, :] = dst[1, :] + (dst_dir[1], -dst_dir[0])

        return src, dst


@TRANSFORMS.register()
class GaussianBlur(TransformGen):
    """
    Gaussian blur transform.
    """

    def __init__(self, sigma, p=1.0):
        """
        Args:
            sigma (List(float)): sigma of gaussian
            p (float): probability of perform this augmentation
        """
        super().__init__()
        self._init(locals())

    def get_transform(self, img, annotations=None):
        return GaussianBlurTransform(self.sigma, self.p)


@TRANSFORMS.register()
class Solarization(TransformGen):
    def __init__(self, threshold=128, p=0.5):
        super().__init__()
        self._init(locals())

    def get_transform(self, img, annotations=None):
        return SolarizationTransform(self.threshold, self.p)


@TRANSFORMS.register()
class GaussianBlurConv(TransformGen):
    def __init__(self, kernel_size, p):
        super().__init__()
        self._init(locals())

    def get_transform(self, img, annotations=None):
        return GaussianBlurConvTransform(self.kernel_size, self.p)


@TRANSFORMS.register()
class Resize(TransformGen):
    """
    Resize image to a target size
    """

    def __init__(self, shape, interp=Image.BILINEAR, scale_jitter=None):
        """
        Args:
            shape: (h, w) tuple or a int.
            interp: PIL interpolation method.
            scale_jitter: None or (0.8, 1.2)
        """
        if isinstance(shape, int):
            shape = (shape, shape)
        shape = tuple(shape)
        assert (scale_jitter is None or isinstance(scale_jitter, tuple))
        self._init(locals())

    def get_transform(self, img, annotations=None):
        if self.scale_jitter is not None:
            if len(self.scale_jitter) > 2:
                assert isinstance(self.scale_jitter[0], tuple)
                idx = np.random.choice(range(len(self.scale_jitter)))
                shape = self.scale_jitter[idx]
            else:
                jitter = np.random.uniform(self.scale_jitter[0], self.scale_jitter[1])
                shape = (int(self.shape[0] * jitter), int(self.shape[1] * jitter))
        else:
            shape = self.shape
        return ResizeTransform(
            img.shape[0], img.shape[1], shape[0], shape[1], self.interp
        )


@TRANSFORMS.register()
class ResizeLongestEdge(TransformGen):
    """
    Scale the longer edge to the given size.
    """

    def __init__(self, long_edge_length, sample_style="range",
                 interp=Image.BILINEAR,
                 jitter=(0.0, 32)):
        """
        Args:
            long_edge_length (list[int]): If ``sample_style=="range"``,
                a [min, max] interval from which to sample the shortest edge length.
                If ``sample_style=="choice"``, a list of shortest edge lengths to sample from.
            sample_style (str): either "range" or "choice".
            interp: PIL interpolation method.
        """
        super().__init__()
        assert sample_style in ["range", "choice"], sample_style

        self.is_range = sample_style == "range"
        if isinstance(long_edge_length, int):
            long_edge_length = (long_edge_length, long_edge_length)
        self._init(locals())

    def get_transform(self, img, annotations=None):
        h, w = img.shape[:2]
        if self.is_range:
            size = np.random.randint(
                self.long_edge_length[0], self.long_edge_length[1] + 1
            )
        else:
            size = np.random.choice(self.long_edge_length)
        if size == 0:
            return NoOpTransform()

        if self.jitter[0] > 0:
            dw = self.jitter[0] * w
            dh = self.jitter[0] * h
            size = max(h, w) + np.random.uniform(low=-max(dw, dh),
                                                 high=max(dw, dh))
            size -= size % self.jitter[1]

        scale = size * 1.0 / max(h, w)
        if h < w:
            newh, neww = scale * h, size
        else:
            newh, neww = size, scale * w

        neww = int(neww + 0.5)
        newh = int(newh + 0.5)

        return ResizeTransform(h, w, newh, neww, self.interp)


@TRANSFORMS.register()
class ResizeShortestEdge(TransformGen):
    """
    Scale the shorter edge to the given size, with a limit of `max_size` on the longer edge.
    If `max_size` is reached, then downscale so that the longer edge does not exceed max_size.
    """

    def __init__(
            self,
            short_edge_length,
            max_size=sys.maxsize,
            sample_style="range",
            interp=Image.BILINEAR,
    ):
        """
        Args:
            short_edge_length (list[int]): If ``sample_style=="range"``,
                a [min, max] interval from which to sample the shortest edge length.
                If ``sample_style=="choice"``, a list of shortest edge lengths to sample from.
            max_size (int): maximum allowed longest edge length.
            sample_style (str): either "range" or "choice".
            interp: PIL interpolation method.
        """
        super().__init__()
        assert sample_style in ["range", "choice"], sample_style

        self.is_range = sample_style == "range"
        if isinstance(short_edge_length, int):
            short_edge_length = (short_edge_length, short_edge_length)
        self._init(locals())

    def get_transform(self, img, annotations=None):
        h, w = img.shape[:2]

        if self.is_range:
            size = np.random.randint(
                self.short_edge_length[0], self.short_edge_length[1] + 1
            )
        else:
            size = np.random.choice(self.short_edge_length)
        if size == 0:
            return NoOpTransform()

        scale = size * 1.0 / min(h, w)
        if h < w:
            newh, neww = size, scale * w
        else:
            newh, neww = scale * h, size
        if max(newh, neww) > self.max_size:
            scale = self.max_size * 1.0 / max(newh, neww)
            newh = newh * scale
            neww = neww * scale
        neww = int(neww + 0.5)
        newh = int(newh + 0.5)
        return ResizeTransform(h, w, newh, neww, self.interp)


@TRANSFORMS.register()
class RandomCrop(TransformGen):
    """
    Randomly crop a subimage out of an image.
    """

    def __init__(self, crop_type: str, crop_size, strict_mode=True):
        """
        Args:
            crop_type (str): one of "relative_range", "relative", "absolute".
                See `config/defaults.py` for explanation.
            crop_size (tuple[float]): the relative ratio or absolute pixels of
                height and width
            strict_mode (bool): if `True`, the target `crop_size` must be smaller than
                the original image size.
        """
        super().__init__()
        assert crop_type in ["relative_range", "relative", "absolute"]
        self._init(locals())

    def get_transform(self, img, annotations=None):
        h, w = img.shape[:2]
        croph, cropw = self.get_crop_size((h, w))
        if self.strict_mode:
            assert h >= croph and w >= cropw, "Shape computation in {} has bugs.".format(
                self
            )
        offset_range_h = max(h - croph, 0)
        offset_range_w = max(w - cropw, 0)
        h0 = np.random.randint(offset_range_h + 1)
        w0 = np.random.randint(offset_range_w + 1)
        return CropTransform(w0, h0, cropw, croph)

    def get_crop_size(self, image_size):
        """
        Args:
            image_size (tuple): height, width

        Returns:
            crop_size (tuple): height, width in absolute pixels
        """
        h, w = image_size
        if self.crop_type == "relative":
            ch, cw = self.crop_size
            return int(h * ch + 0.5), int(w * cw + 0.5)
        elif self.crop_type == "relative_range":
            crop_size = np.asarray(self.crop_size, dtype=np.float32)
            ch, cw = crop_size + np.random.rand(2) * (1 - crop_size)
            return int(h * ch + 0.5), int(w * cw + 0.5)
        elif self.crop_type == "absolute":
            return self.crop_size
        else:
            NotImplementedError("Unknown crop type {}".format(self.crop_type))


@TRANSFORMS.register()
class RandomCropWithInstance(RandomCrop):
    """
    Make sure the cropping region contains the center of a random instance from annotations.
    """

    def get_transform(self, img, annotations=None):
        h, w = img.shape[:2]
        croph, cropw = self.get_crop_size((h, w))
        if self.strict_mode:
            assert h >= croph and w >= cropw, "Shape computation in {} has bugs.".format(
                self
            )
        offset_range_h = max(h - croph, 0)
        offset_range_w = max(w - cropw, 0)
        # Make sure there is always at least one instance in the image
        assert annotations is not None, "Can not get annotations infos."
        instance = np.random.choice(annotations)
        bbox = BoxMode.convert(instance["bbox"], instance["bbox_mode"],
                               BoxMode.XYXY_ABS)
        bbox = torch.tensor(bbox)
        center_xy = (bbox[:2] + bbox[2:]) / 2.0

        offset_range_h_min = max(center_xy[1] - croph, 0)
        offset_range_w_min = max(center_xy[0] - cropw, 0)
        offset_range_h_max = min(offset_range_h, center_xy[1] - 1)
        offset_range_w_max = min(offset_range_w, center_xy[0] - 1)

        h0 = np.random.randint(offset_range_h_min, offset_range_h_max + 1)
        w0 = np.random.randint(offset_range_w_min, offset_range_w_max + 1)
        return CropTransform(w0, h0, cropw, croph)


@TRANSFORMS.register()
class RandomCropWithMaxAreaLimit(RandomCrop):
    """
    Find a cropping window such that no single category occupies more than
    `single_category_max_area` in `sem_seg`.

    The function retries random cropping 10 times max.
    """

    def __init__(self, crop_type: str, crop_size, strict_mode=True,
                 single_category_max_area=1.0, ignore_value=255):
        super().__init__(crop_type, crop_size, strict_mode)
        self._init(locals())

    def get_transform(self, img, annotations=None):
        if self.single_category_max_area >= 1.0:
            crop_tfm = super().get_transform(img, annotations)
        else:
            h, w = img.shape[:2]
            assert "sem_seg" in annotations[0]
            sem_seg = annotations[0]["sem_seg"]
            croph, cropw = self.get_crop_size((h, w))
            for _ in range(10):
                y0 = np.random.randint(h - croph + 1)
                x0 = np.random.randint(w - cropw + 1)
                sem_seg_temp = sem_seg[y0: y0 + croph, x0: x0 + cropw]
                labels, cnt = np.unique(sem_seg_temp, return_counts=True)
                cnt = cnt[labels != self.ignore_value]
                if len(cnt) > 1 and np.max(cnt) / np.sum(
                        cnt) < self.single_category_max_area:
                    break
            crop_tfm = CropTransform(x0, y0, cropw, croph)
        return crop_tfm


@TRANSFORMS.register()
class RandomCropPad(RandomCrop):
    """
    Randomly crop and pad a subimage out of an image.
    """

    def __init__(self,
                 crop_type: str,
                 crop_size,
                 img_value=None,
                 seg_value=None):
        super().__init__(crop_type, crop_size, strict_mode=False)
        self._init(locals())

    def get_transform(self, img, annotations=None):
        h, w = img.shape[:2]
        croph, cropw = self.get_crop_size((h, w))
        h0 = np.random.randint(h - croph + 1) if h >= croph else 0
        w0 = np.random.randint(w - cropw + 1) if w >= cropw else 0
        dh = min(h, croph)
        dw = min(w, cropw)
        # print(w0, h0, dw, dh)
        return CropPadTransform(w0, h0, dw, dh, cropw, croph, self.img_value,
                                self.seg_value)


@TRANSFORMS.register()
class RandomExtent(TransformGen):
    """
    Outputs an image by cropping a random "subrect" of the source image.

    The subrect can be parameterized to include pixels outside the source image,
    in which case they will be set to zeros (i.e. black). The size of the output
    image will vary with the size of the random subrect.
    """

    def __init__(self, scale_range, shift_range):
        """
        Args:
            scale_range (l, h): Range of input-to-output size scaling factor.
            shift_range (x, y): Range of shifts of the cropped subrect. The rect
                is shifted by [w / 2 * Uniform(-x, x), h / 2 * Uniform(-y, y)],
                where (w, h) is the (width, height) of the input image. Set each
                component to zero to crop at the image's center.
        """
        super().__init__()
        self._init(locals())

    def get_transform(self, img, annotations=None):
        img_h, img_w = img.shape[:2]

        # Initialize src_rect to fit the input image.
        src_rect = np.array(
            [-0.5 * img_w, -0.5 * img_h, 0.5 * img_w, 0.5 * img_h])

        # Apply a random scaling to the src_rect.
        src_rect *= np.random.uniform(self.scale_range[0], self.scale_range[1])

        # Apply a random shift to the coordinates origin.
        src_rect[0::2] += self.shift_range[0] * img_w * (
                np.random.rand() - 0.5)
        src_rect[1::2] += self.shift_range[1] * img_h * (
                np.random.rand() - 0.5)

        # Map src_rect coordinates into image coordinates (center at corner).
        src_rect[0::2] += 0.5 * img_w
        src_rect[1::2] += 0.5 * img_h

        return ExtentTransform(
            src_rect=(src_rect[0], src_rect[1], src_rect[2], src_rect[3]),
            output_size=(
                int(src_rect[3] - src_rect[1]),
                int(src_rect[2] - src_rect[0]),
            ),
        )


@TRANSFORMS.register()
class RandomContrast(TransformGen):
    """
    Randomly transforms image contrast.

    Contrast intensity is uniformly sampled in (intensity_min, intensity_max).
    - intensity < 1 will reduce contrast
    - intensity = 1 will preserve the input image
    - intensity > 1 will increase contrast

    See: https://pillow.readthedocs.io/en/3.0.x/reference/ImageEnhance.html
    """

    def __init__(self, intensity_min, intensity_max, prob=1.0):
        """
        Args:
            intensity_min (float): Minimum augmentation.
            intensity_max (float): Maximum augmentation.
            prob (float): probability of transforms image contrast.
        """
        super().__init__()
        self._init(locals())

    def get_transform(self, img, annotations=None):
        do = self._rand_range() < self.prob
        if do:
            w = np.random.uniform(self.intensity_min, self.intensity_max)
            return BlendTransform(src_image=img.mean(), src_weight=1 - w,
                                  dst_weight=w)
        else:
            return NoOpTransform()


@TRANSFORMS.register()
class RandomBrightness(TransformGen):
    """
    Randomly transforms image brightness.

    Brightness intensity is uniformly sampled in (intensity_min, intensity_max).
    - intensity < 1 will reduce brightness
    - intensity = 1 will preserve the input image
    - intensity > 1 will increase brightness

    See: https://pillow.readthedocs.io/en/3.0.x/reference/ImageEnhance.html
    """

    def __init__(self, intensity_min, intensity_max, prob=1.):
        """
        Args:
            intensity_min (float): Minimum augmentation.
            intensity_max (float): Maximum augmentation.
            prob (float): probability of transforms image brightness.
        """
        super().__init__()
        self._init(locals())

    def get_transform(self, img, annotations=None):
        do = self._rand_range() < self.prob
        if do:
            w = np.random.uniform(self.intensity_min, self.intensity_max)
            return BlendTransform(src_image=0, src_weight=1 - w, dst_weight=w)
        else:
            return NoOpTransform()


@TRANSFORMS.register()
class RandomSaturation(TransformGen):
    """
    Randomly transforms image saturation.

    Saturation intensity is uniformly sampled in (intensity_min, intensity_max).
    - intensity < 1 will reduce saturation (make the image more grayscale)
    - intensity = 1 will preserve the input image
    - intensity > 1 will increase saturation

    See: https://pillow.readthedocs.io/en/3.0.x/reference/ImageEnhance.html
    """

    def __init__(self, intensity_min, intensity_max, prob=1.0):
        """
        Args:
            intensity_min (float): Minimum augmentation (1 preserves input).
            intensity_max (float): Maximum augmentation (1 preserves input).
            prob (float): probability of transforms image saturation.
        """
        super().__init__()
        self._init(locals())

    def get_transform(self, img, annotations=None):
        do = self._rand_range() < self.prob
        if do:
            assert img.shape[-1] == 3, "Saturation only works on RGB images"
            w = np.random.uniform(self.intensity_min, self.intensity_max)
            grayscale = img.dot([0.299, 0.587, 0.114])[:, :, np.newaxis]
            return BlendTransform(src_image=grayscale, src_weight=1 - w,
                                  dst_weight=w)
        else:
            return NoOpTransform()


@TRANSFORMS.register()
class RandomLighting(TransformGen):
    """
    Randomly transforms image color using fixed PCA over ImageNet.

    The degree of color jittering is randomly sampled via a normal distribution,
    with standard deviation given by the scale parameter.
    """

    def __init__(self, scale):
        """
        Args:
            scale (float): Standard deviation of principal component weighting.
        """
        super().__init__()
        self._init(locals())
        self.eigen_vecs = np.array(
            [
                [-0.5675, 0.7192, 0.4009],
                [-0.5808, -0.0045, -0.8140],
                [-0.5836, -0.6948, 0.4203],
            ]
        )
        self.eigen_vals = np.array([0.2175, 0.0188, 0.0045])

    def get_transform(self, img, annotations=None):
        assert img.shape[-1] == 3, "Saturation only works on RGB images"
        weights = np.random.normal(scale=self.scale, size=3)
        return BlendTransform(
            src_image=self.eigen_vecs.dot(weights * self.eigen_vals),
            src_weight=1.0,
            dst_weight=1.0,
        )


@TRANSFORMS.register()
class RandomSwapChannels(TransformGen):
    """
    Randomly swap image channels.
    """

    def __init__(self, prob=0.5):
        """
        Args:
            prob (float): probability of swap channels.
        """
        super().__init__()
        self._init(locals())

    def get_transform(self, img, annotations=None):
        _, w = img.shape[:2]
        do = self._rand_range() < self.prob
        if do:
            return RandomSwapChannelsTransform()
        else:
            return NoOpTransform()


@TRANSFORMS.register()
class MinIoURandomCrop(TransformGen):
    """
    Random crop the image & bboxes, the cropped patches have minimum IoU
    requirement with original image & bboxes, the IoU threshold is randomly
    selected from min_ious.
    """

    def __init__(self, min_ious=(0.1, 0.3, 0.5, 0.7, 0.9), min_crop_size=0.3):
        """
        Args:
            min_ious (tuple): minimum IoU threshold for all intersections with bounding boxes
            min_crop_size (float): minimum crop's size
                (i.e. h,w := a*h, a*w, where a >= min_crop_size).
        """
        super().__init__()
        self._init(locals())

    def get_transform(self, img, annotations):
        """
        Args:
            img (ndarray): of shape HxWxC(RGB). The array can be of type uint8
                in range [0, 255], or floating point in range [0, 255].
            annotations (list[dict[str->str]]):
                Each item in the list is a bbox label of an object. The object is
                    represented by a dict,
                which contains:
                 - bbox (list): bbox coordinates, top left and bottom right.
                 - bbox_mode (str): bbox label mode, for example: `XYXY_ABS`,
                    `XYWH_ABS` and so on...
        """
        sample_mode = (1, *self.min_ious, 0)
        h, w = img.shape[:2]

        boxes = list()
        for obj in annotations:
            boxes.append(BoxMode.convert(obj["bbox"], obj["bbox_mode"],
                                         BoxMode.XYXY_ABS))
        boxes = torch.tensor(boxes)

        while True:
            mode = np.random.choice(sample_mode)
            if mode == 1:
                return NoOpTransform()

            min_iou = mode
            for i in range(50):
                new_w = np.random.uniform(self.min_crop_size * w, w)
                new_h = np.random.uniform(self.min_crop_size * h, h)

                # h / w in [0.5, 2]
                if new_h / new_w < 0.5 or new_h / new_w > 2:
                    continue

                left = np.random.uniform(w - new_w)
                top = np.random.uniform(h - new_h)

                patch = np.array(
                    (int(left), int(top), int(left + new_w), int(top + new_h)))

                overlaps = pairwise_iou(
                    Boxes(patch.reshape(-1, 4)),
                    Boxes(boxes.reshape(-1, 4))
                )

                if overlaps.min() < min_iou:
                    continue

                # center of boxes should inside the crop img
                center = (boxes[:, :2] + boxes[:, 2:]) / 2
                mask = ((center[:, 0] > patch[0]) * (center[:, 1] > patch[1])
                        * (center[:, 0] < patch[2]) * (
                                center[:, 1] < patch[3]))
                if not mask.any():
                    continue
                return IoUCropTransform(int(left), int(top), int(new_w),
                                        int(new_h))


@TRANSFORMS.register()
class Expand(TransformGen):
    """
    Random Expand the image & bboxes.
    """

    def __init__(self, ratio_range=(1, 4), mean=(0, 0, 0), prob=0.5):
        """
        Args:
            ratio_range (tuple): range of expand ratio.
            mean (tuple): mean value of dataset.
            prob (float): probability of applying this transformation.
        """
        super().__init__()
        self._init(locals())
        self.min_ratio, self.max_ratio = ratio_range

    def get_transform(self, img, annotations=None):
        if np.random.uniform(0, 1) > self.prob:
            return NoOpTransform()
        h, w, c = img.shape
        ratio = np.random.uniform(self.min_ratio, self.max_ratio)
        left = int(np.random.uniform(0, w * ratio - w))
        top = int(np.random.uniform(0, h * ratio - h))
        return ExpandTransform(left, top, ratio, self.mean)


@TRANSFORMS.register()
class RandomScale(TransformGen):
    """
    Randomly scale the image according to the specified output size and scale ratio range.

    This transform has the following three steps:

        1. select a random scale factor according to the specified scale ratio range.
        2. recompute the accurate scale_factor using rounded scaled image size.
        3. select non-zero random offset (x, y) if scaled image is larger than output_size.
    """

    def __init__(self, output_size, ratio_range=(0.1, 2), interp="BILINEAR"):
        """
        Args:
            output_size (tuple): image output size.
            ratio_range (tuple): range of scale ratio.
            interp (str): the interpolation method. Options includes:
              * "NEAREST"
              * "BILINEAR"
              * "BICUBIC"
              * "LANCZOS"
              * "HAMMING"
              * "BOX"
        """
        super().__init__()
        self._init(locals())
        self.min_ratio, self.max_ratio = ratio_range
        if isinstance(self.output_size, int):
            self.output_size = [self.output_size] * 2

    def get_transform(self, img, annotations=None):
        h, w = img.shape[:2]
        output_h, output_w = self.output_size

        # 1. Select a random scale factor.
        random_scale_factor = np.random.uniform(self.min_ratio, self.max_ratio)

        scaled_size_h = int(random_scale_factor * output_h)
        scaled_size_w = int(random_scale_factor * output_w)

        # 2. Recompute the accurate scale_factor using rounded scaled image size.
        image_scale_h = scaled_size_h * 1.0 / h
        image_scale_w = scaled_size_w * 1.0 / w
        image_scale = min(image_scale_h, image_scale_w)

        # 3. Select non-zero random offset (x, y) if scaled image is larger than output_size.
        scaled_h = int(h * 1.0 * image_scale)
        scaled_w = int(w * 1.0 * image_scale)

        return ScaleTransform(h, w, scaled_h, scaled_w, self.interp)


@TRANSFORMS.register()
class AutoAugment(TransformGen):
    """
    Convert any of AutoAugment into a cvpods-fashion Transform such that can be configured in
        config.py
    """

    def __init__(self, name, prob=0.5, magnitude=10, hparams=None):
        """
        Args:
            name (str): any type of transforms list in _RAND_TRANSFORMS.
            prob (float): probability of perform current augmentation.
            magnitude (int): intensity / magnitude of each augmentation.
            hparams (dict): hyper-parameters required by each augmentation.
        """

        super().__init__()
        self._init(locals())

    def get_transform(self, img, annotations=None):
        return AutoAugmentTransform(self.name, self.prob, self.magnitude,
                                    self.hparams)


@TRANSFORMS.register()
class Pad(TransformGen):
    """
    Pad image with `pad_value` to the specified `target_h` and `target_w`.

    Adds `top` rows of `pad_value` on top, `left` columns of `pad_value` on the left,
    and then pads the image on the bottom and right with `pad_value` until it has
    dimensions `target_h`, `target_w`.

    This op does nothing if `top` and `left` is zero and the image already has size
    `target_h` by `target_w`.
    """

    def __init__(self, top, left, target_h, target_w, pad_value=0):
        """
        Args:
            top (int): number of rows of `pad_value` to add on top.
            left (int): number of columns of `pad_value` to add on the left.
            target_h (int): height of output image.
            target_w (int): width of output image.
            pad_value (int): the value used to pad the image.
        """
        super().__init__()
        self._init(locals())

    def get_transform(self, img, annotations=None):
        return PadTransform(self.top, self.left, self.target_h, self.target_w,
                            self.pad_value)


@TRANSFORMS.register()
class RandomList(TransformGen):
    """
    Random select subset of provided augmentations.
    """

    def __init__(self, transforms, num_layers=2, choice_weights=None):
        """
        Args:
            transforms (List[TorchTransformGen]): list of transforms need to be performed.
            num_layers (int): parameters of np.random.choice.
            choice_weights (optional, float): parameters of np.random.choice.
        """
        self.transforms = transforms
        self.num_layers = num_layers
        self.choice_weights = choice_weights

    def get_transform(self, img, annotations=None):
        tfms = np.random.choice(
            self.transforms,
            self.num_layers,
            replace=self.choice_weights is None,
            p=self.choice_weights)
        return ComposeTransform(tfms)


@TRANSFORMS.register()
class ShuffleList(TransformGen):
    """
    Randomly shuffle the `transforms` order.
    """

    def __init__(self, transforms):
        """
        Args:
            transforms (list[TransformGen]): List of transform to be shuffled.
        """
        super().__init__()
        self.transforms = transforms

    def get_transform(self, img, annotations=None):
        np.random.shuffle(self.transforms)
        return ComposeTransform(self.transforms)


@TRANSFORMS.register()
class RepeatList(TransformGen):
    """
    Forward several times of provided transforms for a given image.
    """

    def __init__(self, transforms, repeat_times):
        """
        Args:
            transforms (list[TransformGen]): List of transform to be repeated.
            repeat_times (int): number of duplicates desired.
        """
        super().__init__()
        self.transforms = transforms
        self.times = repeat_times

    def get_transform(self, img, annotations=None):
        return ComposeTransform(self.transforms)

    def __call__(self, img, annotations=None, **kwargs):
        repeat_imgs = []
        repeat_annotations = []
        for t in range(self.times):
            tmp_img, tmp_anno = self.get_transform(img)(img, annotations,
                                                        **kwargs)
            repeat_imgs.append(tmp_img)
            repeat_annotations.append(tmp_anno)
        repeat_imgs = np.stack(repeat_imgs, axis=0)
        return repeat_imgs, repeat_annotations
```

#### cvpods/data/transforms/transform.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) BaseDetection, Inc. and its affiliates. All Rights Reserved

import inspect
import random
from abc import ABCMeta, abstractmethod
from typing import Callable, TypeVar

import cv2
import numpy as np
from PIL import Image, ImageFilter, ImageOps
import pycocotools.mask as mask_util

import torch
import torchvision.transforms as transforms

import cvpods
from cvpods.structures import BoxMode

from .transform_util import to_float_tensor, to_numpy

__all__ = [
    "ExpandTransform",
    "AffineTransform",
    "BlendTransform",
    "IoUCropTransform",
    "CropTransform",
    "CropPadTransform",
    "JitterCropTransform",
    "GridSampleTransform",
    "RotationTransform",
    "HFlipTransform",
    "VFlipTransform",
    "NoOpTransform",
    "ScaleTransform",
    "DistortTransform",
    "DistortTransform2",
    "ShiftTransform",
    "Transform",
    "TransformList",
    "ExtentTransform",
    "ResizeTransform",
    # Transform used in ssl
    "GaussianBlurTransform",
    "GaussianBlurConvTransform",
    "SolarizationTransform",
    "ComposeTransform",
    "LabSpaceTransform",
    "PadTransform",
]


# NOTE: to document methods in subclasses, it's sufficient to only document those whose
# implemenation needs special attention.


class Transform(metaclass=ABCMeta):
    """
    Base class for implementations of __deterministic__ transformations for
    image and other data structures. "Deterministic" requires that the output of
    all methods of this class are deterministic w.r.t their input arguments. In
    training, there should be a higher-level policy that generates (likely with
    random variations) these transform ops. Each transform op may handle several
    data types, e.g.: image, coordinates, segmentation, bounding boxes. Some of
    them have a default implementation, but can be overwritten if the default
    isn't appropriate. The implementation of each method may choose to modify
    its input data in-place for efficient transformation.
    """

    def _set_attributes(self, params: list = None):
        """
        Set attributes from the input list of parameters.

        Args:
            params (list): list of parameters.
        """

        if params:
            for k, v in params.items():
                if k != "self" and not k.startswith("_"):
                    setattr(self, k, v)

    @abstractmethod
    def apply_image(self, img: np.ndarray):
        """
        Apply the transform on an image.

        Args:
            img (ndarray): of shape NxHxWxC, or HxWxC or HxW. The array can be
                of type uint8 in range [0, 255], or floating point in range
                [0, 1] or [0, 255].

        Returns:
            ndarray: image after apply the transformation.
        """
        pass

    @abstractmethod
    def apply_coords(self, coords: np.ndarray):
        """
        Apply the transform on coordinates.

        Args:
            coords (ndarray): floating point array of shape Nx2. Each row is (x, y).

        Returns:
            ndarray: coordinates after apply the transformation.

        Note:
            The coordinates are not pixel indices. Coordinates on an image of
            shape (H, W) are in range [0, W] or [0, H].
        """

        pass

    def apply_segmentation(self, segmentation: np.ndarray) -> np.ndarray:
        """
        Apply the transform on a full-image segmentation.
        By default will just perform "apply_image".

        Args:
            segmentation (ndarray): of shape HxW. The array should have integer
            or bool dtype.

        Returns:
            ndarray: segmentation after apply the transformation.
        """
        return self.apply_image(segmentation)

    def apply_box(self, box: np.ndarray) -> np.ndarray:
        """
        Apply the transform on an axis-aligned box.
        By default will transform the corner points and use their
        minimum/maximum to create a new axis-aligned box.
        Note that this default may change the size of your box, e.g. in
        rotations.

        Args:
            box (ndarray): Nx4 floating point array of XYXY format in absolute
                coordinates.
        Returns:
            ndarray: box after apply the transformation.

        Note:
            The coordinates are not pixel indices. Coordinates on an image of
            shape (H, W) are in range [0, W] or [0, H].
        """
        # Indexes of converting (x0, y0, x1, y1) box into 4 coordinates of
        # ([x0, y0], [x1, y0], [x0, y1], [x1, y1]).
        idxs = np.array([(0, 1), (2, 1), (0, 3), (2, 3)]).flatten()
        coords = np.asarray(box).reshape(-1, 4)[:, idxs].reshape(-1, 2)
        coords = self.apply_coords(coords).reshape((-1, 4, 2))
        minxy = coords.min(axis=1)
        maxxy = coords.max(axis=1)
        trans_boxes = np.concatenate((minxy, maxxy), axis=1)
        return trans_boxes

    def apply_polygons(self, polygons: list) -> list:
        """
        Apply the transform on a list of polygons, each represented by a Nx2
        array.
        By default will just transform all the points.

        Args:
            polygon (list[ndarray]): each is a Nx2 floating point array of
                (x, y) format in absolute coordinates.
        Returns:
            list[ndarray]: polygon after apply the transformation.

        Note:
            The coordinates are not pixel indices. Coordinates on an image of
            shape (H, W) are in range [0, W] or [0, H].
        """
        return [self.apply_coords(p) for p in polygons]

    def apply_meta_infos(self, meta_infos: dict) -> dict:
        return meta_infos

    def __call__(self, image, annotations=None, **kwargs):
        """
        Apply transfrom to images and annotations (if exist)
        """
        image_size = image.shape[:2]  # h, w
        image = self.apply_image(image)

        if annotations is not None:
            for annotation in annotations:
                if "bbox" in annotation:
                    bbox = BoxMode.convert(
                        annotation["bbox"], annotation["bbox_mode"],
                        BoxMode.XYXY_ABS)
                    # Note that bbox is 1d (per-instance bounding box)
                    annotation["bbox"] = self.apply_box([bbox])[0]
                    annotation["bbox_mode"] = BoxMode.XYXY_ABS

                if "segmentation" in annotation:
                    # each instance contains 1 or more polygons
                    segm = annotation["segmentation"]
                    if isinstance(segm, list):
                        # polygons
                        polygons = [np.asarray(p).reshape(-1, 2) for p in segm]
                        annotation["segmentation"] = [
                            p.reshape(-1) for p in
                            self.apply_polygons(polygons)
                        ]
                    elif isinstance(segm, dict):
                        # RLE
                        mask = mask_util.decode(segm)
                        mask = self.apply_segmentation(mask)
                        assert tuple(mask.shape[:2]) == image_size
                        annotation["segmentation"] = mask
                    else:
                        raise ValueError(
                            "Cannot transform segmentation of type '{}'!"
                            "Supported types are: polygons as list[list[float] or ndarray],"
                            " COCO-style RLE as a dict.".format(type(segm)))

                if "keypoints" in annotation:
                    """
                    Transform keypoint annotation of an image.

                    Args:
                        keypoints (list[float]): Nx3 float in cvpods Dataset format.
                        transforms (TransformList):
                        image_size (tuple): the height, width of the transformed image
                        keypoint_hflip_indices (ndarray[int]): see `create_keypoint_hflip_indices`.
                    """
                    # (N*3,) -> (N, 3)
                    keypoints = annotation["keypoints"]
                    keypoints = np.asarray(keypoints, dtype="float64").reshape(
                        -1, 3)
                    keypoints[:, :2] = self.apply_coords(keypoints[:, :2])

                    # This assumes that HorizFlipTransform is the only one that does flip
                    do_hflip = isinstance(self,
                                          cvpods.data.transforms.transform.HFlipTransform)

                    # Alternative way: check if probe points was horizontally flipped.
                    # probe = np.asarray([[0.0, 0.0], [image_width, 0.0]])
                    # probe_aug = transforms.apply_coords(probe.copy())
                    # do_hflip = np.sign(probe[1][0] - probe[0][0]) != np.sign(probe_aug[1][0] - probe_aug[0][0])  # noqa

                    # If flipped, swap each keypoint with its opposite-handed equivalent
                    if do_hflip:
                        if "keypoint_hflip_indices" in kwargs:
                            keypoints = keypoints[
                                        kwargs["keypoint_hflip_indices"], :]

                    # Maintain COCO convention that if visibility == 0, then x, y = 0
                    # TODO may need to reset visibility for cropped keypoints,
                    # but it does not matter for our existing algorithms
                    keypoints[keypoints[:, 2] == 0] = 0

                    annotation["keypoints"] = keypoints

                # For sem seg task
                if "sem_seg" in annotation:
                    sem_seg = annotation["sem_seg"]
                    if isinstance(sem_seg, np.ndarray):
                        sem_seg = self.apply_segmentation(sem_seg)
                        assert tuple(sem_seg.shape[:2]) == tuple(
                            image.shape[:2]), (
                            f"Image shape is {image.shape[:2]}, "
                            f"but sem_seg shape is {sem_seg.shape[:2]}."
                        )
                        annotation["sem_seg"] = sem_seg
                    else:
                        raise ValueError(
                            "Cannot transform segmentation of type '{}'!"
                            "Supported type is ndarray.".format(type(sem_seg)))

                if "meta_infos" in annotation:
                    meta_infos = annotation["meta_infos"]
                    meta_infos = self.apply_meta_infos(meta_infos)
                    annotation["meta_infos"] = meta_infos
        return image, annotations

    @classmethod
    def register_type(cls, data_type: str, func: Callable):
        """
        Register the given function as a handler that this transform will use
        for a specific data type.

        Args:
            data_type (str): the name of the data type (e.g., box)
            func (callable): takes a transform and a data, returns the
                transformed data.

        Examples:

        .. code-block:: python

            def func(flip_transform, voxel_data):
                return transformed_voxel_data
            HFlipTransform.register_type("voxel", func)

            # ...
            transform = HFlipTransform(...)
            transform.apply_voxel(voxel_data)  # func will be called
        """
        assert callable(
            func
        ), "You can only register a callable to a Transform. Got {} instead.".format(
            func)
        argspec = inspect.getfullargspec(func)
        assert len(argspec.args) == 2, (
            "You can only register a function that takes two positional "
            "arguments to a Transform! Got a function with spec {}".format(
                str(argspec)))
        setattr(cls, "apply_" + data_type, func)


_T = TypeVar("_T")


class ComposeTransform(object):
    """
    Composes several transforms together.
    """

    def __init__(self, tfms):
        """
        Args:
            transforms (list[Transform]): list of transforms to compose.
        """
        super().__init__()
        self.transforms = tfms

    def __eq__(self, other):
        if not isinstance(other, ComposeTransform):
            return False
        return self.transforms == other.transforms

    def __call__(self, img, annotations=None, **kwargs):
        for tfm in self.transforms:
            img, annotations = tfm(img, annotations, **kwargs)
        return img, annotations

    def __repr__(self):
        return "".join([tfm for tfm in self.transforms])


# TODO: Deprecated
# pyre-ignore-all-errors
class TransformList:
    """
    Maintain a list of transform operations which will be applied in sequence.
    Attributes:
        transforms (list[Transform])
    """

    def __init__(self, transforms: list):
        """
        Args:
            transforms (list[Transform]): list of transforms to perform.
        """
        super().__init__()
        for t in transforms:
            assert isinstance(t, Transform), t
        self.transforms = transforms

    def _apply(self, x: _T, meth: str) -> _T:
        """
        Apply the transforms on the input.
        Args:
            x: input to apply the transform operations.
            meth (str): meth.
        Returns:
            x: after apply the transformation.
        """
        for t in self.transforms:
            x = getattr(t, meth)(x)
        return x

    def __getattr__(self, name: str):
        """
        Args:
            name (str): name of the attribute.
        """
        if name.startswith("apply_"):
            return lambda x: self._apply(x, name)
        raise AttributeError(
            "TransformList object has no attribute {}".format(name))

    def __add__(self, other: "TransformList") -> "TransformList":
        """
        Args:
            other (TransformList): transformation to add.
        Returns:
            TransformList: list of transforms.
        """
        others = (other.transforms
                  if isinstance(other, TransformList) else [other])
        return TransformList(self.transforms + others)

    def __iadd__(self, other: "TransformList") -> "TransformList":
        """
        Args:
            other (TransformList): transformation to add.
        Returns:
            TransformList: list of transforms.
        """
        others = (other.transforms
                  if isinstance(other, TransformList) else [other])
        self.transforms.extend(others)
        return self

    def __radd__(self, other: "TransformList") -> "TransformList":
        """
        Args:
            other (TransformList): transformation to add.
        Returns:
            TransformList: list of transforms.
        """
        others = (other.transforms
                  if isinstance(other, TransformList) else [other])
        return TransformList(others + self.transforms)

    def insert(self, idx: int, other: "TransformList") -> "TransformList":
        """
        Args:
            idx (int): insert position.
            other (TransformList): transformation to insert.
        Returns:
            None
        """
        assert idx in range(len(self.transforms))
        others = (other.transforms
                  if isinstance(other, TransformList) else [other])
        self.transforms = self.transforms[:idx] + others + self.transforms[
                                                           idx:]


class DistortTransform(Transform):
    """
    Distort image w.r.t hue, saturation and exposure.
    """

    def __init__(self, hue, saturation, exposure, image_format):
        super().__init__()
        self._set_attributes(locals())
        self.cvt_code = {
            "RGB": (cv2.COLOR_RGB2HSV, cv2.COLOR_HSV2RGB),
            "BGR": (cv2.COLOR_BGR2HSV, cv2.COLOR_HSV2BGR),
        }[image_format]
        if saturation > 1.0:
            saturation /= 255.  # in range [0, 1]

    def apply_image(self, img: np.ndarray) -> np.ndarray:
        """
        Args:
            img (ndarray): of shape HxW, HxWxC, or NxHxWxC. The array can be
                of type uint8 in range [0, 255], or floating point in range
                [0, 1] or [0, 255].

        Returns:
            ndarray: the distorted image(s).
        """
        dhue = np.random.uniform(low=-self.hue, high=self.hue)
        dsat = self._rand_scale(self.saturation)
        dexp = self._rand_scale(self.exposure)

        dtype = img.dtype
        img = cv2.cvtColor(img, self.cvt_code[0])
        img = np.asarray(img, dtype=np.float32) / 255.
        img[:, :, 1] *= dsat
        img[:, :, 2] *= dexp
        H = img[:, :, 0] + dhue

        if dhue > 0:
            H[H > 1.0] -= 1.0
        else:
            H[H < 0.0] += 1.0

        img[:, :, 0] = H
        img = (img * 255).clip(0, 255).astype(np.uint8)
        img = cv2.cvtColor(img, self.cvt_code[1])
        img = np.asarray(img, dtype=dtype)

        return img

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        return coords

    def _rand_scale(self, upper_bound):
        """
        Calculate random scaling factor.

        Args:
            upper_bound (float): range of the random scale.
        Returns:
            random scaling factor (float) whose range is
            from 1 / s to s .
        """
        scale = np.random.uniform(low=1, high=upper_bound)
        if np.random.rand() > 0.5:
            return scale
        return 1 / scale

    def apply_segmentation(self, segmentation: np.ndarray) -> np.ndarray:
        return segmentation


class JitterCropTransform(Transform):
    """JitterCrop data augmentation used in YOLOv4.

    Notes:
        - Rewrite as Yolo.
        - A different method to crop image

    Steps:
        - 1. get random offset of four boundary
        - 2. get target crop size
        - 3. get target crop image
        - 4. filter bbox by valid region

    Args:
        pleft (int): left offset.
        pright (int): right offset.
        ptop (int): top offset.
        pbot (int): bottom offset.
        output_size (tuple(int)): output size (w, h).
    """

    def __init__(self, pleft, pright, ptop, pbot, output_size):
        super().__init__()
        self._set_attributes(locals())

    def apply_image(self, img: np.ndarray) -> np.ndarray:
        """
        Args:
            img (ndarray): of shape HxW, HxWxC, or NxHxWxC. The array can be
                of type uint8 in range [0, 255], or floating point in range
                [0, 1] or [0, 255].

        Returns:
            ndarray: the cropped image(s).
        """
        oh, ow = img.shape[:2]

        swidth, sheight = self.output_size

        src_rect = [self.pleft, self.ptop, swidth + self.pleft,
                    sheight + self.ptop]  # x1,y1,x2,y2
        img_rect = [0, 0, ow, oh]
        # rect intersection
        new_src_rect = [max(src_rect[0], img_rect[0]),
                        max(src_rect[1], img_rect[1]),
                        min(src_rect[2], img_rect[2]),
                        min(src_rect[3], img_rect[3])]
        dst_rect = [max(0, -self.pleft),
                    max(0, -self.ptop),
                    max(0, -self.pleft) + new_src_rect[2] - new_src_rect[0],
                    max(0, -self.ptop) + new_src_rect[3] - new_src_rect[1]]

        # crop the image
        cropped = np.zeros([sheight, swidth, 3], dtype=img.dtype)
        cropped[:, :, ] = np.mean(img, axis=(0, 1))
        cropped[dst_rect[1]:dst_rect[3], dst_rect[0]:dst_rect[2]] = \
            img[new_src_rect[1]:new_src_rect[3],
            new_src_rect[0]:new_src_rect[2]]
        return cropped

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        """
        Crop the coordinates.

        Args:
            coords (ndarray): floating point array of shape Nx2. Each row is
                (x, y).
        Returns:
            ndarray: the flipped coordinates.

        Note:
            The inputs are floating point coordinates, not pixel indices.
            Therefore they are flipped by `(W - x, H - y)`, not
            `(W - 1 - x, H 1 - y)`.
        """
        coords_offset = np.array([self.pleft, self.ptop], dtype=np.float32)
        coords = coords - coords_offset
        swidth, sheight = self.output_size
        coords[..., 0] = np.clip(coords[..., 0], 0, swidth - 1)
        coords[..., 1] = np.clip(coords[..., 1], 0, sheight - 1)
        return coords

    def apply_meta_infos(self, meta_infos: dict) -> dict:
        meta_infos["jitter_pad_left"] = self.pleft
        meta_infos["jitter_pad_right"] = self.pright
        meta_infos["jitter_pad_top"] = self.ptop
        meta_infos["jitter_pad_bot"] = self.pbot
        meta_infos["jitter_swidth"] = self.output_size[0]
        meta_infos["jitter_sheight"] = self.output_size[1]
        return meta_infos


class DistortTransform2(Transform):
    """
    Distort image w.r.t hue, saturation and exposure.
    """

    def __init__(self, hue, saturation, exposure):
        super().__init__()
        self._set_attributes(locals())

    def apply_image(self, img: np.ndarray) -> np.ndarray:
        """
        Args:
            img (ndarray): of shape HxW, HxWxC, or NxHxWxC. The array can be
                of type uint8 in range [0, 255], or floating point in range
                [0, 1] or [0, 255].

        Returns:
            ndarray: the distorted image(s).
        """
        dhue = np.random.uniform(low=-self.hue, high=self.hue)
        dsat = self._rand_scale(self.saturation)
        dexp = self._rand_scale(self.exposure)

        img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
        img = np.asarray(img, dtype=np.float32) / 255.
        img[:, :, 1] *= dsat
        img[:, :, 2] *= dexp
        H = img[:, :, 0] + dhue * 179 / 255.

        if dhue > 0:
            H[H > 1.0] -= 1.0
        else:
            H[H < 0.0] += 1.0

        img[:, :, 0] = H
        img = (img * 255).clip(0, 255).astype(np.uint8)
        img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)
        img = np.asarray(img, dtype=np.float32)

        return img

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        return coords

    def _rand_scale(self, upper_bound):
        """
        Calculate random scaling factor.

        Args:
            upper_bound (float): range of the random scale.
        Returns:
            random scaling factor (float) whose range is
            from 1 / s to s .
        """
        scale = np.random.uniform(low=1, high=upper_bound)
        if np.random.rand() > 0.5:
            return scale
        return 1 / scale


class AffineTransform(Transform):
    """
    Augmentation from CenterNet
    """

    def __init__(self, src, dst, output_size, pad_value=[0, 0, 0]):
        """
        output_size:(w, h)
        """
        super().__init__()
        affine = cv2.getAffineTransform(np.float32(src), np.float32(dst))
        self._set_attributes(locals())

    def apply_image(self, img: np.ndarray) -> np.ndarray:
        """
        Apply AffineTransform for the image(s).

        Args:
            img (ndarray): of shape HxW, HxWxC, or NxHxWxC. The array can be
                of type uint8 in range [0, 255], or floating point in range
                [0, 1] or [0, 255].
        Returns:
            ndarray: the image(s) after applying affine transform.
        """
        return cv2.warpAffine(img,
                              self.affine,
                              self.output_size,
                              flags=cv2.INTER_LINEAR,
                              borderValue=self.pad_value)

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        """
        Affine the coordinates.

        Args:
            coords (ndarray): floating point array of shape Nx2. Each row is
                (x, y).
        Returns:
            ndarray: the flipped coordinates.

        Note:
            The inputs are floating point coordinates, not pixel indices.
            Therefore they are flipped by `(W - x, H - y)`, not
            `(W - 1 - x, H 1 - y)`.
        """
        # aug_coord (N, 3) shape, self.affine (2, 3) shape
        w, h = self.output_size
        aug_coords = np.column_stack((coords, np.ones(coords.shape[0])))
        coords = np.dot(aug_coords, self.affine.T)
        coords[..., 0] = np.clip(coords[..., 0], 0, w - 1)
        coords[..., 1] = np.clip(coords[..., 1], 0, h - 1)
        return coords


class RotationTransform(Transform):
    """
    This method returns a copy of this image, rotated the given
    number of degrees counter clockwise around its center.
    """

    def __init__(self, h, w, angle, expand=True, center=None, interp=None):
        """
        Args:
            h, w (int): original image size
            angle (float): degrees for rotation
            expand (bool): choose if the image should be resized to fit the whole
                rotated image (default), or simply cropped
            center (tuple (width, height)): coordinates of the rotation center
                if left to None, the center will be fit to the center of each image
                center has no effect if expand=True because it only affects shifting
            interp: cv2 interpolation method, default cv2.INTER_LINEAR
        """
        super().__init__()
        image_center = np.array((w / 2, h / 2))
        if center is None:
            center = image_center
        if interp is None:
            interp = cv2.INTER_LINEAR
        abs_cos, abs_sin = (
            abs(np.cos(np.deg2rad(angle))), abs(np.sin(np.deg2rad(angle))))
        if expand:
            # find the new width and height bounds
            bound_w, bound_h = np.rint(
                [h * abs_sin + w * abs_cos, h * abs_cos + w * abs_sin]
            ).astype(int)
        else:
            bound_w, bound_h = w, h

        self._set_attributes(locals())
        self.rm_coords = self.create_rotation_matrix()
        # Needed because of this problem https://github.com/opencv/opencv/issues/11784
        self.rm_image = self.create_rotation_matrix(offset=-0.5)

    def apply_image(self, img, interp=None):
        """
        img should be a numpy array, formatted as Height * Width * Nchannels
        """
        if len(img) == 0 or self.angle % 360 == 0:
            return img
        assert img.shape[:2] == (self.h, self.w)
        interp = interp if interp is not None else self.interp
        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h),
                              flags=interp)

    def apply_coords(self, coords):
        """
        coords should be a N * 2 array-like, containing N couples of (x, y) points
        """
        coords = np.asarray(coords, dtype=float)
        if len(coords) == 0 or self.angle % 360 == 0:
            return coords
        return cv2.transform(coords[:, np.newaxis, :], self.rm_coords)[:, 0, :]

    def apply_segmentation(self, segmentation):
        segmentation = self.apply_image(segmentation, interp=cv2.INTER_NEAREST)
        return segmentation

    def create_rotation_matrix(self, offset=0):
        center = (self.center[0] + offset, self.center[1] + offset)
        rm = cv2.getRotationMatrix2D(tuple(center), self.angle, 1)
        if self.expand:
            # Find the coordinates of the center of rotation in the new image
            # The only point for which we know the future coordinates is the center of the image
            rot_im_center = cv2.transform(
                self.image_center[None, None, :] + offset, rm)[0, 0, :]
            new_center = np.array(
                [self.bound_w / 2, self.bound_h / 2]) + offset - rot_im_center
            # shift the rotation center to the new coordinates
            rm[:, 2] += new_center
        return rm

    def inverse(self):
        """
        The inverse is to rotate it back with expand, and crop to get the original shape.
        """
        if not self.expand:  # Not possible to inverse if a part of the image is lost
            raise NotImplementedError()
        rotation = RotationTransform(
            self.bound_h, self.bound_w, -self.angle, True, None, self.interp
        )
        crop = CropTransform(
            (rotation.bound_w - self.w) // 2, (rotation.bound_h - self.h) // 2,
            self.w, self.h
        )
        return TransformList([rotation, crop])


class HFlipTransform(Transform):
    """
    Perform horizontal flip.
    """

    def __init__(self, width: int):
        super().__init__()
        self._set_attributes(locals())

    def apply_image(self, img: np.ndarray) -> np.ndarray:
        """
        Flip the image(s).

        Args:
            img (ndarray): of shape HxW, HxWxC, or NxHxWxC. The array can be
                of type uint8 in range [0, 255], or floating point in range
                [0, 1] or [0, 255].

        Returns:
            ndarray: the flipped image(s).
        """
        tensor = torch.from_numpy(np.ascontiguousarray(img).copy())
        if len(tensor.shape) == 2:
            # For dimension of HxW.
            tensor = tensor.flip((-1))
        elif len(tensor.shape) > 2:
            # For dimension of HxWxC, NxHxWxC.
            tensor = tensor.flip((-2))
        return tensor.numpy()

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        """
        Flip the coordinates.

        Args:
            coords (ndarray): floating point array of shape Nx2. Each row is (x, y).

        Returns:
            ndarray: the flipped coordinates.

        Note:
            The inputs are floating point coordinates, not pixel indices.
            Therefore they are flipped by `(W - x, H - y)`, not
            `(W - 1 - x, H 1 - y)`.
        """
        coords[:, 0] = self.width - coords[:, 0]
        return coords

    def apply_meta_infos(self, meta_infos: dict) -> dict:
        pleft = meta_infos["jitter_pad_left"]
        pright = meta_infos["jitter_pad_right"]
        pleft, pright = pright, pleft
        meta_infos["jitter_pad_left"] = pleft
        meta_infos["jitter_pad_right"] = pright
        return meta_infos


class VFlipTransform(Transform):
    """
    Perform vertical flip.
    """

    def __init__(self, height: int):
        super().__init__()
        self._set_attributes(locals())

    def apply_image(self, img: np.ndarray) -> np.ndarray:
        """
        Flip the image(s).

        Args:
            img (ndarray): of shape HxW, HxWxC, or NxHxWxC. The array can be
                of type uint8 in range [0, 255], or floating point in range
                [0, 1] or [0, 255].

        Returns:
            ndarray: the flipped image(s).
        """
        tensor = torch.from_numpy(np.ascontiguousarray(img).copy())
        if len(tensor.shape) == 2:
            # For dimension of HxW.
            tensor = tensor.flip((-2))
        elif len(tensor.shape) > 2:
            # For dimension of HxWxC, NxHxWxC.
            tensor = tensor.flip((-3))
        return tensor.numpy()

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        """
        Flip the coordinates.

        Args:
            coords (ndarray): floating point array of shape Nx2. Each row is (x, y).

        Returns:
            ndarray: the flipped coordinates.

        Note:
            The inputs are floating point coordinates, not pixel indices.
            Therefore they are flipped by `(W - x, H - y)`, not
            `(W - 1 - x, H - 1 - y)`.
        """
        coords[:, 1] = self.height - coords[:, 1]
        return coords


class NoOpTransform(Transform):
    """
    A transform that does nothing.
    """

    def __init__(self):
        super().__init__()

    def apply_image(self, img: np.ndarray) -> np.ndarray:
        return img

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        return coords


class GaussianBlurTransform(Transform):
    """
    GaussianBlur using PIL.ImageFilter.GaussianBlur
    """

    def __init__(self, sigma, p=1.0):
        """
        Args:
            sigma (List(float)): sigma of gaussian
            p (float): probability of perform this augmentation
        """
        super().__init__()
        self._set_attributes(locals())

    def apply_image(self, img: np.ndarray) -> np.ndarray:
        if np.random.random() < self.p:
            sigma = random.uniform(self.sigma[0], self.sigma[1])
            img = Image.fromarray(img).filter(
                ImageFilter.GaussianBlur(radius=sigma))
        return np.array(img)

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        return coords


class SolarizationTransform(Transform):
    def __init__(self, thresh=128, p=0.5):
        super().__init__()
        self.thresh = thresh
        self.p = p

    def apply_image(self, img: np.ndarray) -> np.ndarray:
        if np.random.random() < self.p:
            return np.array(
                ImageOps.solarize(Image.fromarray(img), self.thresh))
        else:
            return img

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        return coords


class GaussianBlurConvTransform(Transform):
    def __init__(self, kernel_size, p=1.0):
        super().__init__()
        self._set_attributes(locals())
        radias = kernel_size // 2
        kernel_size = radias * 2 + 1
        self.blur_h = torch.nn.Conv2d(3, 3, kernel_size=(kernel_size, 1),
                                      stride=1, padding=0, bias=False,
                                      groups=3)
        self.blur_v = torch.nn.Conv2d(3, 3, kernel_size=(1, kernel_size),
                                      stride=1, padding=0, bias=False,
                                      groups=3)
        self.k = kernel_size
        self.r = radias

        self.blur = torch.nn.Sequential(
            torch.nn.ReflectionPad2d(radias),
            self.blur_h,
            self.blur_v
        )

        self.pil_to_tensor = transforms.ToTensor()
        self.tensor_to_pil = transforms.ToPILImage()

    def apply_image(self, img: np.ndarray) -> np.ndarray:
        if np.random.random() < self.p:
            img = self.pil_to_tensor(Image.fromarray(img)).unsqueeze(0)

            sigma = np.random.uniform(0.1, 2.0)
            x = np.arange(-self.r, self.r + 1)
            x = np.exp(-np.power(x, 2) / (2 * sigma * sigma))
            x = x / x.sum()
            x = torch.from_numpy(x).view(1, -1).repeat(3, 1)

            self.blur_h.weight.data.copy_(x.view(3, 1, self.k, 1))
            self.blur_v.weight.data.copy_(x.view(3, 1, 1, self.k))

            with torch.no_grad():
                img = self.blur(img)
                img = img.squeeze()

            img = np.array(self.tensor_to_pil(img))
        return img

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        return coords


class LabSpaceTransform(Transform):
    """
    Convert image from RGB into Lab color space
    """

    def __init__(self):
        super().__init__()
        self._set_attributes(locals())

    def apply_image(self, img: np.ndarray) -> np.ndarray:
        assert len(img.shape) == 3, 'Image should have dim H x W x 3'
        assert img.shape[2] == 3, 'Image should have dim H x W x 3'
        img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
        img_lab = img_lab.astype(np.float32)
        img_lab[:, :, 0] = (img_lab[:, :, 0] * (100.0 / 255.0)) - 50.0
        img_lab[:, :, 1:] = img_lab[:, :, 1:] - 128.0
        return img_lab

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        return coords


class PadTransform(Transform):
    """
    Pad image with `pad_value` to the specified `target_h` and `target_w`.

    Adds `top` rows of `pad_value` on top, `left` columns of `pad_value` on the left,
    and then pads the image on the bottom and right with `pad_value` until it has
    dimensions `target_h`, `target_w`.

    This op does nothing if `top` and `left` is zero and the image already has size
    `target_h` by `target_w`.
    """

    def __init__(self,
                 top: int,
                 left: int,
                 target_h: int,
                 target_w: int,
                 pad_value=0,
                 seg_value=255,
                 ):
        """
        Args:
            top (int): number of rows of `pad_value` to add on top.
            left (int): number of columns of `pad_value` to add on the left.
            target_h (int): height of output image.
            target_w (int): width of output image.
            pad_value (int): the value used to pad the image.
            seg_value (int): the value used to pad the semantic seg annotaions.
        """
        super().__init__()
        self._set_attributes(locals())

    def apply_image(self, img: np.ndarray, pad_value=None) -> np.ndarray:
        if pad_value is None:
            pad_value = self.pad_value

        if len(img.shape) == 2:  # semantic segmentation mask
            shape = (self.target_h, self.target_w)
        else:
            shape = (self.target_h, self.target_w, 3)

        pad_img = np.full(shape=shape, fill_value=pad_value).astype(img.dtype)

        rest_h = self.target_h - self.top
        rest_w = self.target_w - self.left

        img_h, img_w = img.shape[:2]
        paste_h, paste_w = min(rest_h, img_h), min(rest_w, img_w)
        pad_img[self.top:self.top + paste_h,
        self.left:self.left + paste_w] = img[:paste_h, :paste_w]
        return pad_img

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        coords[:, 0] = coords[:, 0] + self.left
        coords[:, 1] = coords[:, 1] + self.top
        return coords

    def apply_segmentation(self, segmentation: np.ndarray) -> np.ndarray:
        """
        Apply pad transform on the full-image segmentation.

        Args:
            segmentation (ndarray): of shape HxW. The array should have integer
                or bool dtype.
        Returns:
            ndarray: padded segmentation.
        """
        segmentation = self.apply_image(segmentation, pad_value=self.seg_value)
        return segmentation


class ScaleTransform(Transform):
    """
    Resize the image to a target size.
    """

    def __init__(self,
                 h: int,
                 w: int,
                 new_h: int,
                 new_w: int,
                 interp: str = "BILINEAR"):
        """
        Args:
            h, w (int): original image size.
            new_h, new_w (int): new image size.
            interp (str): the interpolation method. Options includes:
              * "NEAREST"
              * "BILINEAR"
              * "BICUBIC"
              * "LANCZOS"
              * "HAMMING"
              * "BOX"
        """
        super().__init__()
        self._set_attributes(locals())
        _str_to_pil_interpolation = {
            "NEAREST": Image.NEAREST,
            "BILINEAR": Image.BILINEAR,
            "BICUBIC": Image.BICUBIC,
            "LANCZOS": Image.LANCZOS,
            "HAMMING": Image.HAMMING,
            "BOX": Image.BOX,
        }
        assert (interp in _str_to_pil_interpolation.keys(
        )), "This interpolation mode ({}) is not currently supported!".format(
            interp)
        self.interp = _str_to_pil_interpolation[interp]

    def apply_image(self, img: np.ndarray, interp: str = None) -> np.ndarray:
        """
        Resize the image(s).

        Args:
            img (ndarray): of shape NxHxWxC, or HxWxC or HxW. The array can be
                of type uint8 in range [0, 255], or floating point in range
                [0, 1] or [0, 255].

        Returns:
            ndarray: resized image(s).
        """
        # Method 1: second fastest
        # img = cv2.resize(img, (self.new_w, self.new_h), interpolation=cv2.INTER_LINEAR)

        # Method 2: fastest
        pil_image = Image.fromarray(img)
        interp_method = interp if interp is not None else self.interp
        pil_image = pil_image.resize((self.new_w, self.new_h), interp_method)
        img = np.asarray(pil_image)

        return img

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        """
        Compute the coordinates after resize.

        Args:
            coords (ndarray): floating point array of shape Nx2. Each row is
                (x, y).
        Returns:
            ndarray: resized coordinates.
        """
        coords[:, 0] = coords[:, 0] * (self.new_w * 1.0 / self.w)
        coords[:, 1] = coords[:, 1] * (self.new_h * 1.0 / self.h)
        return coords

    def apply_segmentation(self, segmentation: np.ndarray) -> np.ndarray:
        """
        Apply resize on the full-image segmentation.

        Args:
            segmentation (ndarray): of shape HxW. The array should have integer
                or bool dtype.
        Returns:
            ndarray: resized segmentation.
        """
        segmentation = self.apply_image(segmentation, interp=Image.NEAREST)
        return segmentation


class GridSampleTransform(Transform):
    def __init__(self, grid: np.ndarray, interp: str):
        """
        Args:
            grid (ndarray): grid has x and y input pixel locations which are
                used to compute output. Grid has values in the range of [-1, 1],
                which is normalized by the input height and width. The dimension
                is `N x H x W x 2`.
            interp (str): interpolation methods. Options include `nearest` and
                `bilinear`.
        """
        super().__init__()
        self._set_attributes(locals())

    def apply_image(self, img: np.ndarray, interp: str = None) -> np.ndarray:
        """
        Apply grid sampling on the image(s).

        Args:
            img (ndarray): of shape NxHxWxC, or HxWxC or HxW. The array can be
                of type uint8 in range [0, 255], or floating point in range
                [0, 1] or [0, 255].
            interp (str): interpolation methods. Options include `nearest` and
                `bilinear`.
        Returns:
            ndarray: grid sampled image(s).
        """
        interp_method = interp if interp is not None else self.interp
        float_tensor = torch.nn.functional.grid_sample(
            to_float_tensor(img),  # NxHxWxC -> NxCxHxW.
            torch.from_numpy(self.grid),
            mode=interp_method,
            padding_mode="border",
            align_corners=False,
        )
        return to_numpy(float_tensor, img.shape, img.dtype)

    def apply_coords(self, coords: np.ndarray):
        """
        Not supported.
        """
        raise NotImplementedError()

    def apply_segmentation(self, segmentation: np.ndarray) -> np.ndarray:
        """
        Apply grid sampling on the full-image segmentation.

        Args:
            segmentation (ndarray): of shape HxW. The array should have integer
                or bool dtype.
        Returns:
            ndarray: grid sampled segmentation.
        """
        segmentation = self.apply_image(segmentation, interp=Image.NEAREST)
        return segmentation


class IoUCropTransform(Transform):
    """
    Perform crop operations on images.

    This crop operation will checks whether the center of each instance's bbox
    is in the cropped image.
    """

    def __init__(self, x0: int, y0: int, w: int, h: int):
        """
        Args:
            x0, y0, w, h (int): crop the image(s) by img[y0:y0+h, x0:x0+w].
        """
        super().__init__()
        self._set_attributes(locals())

    def apply_image(self, img: np.ndarray) -> np.ndarray:
        """
        Crop the image(s).

        Args:
            img (ndarray): of shape NxHxWxC, or HxWxC or HxW. The array can be
                of type uint8 in range [0, 255], or floating point in range
                [0, 1] or [0, 255].

        Returns:
            ndarray: cropped image(s).
        """
        if len(img.shape) <= 3:
            return img[self.y0:self.y0 + self.h, self.x0:self.x0 + self.w]
        else:
            return img[..., self.y0:self.y0 + self.h,
                   self.x0:self.x0 + self.w, :]

    def apply_box(self, box: np.ndarray) -> np.ndarray:
        """
        Apply the transform on an axis-aligned box.
        By default will transform the corner points and use their
        minimum/maximum to create a new axis-aligned box.
        Note that this default may change the size of your box, e.g. in
        rotations.

        Args:
            box (ndarray): Nx4 floating point array of XYXY format in absolute
                coordinates.

        Returns:
            ndarray: box after apply the transformation.

        Note:
            The coordinates are not pixel indices. Coordinates on an image of
            shape (H, W) are in range [0, W] or [0, H].
        """
        # Indexes of converting (x0, y0, x1, y1) box into 4 coordinates of
        # ([x0, y0], [x1, y0], [x0, y1], [x1, y1]).
        box = np.array(box).reshape(-1, 4)
        center = (box[:, :2] + box[:, 2:]) / 2
        mask = ((center[:, 0] > self.x0) * (center[:, 0] < self.x0 + self.w)
                * (center[:, 1] > self.y0) * (center[:, 1] < self.y0 + self.h))
        if not mask.any():
            return np.zeros_like(box)

        tl = np.array([self.x0, self.y0])
        box[:, :2] = np.maximum(box[:, :2], tl)
        box[:, :2] -= tl

        box[:, 2:] = np.minimum(box[:, 2:],
                                np.array([self.x0 + self.w, self.y0 + self.h]))
        box[:, 2:] -= tl

        return box

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        """
        Apply crop transform on coordinates.

        Args:
            coords (ndarray): floating point array of shape Nx2. Each row is (x, y).

        Returns:
            ndarray: cropped coordinates.
        """
        coords[:, 0] -= self.x0
        coords[:, 1] -= self.y0
        return coords

    def apply_polygons(self, polygons: list) -> list:
        """
        Apply crop transform on a list of polygons, each represented by a Nx2 array.
        It will crop the polygon with the box, therefore the number of points in the
        polygon might change.

        Args:
            polygon (list[ndarray]): each is a Nx2 floating point array of
                (x, y) format in absolute coordinates.

        Returns:
            ndarray: cropped polygons.
        """
        import shapely.geometry as geometry

        # Create a window that will be used to crop
        crop_box = geometry.box(self.x0, self.y0, self.x0 + self.w,
                                self.y0 + self.h).buffer(0.0)

        cropped_polygons = []

        for polygon in polygons:
            polygon = geometry.Polygon(polygon).buffer(0.0)
            # polygon must be valid to perform intersection.
            assert polygon.is_valid, polygon
            cropped = polygon.intersection(crop_box)
            if cropped.is_empty:
                continue
            if not isinstance(cropped,
                              geometry.collection.BaseMultipartGeometry):
                cropped = [cropped]
            # one polygon may be cropped to multiple ones
            for poly in cropped:
                # It could produce lower dimensional objects like lines or
                # points, which we want to ignore
                if not isinstance(poly, geometry.Polygon) or not poly.is_valid:
                    continue
                coords = np.asarray(poly.exterior.coords)
                # NOTE This process will produce an extra identical vertex at
                # the end. So we remove it. This is tested by
                # `tests/test_data_transform.py`
                cropped_polygons.append(coords[:-1])
        return [self.apply_coords(p) for p in cropped_polygons]


class CropTransform(Transform):
    """
    Perform crop operations on images.
    """

    def __init__(self, x0: int, y0: int, w: int, h: int):
        """
        Args:
            x0, y0, w, h (int): crop the image(s) by img[y0:y0+h, x0:x0+w].
        """
        super().__init__()
        self._set_attributes(locals())

    def apply_image(self, img: np.ndarray) -> np.ndarray:
        """
        Crop the image(s).

        Args:
            img (ndarray): of shape NxHxWxC, or HxWxC or HxW. The array can be
                of type uint8 in range [0, 255], or floating point in range
                [0, 1] or [0, 255].

        Returns:
            ndarray: cropped image(s).
        """
        if len(img.shape) <= 3:
            return img[self.y0:self.y0 + self.h, self.x0:self.x0 + self.w]
        else:
            return img[..., self.y0:self.y0 + self.h,
                   self.x0:self.x0 + self.w, :]

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        """
        Apply crop transform on coordinates.

        Args:
            coords (ndarray): floating point array of shape Nx2. Each row is (x, y).

        Returns:
            ndarray: cropped coordinates.
        """
        coords[:, 0] -= self.x0
        coords[:, 1] -= self.y0
        return coords

    def apply_polygons(self, polygons: list) -> list:
        """
        Apply crop transform on a list of polygons, each represented by a Nx2 array.
        It will crop the polygon with the box, therefore the number of points in the
        polygon might change.

        Args:
            polygon (list[ndarray]): each is a Nx2 floating point array of
                (x, y) format in absolute coordinates.

        Returns:
            ndarray: cropped polygons.
        """
        import shapely.geometry as geometry

        # Create a window that will be used to crop
        crop_box = geometry.box(self.x0, self.y0, self.x0 + self.w,
                                self.y0 + self.h).buffer(0.0)

        cropped_polygons = []

        for polygon in polygons:
            polygon = geometry.Polygon(polygon).buffer(0.0)
            # polygon must be valid to perform intersection.
            assert polygon.is_valid, polygon
            cropped = polygon.intersection(crop_box)
            if cropped.is_empty:
                continue
            if not isinstance(cropped,
                              geometry.collection.BaseMultipartGeometry):
                cropped = [cropped]
            # one polygon may be cropped to multiple ones
            for poly in cropped:
                # It could produce lower dimensional objects like lines or
                # points, which we want to ignore
                if not isinstance(poly, geometry.Polygon) or not poly.is_valid:
                    continue
                coords = np.asarray(poly.exterior.coords)
                # NOTE This process will produce an extra identical vertex at
                # the end. So we remove it. This is tested by
                # `tests/test_data_transform.py`
                cropped_polygons.append(coords[:-1])
        return [self.apply_coords(p) for p in cropped_polygons]


class CropPadTransform(Transform):
    def __init__(self,
                 x0: int,
                 y0: int,
                 w: int,
                 h: int,
                 new_w: int,
                 new_h: int,
                 img_value=None,
                 seg_value=None):
        super().__init__()
        self._set_attributes(locals())
        self.crop_trans = CropTransform(x0, y0, w, h)
        pad_top_offset = self.get_pad_offset(h, new_h)
        pad_left_offset = self.get_pad_offset(w, new_w)
        self.pad_trans = PadTransform(
            pad_top_offset, pad_left_offset, new_h, new_w, img_value,
            seg_value)

    def get_pad_offset(self, ori: int, tar: int):
        pad_length = max(tar - ori, 0)
        pad_offset = pad_length // 2
        return pad_offset

    def apply_image(self, img: np.ndarray) -> np.ndarray:
        """
        Crop and Pad the image(s).

        Args:
            img (ndarray): of shape NxHxWxC, or HxWxC or HxW. The array can be
                of type uint8 in range [0, 255], or floating point in range
                [0, 1] or [0, 255].

        Returns:
            ndarray: cropped and padded image(s).
        """
        img = self.crop_trans.apply_image(img)
        img = self.pad_trans.apply_image(img)
        return img

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        """
        Apply crop and pad transform on coordinates.

        Args:
            coords (ndarray): floating point array of shape Nx2. Each row is
                (x, y).

        Returns:
            ndarray: cropped and padded coordinates.
        """
        coords = self.crop_trans.apply_coords(coords)
        coords = self.pad_trans.apply_coords(coords)
        return coords

    def apply_polygons(self, polygons: list) -> list:
        """
        Apply crop and pad transform on a list of polygons, each represented by a Nx2 array.

        Args:
            polygon (list[ndarray]): each is a Nx2 floating point array of
                (x, y) format in absolute coordinates.

        Returns:
            ndarray: cropped and padded polygons.
        """
        polygons = self.crop_trans.apply_polygons(polygons)
        polygons = self.pad_trans.apply_polygons(polygons)
        return polygons

    def apply_segmentation(self, segmentation: np.ndarray) -> np.ndarray:
        """
        Apply crop and pad transform on the full-image segmentation.

        Args:
            segmentation (ndarray): of shape HxW. The array should have integer
                or bool dtype.

        Returns:
            ndarray: cropped and padded segmentation.
        """
        segmentation = self.crop_trans.apply_segmentation(segmentation)
        segmentation = self.pad_trans.apply_segmentation(segmentation)
        return segmentation


class BlendTransform(Transform):
    """
    Transforms pixel colors with PIL enhance functions.
    """

    def __init__(self, src_image: np.ndarray, src_weight: float,
                 dst_weight: float):
        """
        Blends the input image (dst_image) with the src_image using formula:
        ``src_weight * src_image + dst_weight * dst_image``

        Args:
            src_image (ndarray): Input image is blended with this image
            src_weight (float): Blend weighting of src_image
            dst_weight (float): Blend weighting of dst_image
        """
        super().__init__()
        self._set_attributes(locals())

    def apply_image(self, img: np.ndarray, interp: str = None) -> np.ndarray:
        """
        Apply blend transform on the image(s).

        Args:
            img (ndarray): of shape NxHxWxC, or HxWxC or HxW. The array can be
                of type uint8 in range [0, 255], or floating point in range
                [0, 1] or [0, 255].
            interp (str): keep this option for consistency, perform blend would not
                require interpolation.

        Returns:
            ndarray: blended image(s).
        """
        if img.dtype == np.uint8:
            img = img.astype(np.float32)
            img = self.src_weight * self.src_image + self.dst_weight * img
            return np.clip(img, 0, 255).astype(np.uint8)
        else:
            return self.src_weight * self.src_image + self.dst_weight * img

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        """
        Apply no transform on the coordinates.
        """
        return coords

    def apply_segmentation(self, segmentation: np.ndarray) -> np.ndarray:
        """
        Apply no transform on the full-image segmentation.
        """
        return segmentation


class ShiftTransform(Transform):
    """
    Shift the image with random pixels.
    """

    def __init__(self, shift_x: int, shift_y: int):
        """
        Args:
            shift_x (int): the shift pixel for x axis.
            shift_y (int): the shift piexl for y axis.
        """
        super().__init__()
        self._set_attributes(locals())

    def apply_image(self, img: np.ndarray) -> np.ndarray:
        """
        Shift the image(s).

        Args:
            img (ndarray): of shape NxHxWxC, or HxWxC or HxW. The array can be
                of type uint8 in range [0, 255], or floating point in range
                [0, 1] or [0, 255].

        Returns:
            ndarray: shifted image(s).
        """
        new_img = np.zeros_like(img)
        if self.shift_x < 0:
            new_x = 0
            orig_x = - self.shift_x
        else:
            new_x = self.shift_x
            orig_x = 0
        if self.shift_y < 0:
            new_y = 0
            orig_y = - self.shift_y
        else:
            new_y = self.shift_y
            orig_y = 0

        if len(img.shape) <= 3:
            img_h, img_w = img.shape[:2]
            new_h = img_h - np.abs(self.shift_y)
            new_w = img_w - np.abs(self.shift_x)
            new_img[new_y:new_y + new_h, new_x:new_x + new_w] = img[
                                                                orig_y:orig_y + new_h,
                                                                orig_x:orig_x + new_w]
            return new_img
        else:
            img_h, img_w = img.shape[1:3]
            new_h = img_h - np.abs(self.shift_y)
            new_w = img_w - np.abs(self.shift_x)
            new_img[..., new_y:new_y + new_h, new_x:new_x + new_w, :] = img[
                                                                        ...,
                                                                        orig_y:orig_y + new_h,
                                                                        orig_x:orig_x + new_w,
                                                                        :]
            return new_img

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        """
        Apply shift transform on coordinates.

        Args:
            coords (ndarray): floating point array of shape Nx2. Each row is (x, y).

        Returns:
            ndarray: cropped coordinates.
        """
        coords[:, 0] += self.shift_x
        coords[:, 1] += self.shift_y
        return coords


class RandomSwapChannelsTransform(Transform):
    """
    Randomly swap image channels.
    """

    def __init__(self):
        super().__init__()

    def apply_image(self, img):
        assert len(img.shape) > 2
        return img[..., np.random.permutation(3)]

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        """
        Apply no transform on the coordinates.
        """
        return coords

    def apply_segmentation(self, segmentation: np.ndarray) -> np.ndarray:
        """
        Apply no transform on the full-image segmentation.
        """
        return segmentation


class ExpandTransform(Transform):
    """
    Expand the image and boxes according the specified expand ratio.
    """

    def __init__(self, left, top, ratio, mean=(0, 0, 0)):
        """
        Args:
            left, top (int): crop the image by img[top: top+h, left:left+w].
            ratio (float): image expand ratio.
            mean (tuple): mean value of dataset.
        """
        super().__init__()
        self._set_attributes(locals())

    def apply_image(self, img):
        """
        Randomly place the original image on a canvas of 'ratio' x original image
        size filled with mean values. The ratio is in the range of ratio_range.
        """
        h, w, c = img.shape
        expand_img = np.full((int(h * self.ratio), int(w * self.ratio), c),
                             self.mean).astype(img.dtype)
        expand_img[self.top:self.top + h, self.left:self.left + w] = img
        return expand_img

    def apply_coords(self, coords: np.ndarray) -> np.ndarray:
        """
        Apply expand transform on coordinates.

        Args:
            coords (ndarray): floating point array of shape Nx2. Each row is
                (x, y).

        Returns:
            ndarray: expand coordinates.
        """
        coords[:, 0] += self.left
        coords[:, 1] += self.top
        return coords


class ExtentTransform(Transform):
    """
    Extracts a subregion from the source image and scales it to the output size.

    The fill color is used to map pixels from the source rect that fall outside
    the source image.

    See: https://pillow.readthedocs.io/en/latest/PIL.html#PIL.ImageTransform.ExtentTransform
    """

    def __init__(self, src_rect, output_size, interp=Image.LINEAR, fill=0):
        """
        Args:
            src_rect (x0, y0, x1, y1): src coordinates
            output_size (h, w): dst image size
            interp: PIL interpolation methods
            fill: Fill color used when src_rect extends outside image
        """
        super().__init__()
        self._set_attributes(locals())

    def apply_image(self, img, interp=None):
        h, w = self.output_size
        ret = Image.fromarray(img).transform(
            size=(w, h),
            method=Image.EXTENT,
            data=self.src_rect,
            resample=interp if interp else self.interp,
            fill=self.fill,
        )
        return np.asarray(ret)

    def apply_coords(self, coords):
        # Transform image center from source coordinates into output coordinates
        # and then map the new origin to the corner of the output image.
        h, w = self.output_size
        x0, y0, x1, y1 = self.src_rect
        new_coords = coords.astype(np.float32)
        new_coords[:, 0] -= 0.5 * (x0 + x1)
        new_coords[:, 1] -= 0.5 * (y0 + y1)
        new_coords[:, 0] *= w / (x1 - x0)
        new_coords[:, 1] *= h / (y1 - y0)
        new_coords[:, 0] += 0.5 * w
        new_coords[:, 1] += 0.5 * h
        return new_coords

    def apply_segmentation(self, segmentation):
        segmentation = self.apply_image(segmentation, interp=Image.NEAREST)
        return segmentation


class ResizeTransform(Transform):
    """
    Resize the image to a target size.
    """

    def __init__(self, h, w, new_h, new_w, interp):
        """
        Args:
            h, w (int): original image size
            new_h, new_w (int): new image size
            interp: PIL interpolation methods
        """
        # TODO decide on PIL vs opencv
        super().__init__()
        self._set_attributes(locals())

    def apply_image(self, img, interp=None):
        assert img.shape[:2] == (self.h, self.w)
        pil_image = Image.fromarray(img)
        interp_method = interp if interp is not None else self.interp
        pil_image = pil_image.resize((self.new_w, self.new_h), interp_method)
        ret = np.asarray(pil_image)
        return ret

    def apply_coords(self, coords):
        coords[:, 0] = coords[:, 0] * (self.new_w * 1.0 / self.w)
        coords[:, 1] = coords[:, 1] * (self.new_h * 1.0 / self.h)
        return coords

    def apply_segmentation(self, segmentation):
        segmentation = self.apply_image(segmentation, interp=Image.NEAREST)
        return segmentation


def HFlip_rotated_box(transform, rotated_boxes):
    """
    Apply the horizontal flip transform on rotated boxes.

    Args:
        rotated_boxes (ndarray): Nx5 floating point array of
            (x_center, y_center, width, height, angle_degrees) format
            in absolute coordinates.
    """
    # Transform x_center
    rotated_boxes[:, 0] = transform.width - rotated_boxes[:, 0]
    # Transform angle
    rotated_boxes[:, 4] = -rotated_boxes[:, 4]
    return rotated_boxes


def Resize_rotated_box(transform, rotated_boxes):
    """
    Apply the resizing transform on rotated boxes. For details of how these (approximation)
    formulas are derived, please refer to :meth:`RotatedBoxes.scale`.

    Args:
        rotated_boxes (ndarray): Nx5 floating point array of
            (x_center, y_center, width, height, angle_degrees) format
            in absolute coordinates.
    """
    scale_factor_x = transform.new_w * 1.0 / transform.w
    scale_factor_y = transform.new_h * 1.0 / transform.h
    rotated_boxes[:, 0] *= scale_factor_x
    rotated_boxes[:, 1] *= scale_factor_y
    theta = rotated_boxes[:, 4] * np.pi / 180.0
    c = np.cos(theta)
    s = np.sin(theta)
    rotated_boxes[:, 2] *= np.sqrt(
        np.square(scale_factor_x * c) + np.square(scale_factor_y * s))
    rotated_boxes[:, 3] *= np.sqrt(
        np.square(scale_factor_x * s) + np.square(scale_factor_y * c))
    rotated_boxes[:, 4] = np.arctan2(scale_factor_x * s,
                                     scale_factor_y * c) * 180 / np.pi

    return rotated_boxes


HFlipTransform.register_type("rotated_box", HFlip_rotated_box)
NoOpTransform.register_type("rotated_box", lambda t, x: x)
ResizeTransform.register_type("rotated_box", Resize_rotated_box)
```

#### cvpods/data/samplers/__init__.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from .distributed_sampler import (
    DistributedGroupSampler,
    InferenceSampler,
    RepeatFactorTrainingSampler
)

__all__ = [
    "InferenceSampler",
    "RepeatFactorTrainingSampler",
    "DistributedGroupSampler",
]
```

#### cvpods/data/samplers/distributed_sampler.py

```python
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

import itertools
import math
from collections import defaultdict

import numpy as np

import torch
from torch.utils.data.sampler import Sampler

from cvpods.utils import comm

from ..registry import SAMPLERS


@SAMPLERS.register()
class DistributedGroupSampler(Sampler):
    """Sampler that restricts data loading to a subset of the dataset.
    It is especially useful in conjunction with
    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each
    process can pass a DistributedSampler instance as a DataLoader sampler,
    and load a subset of the original dataset that is exclusive to it.
    .. note::
        Dataset is assumed to be of constant size.
    """

    def __init__(self,
                 dataset,
                 samples_per_gpu=1,
                 num_replicas=None,
                 rank=None):
        """
        Args:
            dataset (Dataset): Dataset used for sampling.
            num_replicas (optional): Number of processes participating in
                distributed training.
            rank (optional): Rank of the current process within num_replicas.
        """
        _rank = comm.get_rank()
        _num_replicas = comm.get_world_size()
        if num_replicas is None:
            num_replicas = _num_replicas
        if rank is None:
            rank = _rank
        self.dataset = dataset
        self.samples_per_gpu = samples_per_gpu
        self.num_replicas = num_replicas
        self.rank = rank
        self.epoch = 0

        assert hasattr(self.dataset, 'aspect_ratios')
        self.aspect_ratios = self.dataset.aspect_ratios
        self.group_sizes = np.bincount(self.aspect_ratios)

        self.num_samples = 0
        for i, j in enumerate(self.group_sizes):
            self.num_samples += int(
                math.ceil(
                    self.group_sizes[i] * 1.0 / self.samples_per_gpu / self.num_replicas
                )
            ) * self.samples_per_gpu
        self.total_size = self.num_samples * self.num_replicas

    def __iter__(self):
        # deterministically shuffle based on epoch
        g = torch.Generator()
        g.manual_seed(self.epoch)

        indices = []
        for i, size in enumerate(self.group_sizes):
            if size > 0:
                indice = np.where(self.aspect_ratios == i)[0]
                assert len(indice) == size
                indice = indice[list(torch.randperm(int(size),
                                                    generator=g))].tolist()
                extra = int(
                    math.ceil(
                        size * 1.0 / self.samples_per_gpu / self.num_replicas)
                ) * self.samples_per_gpu * self.num_replicas - len(indice)
                # pad indice
                tmp = indice.copy()
                for _ in range(extra // size):
                    indice.extend(tmp)
                indice.extend(tmp[:extra % size])
                indices.extend(indice)

        assert len(indices) == self.total_size

        indices = [
            indices[j] for i in list(
                torch.randperm(
                    len(indices) // self.samples_per_gpu, generator=g))
            for j in range(i * self.samples_per_gpu, (i + 1) * self.samples_per_gpu)
        ]

        # subsample
        offset = self.num_samples * self.rank
        indices = indices[offset:offset + self.num_samples]
        assert len(indices) == self.num_samples

        return iter(indices)

    def __len__(self):
        return self.num_samples

    def set_epoch(self, epoch):
        self.epoch = epoch


@SAMPLERS.register()
class RepeatFactorTrainingSampler(Sampler):
    """
    Similar to TrainingSampler, but suitable for training on class imbalanced datasets
    like LVIS. In each epoch, an image may appear multiple times based on its "repeat
    factor". The repeat factor for an image is a function of the frequency the rarest
    category labeled in that image. The "frequency of category c" in [0, 1] is defined
    as the fraction of images in the training set (without repeats) in which category c
    appears.

    See https://arxiv.org/abs/1908.03195 (>= v2) Appendix B.2.
    """

    def __init__(self, dataset, repeat_thresh, shuffle=True, seed=None):
        """
        Args:
            dataset (Dataset): dataset used for sampling.
            repeat_thresh (float): frequency threshold below which data is repeated.
            shuffle (bool): whether to shuffle the indices or not.
            seed (int): the initial seed of the shuffle. Must be the same
                across all workers. If None, will use a random seed shared
                among workers (require synchronization among all workers).
        """
        self._shuffle = shuffle
        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()

        dataset_dicts = []
        if hasattr(dataset, "datasets"):
            for d in dataset.datasets:
                dataset_dicts += d.dataset_dicts
        else:
            dataset_dicts = dataset.dataset_dicts

        # Get fractional repeat factors and split into whole number (_int_part)
        # and fractional (_frac_part) parts.
        rep_factors = self._get_repeat_factors(dataset_dicts, repeat_thresh)
        self._int_part = torch.trunc(rep_factors)
        self._frac_part = rep_factors - self._int_part

    def _get_repeat_factors(self, dataset_dicts, repeat_thresh):
        """
        Compute (fractional) per-image repeat factors.

        Args:
            See __init__.

        Returns:
            torch.Tensor: the i-th element is the repeat factor for the dataset image
                at index i.
        """
        # 1. For each category c, compute the fraction of images that contain it: f(c)
        category_freq = defaultdict(int)
        for dataset_dict in dataset_dicts:  # For each image (without repeats)
            cat_ids = {ann["category_id"] for ann in dataset_dict["annotations"]}
            for cat_id in cat_ids:
                category_freq[cat_id] += 1
        num_images = len(dataset_dicts)
        for k, v in category_freq.items():
            category_freq[k] = v / num_images

        # 2. For each category c, compute the category-level repeat factor:
        #    r(c) = max(1, sqrt(t / f(c)))
        category_rep = {
            cat_id: max(1.0, math.sqrt(repeat_thresh / cat_freq))
            for cat_id, cat_freq in category_freq.items()
        }

        # 3. For each image I, compute the image-level repeat factor:
        #    r(I) = max_{c in I} r(c)
        rep_factors = []
        for dataset_dict in dataset_dicts:
            cat_ids = {ann["category_id"] for ann in dataset_dict["annotations"]}
            rep_factor = max({category_rep[cat_id] for cat_id in cat_ids})
            rep_factors.append(rep_factor)

        return torch.tensor(rep_factors, dtype=torch.float32)

    def _get_epoch_indices(self, generator):
        """
        Create a list of dataset indices (with repeats) to use for one epoch.

        Args:
            generator (torch.Generator): pseudo random number generator used for
                stochastic rounding.

        Returns:
            torch.Tensor: list of dataset indices to use in one epoch. Each index
                is repeated based on its calculated repeat factor.
        """
        # Since repeat factors are fractional, we use stochastic rounding so
        # that the target repeat factor is achieved in expectation over the
        # course of training
        rands = torch.rand(len(self._frac_part), generator=generator)
        rep_factors = self._int_part + (rands < self._frac_part).float()
        # Construct a list of indices in which we repeat images as specified
        indices = []
        for dataset_index, rep_factor in enumerate(rep_factors):
            indices.extend([dataset_index] * int(rep_factor.item()))
        return torch.tensor(indices, dtype=torch.int64)

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        g = torch.Generator()
        g.manual_seed(self._seed)
        while True:
            # Sample indices with repeats determined by stochastic rounding; each
            # "epoch" may have a slightly different size due to the rounding.
            indices = self._get_epoch_indices(g)
            if self._shuffle:
                randperm = torch.randperm(len(indices), generator=g)
                yield from indices[randperm]
            else:
                yield from indices


@SAMPLERS.register()
class InferenceSampler(Sampler):
    """
    Produce indices for inference.
    Inference needs to run on the __exact__ set of samples,
    therefore when the total number of samples is not divisible by the number of workers,
    this sampler produces different number of samples on different workers.
    """

    def __init__(self, size: int):
        """
        Args:
            size (int): the total number of data of the underlying dataset to sample from
        """
        self._size = size
        assert size > 0
        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()

        shard_size = (self._size - 1) // self._world_size + 1
        begin = shard_size * self._rank
        end = min(shard_size * (self._rank + 1), self._size)
        self._local_indices = range(begin, end)

    def __iter__(self):
        yield from self._local_indices

    def __len__(self):
        return len(self._local_indices)
```

### cvpods/engine/predictor.py

```python
#!/usr/bin/python3
# -*- coding:utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from copy import deepcopy

import torch

from cvpods.checkpoint import DefaultCheckpointer
from cvpods.data import build_transform_gens

__all__ = ["DefaultPredictor"]


class DefaultPredictor:
    """
    Create a simple end-to-end predictor with the given config that runs on
    single device for a single input image.
    Compared to using the model directly, this class does the following additions:

    1. Load checkpoint from `cfg.MODEL.WEIGHTS`.
    2. Always take BGR image as the input and apply conversion defined by `cfg.INPUT.FORMAT`.
    3. Apply resizing defined by `cfg.INPUT.{MIN,MAX}_SIZE_TEST`.
    4. Take one input image and produce a single output, instead of a batch.

    If you'd like to do anything more fancy, please refer to its source code
    as examples to build and use the model manually.

    Attributes:
        metadata (Metadata): the metadata of the underlying dataset, obtained from
            cfg.DATASETS.TEST.

    Examples:
    .. code-block:: python

        pred = DefaultPredictor(cfg)
        inputs = cv2.imread("input.jpg")
        outputs = pred(inputs)
    """
    def __init__(self, cfg, meta):
        self.cfg = deepcopy(cfg)
        if self.cfg.MODEL.DEVICE.startswith("cuda:"):
            torch.cuda.set_device(self.cfg.MODEL.DEVICE)
            self.cfg.MODEL.DEVICE = "cuda"
        self.model = cfg.build_model(self.cfg)
        self.model.eval()
        self.metadata = meta

        checkpointer = DefaultCheckpointer(self.model)
        checkpointer.load(cfg.MODEL.WEIGHTS)

        self.transform_gen = build_transform_gens(cfg.INPUT.AUG.TEST_PIPELINES)

        self.input_format = cfg.INPUT.FORMAT
        assert self.input_format in ["RGB", "BGR"], self.input_format

    def __call__(self, original_image):
        """
        Args:
            original_image (np.ndarray): an image of shape (H, W, C) (in BGR order).

        Returns:
            predictions (dict):
                the output of the model for one image only.
                See :doc:`/tutorials/models` for details about the format.
        """
        with torch.no_grad(
        ):  # https://github.com/sphinx-doc/sphinx/issues/4258
            # Apply pre-processing to image.
            if self.input_format == "RGB":
                # whether the model expects BGR inputs or RGB
                original_image = original_image[:, :, ::-1]
            height, width = original_image.shape[:2]

            image = original_image
            for tfm_gen in self.transform_gen:
                image = tfm_gen.get_transform(image).apply_image(image)

            image = torch.as_tensor(image.astype("float32").transpose(2, 0, 1))

            inputs = {"image": image, "height": height, "width": width}
            predictions = self.model([inputs])[0]
            return predictions
```

### cvpods/engine/runner.py

```python
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

import logging
import math
import os
from collections import OrderedDict

import torch
from torch.nn.parallel import DistributedDataParallel

from cvpods.checkpoint import DefaultCheckpointer
from cvpods.data import build_test_loader, build_train_loader
from cvpods.evaluation import (
    DatasetEvaluator,
    inference_on_dataset,
    print_csv_format,
    verify_results
)
from cvpods.modeling.nn_utils.module_converter import maybe_convert_module
from cvpods.modeling.nn_utils.precise_bn import get_bn_modules
from cvpods.solver import build_lr_scheduler, build_optimizer
from cvpods.utils import comm, setup_logger
from cvpods.utils.dump.events import CommonMetricPrinter, JSONWriter, TensorboardXWriter

from . import hooks
from .base_runner import RUNNERS, SimpleRunner

logger = logging.getLogger(__name__)


@RUNNERS.register()
class DefaultRunner(SimpleRunner):
    """
    A runner with default training logic. It does the following:

    1. Create a :class:`DefaultRunner` using model, optimizer, dataloader
       defined by the given config. Create a LR scheduler defined by the config.
    2. Load the last checkpoint or `cfg.MODEL.WEIGHTS`, if exists, when
       `resume_or_load` is called.
    3. Register a few common hooks defined by the config.

    It is created to simplify the **standard model training workflow** and reduce code boilerplate
    for users who only need the standard training workflow, with standard features.
    It means this class makes *many assumptions* about your training logic that
    may easily become invalid in a new research. In fact, any assumptions beyond those made in the
    :class:`DefaultRunner` are too much for research.

    The code of this class has been annotated about restrictive assumptions it makes.
    When they do not work for you, you're encouraged to:

    1. Overwrite methods of this class, OR:
    2. Use :class:`DefaultRunner`, which only does minimal SGD training and
       nothing else. You can then add your own hooks if needed. OR:
    3. Write your own training loop similar to `tools/plain_train_net.py`.

    See the :doc:`/tutorials/training` tutorials for more details.

    Note that the behavior of this class, like other functions/classes in
    this file, is not stable, since it is meant to represent the "common default behavior".
    It is only guaranteed to work well with the standard models and training workflow in cvpods.
    To obtain more stable behavior, write your own training logic with other public APIs.

    Examples:
    ::
        runner = DefaultRunner(cfg)
        runner.resume_or_load()  # load last checkpoint or MODEL.WEIGHTS
        runner.train()

    Attributes:
        scheduler:
        checkpointer (DefaultCheckpointer):
        cfg (config dict):
    """

    def __init__(self, cfg, build_model):
        """
        Args:
            cfg (config dict):
        """
        logger = logging.getLogger("cvpods")
        if not logger.isEnabledFor(logging.INFO):  # setup_logger is not called for cvpods
            setup_logger()
        self.logger = logger

        self.data_loader = self.build_train_loader(cfg)
        # Assume these objects must be constructed in this order.
        model = build_model(cfg)
        self.model = maybe_convert_module(model)
        self.logger.info(f"Model: \n{self.model}")

        # Assume these objects must be constructed in this order.
        self.optimizer = self.build_optimizer(cfg, self.model)

        # For training, wrap with DDP. But don't need this for inference.
        if comm.get_world_size() > 1:
            if cfg.TRAINER.FP16.ENABLED:
                self.mixed_precision = True
                if cfg.TRAINER.FP16.TYPE == "APEX":
                    from apex import amp
                    self.model, self.optimizer = amp.initialize(
                        self.model, self.optimizer, opt_level=cfg.TRAINER.FP16.OPTS.OPT_LEVEL
                    )
            else:
                self.mixed_precision = False
            torch.cuda.set_device(comm.get_local_rank())
            self.model = DistributedDataParallel(
                self.model,
                device_ids=[comm.get_local_rank()],
                broadcast_buffers=False,
                find_unused_parameters=True)

        super().__init__(
            self.model,
            self.data_loader,
            self.optimizer,
        )

        if not cfg.SOLVER.LR_SCHEDULER.get("EPOCH_WISE", False):
            epoch_iters = -1
        else:
            epoch_iters = cfg.SOLVER.LR_SCHEDULER.get("EPOCH_ITERS")
            self.logger.warning(f"Setup LR Scheduler in EPOCH mode: {epoch_iters}")

        auto_scale_config(cfg, self.data_loader)
        self.scheduler = self.build_lr_scheduler(cfg, self.optimizer, epoch_iters=epoch_iters)
        # Assume no other objects need to be checkpointed.
        # We can later make it checkpoint the stateful hooks
        self.checkpointer = DefaultCheckpointer(
            # Assume you want to save checkpoints together with logs/statistics
            self.model,
            cfg.OUTPUT_DIR,
            optimizer=self.optimizer,
            scheduler=self.scheduler,
        )

        self.start_iter = 0
        self.start_epoch = 0
        self.max_iter = cfg.SOLVER.LR_SCHEDULER.MAX_ITER
        self.max_epoch = cfg.SOLVER.LR_SCHEDULER.MAX_EPOCH
        self.window_size = cfg.TRAINER.WINDOW_SIZE

        self.cfg = cfg

        self.register_hooks(self.build_hooks())

    def resume_or_load(self, resume=True):
        """
        If `resume==True` and `cfg.OUTPUT_DIR` contains the last checkpoint (defined by
        a `last_checkpoint` file), resume from the file. Resuming means loading all
        available states (eg. optimizer and scheduler) and update iteration counter
        from the checkpoint. ``cfg.MODEL.WEIGHTS`` will not be used.

        Otherwise, this is considered as an independent training. The method will load model
        weights from the file `cfg.MODEL.WEIGHTS` (but will not load other states) and start
        from iteration 0.

        Args:
            resume (bool): whether to do resume or not
        """
        self.checkpointer.resume = resume
        # The checkpoint stores the training iteration that just finished, thus we start
        # at the next iteration (or iter zero if there's no checkpoint).
        self.start_iter = (self.checkpointer.resume_or_load(
            self.cfg.MODEL.WEIGHTS, resume=resume).get("iteration", -1) + 1)
        if self.max_epoch is not None:
            self.start_epoch = self.start_iter // len(self.data_loader)

    def build_hooks(self):
        """
        Build a list of default hooks, including timing, evaluation,
        checkpointing, lr scheduling, precise BN, writing events.

        Returns:
            list[HookBase]:
        """
        cfg = self.cfg
        # cfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN

        ret = [
            hooks.OptimizationHook(
                accumulate_grad_steps=cfg.SOLVER.BATCH_SUBDIVISIONS,
                grad_clipper=None,
                mixed_precision=cfg.TRAINER.FP16.ENABLED
            ),
            hooks.LRScheduler(self.optimizer, self.scheduler),
            hooks.IterationTimer(),
            hooks.PreciseBN(
                # Run at the same freq as (but before) evaluation.
                cfg.TEST.EVAL_PERIOD,
                self.model,
                # Build a new data loader to not affect training
                self.build_train_loader(cfg),
                cfg.TEST.PRECISE_BN.NUM_ITER,
            )
            if cfg.TEST.PRECISE_BN.ENABLED and get_bn_modules(self.model)
            else None,
        ]

        # Do PreciseBN before checkpointer, because it updates the model and need to
        # be saved by checkpointer.
        # This is not always the best: if checkpointing has a different frequency,
        # some checkpoints may have more precise statistics than others.
        if comm.is_main_process():
            ret.append(hooks.PeriodicCheckpointer(
                self.checkpointer,
                cfg.SOLVER.CHECKPOINT_PERIOD,
                max_iter=self.max_iter,
                max_epoch=self.max_epoch
            ))

        def test_and_save_results():
            self._last_eval_results = self.test(self.cfg, self.model)
            return self._last_eval_results

        # Do evaluation after checkpointer, because then if it fails,
        # we can use the saved checkpoint to debug.
        ret.append(hooks.EvalHook(cfg.TEST.EVAL_PERIOD, test_and_save_results))

        if comm.is_main_process():
            # Here the default print/log frequency of each writer is used.
            # run writers in the end, so that evaluation metrics are written
            ret.append(hooks.PeriodicWriter(
                self.build_writers(),
                period=self.window_size))
        return ret

    def build_writers(self):
        """
        Build a list of :class:`EventWriter` to be used.
        It now consists of a :class:`CommonMetricPrinter`,
        :class:`TensorboardXWriter` and :class:`JSONWriter`.

        Args:
            output_dir: directory to store JSON metrics and tensorboard events
            max_iter: the total number of iterations

        Returns:
            list[EventWriter]: a list of :class:`EventWriter` objects.
        """
        return [
            # It may not always print what you want to see, since it prints "common" metrics only.
            CommonMetricPrinter(
                self.max_iter,
                window_size=self.window_size,
                epoch=self.max_epoch,
            ),
            JSONWriter(
                os.path.join(self.cfg.OUTPUT_DIR, "metrics.json"),
                window_size=self.window_size
            ),
            TensorboardXWriter(
                self.cfg.OUTPUT_DIR,
                window_size=self.window_size
            ),
        ]

    def train(self):
        """
        Run training.

        Returns:
            OrderedDict of results, if evaluation is enabled. Otherwise None.
        """
        if self.max_epoch is None:
            logger.info("Starting training from iteration {}".format(self.start_iter))
        else:
            logger.info("Starting training from epoch {}".format(self.start_epoch))

        super().train(self.start_iter, self.start_epoch, self.max_iter)

        if len(self.cfg.TEST.EXPECTED_RESULTS) and comm.is_main_process():
            assert hasattr(
                self, "_last_eval_results"
            ), "No evaluation results obtained during training!"
            verify_results(self.cfg, self._last_eval_results)
            return self._last_eval_results

    @classmethod
    def build_optimizer(cls, cfg, model):
        """
        Returns:
            torch.optim.Optimizer:

        It now calls :func:`cvpods.solver.build_optimizer`.
        Overwrite it if you'd like a different optimizer.
        """
        return build_optimizer(cfg, model)

    @classmethod
    def build_lr_scheduler(cls, cfg, optimizer, **kwargs):
        """
        It now calls :func:`cvpods.solver.build_lr_scheduler`.
        Overwrite it if you'd like a different scheduler.
        """
        return build_lr_scheduler(cfg, optimizer, **kwargs)

    @classmethod
    def build_train_loader(cls, cfg):
        """
        Returns:
            iterable

        It now calls :func:`cvpods.data.build_train_loader`.
        Overwrite it if you'd like a different data loader.
        """
        return build_train_loader(cfg)

    @classmethod
    def build_test_loader(cls, cfg):
        """
        Returns:
            iterable

        It now calls :func:`cvpods.data.build_test_loader`.
        Overwrite it if you'd like a different data loader.
        """
        return build_test_loader(cfg)

    @classmethod
    def build_evaluator(cls, cfg, dataset_name):
        """
        Returns:
            DatasetEvaluator or None

        It is not implemented by default.
        """
        raise NotImplementedError(
            """
If you want DefaultRunner to automatically run evaluation,
please implement `build_evaluator()` in subclasses (see train_net.py for example).
Alternatively, you can call evaluation functions yourself (see Colab balloon tutorial for example).
            """
        )

    @classmethod
    def test(cls, cfg, model, evaluators=None, output_folder=None):
        """
        Args:
            cfg (config dict):
            model (nn.Module):
            evaluators (list[DatasetEvaluator] or None): if None, will call
                :meth:`build_evaluator`. Otherwise, must have the same length as
                ``cfg.DATASETS.TEST``.

        Returns:
            dict: a dict of result metrics
        """
        logger = logging.getLogger(__name__)
        if isinstance(evaluators, DatasetEvaluator):
            evaluators = [evaluators]
        if evaluators is not None:
            assert len(cfg.DATASETS.TEST) == len(evaluators), "{} != {}".format(
                len(cfg.DATASETS.TEST), len(evaluators)
            )

        results = OrderedDict()
        for idx, dataset_name in enumerate(cfg.DATASETS.TEST):
            data_loader = cls.build_test_loader(cfg)
            # When evaluators are passed in as arguments,
            # implicitly assume that evaluators can be created before data_loader.
            if evaluators is not None:
                evaluator = evaluators[idx]
            else:
                try:
                    evaluator = cls.build_evaluator(
                        cfg, dataset_name, data_loader.dataset, output_folder=output_folder)
                except NotImplementedError:
                    logger.warn(
                        "No evaluator found. Use `DefaultTrainer.test(evaluators=)`, "
                        "or implement its `build_evaluator` method.")
                    results[dataset_name] = {}
                    continue
            results_i = inference_on_dataset(model, data_loader, evaluator)
            results[dataset_name] = results_i
            if comm.is_main_process():
                assert isinstance(
                    results_i, dict
                ), "Evaluator must return a dict on the main process. Got {} instead.".format(
                    results_i
                )
                logger.info("Evaluation results for {} in csv format:".format(dataset_name))
                print_csv_format(results_i)

        if len(results) == 1:
            results = list(results.values())[0]
        return results


def auto_scale_config(cfg, dataloader):

    logger = logging.getLogger(__name__)

    max_epoch = cfg.SOLVER.LR_SCHEDULER.MAX_EPOCH
    max_iter = cfg.SOLVER.LR_SCHEDULER.MAX_ITER

    subdivision = cfg.SOLVER.BATCH_SUBDIVISIONS
    # adjust lr by batch_subdivisions
    cfg.SOLVER.OPTIMIZER.BASE_LR *= subdivision

    """
    Here we use batch size * subdivision to simulator large batch training
    """
    if max_epoch:
        epoch_iter = math.ceil(
            len(dataloader.dataset) / (cfg.SOLVER.IMS_PER_BATCH * subdivision))

        if max_iter is not None:
            logger.warning(
                f"Training in EPOCH mode, automatically convert {max_epoch} epochs "
                f"into {max_epoch*epoch_iter} iters...")

        cfg.SOLVER.LR_SCHEDULER.MAX_ITER = max_epoch * epoch_iter
        cfg.SOLVER.LR_SCHEDULER.STEPS = [
            x * epoch_iter for x in cfg.SOLVER.LR_SCHEDULER.STEPS
        ]
        cfg.SOLVER.LR_SCHEDULER.WARMUP_ITERS = int(
            cfg.SOLVER.LR_SCHEDULER.WARMUP_ITERS * epoch_iter)
        cfg.SOLVER.CHECKPOINT_PERIOD = epoch_iter * cfg.SOLVER.CHECKPOINT_PERIOD
        cfg.TEST.EVAL_PERIOD = epoch_iter * cfg.TEST.EVAL_PERIOD
    else:
        epoch_iter = -1

    cfg.SOLVER.LR_SCHEDULER.EPOCH_ITERS = epoch_iter
```

### cvpods/engine/hooks.py

```python
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

import datetime
import itertools
import logging
import os
import tempfile
import time
from collections import Counter

import torch

from cvpods.checkpoint import PeriodicCheckpointer as _PeriodicCheckpointer
from cvpods.evaluation.testing import flatten_results_dict
from cvpods.modeling.nn_utils.precise_bn import get_bn_modules, update_bn_stats
from cvpods.utils import EventStorage, EventWriter, PathManager, Timer, comm

__all__ = [
    "HookBase",
    "CallbackHook",
    "IterationTimer",
    "OptimizationHook",
    "PeriodicWriter",
    "PeriodicCheckpointer",
    "LRScheduler",
    "AutogradProfiler",
    "EvalHook",
    "PreciseBN",
]


"""
Implement some common hooks.
"""


class HookBase:
    """
    Base class for hooks that can be registered with :class:`TrainerBase`.

    Each hook can implement 4 methods. The way they are called is demonstrated
    in the following snippet:

    .. code-block:: python

        hook.before_train()
        for iter in range(start_iter, max_iter):
            hook.before_step()
            trainer.run_step()
            hook.after_step()
        hook.after_train()

    Notes:
        1. In the hook method, users can access `self.trainer` to access more
           properties about the context (e.g., current iteration).

        2. A hook that does something in :meth:`before_step` can often be
           implemented equivalently in :meth:`after_step`.
           If the hook takes non-trivial time, it is strongly recommended to
           implement the hook in :meth:`after_step` instead of :meth:`before_step`.
           The convention is that :meth:`before_step` should only take negligible time.

           Following this convention will allow hooks that do care about the difference
           between :meth:`before_step` and :meth:`after_step` (e.g., timer) to
           function properly.

    Attributes:
        trainer: A weak reference to the trainer object. Set by the trainer when the hook is
            registered.
    """

    def before_train(self):
        """
        Called before the first iteration.
        """
        pass

    def after_train(self):
        """
        Called after the last iteration.
        """
        pass

    def before_step(self):
        """
        Called before each iteration.
        """
        pass

    def after_step(self):
        """
        Called after each iteration.
        """
        pass


class CallbackHook(HookBase):
    """
    Create a hook using callback functions provided by the user.
    """

    def __init__(self, *, before_train=None, after_train=None, before_step=None, after_step=None):
        """
        Each argument is a function that takes one argument: the trainer.
        """
        self._before_train = before_train
        self._before_step = before_step
        self._after_step = after_step
        self._after_train = after_train

    def before_train(self):
        if self._before_train:
            self._before_train(self.trainer)

    def after_train(self):
        if self._after_train:
            self._after_train(self.trainer)
        # The functions may be closures that hold reference to the trainer
        # Therefore, delete them to avoid circular reference.
        del self._before_train, self._after_train
        del self._before_step, self._after_step

    def before_step(self):
        if self._before_step:
            self._before_step(self.trainer)

    def after_step(self):
        if self._after_step:
            self._after_step(self.trainer)


class OptimizationHook(HookBase):
    def __init__(self, accumulate_grad_steps=1, grad_clipper=None, mixed_precision=False):
        self.accumulate_grad_steps = accumulate_grad_steps
        self.grad_clipper = grad_clipper
        self.mixed_precision = mixed_precision

    def before_step(self):
        self.trainer.optimizer.zero_grad()

    def after_step(self):
        losses = self.trainer.step_outputs["loss_for_backward"]
        losses /= self.accumulate_grad_steps

        if self.mixed_precision:
            from apex import amp
            with amp.scale_loss(losses, self.trainer.optimizer) as scaled_loss:
                scaled_loss.backward()
        else:
            losses.backward()

        if self.trainer.iter % self.accumulate_grad_steps == 0:
            if self.grad_clipper is not None:
                self.grad_clipper(self.tariner.model.paramters())
            self.trainer.optimizer.step()
            self.trainer.optimizer.zero_grad()


class IterationTimer(HookBase):
    """
    Track the time spent for each iteration (each run_step call in the trainer).
    Print a summary in the end of training.

    This hook uses the time between the call to its :meth:`before_step`
    and :meth:`after_step` methods.
    Under the convention that :meth:`before_step` of all hooks should only
    take negligible amount of time, the :class:`IterationTimer` hook should be
    placed at the beginning of the list of hooks to obtain accurate timing.
    """

    def __init__(self, warmup_iter=3):
        """
        Args:
            warmup_iter (int): the number of iterations at the beginning to exclude
                from timing.
        """
        self._warmup_iter = warmup_iter
        self._step_timer = Timer()

    def before_train(self):
        self._start_time = time.perf_counter()
        self._total_timer = Timer()
        self._total_timer.pause()

    def after_train(self):
        logger = logging.getLogger(__name__)
        total_time = time.perf_counter() - self._start_time
        total_time_minus_hooks = self._total_timer.seconds()
        hook_time = total_time - total_time_minus_hooks

        num_iter = self.trainer.iter + 1 - self.trainer.start_iter - self._warmup_iter

        if num_iter > 0 and total_time_minus_hooks > 0:
            # Speed is meaningful only after warmup
            # NOTE this format is parsed by grep in some scripts
            logger.info(
                "Overall training speed: {} iterations in {} ({:.4f} s / it)".format(
                    num_iter,
                    str(datetime.timedelta(seconds=int(total_time_minus_hooks))),
                    total_time_minus_hooks / num_iter,
                )
            )

        logger.info(
            "Total training time: {} ({} on hooks)".format(
                str(datetime.timedelta(seconds=int(total_time))),
                str(datetime.timedelta(seconds=int(hook_time))),
            )
        )

    def before_step(self):
        self._step_timer.reset()
        self._total_timer.resume()

    def after_step(self):
        # +1 because we're in after_step
        iter_done = self.trainer.iter - self.trainer.start_iter + 1
        if iter_done >= self._warmup_iter:
            sec = self._step_timer.seconds()
            self.trainer.storage.put_scalars(time=sec)
        else:
            self._start_time = time.perf_counter()
            self._total_timer.reset()

        self._total_timer.pause()


class PeriodicWriter(HookBase):
    """
    Write events to EventStorage periodically.

    It is executed every ``period`` iterations and after the last iteration.
    """

    def __init__(self, writers, period=20):
        """
        Args:
            writers (list[EventWriter]): a list of EventWriter objects
            period (int):
        """
        self._writers = writers
        for w in writers:
            assert isinstance(w, EventWriter), w
        self._period = period

    def after_step(self):
        if (
            (self.trainer.iter + 1) % self._period == 0
            or (self.trainer.iter == self.trainer.max_iter - 1)
            or (self.trainer.iter == 0)
        ):
            for writer in self._writers:
                writer.write()

    def after_train(self):
        for writer in self._writers:
            writer.close()


class PeriodicCheckpointer(_PeriodicCheckpointer, HookBase):
    """
    Same as :class:`cvpods.checkpoint.PeriodicCheckpointer`, but as a hook.

    Note that when used as a hook,
    it is unable to save additional data other than what's defined
    by the given `checkpointer`.

    It is executed every ``period`` iterations and after the last iteration.
    """

    def before_train(self):
        self.max_iter = self.trainer.max_iter

    def after_step(self):
        # No way to use **kwargs
        self.step(self.trainer.iter)


class LRScheduler(HookBase):
    """
    A hook which executes a torch builtin LR scheduler and summarizes the LR.
    It is executed after every iteration.
    """

    def __init__(self, optimizer, scheduler):
        """
        Args:
            optimizer (torch.optim.Optimizer):
            scheduler (torch.optim._LRScheduler)
        """
        self._optimizer = optimizer
        self._scheduler = scheduler

        # NOTE: some heuristics on what LR to summarize
        # summarize the param group with most parameters
        largest_group = max(len(g["params"]) for g in optimizer.param_groups)

        if largest_group == 1:
            # If all groups have one parameter,
            # then find the most common initial LR, and use it for summary
            lr_count = Counter([g["lr"] for g in optimizer.param_groups])
            lr = lr_count.most_common()[0][0]
            for i, g in enumerate(optimizer.param_groups):
                if g["lr"] == lr:
                    self._best_param_group_id = i
                    break
        else:
            for i, g in enumerate(optimizer.param_groups):
                if len(g["params"]) == largest_group:
                    self._best_param_group_id = i
                    break

    def after_step(self):
        lr = self._optimizer.param_groups[self._best_param_group_id]["lr"]
        self.trainer.storage.put_scalar("lr", lr, smoothing_hint=False)
        self._scheduler.step()


class AutogradProfiler(HookBase):
    r"""
    A hook which runs `torch.autograd.profiler.profile`.

    Examples:
    .. code-block:: python

        hooks.AutogradProfiler(
            lambda trainer: trainer.iter > 10 and trainer.iter < 20, self.cfg.OUTPUT_DIR
            )

    The above example will run the profiler for iteration 10~20 and dump
    results to ``OUTPUT_DIR``. We did not profile the first few iterations
    because they are typically slower than the rest.
    The result files can be loaded in the ``chrome://tracing`` page in chrome browser.

    Note:
        When used together with NCCL on older version of GPUs,
        autograd profiler may cause deadlock because it unnecessarily allocates
        memory on every device it sees. The memory management calls, if
        interleaved with NCCL calls, lead to deadlock on GPUs that do not
        support `cudaLaunchCooperativeKernelMultiDevice`.
    """

    def __init__(self, enable_predicate, output_dir, *, use_cuda=True):
        """
        Args:
            enable_predicate (callable[trainer -> bool]): a function which takes a trainer,
                and returns whether to enable the profiler.
                It will be called once every step, and can be used to select which steps to profile.
            output_dir (str): the output directory to dump tracing files.
            use_cuda (bool): same as in `torch.autograd.profiler.profile`.
        """
        self._enable_predicate = enable_predicate
        self._use_cuda = use_cuda
        self._output_dir = output_dir

    def before_step(self):
        if self._enable_predicate(self.trainer):
            self._profiler = torch.autograd.profiler.profile(use_cuda=self._use_cuda)
            self._profiler.__enter__()
        else:
            self._profiler = None

    def after_step(self):
        if self._profiler is None:
            return
        self._profiler.__exit__(None, None, None)
        PathManager.mkdirs(self._output_dir)
        out_file = os.path.join(
            self._output_dir, "profiler-trace-iter{}.json".format(self.trainer.iter)
        )
        if "://" not in out_file:
            self._profiler.export_chrome_trace(out_file)
        else:
            # Support non-posix filesystems
            with tempfile.TemporaryDirectory(prefix="cvpods_profiler") as d:
                tmp_file = os.path.join(d, "tmp.json")
                self._profiler.export_chrome_trace(tmp_file)
                with open(tmp_file) as f:
                    content = f.read()
            with PathManager.open(out_file, "w") as f:
                f.write(content)


class EvalHook(HookBase):
    """
    Run an evaluation function periodically, and at the end of training.

    It is executed every ``eval_period`` iterations and after the last iteration.
    """

    def __init__(self, eval_period, eval_function):
        """
        Args:
            eval_period (int): the period to run `eval_function`.
            eval_function (callable): a function which takes no arguments, and
                returns a nested dict of evaluation metrics.

        Note:
            This hook must be enabled in all or none workers.
            If you would like only certain workers to perform evaluation,
            give other workers a no-op function (`eval_function=lambda: None`).
        """
        self._period = eval_period
        self._func = eval_function

    def _do_eval(self):
        results = self._func()

        if results:
            assert isinstance(
                results, dict
            ), "Eval function must return a dict. Got {} instead.".format(results)

            flattened_results = flatten_results_dict(results)
            for k, v in flattened_results.items():
                try:
                    v = float(v)
                except Exception:
                    raise ValueError(
                        "[EvalHook] eval_function should return a nested dict of float. "
                        "Got '{}: {}' instead.".format(k, v)
                    )
            self.trainer.storage.put_scalars(**flattened_results, smoothing_hint=False)

        # Evaluation may take different time among workers.
        # A barrier make them start the next iteration together.
        comm.synchronize()

    def after_step(self):
        next_iter = self.trainer.iter + 1
        is_final = (next_iter == self.trainer.max_iter) and (self._period >= 0)
        if is_final or (self._period > 0 and next_iter % self._period == 0):
            self._do_eval()

    def after_train(self):
        # func is likely a closure that holds reference to the trainer
        # therefore we clean it to avoid circular reference in the end
        del self._func


class PreciseBN(HookBase):
    """
    The standard implementation of BatchNorm uses EMA in inference, which is
    sometimes suboptimal.
    This class computes the true average of statistics rather than the moving average,
    and put true averages to every BN layer in the given model.

    It is executed every ``period`` iterations and after the last iteration.
    """

    def __init__(self, period, model, data_loader, num_iter):
        """
        Args:
            period (int): the period this hook is run, or 0 to not run during training.
                The hook will always run in the end of training.
            model (nn.Module): a module whose all BN layers in training mode will be
                updated by precise BN.
                Note that user is responsible for ensuring the BN layers to be
                updated are in training mode when this hook is triggered.
            data_loader (iterable): it will produce data to be run by `model(data)`.
            num_iter (int): number of iterations used to compute the precise
                statistics.
        """
        self._logger = logging.getLogger(__name__)
        if len(get_bn_modules(model)) == 0:
            self._logger.info(
                "PreciseBN is disabled because model does not contain BN layers in training mode."
            )
            self._disabled = True
            return

        self._model = model
        self._data_loader = data_loader
        self._num_iter = num_iter
        self._period = period
        self._disabled = False

        self._data_iter = iter(self._data_loader)

    def after_step(self):
        next_iter = self.trainer.iter + 1
        is_final = next_iter == self.trainer.max_iter
        if is_final or (self._period > 0 and next_iter % self._period == 0):
            self.update_stats()

    def update_stats(self):
        """
        Update the model with precise statistics. Users can manually call this method.
        """
        if self._disabled:
            return

        def data_loader():
            for num_iter in itertools.count(1):
                if num_iter % 100 == 0:
                    self._logger.info(
                        "Running precise-BN ... {}/{} iterations.".format(num_iter, self._num_iter)
                    )
                # This way we can reuse the same iterator
                try:
                    item = next(self._data_iter)
                except StopIteration:
                    self._data_iter = iter(self._data_loader)
                    item = next(self._data_iter)

                yield item

        with EventStorage():  # capture events in a new storage to discard them
            self._logger.info(
                "Running precise-BN for {} iterations...  ".format(self._num_iter)
                + "Note that this could produce different statistics every time."
            )
            update_bn_stats(self._model, data_loader(), self._num_iter)
```

### cvpods/engine/base_runner.py

```python
# Copyright (c) Facebook, Inc. and its affiliates.
# Modified by BaseDetection, Inc. and its affiliates.

import logging
import time
import weakref
from typing import Dict

import numpy as np

import torch

from cvpods.utils import comm
from cvpods.utils.dump.events import EventStorage, get_event_storage
from cvpods.utils.registry import Registry

from .hooks import HookBase

RUNNERS = Registry("runners")
logger = logging.getLogger(__name__)


@RUNNERS.register()
class RunnerBase:
    """
    Base class for iterative runner with hooks.

    The only assumption we made here is: the training runs in a loop.
    A subclass can implement what the loop is.
    We made no assumptions about the existence of dataloader, optimizer, model, etc.

    Attributes:
        iter(int): the current iteration.

        start_iter(int): The iteration to start with.
            By convention the minimum possible value is 0.

        max_iter(int): The iteration to end training.

        storage(EventStorage): An EventStorage that's opened during the course of training.
    """

    def __init__(self):
        self._hooks = []

    def register_hooks(self, hooks):
        """
        Register hooks to the runner. The hooks are executed in the order
        they are registered.

        Args:
            hooks (list[Optional[HookBase]]): list of hooks
        """
        hooks = [h for h in hooks if h is not None]
        for h in hooks:
            assert isinstance(h, HookBase)
            # To avoid circular reference, hooks and runner cannot own each other.
            # This normally does not matter, but will cause memory leak if the
            # involved objects contain __del__:
            # See http://engineering.hearsaysocial.com/2013/06/16/circular-references-in-python/
            h.trainer = weakref.proxy(self)
        self._hooks.extend(hooks)

    def train(
        self,
        start_iter: int,
        start_epoch: int,
        max_iter: int,
    ):
        """
        Args:
            start_iter, max_iter (int): See docs above
        """
        self.iter = self.start_iter = start_iter
        self.epoch = self.start_epoch = start_epoch

        with EventStorage(start_iter) as self.storage:
            try:
                self.before_train()
                for self.iter in range(start_iter, max_iter):
                    self.before_step()
                    # by default, a step contains data_loading and model forward,
                    # loss backward is executed in after_step for better expansibility
                    self.run_step()
                    self.after_step()
                # self.iter == max_iter can be used by `after_train` to
                # tell whether the training successfully finished or failed
                # due to exceptions.
                self.iter += 1
            except Exception:
                logger.exception("Exception during training:")
                raise
            finally:
                self.after_train()

    def before_train(self):
        for h in self._hooks:
            h.before_train()

    def after_train(self):
        self.storage._iter = self.iter
        for h in self._hooks:
            h.after_train()

    def before_step(self):
        # Maintain the invariant that storage.iter == runner.iter
        # for the entire execution of each step
        self.storage._iter = self.iter

        for h in self._hooks:
            h.before_step()

    def after_step(self):
        for h in self._hooks:
            h.after_step()

    def run_step(self):
        raise NotImplementedError


@RUNNERS.register()
class SimpleRunner(RunnerBase):
    """
    A simple runner for the most common type of task:
    fetch a data batch and execute model forwarding, optionally using data-parallelism.
    It assumes that every step, you:

    1. Compute the loss with a data from the data_loader.

    Note that all other tasks during training (checkpointing, logging, evaluation,
    LR schedule, gradients compute, parameters udpate) are maintained by hooks,
    which can be registered by :meth:`RunnerBase.register_hooks`.

    If you want to do anything fancier than this,
    either subclass RunnerBase and implement your own `run_step`,
    or write your own training loop.
    """

    def __init__(self, model, data_loader, optimizer):
        """
        Args:
            model: a torch Module. Takes a data from data_loader and returns a
                dict of losses.
            data_loader: an iterable. Contains data to be used to call model.
            optimizer: a torch optimizer.
        """
        super().__init__()

        """
        We set the model to training mode in the runner.
        However it's valid to train a model that's in eval mode.
        If you want your model (or a submodule of it) to behave
        like evaluation during training, you can overwrite its train() method.
        """
        model.train()

        self.model = model
        self.data_loader = data_loader
        self._data_loader_iter = iter(data_loader)
        self.optimizer = optimizer

    def run_step(self):
        """
        Implement the standard training logic described above.
        """
        assert self.model.training, "[IterRunner] model was changed to eval mode!"
        start = time.perf_counter()
        """
        If you want to do something with the data, you can wrap the dataloader.
        """
        try:
            data = next(self._data_loader_iter)
        except StopIteration:
            self.epoch += 1
            if hasattr(self.data_loader.sampler, 'set_epoch'):
                self.data_loader.sampler.set_epoch(self.epoch)
            self._data_loader_iter = iter(self.data_loader)
            data = next(self._data_loader_iter)

        data_time = time.perf_counter() - start

        """
        If you want to do something with the losses, you can wrap the model.
        """
        loss_dict = self.model(data)
        losses = sum([
            metrics_value for metrics_value in loss_dict.values()
            if metrics_value.requires_grad
        ])
        self._detect_anomaly(losses, loss_dict)
        self._write_metrics(loss_dict, data_time)

        self.step_outputs = {
            "loss_for_backward": losses,
        }

    def _detect_anomaly(self, losses, loss_dict):
        if not torch.isfinite(losses).all():
            raise FloatingPointError(
                "Loss became infinite or NaN at iteration={}!\nloss_dict = {}".format(
                    self.iter, loss_dict
                )
            )

    def _write_metrics(
        self,
        loss_dict: Dict[str, torch.Tensor],
        data_time: float,
        prefix: str = "",
    ):
        """
        Args:
            loss_dict (dict): dict of scalar losses
            data_time (float): time taken by the dataloader iteration
        """
        device = next(iter(loss_dict.values())).device

        # Use a new stream so these ops don't wait for DDP or backward
        with torch.cuda.stream(torch.cuda.Stream() if device.type == "cuda" else None):
            metrics_dict = {k: v.detach().cpu().item() for k, v in loss_dict.items()}
            metrics_dict["data_time"] = data_time

            # Gather metrics among all workers for logging
            # This assumes we do DDP-style training, which is currently the only
            # supported method in cvpods.
            all_metrics_dict = comm.gather(metrics_dict)

        if comm.is_main_process():
            storage = get_event_storage()

            # data_time among workers can have high variance. The actual latency
            # caused by data_time is the maximum among workers.
            data_time = np.max([x.pop("data_time") for x in all_metrics_dict])
            storage.put_scalar("data_time", data_time)

            # average the rest metrics
            metrics_dict = {
                k: np.mean([x[k] for x in all_metrics_dict]) for k in all_metrics_dict[0].keys()
            }
            total_losses_reduced = sum(loss for key, loss in metrics_dict.items() if "loss" in key)
            storage.put_scalar("{}total_loss".format(prefix), total_losses_reduced)
            if len(metrics_dict) > 1:
                storage.put_scalars(**metrics_dict)
```

### cvpods/engine/__init__.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.

from .hooks import *
from .launch import *
from .predictor import *
from .base_runner import *
from .runner import *
from .setup import *

__all__ = [k for k in globals().keys() if not k.startswith("_")]
```

### cvpods/engine/launch.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import logging

import torch
import torch.distributed as dist
import torch.multiprocessing as mp

from cvpods.utils import comm

__all__ = ["launch"]


def _find_free_port():
    """
    Find an available port of current machine / node.
    """
    import socket

    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    # Binding to port 0 will cause the OS to find an available port for us
    sock.bind(("", 0))
    port = sock.getsockname()[1]
    sock.close()
    # NOTE: there is still a chance the port could be taken by other processes.
    return port


def launch(main_func, num_gpus_per_machine, num_machines=1, machine_rank=0, dist_url=None, args=()):
    """
    Args:
        main_func: a function that will be called by `main_func(*args)`
        num_machines (int): the total number of machines
        machine_rank (int): the rank of this machine (one per machine)
        dist_url (str): url to connect to for distributed training, including protocol
                       e.g. "tcp://127.0.0.1:8686".
                       Can be set to auto to automatically select a free port on localhost
        args (tuple): arguments passed to main_func
    """
    world_size = num_machines * num_gpus_per_machine
    if world_size > 1:
        # https://github.com/pytorch/pytorch/pull/14391
        # TODO prctl in spawned processes

        if dist_url == "auto":
            assert num_machines == 1, "dist_url=auto cannot work with distributed training."
            port = _find_free_port()
            dist_url = f"tcp://127.0.0.1:{port}"

        mp.spawn(
            _distributed_worker,
            nprocs=num_gpus_per_machine,
            args=(main_func, world_size, num_gpus_per_machine, machine_rank, dist_url, args),
            daemon=False,
        )
    else:
        main_func(*args)


def _distributed_worker(
    local_rank, main_func, world_size, num_gpus_per_machine, machine_rank, dist_url, args
):
    assert torch.cuda.is_available(), "cuda is not available. Please check your installation."
    global_rank = machine_rank * num_gpus_per_machine + local_rank
    try:
        dist.init_process_group(
            backend="NCCL", init_method=dist_url, world_size=world_size, rank=global_rank
        )
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error("Process group URL: {}".format(dist_url))
        raise e
    # synchronize is needed here to prevent a possible timeout after calling init_process_group
    # See: https://github.com/facebookresearch/maskrcnn-benchmark/issues/172
    comm.synchronize()

    assert num_gpus_per_machine <= torch.cuda.device_count()
    torch.cuda.set_device(local_rank)

    # Setup the local process group (which contains ranks within the same machine)
    assert comm._LOCAL_PROCESS_GROUP is None
    num_machines = world_size // num_gpus_per_machine
    for i in range(num_machines):
        ranks_on_i = list(range(i * num_gpus_per_machine, (i + 1) * num_gpus_per_machine))
        pg = dist.new_group(ranks_on_i)
        if i == machine_rank:
            comm._LOCAL_PROCESS_GROUP = pg

    main_func(*args)
```

###### playground/detection/coco/yolof/yolof.res50.DC5.1x/config.py

```python
from cvpods.configs.base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        # Backbone NAME: "build_resnet_backbone"
        WEIGHTS="detectron2://ImageNetPretrained/MSRA/R-50.pkl",
        RESNETS=dict(DEPTH=50, OUT_FEATURES=["res5"], RES5_DILATION=2),
        ANCHOR_GENERATOR=dict(
            SIZES=[[16, 32, 64, 128, 256, 512]],
            ASPECT_RATIOS=[[1.0]]
        ),
        YOLOF=dict(
            ENCODER=dict(
                IN_FEATURES=["res5"],
                NUM_CHANNELS=512,
                BLOCK_MID_CHANNELS=128,
                NUM_RESIDUAL_BLOCKS=4,
                BLOCK_DILATIONS=[4, 8, 12, 16],
                NORM="BN",
                ACTIVATION="ReLU"
            ),
            DECODER=dict(
                IN_CHANNELS=512,
                NUM_CLASSES=80,
                NUM_ANCHORS=6,
                CLS_NUM_CONVS=2,
                REG_NUM_CONVS=4,
                NORM="BN",
                ACTIVATION="ReLU",
                PRIOR_PROB=0.01
            ),
            BBOX_REG_WEIGHTS=(1.0, 1.0, 1.0, 1.0),
            ADD_CTR_CLAMP=True,
            CTR_CLAMP=32,
            MATCHER_TOPK=8,
            POS_IGNORE_THRESHOLD=0.1,
            NEG_IGNORE_THRESHOLD=0.7,
            FOCAL_LOSS_GAMMA=2.0,
            FOCAL_LOSS_ALPHA=0.25,
            SCORE_THRESH_TEST=0.05,
            TOPK_CANDIDATES_TEST=1000,
            NMS_THRESH_TEST=0.6
        ),
    ),
    DATASETS=dict(
        TRAIN=("coco_2017_train",),
        TEST=("coco_2017_val",),
    ),
    DATALOADER=dict(NUM_WORKERS=8),
    SOLVER=dict(
        LR_SCHEDULER=dict(
            STEPS=(15000, 20000),
            MAX_ITER=22500,
            WARMUP_FACTOR=0.00066667,
            WARMUP_ITERS=1500
        ),
        OPTIMIZER=dict(
            NAME="D2SGD",
            BASE_LR=0.12,
            BIAS_LR_FACTOR=1.0,
            WEIGHT_DECAY=0.0001,
            WEIGHT_DECAY_NORM=0.0,
            MOMENTUM=0.9,
            BACKBONE_LR_FACTOR=0.334
        ),
        IMS_PER_BATCH=64,
        IMS_PER_DEVICE=8,
    ),
    INPUT=dict(
        AUG=dict(
            TRAIN_PIPELINES=[
                ("ResizeShortestEdge", dict(
                    short_edge_length=(800,), max_size=1333,
                    sample_style="choice")),
                ("RandomFlip", dict()),
                ("RandomShift", dict(max_shifts=32))
            ],
            TEST_PIPELINES=[
                ("ResizeShortestEdge", dict(
                    short_edge_length=800, max_size=1333,
                    sample_style="choice")),
            ],
        ),
        # Whether the model needs RGB, YUV, HSV etc.
        FORMAT="BGR",
    ),
)


class YOLOFConfig(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = YOLOFConfig()
```

###### playground/detection/coco/yolof/yolof.res50.DC5.1x/net.py

```python
import logging
import sys

sys.path.append("..")

from cvpods.layers import ShapeSpec
from cvpods.modeling.backbone import Backbone
from cvpods.modeling.backbone import build_resnet_backbone
from cvpods.modeling.anchor_generator import DefaultAnchorGenerator

from yolof_base import build_encoder, build_decoder, YOLOF


def build_backbone(cfg, input_shape=None):
    """
    Build a backbone from `cfg.MODEL.BACKBONE.NAME`.
    Returns:
        an instance of :class:`Backbone`
    """
    if input_shape is None:
        input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))

    backbone = build_resnet_backbone(cfg, input_shape)
    assert isinstance(backbone, Backbone)
    return backbone


def build_anchor_generator(cfg, input_shape):
    return DefaultAnchorGenerator(cfg, input_shape)


def build_model(cfg):
    cfg.build_backbone = build_backbone
    cfg.build_anchor_generator = build_anchor_generator
    cfg.build_encoder = build_encoder
    cfg.build_decoder = build_decoder
    model = YOLOF(cfg)

    logger = logging.getLogger(__name__)
    logger.info("Model:\n{}".format(model))
    return model
```

###### playground/detection/coco/yolof/yolof.res101.C5.1x/config.py

```python
from cvpods.configs.base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        # Backbone NAME: "build_resnet_backbone"
        WEIGHTS="detectron2://ImageNetPretrained/MSRA/R-101.pkl",
        RESNETS=dict(DEPTH=101, OUT_FEATURES=["res5"]),
        ANCHOR_GENERATOR=dict(
            SIZES=[[32, 64, 128, 256, 512]],
            ASPECT_RATIOS=[[1.0]]
        ),
        YOLOF=dict(
            ENCODER=dict(
                IN_FEATURES=["res5"],
                NUM_CHANNELS=512,
                BLOCK_MID_CHANNELS=128,
                NUM_RESIDUAL_BLOCKS=4,
                BLOCK_DILATIONS=[2, 4, 6, 8],
                NORM="BN",
                ACTIVATION="ReLU"
            ),
            DECODER=dict(
                IN_CHANNELS=512,
                NUM_CLASSES=80,
                NUM_ANCHORS=5,
                CLS_NUM_CONVS=2,
                REG_NUM_CONVS=4,
                NORM="BN",
                ACTIVATION="ReLU",
                PRIOR_PROB=0.01
            ),
            BBOX_REG_WEIGHTS=(1.0, 1.0, 1.0, 1.0),
            ADD_CTR_CLAMP=True,
            CTR_CLAMP=32,
            MATCHER_TOPK=4,
            POS_IGNORE_THRESHOLD=0.15,
            NEG_IGNORE_THRESHOLD=0.7,
            FOCAL_LOSS_GAMMA=2.0,
            FOCAL_LOSS_ALPHA=0.25,
            SCORE_THRESH_TEST=0.05,
            TOPK_CANDIDATES_TEST=1000,
            NMS_THRESH_TEST=0.6
        ),
    ),
    DATASETS=dict(
        TRAIN=("coco_2017_train",),
        TEST=("coco_2017_val",),
    ),
    DATALOADER=dict(NUM_WORKERS=8),
    SOLVER=dict(
        LR_SCHEDULER=dict(
            STEPS=(15000, 20000),
            MAX_ITER=22500,
            WARMUP_FACTOR=0.00066667,
            WARMUP_ITERS=1500
        ),
        OPTIMIZER=dict(
            NAME="D2SGD",
            BASE_LR=0.12,
            BIAS_LR_FACTOR=1.0,
            WEIGHT_DECAY=0.0001,
            WEIGHT_DECAY_NORM=0.0,
            MOMENTUM=0.9,
            BACKBONE_LR_FACTOR=0.334
        ),
        IMS_PER_BATCH=64,
        IMS_PER_DEVICE=8,
    ),
    INPUT=dict(
        AUG=dict(
            TRAIN_PIPELINES=[
                ("ResizeShortestEdge", dict(
                    short_edge_length=(800,), max_size=1333,
                    sample_style="choice")),
                ("RandomFlip", dict()),
                ("RandomShift", dict(max_shifts=32))
            ],
            TEST_PIPELINES=[
                ("ResizeShortestEdge", dict(
                    short_edge_length=800, max_size=1333,
                    sample_style="choice")),
            ],
        ),
        # Whether the model needs RGB, YUV, HSV etc.
        FORMAT="BGR",
    ),
)


class YOLOFConfig(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = YOLOFConfig()
```

###### playground/detection/coco/yolof/yolof.res101.C5.1x/net.py

```python
import logging
import sys

sys.path.append("..")

from cvpods.layers import ShapeSpec
from cvpods.modeling.backbone import Backbone
from cvpods.modeling.backbone import build_resnet_backbone
from cvpods.modeling.anchor_generator import DefaultAnchorGenerator

from yolof_base import build_encoder, build_decoder, YOLOF


def build_backbone(cfg, input_shape=None):
    """
    Build a backbone from `cfg.MODEL.BACKBONE.NAME`.
    Returns:
        an instance of :class:`Backbone`
    """
    if input_shape is None:
        input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))

    backbone = build_resnet_backbone(cfg, input_shape)
    assert isinstance(backbone, Backbone)
    return backbone


def build_anchor_generator(cfg, input_shape):
    return DefaultAnchorGenerator(cfg, input_shape)


def build_model(cfg):
    cfg.build_backbone = build_backbone
    cfg.build_anchor_generator = build_anchor_generator
    cfg.build_encoder = build_encoder
    cfg.build_decoder = build_decoder
    model = YOLOF(cfg)

    logger = logging.getLogger(__name__)
    logger.info("Model:\n{}".format(model))
    return model
```

###### playground/detection/coco/yolof/yolof.res50.C5.1x/config.py

```python
from cvpods.configs.base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        # Backbone NAME: "build_resnet_backbone"
        WEIGHTS="detectron2://ImageNetPretrained/MSRA/R-50.pkl",
        RESNETS=dict(DEPTH=50, OUT_FEATURES=["res5"]),
        ANCHOR_GENERATOR=dict(
            SIZES=[[32, 64, 128, 256, 512]],
            ASPECT_RATIOS=[[1.0]]
        ),
        YOLOF=dict(
            ENCODER=dict(
                IN_FEATURES=["res5"],
                NUM_CHANNELS=512,
                BLOCK_MID_CHANNELS=128,
                NUM_RESIDUAL_BLOCKS=4,
                BLOCK_DILATIONS=[2, 4, 6, 8],
                NORM="BN",
                ACTIVATION="ReLU"
            ),
            DECODER=dict(
                IN_CHANNELS=512,
                NUM_CLASSES=80,
                NUM_ANCHORS=5,
                CLS_NUM_CONVS=2,
                REG_NUM_CONVS=4,
                NORM="BN",
                ACTIVATION="ReLU",
                PRIOR_PROB=0.01
            ),
            BBOX_REG_WEIGHTS=(1.0, 1.0, 1.0, 1.0),
            ADD_CTR_CLAMP=True,
            CTR_CLAMP=32,
            MATCHER_TOPK=4,
            POS_IGNORE_THRESHOLD=0.15,
            NEG_IGNORE_THRESHOLD=0.7,
            FOCAL_LOSS_GAMMA=2.0,
            FOCAL_LOSS_ALPHA=0.25,
            SCORE_THRESH_TEST=0.05,
            TOPK_CANDIDATES_TEST=1000,
            NMS_THRESH_TEST=0.6
        ),
    ),
    DATASETS=dict(
        TRAIN=("coco_2017_train",),
        TEST=("coco_2017_val",),
    ),
    DATALOADER=dict(NUM_WORKERS=8),
    SOLVER=dict(
        LR_SCHEDULER=dict(
            STEPS=(15000, 20000),
            MAX_ITER=22500,
            WARMUP_FACTOR=0.00066667,
            WARMUP_ITERS=1500
        ),
        OPTIMIZER=dict(
            NAME="D2SGD",
            BASE_LR=0.12,
            BIAS_LR_FACTOR=1.0,
            WEIGHT_DECAY=0.0001,
            WEIGHT_DECAY_NORM=0.0,
            MOMENTUM=0.9,
            BACKBONE_LR_FACTOR=0.334
        ),
        IMS_PER_BATCH=64,
        IMS_PER_DEVICE=8,
    ),
    INPUT=dict(
        AUG=dict(
            TRAIN_PIPELINES=[
                ("ResizeShortestEdge", dict(
                    short_edge_length=(800,), max_size=1333,
                    sample_style="choice")),
                ("RandomFlip", dict()),
                ("RandomShift", dict(max_shifts=32))
            ],
            TEST_PIPELINES=[
                ("ResizeShortestEdge", dict(
                    short_edge_length=800, max_size=1333,
                    sample_style="choice")),
            ],
        ),
        # Whether the model needs RGB, YUV, HSV etc.
        FORMAT="BGR",
    ),
)


class YOLOFConfig(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = YOLOFConfig()
```

###### playground/detection/coco/yolof/yolof.res50.C5.1x/net.py

```python
import logging
import sys

sys.path.append("..")

from cvpods.layers import ShapeSpec
from cvpods.modeling.backbone import Backbone
from cvpods.modeling.backbone import build_resnet_backbone
from cvpods.modeling.anchor_generator import DefaultAnchorGenerator

from yolof_base import build_encoder, build_decoder, YOLOF


def build_backbone(cfg, input_shape=None):
    """
    Build a backbone from `cfg.MODEL.BACKBONE.NAME`.
    Returns:
        an instance of :class:`Backbone`
    """
    if input_shape is None:
        input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))

    backbone = build_resnet_backbone(cfg, input_shape)
    assert isinstance(backbone, Backbone)
    return backbone


def build_anchor_generator(cfg, input_shape):
    return DefaultAnchorGenerator(cfg, input_shape)


def build_model(cfg):
    cfg.build_backbone = build_backbone
    cfg.build_anchor_generator = build_anchor_generator
    cfg.build_encoder = build_encoder
    cfg.build_decoder = build_decoder
    model = YOLOF(cfg)

    logger = logging.getLogger(__name__)
    logger.info("Model:\n{}".format(model))
    return model
```

###### playground/detection/coco/yolof/yolof.cspdarknet53.DC5.9x.stage2.3x/config.py

```python
from cvpods.configs.base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        # Backbone NAME: "build_darknet_backbone"
        WEIGHTS="../../../../../pretrained_models/YOLOF_CSP_D_53_DC5_9x.pth",
        # or
        # WEIGHTS="../yolof.cspdarknet53.DC5.9x/log/model_final.pth",
        DARKNET=dict(
            DEPTH=53,
            WITH_CSP=True,
            NORM="SyncBN",
            OUT_FEATURES=["res5"],
            RES5_DILATION=2
        ),
        ANCHOR_GENERATOR=dict(
            SIZES=[[16, 32, 64, 128, 256, 512]],
            ASPECT_RATIOS=[[1.0]]
        ),
        YOLOF=dict(
            ENCODER=dict(
                IN_FEATURES=["res5"],
                NUM_CHANNELS=512,
                BLOCK_MID_CHANNELS=128,
                NUM_RESIDUAL_BLOCKS=8,
                BLOCK_DILATIONS=[1, 2, 3, 4, 5, 6, 7, 8],
                NORM="SyncBN",
                ACTIVATION="LeakyReLU"
            ),
            DECODER=dict(
                IN_CHANNELS=512,
                NUM_CLASSES=80,
                NUM_ANCHORS=6,
                CLS_NUM_CONVS=2,
                REG_NUM_CONVS=4,
                NORM="SyncBN",
                ACTIVATION="LeakyReLU",
                PRIOR_PROB=0.01
            ),
            BBOX_REG_WEIGHTS=(1.0, 1.0, 1.0, 1.0),
            ADD_CTR_CLAMP=True,
            CTR_CLAMP=32,
            MATCHER_TOPK=8,
            POS_IGNORE_THRESHOLD=0.1,
            NEG_IGNORE_THRESHOLD=0.8,
            FOCAL_LOSS_GAMMA=2.0,
            FOCAL_LOSS_ALPHA=0.25,
            SCORE_THRESH_TEST=0.05,
            TOPK_CANDIDATES_TEST=1000,
            NMS_THRESH_TEST=0.6
        ),
    ),
    DATASETS=dict(
        TRAIN=("coco_2017_train",),
        TEST=("coco_2017_val",),
    ),
    DATALOADER=dict(NUM_WORKERS=8),
    SOLVER=dict(
        LR_SCHEDULER=dict(
            STEPS=(52500, 62500),
            MAX_ITER=67500,
            WARMUP_FACTOR=0.00066667,
            WARMUP_ITERS=1500
        ),
        OPTIMIZER=dict(
            NAME="D2SGD",
            BASE_LR=0.04,
            BIAS_LR_FACTOR=1.0,
            WEIGHT_DECAY=0.0001,
            WEIGHT_DECAY_NORM=0.0,
            MOMENTUM=0.9,
            BACKBONE_LR_FACTOR=1.0
        ),
        IMS_PER_BATCH=64,
        IMS_PER_DEVICE=8,
    ),
    INPUT=dict(
        AUG=dict(
            TRAIN_PIPELINES=[
                ("JitterCrop", dict(jitter_ratio=0.3)),
                ("Resize", dict(shape=(640, 640), scale_jitter=(0.8, 1.2))),
                ("RandomDistortion2",
                 dict(hue=0.1, saturation=1.5, exposure=1.5)),
                ("RandomFlip", dict()),
                ("RandomShift", dict(max_shifts=32))
            ],
            TEST_PIPELINES=[
                ("Resize", dict(shape=(608, 608))),
            ],
        ),
        MOSAIC=dict(
            MIN_OFFSET=0.2,
            MOSAIC_WIDTH=640,
            MOSAIC_HEIGHT=640
        ),
        # Whether the model needs RGB, YUV, HSV etc.
        FORMAT="BGR",
    ),
)


class YOLOFConfig(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = YOLOFConfig()
```

###### playground/detection/coco/yolof/yolof.cspdarknet53.DC5.9x.stage2.3x/net.py

```python
import logging
import sys

from cvpods.layers import ShapeSpec
from cvpods.modeling.backbone import Backbone
from cvpods.modeling.anchor_generator import DefaultAnchorGenerator

from .cspdarknet import build_darknet_backbone
sys.path.append("..")
from yolof_base import build_encoder, build_decoder, YOLOF


def build_backbone(cfg, input_shape=None):
    """
    Build a backbone from `cfg.MODEL.BACKBONE.NAME`.
    Returns:
        an instance of :class:`Backbone`
    """
    if input_shape is None:
        input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))

    backbone = build_darknet_backbone(cfg, input_shape)
    assert isinstance(backbone, Backbone)
    return backbone


def build_anchor_generator(cfg, input_shape):
    return DefaultAnchorGenerator(cfg, input_shape)


def build_model(cfg):
    cfg.build_backbone = build_backbone
    cfg.build_anchor_generator = build_anchor_generator
    cfg.build_encoder = build_encoder
    cfg.build_decoder = build_decoder
    model = YOLOF(cfg)

    logger = logging.getLogger(__name__)
    logger.info("Model:\n{}".format(model))
    return model
```

###### playground/detection/coco/yolof/yolof.cspdarknet53.DC5.9x.stage2.3x/cspdarknet.py

```python
import logging
import sys

sys.path.append("..")

import torch
import torch.nn as nn
import torch.nn.functional as F

from cvpods.modeling.backbone import Backbone
from cvpods.layers import ShapeSpec

from yolof_base.utils import get_norm

try:
    from mish_cuda import MishCuda as Mish
except Exception:
    logger = logging.getLogger(__name__)
    logger.warning(
        "Install mish-cuda to speed up training and inference. More "
        "importantly, replace the naive Mish with MishCuda will give a "
        "~1.5G memory saving during training."
    )


    def mish(x):
        return x.mul(F.softplus(x).tanh())


    class Mish(nn.Module):
        def __init__(self):
            super(Mish, self).__init__()

        def forward(self, x):
            return mish(x)


def ConvNormActivation(inplanes,
                       planes,
                       kernel_size=3,
                       stride=1,
                       padding=0,
                       dilation=1,
                       groups=1,
                       norm_type="BN"):
    """
    A help function to build a 'conv-bn-activation' module
    """
    layers = []
    layers.append(nn.Conv2d(inplanes,
                            planes,
                            kernel_size=kernel_size,
                            stride=stride,
                            padding=padding,
                            dilation=dilation,
                            groups=groups,
                            bias=False))
    layers.append(get_norm(norm_type, planes, eps=1e-4, momentum=0.03))
    layers.append(Mish())
    return nn.Sequential(*layers)


class DarkBlock(nn.Module):

    def __init__(self,
                 inplanes,
                 planes,
                 dilation=1,
                 downsample=None,
                 norm_type="BN"):
        """Residual Block for DarkNet.

        This module has the dowsample layer (optional),
        1x1 conv layer and 3x3 conv layer.
        """
        super(DarkBlock, self).__init__()

        self.downsample = downsample

        self.bn1 = get_norm(norm_type, inplanes, eps=1e-4, momentum=0.03)
        self.bn2 = get_norm(norm_type, planes, eps=1e-4, momentum=0.03)

        self.conv1 = nn.Conv2d(
            planes,
            inplanes,
            kernel_size=1,
            stride=1,
            padding=0,
            bias=False
        )

        self.conv2 = nn.Conv2d(
            inplanes,
            planes,
            kernel_size=3,
            stride=1,
            padding=dilation,
            dilation=dilation,
            bias=False
        )

        self.activation = Mish()

    def forward(self, x):
        if self.downsample is not None:
            x = self.downsample(x)

        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.activation(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.activation(out)

        out += identity

        return out


class CrossStagePartialBlock(nn.Module):
    """CSPNet: A New Backbone that can Enhance Learning Capability of CNN.
    Refer to the paper for more details: https://arxiv.org/abs/1911.11929.
    In this module, the inputs go throuth the base conv layer at the first,
    and then pass the two partial transition layers.
    1. go throuth basic block (like DarkBlock)
        and one partial transition layer.
    2. go throuth the other partial transition layer.
    At last, They are concat into fuse transition layer.

    Args:
        inplanes (int): number of input channels.
        planes (int): number of output channels
        stage_layers (nn.Module): the basic block which applying CSPNet.
        is_csp_first_stage (bool): Is the first stage or not.
            The number of input and output channels in the first stage of
            CSPNet is different from other stages.
        dilation (int): conv dilation
        stride (int): stride for the base layer
        norm_type (str): normalization layer type.
    """

    def __init__(self,
                 inplanes,
                 planes,
                 stage_layers,
                 is_csp_first_stage,
                 dilation=1,
                 stride=2,
                 norm_type="BN"):
        super(CrossStagePartialBlock, self).__init__()

        self.base_layer = ConvNormActivation(
            inplanes,
            planes,
            kernel_size=3,
            stride=stride,
            padding=dilation,
            dilation=dilation,
            norm_type=norm_type
        )
        self.partial_transition1 = ConvNormActivation(
            inplanes=planes,
            planes=inplanes if not is_csp_first_stage else planes,
            kernel_size=1,
            stride=1,
            padding=0,
            norm_type=norm_type
        )
        self.stage_layers = stage_layers

        self.partial_transition2 = ConvNormActivation(
            inplanes=inplanes if not is_csp_first_stage else planes,
            planes=inplanes if not is_csp_first_stage else planes,
            kernel_size=1,
            stride=1,
            padding=0,
            norm_type=norm_type
        )
        self.fuse_transition = ConvNormActivation(
            inplanes=planes if not is_csp_first_stage else planes * 2,
            planes=planes,
            kernel_size=1,
            stride=1,
            padding=0,
            norm_type=norm_type
        )

    def forward(self, x):
        x = self.base_layer(x)

        out1 = self.partial_transition1(x)

        out2 = self.stage_layers(x)
        out2 = self.partial_transition2(out2)

        out = torch.cat([out2, out1], dim=1)
        out = self.fuse_transition(out)

        return out


def make_dark_layer(block,
                    inplanes,
                    planes,
                    num_blocks,
                    dilation=1,
                    stride=2,
                    norm_type="BN"):
    downsample = ConvNormActivation(
        inplanes=inplanes,
        planes=planes,
        kernel_size=3,
        stride=stride,
        padding=dilation,
        dilation=dilation,
        norm_type=norm_type
    )

    layers = []
    for i in range(0, num_blocks):
        layers.append(
            block(
                inplanes=inplanes,
                planes=planes,
                downsample=downsample if i == 0 else None,
                dilation=dilation,
                norm_type=norm_type
            )
        )
    return nn.Sequential(*layers)


def make_cspdark_layer(block,
                       inplanes,
                       planes,
                       num_blocks,
                       is_csp_first_stage,
                       dilation=1,
                       norm_type="BN"):
    downsample = ConvNormActivation(
        inplanes=planes,
        planes=planes if is_csp_first_stage else inplanes,
        kernel_size=1,
        stride=1,
        padding=0,
        norm_type=norm_type
    )

    layers = []
    for i in range(0, num_blocks):
        layers.append(
            block(
                inplanes=inplanes,
                planes=planes if is_csp_first_stage else inplanes,
                downsample=downsample if i == 0 else None,
                dilation=dilation,
                norm_type=norm_type
            )
        )
    return nn.Sequential(*layers)


class DarkNet(Backbone):
    """DarkNet backbone.
    Refer to the paper for more details: https://arxiv.org/pdf/1804.02767

    Args:
        depth (int): Depth of Darknet, from {53}.
        num_stages (int): Darknet stages, normally 5.
        with_csp (bool): Use cross stage partial connection or not.
        out_features (List[str]): Output features.
        norm_type (str): type of normalization layer.
        res5_dilation (int): dilation for the last stage
    """

    arch_settings = {
        53: (DarkBlock, (1, 2, 8, 8, 4))
    }

    def __init__(self,
                 depth,
                 with_csp=False,
                 out_features=["res5"],
                 norm_type="BN",
                 res5_dilation=1):
        super(DarkNet, self).__init__()
        if depth not in self.arch_settings:
            raise KeyError('invalid depth {} for resnet'.format(depth))
        self.with_csp = with_csp
        self._out_features = out_features
        self.norm_type = norm_type
        self.res5_dilation = res5_dilation

        self.block, self.stage_blocks = self.arch_settings[depth]
        self.inplanes = 32

        self._make_stem_layer()

        self.dark_layers = []
        for i, num_blocks in enumerate(self.stage_blocks):
            planes = 64 * 2 ** i
            dilation = 1
            stride = 2
            if i == 4 and self.res5_dilation == 2:
                dilation = self.res5_dilation
                stride = 1
            if not self.with_csp:
                layer = make_dark_layer(
                    block=self.block,
                    inplanes=self.inplanes,
                    planes=planes,
                    num_blocks=num_blocks,
                    dilation=dilation,
                    stride=stride,
                    norm_type=self.norm_type
                )
            else:
                layer = make_cspdark_layer(
                    block=self.block,
                    inplanes=self.inplanes,
                    planes=planes,
                    num_blocks=num_blocks,
                    is_csp_first_stage=True if i == 0 else False,
                    dilation=dilation,
                    norm_type=self.norm_type
                )
                layer = CrossStagePartialBlock(
                    self.inplanes,
                    planes,
                    stage_layers=layer,
                    is_csp_first_stage=True if i == 0 else False,
                    dilation=dilation,
                    stride=stride,
                    norm_type=self.norm_type
                )
            self.inplanes = planes
            layer_name = 'layer{}'.format(i + 1)
            self.add_module(layer_name, layer)
            self.dark_layers.append(layer_name)

        # freeze stage<=2
        for p in self.conv1.parameters():
            p.requires_grad = False
        for p in self.bn1.parameters():
            p.requires_grad = False
        for p in self.layer1.parameters():
            p.requires_grad = False
        for p in self.layer2.parameters():
            p.requires_grad = False

    def _make_stem_layer(self):
        self.conv1 = nn.Conv2d(
            3,
            self.inplanes,
            kernel_size=3,
            stride=1,
            padding=1,
            bias=False
        )
        self.bn1 = get_norm(
            self.norm_type, self.inplanes, eps=1e-4, momentum=0.03
        )
        self.act1 = Mish()

    def forward(self, x):
        outputs = {}
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.act1(x)

        for i, layer_name in enumerate(self.dark_layers):
            layer = getattr(self, layer_name)
            x = layer(x)
        outputs[self._out_features[-1]] = x
        return outputs

    def output_shape(self):
        return {
            "res5": ShapeSpec(
                channels=1024, stride=16 if self.res5_dilation == 2 else 32
            )
        }


def build_darknet_backbone(cfg, input_shape=None):
    """
    Create a DarkNet/CSPDarkNet instance from config.

    Returns:
        DarkNet: a :class:`DarkNet` instance.
    """
    depth = cfg.MODEL.DARKNET.DEPTH
    with_csp = cfg.MODEL.DARKNET.WITH_CSP
    out_features = cfg.MODEL.DARKNET.OUT_FEATURES
    norm_type = cfg.MODEL.DARKNET.NORM
    res5_dilation = cfg.MODEL.DARKNET.RES5_DILATION
    return DarkNet(
        depth=depth,
        with_csp=with_csp,
        out_features=out_features,
        norm_type=norm_type,
        res5_dilation=res5_dilation
    )
```

###### playground/detection/coco/yolof/yolof_base/decoder.py

```python
import math
from typing import Tuple

import torch
import torch.nn as nn

from .utils import get_activation, get_norm


class Decoder(nn.Module):
    """
    Head Decoder for YOLOF.

    This module contains two types of components:
        - A classification head with two 3x3 convolutions and one
            classification 3x3 convolution
        - A regression head with four 3x3 convolutions, one regression 3x3
          convolution, and one implicit objectness 3x3 convolution
    """

    def __init__(self, cfg):
        super(Decoder, self).__init__()
        # fmt: off
        self.in_channels = cfg.MODEL.YOLOF.DECODER.IN_CHANNELS
        self.num_classes = cfg.MODEL.YOLOF.DECODER.NUM_CLASSES
        self.num_anchors = cfg.MODEL.YOLOF.DECODER.NUM_ANCHORS
        self.cls_num_convs = cfg.MODEL.YOLOF.DECODER.CLS_NUM_CONVS
        self.reg_num_convs = cfg.MODEL.YOLOF.DECODER.REG_NUM_CONVS
        self.norm_type = cfg.MODEL.YOLOF.DECODER.NORM
        self.act_type = cfg.MODEL.YOLOF.DECODER.ACTIVATION
        self.prior_prob = cfg.MODEL.YOLOF.DECODER.PRIOR_PROB
        # fmt: on

        self.INF = 1e8
        # init
        self._init_layers()
        self._init_weight()

    def _init_layers(self):
        cls_subnet = []
        bbox_subnet = []
        for i in range(self.cls_num_convs):
            cls_subnet.append(
                nn.Conv2d(self.in_channels,
                          self.in_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1))
            cls_subnet.append(get_norm(self.norm_type, self.in_channels))
            cls_subnet.append(get_activation(self.act_type))
        for i in range(self.reg_num_convs):
            bbox_subnet.append(
                nn.Conv2d(self.in_channels,
                          self.in_channels,
                          kernel_size=3,
                          stride=1,
                          padding=1))
            bbox_subnet.append(get_norm(self.norm_type, self.in_channels))
            bbox_subnet.append(get_activation(self.act_type))
        self.cls_subnet = nn.Sequential(*cls_subnet)
        self.bbox_subnet = nn.Sequential(*bbox_subnet)
        self.cls_score = nn.Conv2d(self.in_channels,
                                   self.num_anchors * self.num_classes,
                                   kernel_size=3,
                                   stride=1,
                                   padding=1)
        self.bbox_pred = nn.Conv2d(self.in_channels,
                                   self.num_anchors * 4,
                                   kernel_size=3,
                                   stride=1,
                                   padding=1)
        self.object_pred = nn.Conv2d(self.in_channels,
                                     self.num_anchors,
                                     kernel_size=3,
                                     stride=1,
                                     padding=1)

    def _init_weight(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.normal_(m.weight, mean=0, std=0.01)
                if hasattr(m, 'bias') and m.bias is not None:
                    nn.init.constant_(m.bias, 0)

            if isinstance(m, (nn.GroupNorm, nn.BatchNorm2d, nn.SyncBatchNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # Use prior in model initialization to improve stability
        bias_value = -math.log((1 - self.prior_prob) / self.prior_prob)
        torch.nn.init.constant_(self.cls_score.bias, bias_value)

    def forward(self,
                feature: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        cls_score = self.cls_score(self.cls_subnet(feature))
        N, _, H, W = cls_score.shape
        cls_score = cls_score.view(N, -1, self.num_classes, H, W)

        reg_feat = self.bbox_subnet(feature)
        bbox_reg = self.bbox_pred(reg_feat)
        objectness = self.object_pred(reg_feat)

        # implicit objectness
        objectness = objectness.view(N, -1, 1, H, W)
        normalized_cls_score = cls_score + objectness - torch.log(
            1. + torch.clamp(cls_score.exp(), max=self.INF) + torch.clamp(
                objectness.exp(), max=self.INF))
        normalized_cls_score = normalized_cls_score.view(N, -1, H, W)
        return normalized_cls_score, bbox_reg


def build_decoder(cfg):
    return Decoder(cfg)
```

###### playground/detection/coco/yolof/yolof_base/box_ops.py

```python
import torch
from torchvision.ops.boxes import box_area


def box_cxcywh_to_xyxy(x):
    x_c, y_c, w, h = x.unbind(-1)
    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),
         (x_c + 0.5 * w), (y_c + 0.5 * h)]
    return torch.stack(b, dim=-1)


def box_xyxy_to_cxcywh(x):
    x0, y0, x1, y1 = x.unbind(-1)
    b = [(x0 + x1) / 2, (y0 + y1) / 2,
         (x1 - x0), (y1 - y0)]
    return torch.stack(b, dim=-1)


# modified from torchvision to also return the union
def box_iou(boxes1, boxes2):
    area1 = box_area(boxes1)
    area2 = box_area(boxes2)

    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]
    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]

    wh = (rb - lt).clamp(min=0)  # [N,M,2]
    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]

    union = area1[:, None] + area2 - inter

    iou = inter / union
    return iou, union


def generalized_box_iou(boxes1, boxes2):
    """
    Generalized IoU from https://giou.stanford.edu/

    The boxes should be in [x0, y0, x1, y1] format

    Returns a [N, M] pairwise matrix, where N = len(boxes1)
    and M = len(boxes2)
    """
    # degenerate boxes gives inf / nan results
    # so do an early check
    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()
    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()
    iou, union = box_iou(boxes1, boxes2)

    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])
    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])

    wh = (rb - lt).clamp(min=0)  # [N,M,2]
    area = wh[:, :, 0] * wh[:, :, 1]

    return iou - (area - union) / area
```

###### playground/detection/coco/yolof/yolof_base/uniform_matcher.py

```python
import numpy as np
import torch
from torch import nn


def box_xyxy_to_cxcywh(x):
    x0, y0, x1, y1 = x.unbind(-1)
    b = [(x0 + x1) / 2, (y0 + y1) / 2,
         (x1 - x0), (y1 - y0)]
    return torch.stack(b, dim=-1)


class UniformMatcher(nn.Module):
    """
    Uniform Matching between the anchors and gt boxes, which can achieve
    balance in positive anchors.

    Args:
        match_times(int): Number of positive anchors for each gt box.
    """

    def __init__(self, match_times: int = 4):
        super().__init__()
        self.match_times = match_times

    @torch.no_grad()
    def forward(self, pred_boxes, anchors, targets):
        bs, num_queries = pred_boxes.shape[:2]

        # We flatten to compute the cost matrices in a batch
        # [batch_size * num_anchors, 4]
        out_bbox = pred_boxes.flatten(0, 1)
        anchors = anchors.flatten(0, 1)

        # Also concat the target boxes
        tgt_bbox = torch.cat([v.gt_boxes.tensor for v in targets])

        # Compute the L1 cost between boxes
        # Note that we use anchors and predict boxes both
        cost_bbox = torch.cdist(
            box_xyxy_to_cxcywh(out_bbox), box_xyxy_to_cxcywh(tgt_bbox), p=1)
        cost_bbox_anchors = torch.cdist(
            box_xyxy_to_cxcywh(anchors), box_xyxy_to_cxcywh(tgt_bbox), p=1)

        # Final cost matrix
        C = cost_bbox
        C = C.view(bs, num_queries, -1).cpu()
        C1 = cost_bbox_anchors
        C1 = C1.view(bs, num_queries, -1).cpu()

        sizes = [len(v.gt_boxes.tensor) for v in targets]
        all_indices_list = [[] for _ in range(bs)]
        # positive indices when matching predict boxes and gt boxes
        indices = [
            tuple(
                torch.topk(
                    c[i],
                    k=self.match_times,
                    dim=0,
                    largest=False)[1].numpy().tolist()
            )
            for i, c in enumerate(C.split(sizes, -1))
        ]
        # positive indices when matching anchor boxes and gt boxes
        indices1 = [
            tuple(
                torch.topk(
                    c[i],
                    k=self.match_times,
                    dim=0,
                    largest=False)[1].numpy().tolist())
            for i, c in enumerate(C1.split(sizes, -1))]

        # concat the indices according to image ids
        for img_id, (idx, idx1) in enumerate(zip(indices, indices1)):
            img_idx_i = [
                np.array(idx_ + idx1_)
                for (idx_, idx1_) in zip(idx, idx1)
            ]
            img_idx_j = [
                np.array(list(range(len(idx_))) + list(range(len(idx1_))))
                for (idx_, idx1_) in zip(idx, idx1)
            ]
            all_indices_list[img_id] = [*zip(img_idx_i, img_idx_j)]

        # re-organize the positive indices
        all_indices = []
        for img_id in range(bs):
            all_idx_i = []
            all_idx_j = []
            for idx_list in all_indices_list[img_id]:
                idx_i, idx_j = idx_list
                all_idx_i.append(idx_i)
                all_idx_j.append(idx_j)
            all_idx_i = np.hstack(all_idx_i)
            all_idx_j = np.hstack(all_idx_j)
            all_indices.append((all_idx_i, all_idx_j))
        return [
            (torch.as_tensor(i, dtype=torch.int64),
             torch.as_tensor(j, dtype=torch.int64))
            for i, j in all_indices
        ]
```

###### playground/detection/coco/yolof/yolof_base/__init__.py

```python
from .encoder import build_encoder
from .decoder import build_decoder
from .yolof import YOLOF

__all__ = [
    "build_encoder", "build_decoder", "YOLOF"
]
```

###### playground/detection/coco/yolof/yolof_base/encoder.py

```python
from typing import List

import torch
import torch.nn as nn

from cvpods.layers import ShapeSpec
from cvpods.modeling.nn_utils import weight_init

from .utils import get_activation, get_norm


class DilatedEncoder(nn.Module):
    """
    Dilated Encoder for YOLOF.

    This module contains two types of components:
        - the original FPN lateral convolution layer and fpn convolution layer,
          which are 1x1 conv + 3x3 conv
        - the dilated residual block
    """

    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super(DilatedEncoder, self).__init__()
        # fmt: off
        self.backbone_level = cfg.MODEL.YOLOF.ENCODER.IN_FEATURES
        self.in_channels = input_shape[self.backbone_level[0]].channels
        self.encoder_channels = cfg.MODEL.YOLOF.ENCODER.NUM_CHANNELS
        self.block_mid_channels = cfg.MODEL.YOLOF.ENCODER.BLOCK_MID_CHANNELS
        self.num_residual_blocks = cfg.MODEL.YOLOF.ENCODER.NUM_RESIDUAL_BLOCKS
        self.block_dilations = cfg.MODEL.YOLOF.ENCODER.BLOCK_DILATIONS
        self.norm_type = cfg.MODEL.YOLOF.ENCODER.NORM
        self.act_type = cfg.MODEL.YOLOF.ENCODER.ACTIVATION
        # fmt: on
        assert len(self.block_dilations) == self.num_residual_blocks

        # init
        self._init_layers()
        self._init_weight()

    def _init_layers(self):
        self.lateral_conv = nn.Conv2d(self.in_channels,
                                      self.encoder_channels,
                                      kernel_size=1)
        self.lateral_norm = get_norm(self.norm_type, self.encoder_channels)
        self.fpn_conv = nn.Conv2d(self.encoder_channels,
                                  self.encoder_channels,
                                  kernel_size=3,
                                  padding=1)
        self.fpn_norm = get_norm(self.norm_type, self.encoder_channels)
        encoder_blocks = []
        for i in range(self.num_residual_blocks):
            dilation = self.block_dilations[i]
            encoder_blocks.append(
                Bottleneck(
                    self.encoder_channels,
                    self.block_mid_channels,
                    dilation=dilation,
                    norm_type=self.norm_type,
                    act_type=self.act_type
                )
            )
        self.dilated_encoder_blocks = nn.Sequential(*encoder_blocks)

    def _init_weight(self):
        weight_init.c2_xavier_fill(self.lateral_conv)
        weight_init.c2_xavier_fill(self.fpn_conv)
        for m in [self.lateral_norm, self.fpn_norm]:
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
        for m in self.dilated_encoder_blocks.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.normal_(m.weight, mean=0, std=0.01)
                if hasattr(m, 'bias') and m.bias is not None:
                    nn.init.constant_(m.bias, 0)

            if isinstance(m, (nn.GroupNorm, nn.BatchNorm2d, nn.SyncBatchNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, feature: torch.Tensor) -> torch.Tensor:
        out = self.lateral_norm(self.lateral_conv(feature))
        out = self.fpn_norm(self.fpn_conv(out))
        return self.dilated_encoder_blocks(out)


class Bottleneck(nn.Module):

    def __init__(self,
                 in_channels: int = 512,
                 mid_channels: int = 128,
                 dilation: int = 1,
                 norm_type: str = 'BN',
                 act_type: str = 'ReLU'):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, mid_channels, kernel_size=1, padding=0),
            get_norm(norm_type, mid_channels),
            get_activation(act_type)
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(mid_channels, mid_channels,
                      kernel_size=3, padding=dilation, dilation=dilation),
            get_norm(norm_type, mid_channels),
            get_activation(act_type)
        )
        self.conv3 = nn.Sequential(
            nn.Conv2d(mid_channels, in_channels, kernel_size=1, padding=0),
            get_norm(norm_type, in_channels),
            get_activation(act_type)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        identity = x
        out = self.conv1(x)
        out = self.conv2(out)
        out = self.conv3(out)
        out = out + identity
        return out


def build_encoder(cfg, input_shape: ShapeSpec):
    return DilatedEncoder(cfg, input_shape=input_shape)
```

###### playground/detection/coco/yolof/yolof_base/yolof.py

```python
import logging
from typing import List

import torch
from torch import nn
import torch.distributed as dist

from cvpods.layers import ShapeSpec, cat, generalized_batched_nms
from cvpods.modeling.basenet import basenet
from cvpods.modeling.box_regression import Box2BoxTransform
from cvpods.modeling.losses import sigmoid_focal_loss_jit
from cvpods.modeling.postprocessing import detector_postprocess
from cvpods.structures import Boxes, ImageList, Instances
from cvpods.utils import log_first_n

from .box_ops import box_iou, generalized_box_iou
from .uniform_matcher import UniformMatcher


def permute_to_N_HWA_K(tensor, K):
    """
    Transpose/reshape a tensor from (N, (A x K), H, W) to (N, (HxWxA), K)
    """
    assert tensor.dim() == 4, tensor.shape
    N, _, H, W = tensor.shape
    tensor = tensor.view(N, -1, K, H, W)
    tensor = tensor.permute(0, 3, 4, 1, 2)
    tensor = tensor.reshape(N, -1, K)  # Size=(N,HWA,K)
    return tensor


@basenet
class YOLOF(nn.Module):
    """
    Implementation of YOLOF.
    """
    def __init__(self, cfg):
        super().__init__()

        self.device = torch.device(cfg.MODEL.DEVICE)

        # fmt: off
        self.num_classes = cfg.MODEL.YOLOF.DECODER.NUM_CLASSES
        self.in_features = cfg.MODEL.YOLOF.ENCODER.IN_FEATURES
        self.pos_ignore_thresh = cfg.MODEL.YOLOF.POS_IGNORE_THRESHOLD
        self.neg_ignore_thresh = cfg.MODEL.YOLOF.NEG_IGNORE_THRESHOLD
        # Loss parameters:
        self.focal_loss_alpha = cfg.MODEL.YOLOF.FOCAL_LOSS_ALPHA
        self.focal_loss_gamma = cfg.MODEL.YOLOF.FOCAL_LOSS_GAMMA
        # Inference parameters:
        self.score_threshold = cfg.MODEL.YOLOF.SCORE_THRESH_TEST
        self.topk_candidates = cfg.MODEL.YOLOF.TOPK_CANDIDATES_TEST
        self.nms_threshold = cfg.MODEL.YOLOF.NMS_THRESH_TEST
        self.nms_type = cfg.MODEL.NMS_TYPE
        self.max_detections_per_image = cfg.TEST.DETECTIONS_PER_IMAGE
        # fmt: on

        self.backbone = cfg.build_backbone(
            cfg, input_shape=ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN)))

        backbone_shape = self.backbone.output_shape()
        feature_shapes = [backbone_shape[f] for f in self.in_features]
        self.encoder = cfg.build_encoder(
            cfg, backbone_shape
        )
        self.decoder = cfg.build_decoder(cfg)
        self.anchor_generator = cfg.build_anchor_generator(cfg, feature_shapes)

        # Matching and loss
        self.box2box_transform = Box2BoxTransform(
            weights=cfg.MODEL.YOLOF.BBOX_REG_WEIGHTS,
            add_ctr_clamp=cfg.MODEL.YOLOF.ADD_CTR_CLAMP,
            ctr_clamp=cfg.MODEL.YOLOF.CTR_CLAMP
        )
        self.matcher = UniformMatcher(cfg.MODEL.YOLOF.MATCHER_TOPK)

        self.register_buffer(
            "pixel_mean",
            torch.Tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(3, 1, 1)
        )
        self.register_buffer(
            "pixel_std",
            torch.Tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(3, 1, 1)
        )
        self.to(self.device)

    def forward(self, batched_inputs):
        """
        Args:
            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
                Each item in the list contains the inputs for one image.
                For now, each item in the list is a dict that contains:

                * image: Tensor, image in (C, H, W) format.
                * instances: Instances

                Other information that's included in the original dicts, such as:

                * "height", "width" (int): the output resolution of the model, used in inference.
                  See :meth:`postprocess` for details.
        Returns:
            dict[str: Tensor]:
                mapping from a named loss to a tensor storing the loss. Used during training only.
        """
        images = self.preprocess_image(batched_inputs)
        if "instances" in batched_inputs[0]:
            gt_instances = [
                x["instances"].to(self.device) for x in batched_inputs
            ]
        elif "targets" in batched_inputs[0]:
            log_first_n(
                logging.WARN,
                "'targets' in the model inputs is now renamed to 'instances'!",
                n=10)
            gt_instances = [
                x["targets"].to(self.device) for x in batched_inputs
            ]
        else:
            gt_instances = None

        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]
        box_cls, box_delta = self.decoder(self.encoder(features[0]))
        anchors = self.anchor_generator(features)
        # Transpose the Hi*Wi*A dimension to the middle:
        pred_logits = [permute_to_N_HWA_K(box_cls, self.num_classes)]
        pred_anchor_deltas = [permute_to_N_HWA_K(box_delta, 4)]

        if self.training:
            indices = self.get_ground_truth(
                anchors, pred_anchor_deltas, gt_instances)
            losses = self.losses(
                indices, gt_instances, anchors,
                pred_logits, pred_anchor_deltas)
            return losses
        else:
            results = self.inference(
                [box_cls], [box_delta], anchors, images.image_sizes)
            processed_results = []
            for results_per_image, input_per_image, image_size in zip(
                    results, batched_inputs, images.image_sizes):
                height = input_per_image.get("height", image_size[0])
                width = input_per_image.get("width", image_size[1])
                r = detector_postprocess(results_per_image, height, width)
                processed_results.append({"instances": r})
            return processed_results

    def losses(self,
               indices,
               gt_instances,
               anchors,
               pred_class_logits,
               pred_anchor_deltas):
        pred_class_logits = cat(
            pred_class_logits, dim=1).view(-1, self.num_classes)
        pred_anchor_deltas = cat(pred_anchor_deltas, dim=1).view(-1, 4)

        anchors = [Boxes.cat(anchors_i) for anchors_i in anchors]
        N = len(anchors)
        # list[Tensor(R, 4)], one for each image
        all_anchors = Boxes.cat(anchors).tensor
        # Boxes(Tensor(N*R, 4))
        predicted_boxes = self.box2box_transform.apply_deltas(
            pred_anchor_deltas, all_anchors)
        predicted_boxes = predicted_boxes.reshape(N, -1, 4)

        ious = []
        pos_ious = []
        for i in range(N):
            src_idx, tgt_idx = indices[i]
            iou, _ = box_iou(predicted_boxes[i, ...],
                          gt_instances[i].gt_boxes.tensor)
            if iou.numel() == 0:
                max_iou = iou.new_full((iou.size(0),), 0)
            else:
                max_iou = iou.max(dim=1)[0]
            a_iou, _ = box_iou(anchors[i].tensor,
                            gt_instances[i].gt_boxes.tensor)
            if a_iou.numel() == 0:
                pos_iou = a_iou.new_full((0,), 0)
            else:
                pos_iou = a_iou[src_idx, tgt_idx]
            ious.append(max_iou)
            pos_ious.append(pos_iou)
        ious = torch.cat(ious)
        ignore_idx = ious > self.neg_ignore_thresh
        pos_ious = torch.cat(pos_ious)
        pos_ignore_idx = pos_ious < self.pos_ignore_thresh

        src_idx = torch.cat(
            [src + idx * anchors[0].tensor.shape[0] for idx, (src, _) in
             enumerate(indices)])
        gt_classes = torch.full(pred_class_logits.shape[:1],
                                self.num_classes,
                                dtype=torch.int64,
                                device=pred_class_logits.device)
        gt_classes[ignore_idx] = -1
        target_classes_o = torch.cat(
            [t.gt_classes[J] for t, (_, J) in zip(gt_instances, indices)])
        target_classes_o[pos_ignore_idx] = -1
        gt_classes[src_idx] = target_classes_o

        valid_idxs = gt_classes >= 0
        foreground_idxs = (gt_classes >= 0) & (gt_classes != self.num_classes)
        num_foreground = foreground_idxs.sum()

        gt_classes_target = torch.zeros_like(pred_class_logits)
        gt_classes_target[foreground_idxs, gt_classes[foreground_idxs]] = 1

        dist.all_reduce(num_foreground)
        num_foreground = num_foreground * 1.0 / dist.get_world_size()

        # cls loss
        loss_cls = sigmoid_focal_loss_jit(
            pred_class_logits[valid_idxs],
            gt_classes_target[valid_idxs],
            alpha=self.focal_loss_alpha,
            gamma=self.focal_loss_gamma,
            reduction="sum",
        )
        # reg loss
        target_boxes = torch.cat(
            [t.gt_boxes.tensor[i] for t, (_, i) in zip(gt_instances, indices)],
            dim=0)
        target_boxes = target_boxes[~pos_ignore_idx]
        matched_predicted_boxes = predicted_boxes.reshape(-1, 4)[
            src_idx[~pos_ignore_idx]]
        loss_box_reg = (1 - torch.diag(generalized_box_iou(
            matched_predicted_boxes, target_boxes))).sum()

        return {
            "loss_cls": loss_cls / max(1, num_foreground),
            "loss_box_reg": loss_box_reg / max(1, num_foreground),
        }

    @torch.no_grad()
    def get_ground_truth(self, anchors, bbox_preds, targets):
        anchors = [Boxes.cat(anchors_i) for anchors_i in anchors]
        N = len(anchors)
        # list[Tensor(R, 4)], one for each image
        all_anchors = Boxes.cat(anchors).tensor.reshape(N, -1, 4)
        # Boxes(Tensor(N*R, 4))
        box_delta = cat(bbox_preds, dim=1)
        # box_pred: xyxy; targets: xyxy
        box_pred = self.box2box_transform.apply_deltas(box_delta, all_anchors)
        indices = self.matcher(box_pred, all_anchors, targets)
        return indices

    def inference(self, box_cls, box_delta, anchors, image_sizes):
        """
        Arguments:
            box_cls, box_delta: Same as the output of :meth:`YOLOFHead.forward`
            anchors (list[list[Boxes]]): a list of #images elements. Each is a
                list of #feature level Boxes. The Boxes contain anchors of this
                image on the specific feature level.
            image_sizes (List[torch.Size]): the input image sizes

        Returns:
            results (List[Instances]): a list of #images elements.
        """
        assert len(anchors) == len(image_sizes)
        results = []

        box_cls = [permute_to_N_HWA_K(x, self.num_classes) for x in box_cls]
        box_delta = [permute_to_N_HWA_K(x, 4) for x in box_delta]
        # list[Tensor], one per level, each has shape (N, Hi x Wi x A, K or 4)

        for img_idx, anchors_per_image in enumerate(anchors):
            image_size = image_sizes[img_idx]
            box_cls_per_image = [
                box_cls_per_level[img_idx] for box_cls_per_level in box_cls
            ]
            box_reg_per_image = [
                box_reg_per_level[img_idx] for box_reg_per_level in box_delta
            ]
            results_per_image = self.inference_single_image(
                box_cls_per_image, box_reg_per_image, anchors_per_image,
                tuple(image_size))
            results.append(results_per_image)
        return results

    def inference_single_image(self, box_cls, box_delta, anchors, image_size):
        """
        Single-image inference. Return bounding-box detection results by thresholding
        on scores and applying non-maximum suppression (NMS).

        Arguments:
            box_cls (list[Tensor]): list of #feature levels. Each entry contains
                tensor of size (H x W x A, K)
            box_delta (list[Tensor]): Same shape as 'box_cls' except that K becomes 4.
            anchors (list[Boxes]): list of #feature levels. Each entry contains
                a Boxes object, which contains all the anchors for that
                image in that feature level.
            image_size (tuple(H, W)): a tuple of the image height and width.

        Returns:
            Same as `inference`, but for only one image.
        """
        boxes_all = []
        scores_all = []
        class_idxs_all = []

        # Iterate over every feature level
        for box_cls_i, box_reg_i, anchors_i in zip(box_cls, box_delta,
                                                   anchors):
            # (HxWxAxK,)
            box_cls_i = box_cls_i.flatten().sigmoid_()

            # Keep top k top scoring indices only.
            num_topk = min(self.topk_candidates, box_reg_i.size(0))
            # torch.sort is actually faster than .topk (at least on GPUs)
            predicted_prob, topk_idxs = box_cls_i.sort(descending=True)
            predicted_prob = predicted_prob[:num_topk]
            topk_idxs = topk_idxs[:num_topk]

            # filter out the proposals with low confidence score
            keep_idxs = predicted_prob > self.score_threshold
            predicted_prob = predicted_prob[keep_idxs]
            topk_idxs = topk_idxs[keep_idxs]

            anchor_idxs = topk_idxs // self.num_classes
            classes_idxs = topk_idxs % self.num_classes

            box_reg_i = box_reg_i[anchor_idxs]
            anchors_i = anchors_i[anchor_idxs]
            # predict boxes
            predicted_boxes = self.box2box_transform.apply_deltas(
                box_reg_i, anchors_i.tensor)

            boxes_all.append(predicted_boxes)
            scores_all.append(predicted_prob)
            class_idxs_all.append(classes_idxs)

        boxes_all, scores_all, class_idxs_all = [
            cat(x) for x in [boxes_all, scores_all, class_idxs_all]
        ]

        keep = generalized_batched_nms(boxes_all, scores_all, class_idxs_all,
                                       self.nms_threshold, nms_type=self.nms_type)
        keep = keep[:self.max_detections_per_image]

        result = Instances(image_size)
        result.pred_boxes = Boxes(boxes_all[keep])
        result.scores = scores_all[keep]
        result.pred_classes = class_idxs_all[keep]
        return result

    def preprocess_image(self, batched_inputs):
        """
        Normalize, pad and batch the input images.
        """
        images = [x["image"].to(self.device) for x in batched_inputs]
        images = [(x - self.pixel_mean) / self.pixel_std for x in images]
        images = ImageList.from_tensors(images,
                                        self.backbone.size_divisibility)
        return images

    def _inference_for_ms_test(self, batched_inputs):
        """
        function used for multiscale test, will be refactor in the future.
        The same input with `forward` function.
        """
        assert not self.training, "inference mode with training=True"
        assert len(batched_inputs) == 1, "inference image number > 1"
        images = self.preprocess_image(batched_inputs)

        features = self.backbone(images.tensor)
        features = [features[f] for f in self.in_features]
        box_cls, box_delta = self.head(features)
        anchors = self.anchor_generator(features)

        results = self.inference(box_cls, box_delta, anchors, images.image_sizes)
        for results_per_image, input_per_image, image_size in zip(
                results, batched_inputs, images.image_sizes
        ):
            height = input_per_image.get("height", image_size[0])
            width = input_per_image.get("width", image_size[1])
            processed_results = detector_postprocess(results_per_image, height, width)
        return processed_results
```

###### playground/detection/coco/yolof/yolof_base/utils.py

```python
from functools import partial

import torch.nn as nn

from cvpods.layers import BatchNorm2d, NaiveSyncBatchNorm, FrozenBatchNorm2d
from cvpods.utils import env


def get_norm(norm, out_channels, **kwargs):
    """
    Args:
        norm (str or callable): either one of BN, SyncBN, FrozenBN, GN;
            or a callable that takes a channel number and returns
            the normalization layer as a nn.Module.
        kwargs: Additional parameters in normalization layers,
            such as, eps, momentum

    Returns:
        nn.Module or None: the normalization layer
    """
    if norm is None:
        return None
    if isinstance(norm, str):
        if len(norm) == 0:
            return None
        norm = {
            "BN": BatchNorm2d,
            # Fixed in https://github.com/pytorch/pytorch/pull/36382
            "SyncBN": NaiveSyncBatchNorm if env.TORCH_VERSION <= (
                1, 5) else nn.SyncBatchNorm,
            "FrozenBN": FrozenBatchNorm2d,
            "GN": lambda channels: nn.GroupNorm(32, channels),
            # for debugging:
            "nnSyncBN": nn.SyncBatchNorm,
            "naiveSyncBN": NaiveSyncBatchNorm,
        }[norm]
    return norm(out_channels, **kwargs)


def get_activation(activation):
    """
    Only support `ReLU` and `LeakyReLU` now.

    Args:
        activation (str or callable):

    Returns:
        nn.Module: the activation layer
    """

    act = {
        "ReLU": nn.ReLU,
        "LeakyReLU": nn.LeakyReLU,
    }[activation]
    if activation == "LeakyReLU":
        act = partial(act, negative_slope=0.1)
    return act(inplace=True)
```

###### playground/detection/coco/yolof/yolof.res101.DC5.1x/config.py

```python
from cvpods.configs.base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        # Backbone NAME: "build_resnet_backbone"
        WEIGHTS="detectron2://ImageNetPretrained/MSRA/R-101.pkl",
        RESNETS=dict(DEPTH=101, OUT_FEATURES=["res5"], RES5_DILATION=2),
        ANCHOR_GENERATOR=dict(
            SIZES=[[16, 32, 64, 128, 256, 512]],
            ASPECT_RATIOS=[[1.0]]
        ),
        YOLOF=dict(
            ENCODER=dict(
                IN_FEATURES=["res5"],
                NUM_CHANNELS=512,
                BLOCK_MID_CHANNELS=128,
                NUM_RESIDUAL_BLOCKS=4,
                BLOCK_DILATIONS=[4, 8, 12, 16],
                NORM="BN",
                ACTIVATION="ReLU"
            ),
            DECODER=dict(
                IN_CHANNELS=512,
                NUM_CLASSES=80,
                NUM_ANCHORS=6,
                CLS_NUM_CONVS=2,
                REG_NUM_CONVS=4,
                NORM="BN",
                ACTIVATION="ReLU",
                PRIOR_PROB=0.01
            ),
            BBOX_REG_WEIGHTS=(1.0, 1.0, 1.0, 1.0),
            ADD_CTR_CLAMP=True,
            CTR_CLAMP=32,
            MATCHER_TOPK=8,
            POS_IGNORE_THRESHOLD=0.1,
            NEG_IGNORE_THRESHOLD=0.7,
            FOCAL_LOSS_GAMMA=2.0,
            FOCAL_LOSS_ALPHA=0.25,
            SCORE_THRESH_TEST=0.05,
            TOPK_CANDIDATES_TEST=1000,
            NMS_THRESH_TEST=0.6
        ),
    ),
    DATASETS=dict(
        TRAIN=("coco_2017_train",),
        TEST=("coco_2017_val",),
    ),
    DATALOADER=dict(NUM_WORKERS=8),
    SOLVER=dict(
        LR_SCHEDULER=dict(
            STEPS=(30000, 40000),
            MAX_ITER=45000,
            WARMUP_FACTOR=0.000334,
            WARMUP_ITERS=3000
        ),
        OPTIMIZER=dict(
            NAME="D2SGD",
            BASE_LR=0.06,
            BIAS_LR_FACTOR=1.0,
            WEIGHT_DECAY=0.0001,
            WEIGHT_DECAY_NORM=0.0,
            MOMENTUM=0.9,
            BACKBONE_LR_FACTOR=0.334
        ),
        IMS_PER_BATCH=32,
        IMS_PER_DEVICE=4,
    ),
    INPUT=dict(
        AUG=dict(
            TRAIN_PIPELINES=[
                ("ResizeShortestEdge", dict(
                    short_edge_length=(800,), max_size=1333,
                    sample_style="choice")),
                ("RandomFlip", dict()),
                ("RandomShift", dict(max_shifts=32))
            ],
            TEST_PIPELINES=[
                ("ResizeShortestEdge", dict(
                    short_edge_length=800, max_size=1333,
                    sample_style="choice")),
            ],
        ),
        # Whether the model needs RGB, YUV, HSV etc.
        FORMAT="BGR",
    ),
)


class YOLOFConfig(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = YOLOFConfig()
```

###### playground/detection/coco/yolof/yolof.res101.DC5.1x/net.py

```python
import logging
import sys

sys.path.append("..")

from cvpods.layers import ShapeSpec
from cvpods.modeling.backbone import Backbone
from cvpods.modeling.backbone import build_resnet_backbone
from cvpods.modeling.anchor_generator import DefaultAnchorGenerator

from yolof_base import build_encoder, build_decoder, YOLOF


def build_backbone(cfg, input_shape=None):
    """
    Build a backbone from `cfg.MODEL.BACKBONE.NAME`.
    Returns:
        an instance of :class:`Backbone`
    """
    if input_shape is None:
        input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))

    backbone = build_resnet_backbone(cfg, input_shape)
    assert isinstance(backbone, Backbone)
    return backbone


def build_anchor_generator(cfg, input_shape):
    return DefaultAnchorGenerator(cfg, input_shape)


def build_model(cfg):
    cfg.build_backbone = build_backbone
    cfg.build_anchor_generator = build_anchor_generator
    cfg.build_encoder = build_encoder
    cfg.build_decoder = build_decoder
    model = YOLOF(cfg)

    logger = logging.getLogger(__name__)
    logger.info("Model:\n{}".format(model))
    return model
```

###### playground/detection/coco/yolof/yolof.cspdarknet53.DC5.3x/config.py

```python
from cvpods.configs.base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        # Backbone NAME: "build_darknet_backbone"
        WEIGHTS="../../../../../pretrained_models/cspdarknet53.pth",
        DARKNET=dict(
            DEPTH=53,
            WITH_CSP=True,
            NORM="SyncBN",
            OUT_FEATURES=["res5"],
            RES5_DILATION=2
        ),
        ANCHOR_GENERATOR=dict(
            SIZES=[[16, 32, 64, 128, 256, 512]],
            ASPECT_RATIOS=[[1.0]]
        ),
        YOLOF=dict(
            ENCODER=dict(
                IN_FEATURES=["res5"],
                NUM_CHANNELS=512,
                BLOCK_MID_CHANNELS=128,
                NUM_RESIDUAL_BLOCKS=8,
                BLOCK_DILATIONS=[1, 2, 3, 4, 5, 6, 7, 8],
                NORM="SyncBN",
                ACTIVATION="LeakyReLU"
            ),
            DECODER=dict(
                IN_CHANNELS=512,
                NUM_CLASSES=80,
                NUM_ANCHORS=6,
                CLS_NUM_CONVS=2,
                REG_NUM_CONVS=4,
                NORM="SyncBN",
                ACTIVATION="LeakyReLU",
                PRIOR_PROB=0.01
            ),
            BBOX_REG_WEIGHTS=(1.0, 1.0, 1.0, 1.0),
            ADD_CTR_CLAMP=True,
            CTR_CLAMP=32,
            MATCHER_TOPK=8,
            POS_IGNORE_THRESHOLD=0.1,
            NEG_IGNORE_THRESHOLD=0.7,
            FOCAL_LOSS_GAMMA=2.0,
            FOCAL_LOSS_ALPHA=0.25,
            SCORE_THRESH_TEST=0.05,
            TOPK_CANDIDATES_TEST=1000,
            NMS_THRESH_TEST=0.6
        ),
    ),
    DATASETS=dict(
        TRAIN=("coco_2017_train",),
        TEST=("coco_2017_val",),
    ),
    DATALOADER=dict(NUM_WORKERS=8),
    SOLVER=dict(
        LR_SCHEDULER=dict(
            STEPS=(52500, 62500),
            MAX_ITER=67500,
            WARMUP_FACTOR=0.00066667,
            WARMUP_ITERS=1500
        ),
        OPTIMIZER=dict(
            NAME="D2SGD",
            BASE_LR=0.04,
            BIAS_LR_FACTOR=1.0,
            WEIGHT_DECAY=0.0001,
            WEIGHT_DECAY_NORM=0.0,
            MOMENTUM=0.9,
            BACKBONE_LR_FACTOR=1.0
        ),
        IMS_PER_BATCH=64,
        IMS_PER_DEVICE=8,
    ),
    INPUT=dict(
        AUG=dict(
            TRAIN_PIPELINES=[
                ("JitterCrop", dict(jitter_ratio=0.3)),
                ("Resize", dict(shape=(640, 640), scale_jitter=(0.8, 1.2))),
                ("RandomDistortion2",
                 dict(hue=0.1, saturation=1.5, exposure=1.5)),
                ("RandomFlip", dict()),
                ("RandomShift", dict(max_shifts=32))
            ],
            TEST_PIPELINES=[
                ("Resize", dict(shape=(608, 608))),
            ],
        ),
        MOSAIC=dict(
            MIN_OFFSET=0.2,
            MOSAIC_WIDTH=640,
            MOSAIC_HEIGHT=640
        ),
        # Whether the model needs RGB, YUV, HSV etc.
        FORMAT="BGR",
    ),
)


class YOLOFConfig(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = YOLOFConfig()
```

###### playground/detection/coco/yolof/yolof.cspdarknet53.DC5.3x/net.py

```python
import logging
import sys

from cvpods.layers import ShapeSpec
from cvpods.modeling.backbone import Backbone
from cvpods.modeling.anchor_generator import DefaultAnchorGenerator

from .cspdarknet import build_darknet_backbone
sys.path.append("..")
from yolof_base import build_encoder, build_decoder, YOLOF


def build_backbone(cfg, input_shape=None):
    """
    Build a backbone from `cfg.MODEL.BACKBONE.NAME`.
    Returns:
        an instance of :class:`Backbone`
    """
    if input_shape is None:
        input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))

    backbone = build_darknet_backbone(cfg, input_shape)
    assert isinstance(backbone, Backbone)
    return backbone


def build_anchor_generator(cfg, input_shape):
    return DefaultAnchorGenerator(cfg, input_shape)


def build_model(cfg):
    cfg.build_backbone = build_backbone
    cfg.build_anchor_generator = build_anchor_generator
    cfg.build_encoder = build_encoder
    cfg.build_decoder = build_decoder
    model = YOLOF(cfg)

    logger = logging.getLogger(__name__)
    logger.info("Model:\n{}".format(model))
    return model
```

###### playground/detection/coco/yolof/yolof.cspdarknet53.DC5.3x/cspdarknet.py

```python
import logging
import sys

sys.path.append("..")

import torch
import torch.nn as nn
import torch.nn.functional as F

from cvpods.modeling.backbone import Backbone
from cvpods.layers import ShapeSpec

from yolof_base.utils import get_norm

try:
    from mish_cuda import MishCuda as Mish
except Exception:
    logger = logging.getLogger(__name__)
    logger.warning(
        "Install mish-cuda to speed up training and inference. More "
        "importantly, replace the naive Mish with MishCuda will give a "
        "~1.5G memory saving during training."
    )


    def mish(x):
        return x.mul(F.softplus(x).tanh())


    class Mish(nn.Module):
        def __init__(self):
            super(Mish, self).__init__()

        def forward(self, x):
            return mish(x)


def ConvNormActivation(inplanes,
                       planes,
                       kernel_size=3,
                       stride=1,
                       padding=0,
                       dilation=1,
                       groups=1,
                       norm_type="BN"):
    """
    A help function to build a 'conv-bn-activation' module
    """
    layers = []
    layers.append(nn.Conv2d(inplanes,
                            planes,
                            kernel_size=kernel_size,
                            stride=stride,
                            padding=padding,
                            dilation=dilation,
                            groups=groups,
                            bias=False))
    layers.append(get_norm(norm_type, planes, eps=1e-4, momentum=0.03))
    layers.append(Mish())
    return nn.Sequential(*layers)


class DarkBlock(nn.Module):

    def __init__(self,
                 inplanes,
                 planes,
                 dilation=1,
                 downsample=None,
                 norm_type="BN"):
        """Residual Block for DarkNet.

        This module has the dowsample layer (optional),
        1x1 conv layer and 3x3 conv layer.
        """
        super(DarkBlock, self).__init__()

        self.downsample = downsample

        self.bn1 = get_norm(norm_type, inplanes, eps=1e-4, momentum=0.03)
        self.bn2 = get_norm(norm_type, planes, eps=1e-4, momentum=0.03)

        self.conv1 = nn.Conv2d(
            planes,
            inplanes,
            kernel_size=1,
            stride=1,
            padding=0,
            bias=False
        )

        self.conv2 = nn.Conv2d(
            inplanes,
            planes,
            kernel_size=3,
            stride=1,
            padding=dilation,
            dilation=dilation,
            bias=False
        )

        self.activation = Mish()

    def forward(self, x):
        if self.downsample is not None:
            x = self.downsample(x)

        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.activation(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.activation(out)

        out += identity

        return out


class CrossStagePartialBlock(nn.Module):
    """CSPNet: A New Backbone that can Enhance Learning Capability of CNN.
    Refer to the paper for more details: https://arxiv.org/abs/1911.11929.
    In this module, the inputs go throuth the base conv layer at the first,
    and then pass the two partial transition layers.
    1. go throuth basic block (like DarkBlock)
        and one partial transition layer.
    2. go throuth the other partial transition layer.
    At last, They are concat into fuse transition layer.

    Args:
        inplanes (int): number of input channels.
        planes (int): number of output channels
        stage_layers (nn.Module): the basic block which applying CSPNet.
        is_csp_first_stage (bool): Is the first stage or not.
            The number of input and output channels in the first stage of
            CSPNet is different from other stages.
        dilation (int): conv dilation
        stride (int): stride for the base layer
        norm_type (str): normalization layer type.
    """

    def __init__(self,
                 inplanes,
                 planes,
                 stage_layers,
                 is_csp_first_stage,
                 dilation=1,
                 stride=2,
                 norm_type="BN"):
        super(CrossStagePartialBlock, self).__init__()

        self.base_layer = ConvNormActivation(
            inplanes,
            planes,
            kernel_size=3,
            stride=stride,
            padding=dilation,
            dilation=dilation,
            norm_type=norm_type
        )
        self.partial_transition1 = ConvNormActivation(
            inplanes=planes,
            planes=inplanes if not is_csp_first_stage else planes,
            kernel_size=1,
            stride=1,
            padding=0,
            norm_type=norm_type
        )
        self.stage_layers = stage_layers

        self.partial_transition2 = ConvNormActivation(
            inplanes=inplanes if not is_csp_first_stage else planes,
            planes=inplanes if not is_csp_first_stage else planes,
            kernel_size=1,
            stride=1,
            padding=0,
            norm_type=norm_type
        )
        self.fuse_transition = ConvNormActivation(
            inplanes=planes if not is_csp_first_stage else planes * 2,
            planes=planes,
            kernel_size=1,
            stride=1,
            padding=0,
            norm_type=norm_type
        )

    def forward(self, x):
        x = self.base_layer(x)

        out1 = self.partial_transition1(x)

        out2 = self.stage_layers(x)
        out2 = self.partial_transition2(out2)

        out = torch.cat([out2, out1], dim=1)
        out = self.fuse_transition(out)

        return out


def make_dark_layer(block,
                    inplanes,
                    planes,
                    num_blocks,
                    dilation=1,
                    stride=2,
                    norm_type="BN"):
    downsample = ConvNormActivation(
        inplanes=inplanes,
        planes=planes,
        kernel_size=3,
        stride=stride,
        padding=dilation,
        dilation=dilation,
        norm_type=norm_type
    )

    layers = []
    for i in range(0, num_blocks):
        layers.append(
            block(
                inplanes=inplanes,
                planes=planes,
                downsample=downsample if i == 0 else None,
                dilation=dilation,
                norm_type=norm_type
            )
        )
    return nn.Sequential(*layers)


def make_cspdark_layer(block,
                       inplanes,
                       planes,
                       num_blocks,
                       is_csp_first_stage,
                       dilation=1,
                       norm_type="BN"):
    downsample = ConvNormActivation(
        inplanes=planes,
        planes=planes if is_csp_first_stage else inplanes,
        kernel_size=1,
        stride=1,
        padding=0,
        norm_type=norm_type
    )

    layers = []
    for i in range(0, num_blocks):
        layers.append(
            block(
                inplanes=inplanes,
                planes=planes if is_csp_first_stage else inplanes,
                downsample=downsample if i == 0 else None,
                dilation=dilation,
                norm_type=norm_type
            )
        )
    return nn.Sequential(*layers)


class DarkNet(Backbone):
    """DarkNet backbone.
    Refer to the paper for more details: https://arxiv.org/pdf/1804.02767

    Args:
        depth (int): Depth of Darknet, from {53}.
        num_stages (int): Darknet stages, normally 5.
        with_csp (bool): Use cross stage partial connection or not.
        out_features (List[str]): Output features.
        norm_type (str): type of normalization layer.
        res5_dilation (int): dilation for the last stage
    """

    arch_settings = {
        53: (DarkBlock, (1, 2, 8, 8, 4))
    }

    def __init__(self,
                 depth,
                 with_csp=False,
                 out_features=["res5"],
                 norm_type="BN",
                 res5_dilation=1):
        super(DarkNet, self).__init__()
        if depth not in self.arch_settings:
            raise KeyError('invalid depth {} for resnet'.format(depth))
        self.with_csp = with_csp
        self._out_features = out_features
        self.norm_type = norm_type
        self.res5_dilation = res5_dilation

        self.block, self.stage_blocks = self.arch_settings[depth]
        self.inplanes = 32

        self._make_stem_layer()

        self.dark_layers = []
        for i, num_blocks in enumerate(self.stage_blocks):
            planes = 64 * 2 ** i
            dilation = 1
            stride = 2
            if i == 4 and self.res5_dilation == 2:
                dilation = self.res5_dilation
                stride = 1
            if not self.with_csp:
                layer = make_dark_layer(
                    block=self.block,
                    inplanes=self.inplanes,
                    planes=planes,
                    num_blocks=num_blocks,
                    dilation=dilation,
                    stride=stride,
                    norm_type=self.norm_type
                )
            else:
                layer = make_cspdark_layer(
                    block=self.block,
                    inplanes=self.inplanes,
                    planes=planes,
                    num_blocks=num_blocks,
                    is_csp_first_stage=True if i == 0 else False,
                    dilation=dilation,
                    norm_type=self.norm_type
                )
                layer = CrossStagePartialBlock(
                    self.inplanes,
                    planes,
                    stage_layers=layer,
                    is_csp_first_stage=True if i == 0 else False,
                    dilation=dilation,
                    stride=stride,
                    norm_type=self.norm_type
                )
            self.inplanes = planes
            layer_name = 'layer{}'.format(i + 1)
            self.add_module(layer_name, layer)
            self.dark_layers.append(layer_name)

        # freeze stage<=2
        for p in self.conv1.parameters():
            p.requires_grad = False
        for p in self.bn1.parameters():
            p.requires_grad = False
        for p in self.layer1.parameters():
            p.requires_grad = False
        for p in self.layer2.parameters():
            p.requires_grad = False

    def _make_stem_layer(self):
        self.conv1 = nn.Conv2d(
            3,
            self.inplanes,
            kernel_size=3,
            stride=1,
            padding=1,
            bias=False
        )
        self.bn1 = get_norm(
            self.norm_type, self.inplanes, eps=1e-4, momentum=0.03
        )
        self.act1 = Mish()

    def forward(self, x):
        outputs = {}
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.act1(x)

        for i, layer_name in enumerate(self.dark_layers):
            layer = getattr(self, layer_name)
            x = layer(x)
        outputs[self._out_features[-1]] = x
        return outputs

    def output_shape(self):
        return {
            "res5": ShapeSpec(
                channels=1024, stride=16 if self.res5_dilation == 2 else 32
            )
        }


def build_darknet_backbone(cfg, input_shape=None):
    """
    Create a DarkNet/CSPDarkNet instance from config.

    Returns:
        DarkNet: a :class:`DarkNet` instance.
    """
    depth = cfg.MODEL.DARKNET.DEPTH
    with_csp = cfg.MODEL.DARKNET.WITH_CSP
    out_features = cfg.MODEL.DARKNET.OUT_FEATURES
    norm_type = cfg.MODEL.DARKNET.NORM
    res5_dilation = cfg.MODEL.DARKNET.RES5_DILATION
    return DarkNet(
        depth=depth,
        with_csp=with_csp,
        out_features=out_features,
        norm_type=norm_type,
        res5_dilation=res5_dilation
    )
```

###### playground/detection/coco/yolof/yolof.X101.64x4d.C5.1x/config.py

```python
from cvpods.configs.base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        # Backbone NAME: "build_resnet_backbone"
        WEIGHTS="detectron2://ImageNetPretrained/FBResNeXt/X-101-64x4d.pkl",
        RESNETS=dict(
            DEPTH=101,
            NUM_GROUPS=64,
            WIDTH_PER_GROUP=4,
            STRIDE_IN_1X1=False,
            OUT_FEATURES=["res5"]
        ),
        YOLOF=dict(
            ENCODER=dict(
                IN_FEATURES=["res5"],
                NUM_CHANNELS=512,
                BLOCK_MID_CHANNELS=128,
                NUM_RESIDUAL_BLOCKS=4,
                BLOCK_DILATIONS=[2, 4, 6, 8],
                NORM="BN",
                ACTIVATION="ReLU"
            ),
            DECODER=dict(
                IN_CHANNELS=512,
                NUM_CLASSES=80,
                NUM_ANCHORS=5,
                CLS_NUM_CONVS=2,
                REG_NUM_CONVS=4,
                NORM="BN",
                ACTIVATION="ReLU",
                PRIOR_PROB=0.01
            ),
            BBOX_REG_WEIGHTS=(1.0, 1.0, 1.0, 1.0),
            ADD_CTR_CLAMP=True,
            CTR_CLAMP=32,
            MATCHER_TOPK=4,
            POS_IGNORE_THRESHOLD=0.15,
            NEG_IGNORE_THRESHOLD=0.7,
            FOCAL_LOSS_GAMMA=2.0,
            FOCAL_LOSS_ALPHA=0.25,
            SCORE_THRESH_TEST=0.05,
            TOPK_CANDIDATES_TEST=1000,
            NMS_THRESH_TEST=0.6
        ),
    ),
    DATASETS=dict(
        TRAIN=("coco_2017_train",),
        TEST=("coco_2017_val",),
    ),
    DATALOADER=dict(NUM_WORKERS=4),
    SOLVER=dict(
        LR_SCHEDULER=dict(
            STEPS=(30000, 40000),
            MAX_ITER=45000,
            WARMUP_FACTOR=0.000334,
            WARMUP_ITERS=3000
        ),
        OPTIMIZER=dict(
            NAME="D2SGD",
            BASE_LR=0.06,
            BIAS_LR_FACTOR=1.0,
            WEIGHT_DECAY=0.0001,
            WEIGHT_DECAY_NORM=0.0,
            MOMENTUM=0.9,
            BACKBONE_LR_FACTOR=0.334
        ),
        IMS_PER_BATCH=32,
        IMS_PER_DEVICE=4,
    ),
    INPUT=dict(
        AUG=dict(
            TRAIN_PIPELINES=[
                ("ResizeShortestEdge", dict(
                    short_edge_length=(800,), max_size=1333,
                    sample_style="choice")),
                ("RandomFlip", dict()),
                ("RandomShift", dict(max_shifts=32))
            ],
            TEST_PIPELINES=[
                ("ResizeShortestEdge", dict(
                    short_edge_length=800, max_size=1333,
                    sample_style="choice")),
            ],
        ),
        # Whether the model needs RGB, YUV, HSV etc.
        FORMAT="BGR",
    ),
)


class YOLOFConfig(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = YOLOFConfig()
```

###### playground/detection/coco/yolof/yolof.X101.64x4d.C5.1x/net.py

```python
import logging
import sys

sys.path.append("..")

from cvpods.layers import ShapeSpec
from cvpods.modeling.backbone import Backbone
from cvpods.modeling.backbone import build_resnet_backbone
from cvpods.modeling.anchor_generator import DefaultAnchorGenerator

from yolof_base import build_encoder, build_decoder, YOLOF


def build_backbone(cfg, input_shape=None):
    """
    Build a backbone from `cfg.MODEL.BACKBONE.NAME`.
    Returns:
        an instance of :class:`Backbone`
    """
    if input_shape is None:
        input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))

    backbone = build_resnet_backbone(cfg, input_shape)
    assert isinstance(backbone, Backbone)
    return backbone


def build_anchor_generator(cfg, input_shape):
    return DefaultAnchorGenerator(cfg, input_shape)


def build_model(cfg):
    cfg.build_backbone = build_backbone
    cfg.build_anchor_generator = build_anchor_generator
    cfg.build_encoder = build_encoder
    cfg.build_decoder = build_decoder
    model = YOLOF(cfg)

    logger = logging.getLogger(__name__)
    logger.info("Model:\n{}".format(model))
    return model
```

###### playground/detection/coco/yolof/yolof.cspdarknet53.DC5.9x/config.py

```python
from cvpods.configs.base_detection_config import BaseDetectionConfig

_config_dict = dict(
    MODEL=dict(
        # Backbone NAME: "build_darknet_backbone"
        WEIGHTS="../../../../../pretrained_models/cspdarknet53.pth",
        DARKNET=dict(
            DEPTH=53,
            WITH_CSP=True,
            NORM="SyncBN",
            OUT_FEATURES=["res5"],
            RES5_DILATION=2
        ),
        ANCHOR_GENERATOR=dict(
            SIZES=[[16, 32, 64, 128, 256, 512]],
            ASPECT_RATIOS=[[1.0]]
        ),
        YOLOF=dict(
            ENCODER=dict(
                IN_FEATURES=["res5"],
                NUM_CHANNELS=512,
                BLOCK_MID_CHANNELS=128,
                NUM_RESIDUAL_BLOCKS=8,
                BLOCK_DILATIONS=[1, 2, 3, 4, 5, 6, 7, 8],
                NORM="SyncBN",
                ACTIVATION="LeakyReLU"
            ),
            DECODER=dict(
                IN_CHANNELS=512,
                NUM_CLASSES=80,
                NUM_ANCHORS=6,
                CLS_NUM_CONVS=2,
                REG_NUM_CONVS=4,
                NORM="SyncBN",
                ACTIVATION="LeakyReLU",
                PRIOR_PROB=0.01
            ),
            BBOX_REG_WEIGHTS=(1.0, 1.0, 1.0, 1.0),
            ADD_CTR_CLAMP=True,
            CTR_CLAMP=32,
            MATCHER_TOPK=8,
            POS_IGNORE_THRESHOLD=0.1,
            NEG_IGNORE_THRESHOLD=0.7,
            FOCAL_LOSS_GAMMA=2.0,
            FOCAL_LOSS_ALPHA=0.25,
            SCORE_THRESH_TEST=0.05,
            TOPK_CANDIDATES_TEST=1000,
            NMS_THRESH_TEST=0.6
        ),
    ),
    DATASETS=dict(
        TRAIN=("coco_2017_train",),
        TEST=("coco_2017_val",),
    ),
    DATALOADER=dict(NUM_WORKERS=8),
    SOLVER=dict(
        LR_SCHEDULER=dict(
            STEPS=(187500, 197500),
            MAX_ITER=202500,
            WARMUP_FACTOR=0.00066667,
            WARMUP_ITERS=1500
        ),
        OPTIMIZER=dict(
            NAME="D2SGD",
            BASE_LR=0.04,
            BIAS_LR_FACTOR=1.0,
            WEIGHT_DECAY=0.0001,
            WEIGHT_DECAY_NORM=0.0,
            MOMENTUM=0.9,
            BACKBONE_LR_FACTOR=1.0
        ),
        IMS_PER_BATCH=64,
        IMS_PER_DEVICE=8,
    ),
    INPUT=dict(
        AUG=dict(
            TRAIN_PIPELINES=[
                ("JitterCrop", dict(jitter_ratio=0.3)),
                ("Resize", dict(shape=(640, 640), scale_jitter=(0.8, 1.2))),
                ("RandomDistortion2",
                 dict(hue=0.1, saturation=1.5, exposure=1.5)),
                ("RandomFlip", dict()),
                ("RandomShift", dict(max_shifts=32))
            ],
            TEST_PIPELINES=[
                ("Resize", dict(shape=(608, 608))),
            ],
        ),
        MOSAIC=dict(
            MIN_OFFSET=0.2,
            MOSAIC_WIDTH=640,
            MOSAIC_HEIGHT=640
        ),
        # Whether the model needs RGB, YUV, HSV etc.
        FORMAT="BGR",
    ),
)


class YOLOFConfig(BaseDetectionConfig):
    def __init__(self, d=None, **kwargs):
        super().__init__(d, **kwargs)
        self._register_configuration(_config_dict)


config = YOLOFConfig()
```

###### playground/detection/coco/yolof/yolof.cspdarknet53.DC5.9x/net.py

```python
import logging
import sys

from cvpods.layers import ShapeSpec
from cvpods.modeling.backbone import Backbone
from cvpods.modeling.anchor_generator import DefaultAnchorGenerator

from .cspdarknet import build_darknet_backbone
sys.path.append("..")
from yolof_base import build_encoder, build_decoder, YOLOF


def build_backbone(cfg, input_shape=None):
    """
    Build a backbone from `cfg.MODEL.BACKBONE.NAME`.
    Returns:
        an instance of :class:`Backbone`
    """
    if input_shape is None:
        input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))

    backbone = build_darknet_backbone(cfg, input_shape)
    assert isinstance(backbone, Backbone)
    return backbone


def build_anchor_generator(cfg, input_shape):
    return DefaultAnchorGenerator(cfg, input_shape)


def build_model(cfg):
    cfg.build_backbone = build_backbone
    cfg.build_anchor_generator = build_anchor_generator
    cfg.build_encoder = build_encoder
    cfg.build_decoder = build_decoder
    model = YOLOF(cfg)

    logger = logging.getLogger(__name__)
    logger.info("Model:\n{}".format(model))
    return model
```

###### playground/detection/coco/yolof/yolof.cspdarknet53.DC5.9x/cspdarknet.py

```python
import logging
import sys

sys.path.append("..")

import torch
import torch.nn as nn
import torch.nn.functional as F

from cvpods.modeling.backbone import Backbone
from cvpods.layers import ShapeSpec

from yolof_base.utils import get_norm

try:
    from mish_cuda import MishCuda as Mish
except Exception:
    logger = logging.getLogger(__name__)
    logger.warning(
        "Install mish-cuda to speed up training and inference. More "
        "importantly, replace the naive Mish with MishCuda will give a "
        "~1.5G memory saving during training."
    )


    def mish(x):
        return x.mul(F.softplus(x).tanh())


    class Mish(nn.Module):
        def __init__(self):
            super(Mish, self).__init__()

        def forward(self, x):
            return mish(x)


def ConvNormActivation(inplanes,
                       planes,
                       kernel_size=3,
                       stride=1,
                       padding=0,
                       dilation=1,
                       groups=1,
                       norm_type="BN"):
    """
    A help function to build a 'conv-bn-activation' module
    """
    layers = []
    layers.append(nn.Conv2d(inplanes,
                            planes,
                            kernel_size=kernel_size,
                            stride=stride,
                            padding=padding,
                            dilation=dilation,
                            groups=groups,
                            bias=False))
    layers.append(get_norm(norm_type, planes, eps=1e-4, momentum=0.03))
    layers.append(Mish())
    return nn.Sequential(*layers)


class DarkBlock(nn.Module):

    def __init__(self,
                 inplanes,
                 planes,
                 dilation=1,
                 downsample=None,
                 norm_type="BN"):
        """Residual Block for DarkNet.

        This module has the dowsample layer (optional),
        1x1 conv layer and 3x3 conv layer.
        """
        super(DarkBlock, self).__init__()

        self.downsample = downsample

        self.bn1 = get_norm(norm_type, inplanes, eps=1e-4, momentum=0.03)
        self.bn2 = get_norm(norm_type, planes, eps=1e-4, momentum=0.03)

        self.conv1 = nn.Conv2d(
            planes,
            inplanes,
            kernel_size=1,
            stride=1,
            padding=0,
            bias=False
        )

        self.conv2 = nn.Conv2d(
            inplanes,
            planes,
            kernel_size=3,
            stride=1,
            padding=dilation,
            dilation=dilation,
            bias=False
        )

        self.activation = Mish()

    def forward(self, x):
        if self.downsample is not None:
            x = self.downsample(x)

        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.activation(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.activation(out)

        out += identity

        return out


class CrossStagePartialBlock(nn.Module):
    """CSPNet: A New Backbone that can Enhance Learning Capability of CNN.
    Refer to the paper for more details: https://arxiv.org/abs/1911.11929.
    In this module, the inputs go throuth the base conv layer at the first,
    and then pass the two partial transition layers.
    1. go throuth basic block (like DarkBlock)
        and one partial transition layer.
    2. go throuth the other partial transition layer.
    At last, They are concat into fuse transition layer.

    Args:
        inplanes (int): number of input channels.
        planes (int): number of output channels
        stage_layers (nn.Module): the basic block which applying CSPNet.
        is_csp_first_stage (bool): Is the first stage or not.
            The number of input and output channels in the first stage of
            CSPNet is different from other stages.
        dilation (int): conv dilation
        stride (int): stride for the base layer
        norm_type (str): normalization layer type.
    """

    def __init__(self,
                 inplanes,
                 planes,
                 stage_layers,
                 is_csp_first_stage,
                 dilation=1,
                 stride=2,
                 norm_type="BN"):
        super(CrossStagePartialBlock, self).__init__()

        self.base_layer = ConvNormActivation(
            inplanes,
            planes,
            kernel_size=3,
            stride=stride,
            padding=dilation,
            dilation=dilation,
            norm_type=norm_type
        )
        self.partial_transition1 = ConvNormActivation(
            inplanes=planes,
            planes=inplanes if not is_csp_first_stage else planes,
            kernel_size=1,
            stride=1,
            padding=0,
            norm_type=norm_type
        )
        self.stage_layers = stage_layers

        self.partial_transition2 = ConvNormActivation(
            inplanes=inplanes if not is_csp_first_stage else planes,
            planes=inplanes if not is_csp_first_stage else planes,
            kernel_size=1,
            stride=1,
            padding=0,
            norm_type=norm_type
        )
        self.fuse_transition = ConvNormActivation(
            inplanes=planes if not is_csp_first_stage else planes * 2,
            planes=planes,
            kernel_size=1,
            stride=1,
            padding=0,
            norm_type=norm_type
        )

    def forward(self, x):
        x = self.base_layer(x)

        out1 = self.partial_transition1(x)

        out2 = self.stage_layers(x)
        out2 = self.partial_transition2(out2)

        out = torch.cat([out2, out1], dim=1)
        out = self.fuse_transition(out)

        return out


def make_dark_layer(block,
                    inplanes,
                    planes,
                    num_blocks,
                    dilation=1,
                    stride=2,
                    norm_type="BN"):
    downsample = ConvNormActivation(
        inplanes=inplanes,
        planes=planes,
        kernel_size=3,
        stride=stride,
        padding=dilation,
        dilation=dilation,
        norm_type=norm_type
    )

    layers = []
    for i in range(0, num_blocks):
        layers.append(
            block(
                inplanes=inplanes,
                planes=planes,
                downsample=downsample if i == 0 else None,
                dilation=dilation,
                norm_type=norm_type
            )
        )
    return nn.Sequential(*layers)


def make_cspdark_layer(block,
                       inplanes,
                       planes,
                       num_blocks,
                       is_csp_first_stage,
                       dilation=1,
                       norm_type="BN"):
    downsample = ConvNormActivation(
        inplanes=planes,
        planes=planes if is_csp_first_stage else inplanes,
        kernel_size=1,
        stride=1,
        padding=0,
        norm_type=norm_type
    )

    layers = []
    for i in range(0, num_blocks):
        layers.append(
            block(
                inplanes=inplanes,
                planes=planes if is_csp_first_stage else inplanes,
                downsample=downsample if i == 0 else None,
                dilation=dilation,
                norm_type=norm_type
            )
        )
    return nn.Sequential(*layers)


class DarkNet(Backbone):
    """DarkNet backbone.
    Refer to the paper for more details: https://arxiv.org/pdf/1804.02767

    Args:
        depth (int): Depth of Darknet, from {53}.
        num_stages (int): Darknet stages, normally 5.
        with_csp (bool): Use cross stage partial connection or not.
        out_features (List[str]): Output features.
        norm_type (str): type of normalization layer.
        res5_dilation (int): dilation for the last stage
    """

    arch_settings = {
        53: (DarkBlock, (1, 2, 8, 8, 4))
    }

    def __init__(self,
                 depth,
                 with_csp=False,
                 out_features=["res5"],
                 norm_type="BN",
                 res5_dilation=1):
        super(DarkNet, self).__init__()
        if depth not in self.arch_settings:
            raise KeyError('invalid depth {} for resnet'.format(depth))
        self.with_csp = with_csp
        self._out_features = out_features
        self.norm_type = norm_type
        self.res5_dilation = res5_dilation

        self.block, self.stage_blocks = self.arch_settings[depth]
        self.inplanes = 32

        self._make_stem_layer()

        self.dark_layers = []
        for i, num_blocks in enumerate(self.stage_blocks):
            planes = 64 * 2 ** i
            dilation = 1
            stride = 2
            if i == 4 and self.res5_dilation == 2:
                dilation = self.res5_dilation
                stride = 1
            if not self.with_csp:
                layer = make_dark_layer(
                    block=self.block,
                    inplanes=self.inplanes,
                    planes=planes,
                    num_blocks=num_blocks,
                    dilation=dilation,
                    stride=stride,
                    norm_type=self.norm_type
                )
            else:
                layer = make_cspdark_layer(
                    block=self.block,
                    inplanes=self.inplanes,
                    planes=planes,
                    num_blocks=num_blocks,
                    is_csp_first_stage=True if i == 0 else False,
                    dilation=dilation,
                    norm_type=self.norm_type
                )
                layer = CrossStagePartialBlock(
                    self.inplanes,
                    planes,
                    stage_layers=layer,
                    is_csp_first_stage=True if i == 0 else False,
                    dilation=dilation,
                    stride=stride,
                    norm_type=self.norm_type
                )
            self.inplanes = planes
            layer_name = 'layer{}'.format(i + 1)
            self.add_module(layer_name, layer)
            self.dark_layers.append(layer_name)

        # freeze stage<=2
        for p in self.conv1.parameters():
            p.requires_grad = False
        for p in self.bn1.parameters():
            p.requires_grad = False
        for p in self.layer1.parameters():
            p.requires_grad = False
        for p in self.layer2.parameters():
            p.requires_grad = False

    def _make_stem_layer(self):
        self.conv1 = nn.Conv2d(
            3,
            self.inplanes,
            kernel_size=3,
            stride=1,
            padding=1,
            bias=False
        )
        self.bn1 = get_norm(
            self.norm_type, self.inplanes, eps=1e-4, momentum=0.03
        )
        self.act1 = Mish()

    def forward(self, x):
        outputs = {}
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.act1(x)

        for i, layer_name in enumerate(self.dark_layers):
            layer = getattr(self, layer_name)
            x = layer(x)
        outputs[self._out_features[-1]] = x
        return outputs

    def output_shape(self):
        return {
            "res5": ShapeSpec(
                channels=1024, stride=16 if self.res5_dilation == 2 else 32
            )
        }


def build_darknet_backbone(cfg, input_shape=None):
    """
    Create a DarkNet/CSPDarkNet instance from config.

    Returns:
        DarkNet: a :class:`DarkNet` instance.
    """
    depth = cfg.MODEL.DARKNET.DEPTH
    with_csp = cfg.MODEL.DARKNET.WITH_CSP
    out_features = cfg.MODEL.DARKNET.OUT_FEATURES
    norm_type = cfg.MODEL.DARKNET.NORM
    res5_dilation = cfg.MODEL.DARKNET.RES5_DILATION
    return DarkNet(
        depth=depth,
        with_csp=with_csp,
        out_features=out_features,
        norm_type=norm_type,
        res5_dilation=res5_dilation
    )
```

## tests/__init__.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
```

## tests/test_psroi_pool.py

```python
import unittest

import numpy as np

import torch
from torch.autograd import gradcheck

from cvpods.layers.psroi_pool import PSROIPool


class PSROIPoolTest(unittest.TestCase):
    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_psroipool_forward_cuda(self):
        input = torch.randn(2, 21 * 7 * 7, 5, 7, requires_grad=True).float()
        rois = torch.from_numpy(
            np.array([
                [0.0000, 350.6689, 211.0240, 779.0886, 777.7496],
                [0.0000, 744.0627, 277.4919, 988.4307, 602.7589],
                [1.0000, 350.6689, 211.0240, 779.0886, 777.7496],
                [1.0000, 744.0627, 277.4919, 988.4307, 602.7589],
            ])
        ).float()

        pool = PSROIPool((7, 7), 1 / 160.0, 7, 21)
        input = input.cuda()
        rois = rois.cuda()
        out = pool(input, rois)
        assert out.shape == (4, 21, 7, 7), out.shape

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_psroipool_backward_cuda(self):
        input = torch.randn(2, 21 * 7 * 7, 5, 7, requires_grad=True).double()
        rois = torch.from_numpy(
            np.array([
                [0.0000, 350.6689, 211.0240, 779.0886, 777.7496],
                [0.0000, 744.0627, 277.4919, 988.4307, 602.7589],
                [1.0000, 350.6689, 211.0240, 779.0886, 777.7496],
                [1.0000, 744.0627, 277.4919, 988.4307, 602.7589],
            ])
        ).double()

        pool = PSROIPool((7, 7), 1 / 160.0, 7, 21)
        input = input.cuda()
        rois = rois.cuda()
        func = lambda x: pool(x, rois).mean()  # noqa
        gradcheck(func, input, atol=3e-5)


if __name__ == "__main__":
    unittest.main()
```

### tests/checkpoint/test_checkpoint.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import unittest
from collections import OrderedDict
import torch
from torch import nn

from cvpods.checkpoint.c2_model_loading import align_and_update_state_dicts
from cvpods.utils import setup_logger


class TestCheckpointer(unittest.TestCase):
    def setUp(self):
        setup_logger()

    def create_complex_model(self):
        m = nn.Module()
        m.block1 = nn.Module()
        m.block1.layer1 = nn.Linear(2, 3)
        m.layer2 = nn.Linear(3, 2)
        m.res = nn.Module()
        m.res.layer2 = nn.Linear(3, 2)

        state_dict = OrderedDict()
        state_dict["layer1.weight"] = torch.rand(3, 2)
        state_dict["layer1.bias"] = torch.rand(3)
        state_dict["layer2.weight"] = torch.rand(2, 3)
        state_dict["layer2.bias"] = torch.rand(2)
        state_dict["res.layer2.weight"] = torch.rand(2, 3)
        state_dict["res.layer2.bias"] = torch.rand(2)
        return m, state_dict

    def test_complex_model_loaded(self):
        for add_data_parallel in [False, True]:
            model, state_dict = self.create_complex_model()
            if add_data_parallel:
                model = nn.DataParallel(model)
            model_sd = model.state_dict()

            align_and_update_state_dicts(model_sd, state_dict)
            for loaded, stored in zip(model_sd.values(), state_dict.values()):
                # different tensor references
                self.assertFalse(id(loaded) == id(stored))
                # same content
                self.assertTrue(loaded.equal(stored))


if __name__ == "__main__":
    unittest.main()
```

### tests/layers/test_nms.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from __future__ import absolute_import, division, print_function, unicode_literals
import unittest
import torch

from cvpods.layers import batched_nms
from cvpods.utils.env import TORCH_VERSION


class TestNMS(unittest.TestCase):
    def _create_tensors(self, N):
        boxes = torch.rand(N, 4) * 100
        # Note: the implementation of this function in torchvision is:
        # boxes[:, 2:] += torch.rand(N, 2) * 100
        # but it does not guarantee non-negative widths/heights constraints:
        # boxes[:, 2] >= boxes[:, 0] and boxes[:, 3] >= boxes[:, 1]:
        boxes[:, 2:] += boxes[:, :2]
        scores = torch.rand(N)
        return boxes, scores

    # TODO: make cvpods.layers.batched_nms scriptable
    @unittest.skip("Don't support scriptable cvpods.layers.batched_nms")
    @unittest.skipIf(TORCH_VERSION < (1, 6), "Insufficient pytorch version")
    def test_nms_scriptability(self):
        N = 2000
        num_classes = 50
        boxes, scores = self._create_tensors(N)
        idxs = torch.randint(0, num_classes, (N,))
        scripted_batched_nms = torch.jit.script(batched_nms)
        err_msg = "NMS is incompatible with jit-scripted NMS for IoU={}"

        for iou in [0.2, 0.5, 0.8]:
            keep_ref = batched_nms(boxes, scores, idxs, iou)
            backup = boxes.clone()
            scripted_keep = scripted_batched_nms(boxes, scores, idxs, iou)
            assert torch.allclose(boxes, backup), "boxes modified by jit-scripted batched_nms"
            self.assertTrue(torch.equal(keep_ref, scripted_keep), err_msg.format(iou))


if __name__ == "__main__":
    unittest.main()
```

### tests/layers/test_roi_align.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import numpy as np
import unittest
import cv2
import torch
from cvpods.utils import benchmark

from cvpods.layers.roi_align import ROIAlign


class ROIAlignTest(unittest.TestCase):
    def test_forward_output(self):
        input = np.arange(25).reshape(5, 5).astype("float32")
        """
        0  1  2   3 4
        5  6  7   8 9
        10 11 12 13 14
        15 16 17 18 19
        20 21 22 23 24
        """

        output = self._simple_roialign(input, [1, 1, 3, 3], (4, 4), aligned=False)
        output_correct = self._simple_roialign(input, [1, 1, 3, 3], (4, 4), aligned=True)

        # without correction:
        old_results = [
            [7.5, 8, 8.5, 9],
            [10, 10.5, 11, 11.5],
            [12.5, 13, 13.5, 14],
            [15, 15.5, 16, 16.5],
        ]

        # with 0.5 correction:
        correct_results = [
            [4.5, 5.0, 5.5, 6.0],
            [7.0, 7.5, 8.0, 8.5],
            [9.5, 10.0, 10.5, 11.0],
            [12.0, 12.5, 13.0, 13.5],
        ]
        # This is an upsampled version of [[6, 7], [11, 12]]

        self.assertTrue(np.allclose(output.flatten(), np.asarray(old_results).flatten()))
        self.assertTrue(
            np.allclose(output_correct.flatten(), np.asarray(correct_results).flatten())
        )

        # Also see similar issues in tensorflow at
        # https://github.com/tensorflow/tensorflow/issues/26278

    def test_resize(self):
        H, W = 30, 30
        input = np.random.rand(H, W).astype("float32") * 100
        box = [10, 10, 20, 20]
        output = self._simple_roialign(input, box, (5, 5), aligned=True)

        input2x = cv2.resize(input, (W // 2, H // 2), interpolation=cv2.INTER_LINEAR)
        box2x = [x / 2 for x in box]
        output2x = self._simple_roialign(input2x, box2x, (5, 5), aligned=True)
        diff = np.abs(output2x - output)
        self.assertTrue(diff.max() < 1e-4)

    def _simple_roialign(self, img, box, resolution, aligned=True):
        """
        RoiAlign with scale 1.0 and 0 sample ratio.
        """
        if isinstance(resolution, int):
            resolution = (resolution, resolution)
        op = ROIAlign(resolution, 1.0, 0, aligned=aligned)
        input = torch.from_numpy(img[None, None, :, :].astype("float32"))

        rois = [0] + list(box)
        rois = torch.from_numpy(np.asarray(rois)[None, :].astype("float32"))
        output = op.forward(input, rois)
        if torch.cuda.is_available():
            output_cuda = op.forward(input.cuda(), rois.cuda()).cpu()
            self.assertTrue(torch.allclose(output, output_cuda))
        return output[0, 0]

    def _simple_roialign_with_grad(self, img, box, resolution, device):
        if isinstance(resolution, int):
            resolution = (resolution, resolution)

        op = ROIAlign(resolution, 1.0, 0, aligned=True)
        input = torch.from_numpy(img[None, None, :, :].astype("float32"))

        rois = [0] + list(box)
        rois = torch.from_numpy(np.asarray(rois)[None, :].astype("float32"))
        input = input.to(device=device)
        rois = rois.to(device=device)
        input.requires_grad = True
        output = op.forward(input, rois)
        return input, output

    def test_empty_box(self):
        img = np.random.rand(5, 5)
        box = [3, 4, 5, 4]
        o = self._simple_roialign(img, box, 7)
        self.assertTrue(o.shape == (7, 7))
        self.assertTrue((o == 0).all())

        for dev in ["cpu"] + ["cuda"] if torch.cuda.is_available() else []:
            input, output = self._simple_roialign_with_grad(img, box, 7, torch.device(dev))
            output.sum().backward()
            self.assertTrue(torch.allclose(input.grad, torch.zeros_like(input)))

    def test_empty_batch(self):
        input = torch.zeros(0, 3, 10, 10, dtype=torch.float32)
        rois = torch.zeros(0, 5, dtype=torch.float32)
        op = ROIAlign((7, 7), 1.0, 0, aligned=True)
        output = op.forward(input, rois)
        self.assertTrue(output.shape == (0, 3, 7, 7))


def benchmark_roi_align():
    from cvpods import _C

    def random_boxes(mean_box, stdev, N, maxsize):
        ret = torch.rand(N, 4) * stdev + torch.tensor(mean_box, dtype=torch.float)
        ret.clamp_(min=0, max=maxsize)
        return ret

    def func(N, C, H, W, nboxes_per_img):
        input = torch.rand(N, C, H, W)
        boxes = []
        batch_idx = []
        for k in range(N):
            b = random_boxes([80, 80, 130, 130], 24, nboxes_per_img, H)
            # try smaller boxes:
            # b = random_boxes([100, 100, 110, 110], 4, nboxes_per_img, H)
            boxes.append(b)
            batch_idx.append(torch.zeros(nboxes_per_img, 1, dtype=torch.float32) + k)
        boxes = torch.cat(boxes, axis=0)
        batch_idx = torch.cat(batch_idx, axis=0)
        boxes = torch.cat([batch_idx, boxes], axis=1)

        input = input.cuda()
        boxes = boxes.cuda()

        def bench():
            _C.roi_align_forward(input, boxes, 1.0, 7, 7, 0, True)
            torch.cuda.synchronize()

        return bench

    args = [dict(N=2, C=512, H=256, W=256, nboxes_per_img=500)]
    benchmark(func, "cuda_roialign", args, num_iters=20, warmup_iters=1)


if __name__ == "__main__":
    if torch.cuda.is_available():
        benchmark_roi_align()
    unittest.main()
```

### tests/layers/test_mask_ops.py

```python
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

import contextlib
import io
import numpy as np
import os
import unittest
from collections import defaultdict
import torch
import tqdm
from cvpods.utils import benchmark
from pycocotools.coco import COCO
from tabulate import tabulate
from torch.nn import functional as F

from cvpods.layers.mask_ops import (
    pad_masks,
    paste_mask_in_image_old,
    paste_masks_in_image,
    scale_boxes,
)
from cvpods.structures import BitMasks, Boxes, BoxMode, PolygonMasks
from cvpods.structures.masks import polygons_to_bitmask


def iou_between_full_image_bit_masks(a, b, eps=0.001):
    intersect = (a & b).sum() + eps
    union = (a | b).sum() + eps
    return intersect / union


def rasterize_polygons_with_grid_sample(full_image_bit_mask, box, mask_size, threshold=0.5):
    x0, y0, x1, y1 = box[0], box[1], box[2], box[3]

    img_h, img_w = full_image_bit_mask.shape

    mask_y = np.arange(0.0, mask_size) + 0.5  # mask y sample coords in [0.5, mask_size - 0.5]
    mask_x = np.arange(0.0, mask_size) + 0.5  # mask x sample coords in [0.5, mask_size - 0.5]
    mask_y = (mask_y) / (mask_size) * (y1 - y0) + y0
    mask_x = (mask_x) / (mask_size) * (x1 - x0) + x0

    mask_x = (mask_x - 0.5) / (img_w - 1) * 2 + -1
    mask_y = (mask_y - 0.5) / (img_h - 1) * 2 + -1
    gy, gx = torch.meshgrid(torch.from_numpy(mask_y), torch.from_numpy(mask_x))
    ind = torch.stack([gx, gy], dim=-1).to(dtype=torch.float32)

    full_image_bit_mask = torch.from_numpy(full_image_bit_mask)
    mask = F.grid_sample(
        full_image_bit_mask[None, None, :, :].to(dtype=torch.float32),
        ind[None, :, :, :],
        align_corners=True,
    )

    return mask[0, 0] >= threshold


class TestMaskCropPaste(unittest.TestCase):
    def setUp(self):
        json_file = "datasets/coco/annotations/instances_train2017.json"
        if not os.path.isfile(json_file):
            print("{} not found".format(json_file))
            raise unittest.SkipTest("{} not found".format(json_file))
        with contextlib.redirect_stdout(io.StringIO()):
            self.coco = COCO(json_file)

    def test_crop_paste_consistency(self):
        """
        rasterize_polygons_within_box (used in training)
        and
        paste_masks_in_image (used in inference)
        should be inverse operations to each other.

        This function runs several implementation of the above two operations and prints
        the reconstruction error.
        """

        anns = self.coco.loadAnns(self.coco.getAnnIds(iscrowd=False))  # avoid crowd annotations

        selected_anns = anns[:100]

        ious = []
        for ann in tqdm.tqdm(selected_anns):
            results = self.process_annotation(ann)
            ious.append([k[2] for k in results])

        ious = np.array(ious)
        mean_ious = ious.mean(axis=0)
        table = []
        res_dic = defaultdict(dict)
        for row, iou in zip(results, mean_ious):
            table.append((row[0], row[1], iou))
            res_dic[row[0]][row[1]] = iou
        print(tabulate(table, headers=["rasterize", "paste", "iou"], tablefmt="simple"))
        # assert that the reconstruction is good:
        self.assertTrue(
            res_dic["polygon"]["aligned"] > 0.94,
            msg="res_dic['polygon']['aligned']: {}".format(res_dic["polygon"]["aligned"])
        )
        self.assertTrue(
            res_dic["roialign"]["aligned"] > 0.95,
            msg="res_dic['roialign']['aligned']: {}".format(res_dic["roialign"]["aligned"])
        )

    def process_annotation(self, ann, mask_side_len=28):
        # Parse annotation data
        img_info = self.coco.loadImgs(ids=[ann["image_id"]])[0]
        height, width = img_info["height"], img_info["width"]
        gt_polygons = [np.array(p, dtype=np.float64) for p in ann["segmentation"]]
        gt_bbox = BoxMode.convert(ann["bbox"], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)
        gt_bbox = np.array(gt_bbox)
        gt_bit_mask = polygons_to_bitmask(gt_polygons, height, width)

        # Run rasterize ..
        torch_gt_bbox = torch.Tensor(gt_bbox)[None, :].to(dtype=torch.float32)
        box_bitmasks = {
            "polygon": PolygonMasks([gt_polygons]).crop_and_resize(torch_gt_bbox, mask_side_len)[0],
            "gridsample": rasterize_polygons_with_grid_sample(gt_bit_mask, gt_bbox, mask_side_len),
            "roialign": BitMasks(torch.from_numpy(gt_bit_mask[None, :, :])).crop_and_resize(
                torch_gt_bbox, mask_side_len
            )[0],
        }

        # Run paste ..
        results = defaultdict(dict)
        for k, box_bitmask in box_bitmasks.items():
            padded_bitmask, scale = pad_masks(box_bitmask[None, :, :], 1)
            scaled_boxes = scale_boxes(torch_gt_bbox, scale)

            r = results[k]
            r["old"] = paste_mask_in_image_old(
                padded_bitmask[0], scaled_boxes[0], height, width, threshold=0.5
            )
            r["aligned"] = paste_masks_in_image(
                box_bitmask[None, :, :], Boxes(gt_bbox[None, :]), (height, width)
            )[0]

        table = []
        for rasterize_method, r in results.items():
            for paste_method, mask in r.items():
                mask = np.asarray(mask)
                iou = iou_between_full_image_bit_masks(gt_bit_mask.astype("uint8"), mask)
                table.append((rasterize_method, paste_method, iou))
        return table

    def test_polygon_area(self):
        # Draw polygon boxes
        for d in [5.0, 10.0, 1000.0]:
            polygon = PolygonMasks([[[0, 0, 0, d, d, d, d, 0]]])
            area = polygon.area()[0]
            target = d ** 2
            self.assertEqual(area, target)

        # Draw polygon triangles
        for d in [5.0, 10.0, 1000.0]:
            polygon = PolygonMasks([[[0, 0, 0, d, d, d]]])
            area = polygon.area()[0]
            target = d ** 2 / 2
            self.assertEqual(area, target)


def benchmark_paste():
    S = 800
    H, W = image_shape = (S, S)
    N = 64
    torch.manual_seed(42)
    masks = torch.rand(N, 28, 28)

    center = torch.rand(N, 2) * 600 + 100
    wh = torch.clamp(torch.randn(N, 2) * 40 + 200, min=50)
    x0y0 = torch.clamp(center - wh * 0.5, min=0.0)
    x1y1 = torch.clamp(center + wh * 0.5, max=S)
    boxes = Boxes(torch.cat([x0y0, x1y1], axis=1))

    def func(device, n=3):
        m = masks.to(device=device)
        b = boxes.to(device=device)

        def bench():
            for _ in range(n):
                paste_masks_in_image(m, b, image_shape)
            if device.type == "cuda":
                torch.cuda.synchronize()

        return bench

    specs = [{"device": torch.device("cpu"), "n": 3}]
    if torch.cuda.is_available():
        specs.append({"device": torch.device("cuda"), "n": 3})

    benchmark(func, "paste_masks", specs, num_iters=10, warmup_iters=2)


if __name__ == "__main__":
    benchmark_paste()
    unittest.main()
```

### tests/layers/test_roi_align_rotated.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import logging
import unittest
import cv2
import torch
from torch.autograd import Variable, gradcheck

from cvpods.layers.roi_align import ROIAlign
from cvpods.layers.roi_align_rotated import ROIAlignRotated

logger = logging.getLogger(__name__)


class ROIAlignRotatedTest(unittest.TestCase):
    def _box_to_rotated_box(self, box, angle):
        return [
            (box[0] + box[2]) / 2.0,
            (box[1] + box[3]) / 2.0,
            box[2] - box[0],
            box[3] - box[1],
            angle,
        ]

    def _rot90(self, img, num):
        num = num % 4  # note: -1 % 4 == 3
        for _ in range(num):
            img = img.transpose(0, 1).flip(0)
        return img

    def test_forward_output_0_90_180_270(self):
        for i in range(4):
            # i = 0, 1, 2, 3 corresponding to 0, 90, 180, 270 degrees
            img = torch.arange(25, dtype=torch.float32).reshape(5, 5)
            """
            0  1  2   3 4
            5  6  7   8 9
            10 11 12 13 14
            15 16 17 18 19
            20 21 22 23 24
            """
            box = [1, 1, 3, 3]
            rotated_box = self._box_to_rotated_box(box=box, angle=90 * i)

            result = self._simple_roi_align_rotated(img=img, box=rotated_box, resolution=(4, 4))

            # Here's an explanation for 0 degree case:
            # point 0 in the original input lies at [0.5, 0.5]
            # (the center of bin [0, 1] x [0, 1])
            # point 1 in the original input lies at [1.5, 0.5], etc.
            # since the resolution is (4, 4) that divides [1, 3] x [1, 3]
            # into 4 x 4 equal bins,
            # the top-left bin is [1, 1.5] x [1, 1.5], and its center
            # (1.25, 1.25) lies at the 3/4 position
            # between point 0 and point 1, point 5 and point 6,
            # point 0 and point 5, point 1 and point 6, so it can be calculated as
            # 0.25*(0*0.25+1*0.75)+(5*0.25+6*0.75)*0.75 = 4.5
            result_expected = torch.tensor(
                [
                    [4.5, 5.0, 5.5, 6.0],
                    [7.0, 7.5, 8.0, 8.5],
                    [9.5, 10.0, 10.5, 11.0],
                    [12.0, 12.5, 13.0, 13.5],
                ]
            )
            # This is also an upsampled version of [[6, 7], [11, 12]]

            # When the box is rotated by 90 degrees CCW,
            # the result would be rotated by 90 degrees CW, thus it's -i here
            result_expected = self._rot90(result_expected, -i)

            assert torch.allclose(result, result_expected)

    def test_resize(self):
        H, W = 30, 30
        input = torch.rand(H, W) * 100
        box = [10, 10, 20, 20]
        rotated_box = self._box_to_rotated_box(box, angle=0)
        output = self._simple_roi_align_rotated(img=input, box=rotated_box, resolution=(5, 5))

        input2x = cv2.resize(input.numpy(), (W // 2, H // 2), interpolation=cv2.INTER_LINEAR)
        input2x = torch.from_numpy(input2x)
        box2x = [x / 2 for x in box]
        rotated_box2x = self._box_to_rotated_box(box2x, angle=0)
        output2x = self._simple_roi_align_rotated(img=input2x, box=rotated_box2x, resolution=(5, 5))
        assert torch.allclose(output2x, output)

    def _simple_roi_align_rotated(self, img, box, resolution):
        """
        RoiAlignRotated with scale 1.0 and 0 sample ratio.
        """
        op = ROIAlignRotated(output_size=resolution, spatial_scale=1.0, sampling_ratio=0)
        input = img[None, None, :, :]

        rois = [0] + list(box)
        rois = torch.tensor(rois, dtype=torch.float32)[None, :]
        result_cpu = op.forward(input, rois)
        if torch.cuda.is_available():
            result_cuda = op.forward(input.cuda(), rois.cuda())
            assert torch.allclose(result_cpu, result_cuda.cpu())
        return result_cpu[0, 0]

    def test_empty_box(self):
        img = torch.rand(5, 5)
        out = self._simple_roi_align_rotated(img, [2, 3, 0, 0, 0], (7, 7))
        self.assertTrue((out == 0).all())

    def test_roi_align_rotated_gradcheck_cpu(self):
        dtype = torch.float64
        device = torch.device("cpu")
        roi_align_rotated_op = ROIAlignRotated(
            output_size=(5, 5), spatial_scale=0.5, sampling_ratio=1
        ).to(dtype=dtype, device=device)
        x = torch.rand(1, 1, 10, 10, dtype=dtype, device=device, requires_grad=True)
        # roi format is (batch index, x_center, y_center, width, height, angle)
        rois = torch.tensor(
            [[0, 4.5, 4.5, 9, 9, 0], [0, 2, 7, 4, 4, 0], [0, 7, 7, 4, 4, 0]],
            dtype=dtype,
            device=device,
        )

        def func(input):
            return roi_align_rotated_op(input, rois)

        assert gradcheck(func, (x,)), "gradcheck failed for RoIAlignRotated CPU"
        assert gradcheck(func, (x.transpose(2, 3),)), "gradcheck failed for RoIAlignRotated CPU"

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_roi_align_rotated_gradient_cuda(self):
        """
        Compute gradients for ROIAlignRotated with multiple bounding boxes on the GPU,
        and compare the result with ROIAlign
        """
        # torch.manual_seed(123)
        dtype = torch.float64
        device = torch.device("cuda")
        pool_h, pool_w = (5, 5)

        roi_align = ROIAlign(output_size=(pool_h, pool_w), spatial_scale=1, sampling_ratio=2).to(
            device=device
        )

        roi_align_rotated = ROIAlignRotated(
            output_size=(pool_h, pool_w), spatial_scale=1, sampling_ratio=2
        ).to(device=device)

        x = torch.rand(1, 1, 10, 10, dtype=dtype, device=device, requires_grad=True)
        # x_rotated = x.clone() won't work (will lead to grad_fun=CloneBackward)!
        x_rotated = Variable(x.data.clone(), requires_grad=True)

        # roi_rotated format is (batch index, x_center, y_center, width, height, angle)
        rois_rotated = torch.tensor(
            [[0, 4.5, 4.5, 9, 9, 0], [0, 2, 7, 4, 4, 0], [0, 7, 7, 4, 4, 0]],
            dtype=dtype,
            device=device,
        )

        y_rotated = roi_align_rotated(x_rotated, rois_rotated)
        s_rotated = y_rotated.sum()
        s_rotated.backward()

        # roi format is (batch index, x1, y1, x2, y2)
        rois = torch.tensor(
            [[0, 0, 0, 9, 9], [0, 0, 5, 4, 9], [0, 5, 5, 9, 9]], dtype=dtype, device=device
        )

        y = roi_align(x, rois)
        s = y.sum()
        s.backward()

        assert torch.allclose(
            x.grad, x_rotated.grad
        ), "gradients for ROIAlign and ROIAlignRotated mismatch on CUDA"


if __name__ == "__main__":
    unittest.main()
```

### tests/layers/test_nms_rotated.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from __future__ import absolute_import, division, print_function, unicode_literals
import numpy as np
import unittest
import torch
from torchvision import ops

from cvpods.layers import (
    batched_softnms,
    batched_softnms_rotated,
    batched_nms,
    batched_nms_rotated,
    nms_rotated
)


def nms_edit_distance(keep1, keep2):
    """
    Compare the "keep" result of two nms call.
    They are allowed to be different in terms of edit distance
    due to floating point precision issues, e.g.,
    if a box happen to have an IoU of 0.5 with another box,
    one implentation may choose to keep it while another may discard it.
    """
    if torch.equal(keep1, keep2):
        # they should be equal most of the time
        return 0
    keep1, keep2 = tuple(keep1.cpu()), tuple(keep2.cpu())
    m, n = len(keep1), len(keep2)

    # edit distance with DP
    f = [np.arange(n + 1), np.arange(n + 1)]
    for i in range(m):
        cur_row = i % 2
        other_row = (i + 1) % 2
        f[other_row][0] = i + 1
        for j in range(n):
            f[other_row][j + 1] = (
                f[cur_row][j]
                if keep1[i] == keep2[j]
                else min(min(f[cur_row][j], f[cur_row][j + 1]), f[other_row][j]) + 1
            )
    return f[m % 2][n]


class TestNMSRotated(unittest.TestCase):
    def reference_horizontal_nms(self, boxes, scores, iou_threshold):
        """
        Args:
            box_scores (N, 5): boxes in corner-form and probabilities.
                (Note here 5 == 4 + 1, i.e., 4-dim horizontal box + 1-dim prob)
            iou_threshold: intersection over union threshold.
        Returns:
             picked: a list of indexes of the kept boxes
        """
        picked = []
        _, indexes = scores.sort(descending=True)
        while len(indexes) > 0:
            current = indexes[0]
            picked.append(current.item())
            if len(indexes) == 1:
                break
            current_box = boxes[current, :]
            indexes = indexes[1:]
            rest_boxes = boxes[indexes, :]
            iou = ops.box_iou(rest_boxes, current_box.unsqueeze(0)).squeeze(1)
            indexes = indexes[iou <= iou_threshold]

        return torch.as_tensor(picked)

    def _create_tensors(self, N):
        boxes = torch.rand(N, 4) * 100
        # Note: the implementation of this function in torchvision is:
        # boxes[:, 2:] += torch.rand(N, 2) * 100
        # but it does not guarantee non-negative widths/heights constraints:
        # boxes[:, 2] >= boxes[:, 0] and boxes[:, 3] >= boxes[:, 1]:
        boxes[:, 2:] += boxes[:, :2]
        scores = torch.rand(N)
        return boxes, scores

    def _rotate_box(self, box, deg):
        assert len(box.shape) == 2 and box.shape[1] == 5, box.shape  # (x, y, w, h, a)
        assert isinstance(deg, float)

        rad = torch.Tensor([np.deg2rad(deg)])
        box = box.clone()
        rotation_mat = torch.Tensor([[torch.cos(rad), -torch.sin(rad)],
                                     [torch.sin(rad), torch.cos(rad)]])

        box[:, :2] = torch.matmul(box[:, :2], rotation_mat)
        box[:, 4] += deg
        return box

    def test_batched_nms_rotated_0_degree_cpu(self):
        # torch.manual_seed(0)
        N = 2000
        num_classes = 50
        boxes, scores = self._create_tensors(N)
        idxs = torch.randint(0, num_classes, (N,))
        rotated_boxes = torch.zeros(N, 5)
        rotated_boxes[:, 0] = (boxes[:, 0] + boxes[:, 2]) / 2.0
        rotated_boxes[:, 1] = (boxes[:, 1] + boxes[:, 3]) / 2.0
        rotated_boxes[:, 2] = boxes[:, 2] - boxes[:, 0]
        rotated_boxes[:, 3] = boxes[:, 3] - boxes[:, 1]
        err_msg = "Rotated NMS with 0 degree is incompatible with horizontal NMS for IoU={}"
        for iou in [0.2, 0.5, 0.8]:
            backup = boxes.clone()
            keep_ref = batched_nms(boxes, scores, idxs, iou)
            assert torch.allclose(boxes, backup), "boxes modified by batched_nms"
            backup = rotated_boxes.clone()
            keep = batched_nms_rotated(rotated_boxes, scores, idxs, iou)
            assert torch.allclose(
                rotated_boxes, backup
            ), "rotated_boxes modified by batched_nms_rotated"
            self.assertLessEqual(nms_edit_distance(keep, keep_ref), 5, err_msg.format(iou))

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_batched_nms_rotated_0_degree_cuda(self):
        # torch.manual_seed(0)
        N = 2000
        num_classes = 50
        boxes, scores = self._create_tensors(N)
        idxs = torch.randint(0, num_classes, (N,))
        rotated_boxes = torch.zeros(N, 5)
        rotated_boxes[:, 0] = (boxes[:, 0] + boxes[:, 2]) / 2.0
        rotated_boxes[:, 1] = (boxes[:, 1] + boxes[:, 3]) / 2.0
        rotated_boxes[:, 2] = boxes[:, 2] - boxes[:, 0]
        rotated_boxes[:, 3] = boxes[:, 3] - boxes[:, 1]
        err_msg = "Rotated NMS with 0 degree is incompatible with horizontal NMS for IoU={}"
        for iou in [0.2, 0.5, 0.8]:
            backup = boxes.clone()
            keep_ref = batched_nms(boxes.cuda(), scores.cuda(), idxs, iou)
            assert torch.allclose(boxes, backup), "boxes modified by batched_nms"
            backup = rotated_boxes.clone()
            keep = batched_nms_rotated(rotated_boxes.cuda(), scores.cuda(), idxs, iou)
            assert torch.allclose(
                rotated_boxes, backup
            ), "rotated_boxes modified by batched_nms_rotated"
            self.assertLessEqual(nms_edit_distance(keep, keep_ref), 1, err_msg.format(iou))

    def test_batched_softnms_rotated_random_degree_cpu(self, device="cpu"):
        torch.manual_seed(0)
        NUM_ANGLES = 5
        N = 2000
        num_classes = 50
        boxes, scores = self._create_tensors(N)
        idxs = torch.randint(0, num_classes, (N,))
        rotated_boxes = torch.zeros(N, 5)
        rotated_boxes[:, 0] = (boxes[:, 0] + boxes[:, 2]) / 2.0
        rotated_boxes[:, 1] = (boxes[:, 1] + boxes[:, 3]) / 2.0
        rotated_boxes[:, 2] = boxes[:, 2] - boxes[:, 0]
        rotated_boxes[:, 3] = boxes[:, 3] - boxes[:, 1]

        backup = boxes.clone()
        rotated_backup = rotated_boxes.clone()
        scores_backup = scores.clone()
        err_msg = "Rotated NMS with 0 degree is incompatible with " \
                  "horizontal NMS for IoU={} Degree#{}={} Max(Error)={}"
        for i in range(NUM_ANGLES):
            deg = np.random.random(1)[0] * 180
            # deg = 60.0
            for iou in [0.2, 0.5, 0.8]:
                boxes = backup.clone().to(device)
                scores_ref = scores_backup.clone().to(device)
                keep_ref = batched_softnms(boxes, scores_ref, idxs.to(device), iou,
                                           score_threshold=0)
                assert torch.allclose(boxes, backup.to(device)), "boxes modified by batched_nms"
                assert torch.equal(
                    keep_ref,
                    scores_ref.argsort(descending=True)), "keep not sorted according to scores"

                scores = scores_backup.clone().to(device)
                rotated_boxes = self._rotate_box(rotated_backup, deg).to(device)
                rotated_boxes_ref = rotated_boxes.clone().to(device)

                keep = batched_softnms_rotated(rotated_boxes, scores, idxs, iou,
                                               score_threshold=0)
                assert torch.allclose(
                    rotated_boxes, rotated_boxes_ref
                ), "rotated_boxes modified by batched_nms_rotated"
                assert torch.equal(
                    keep,
                    scores.argsort(descending=True)), "keep not sorted according to scores"
                assert (scores - scores_ref).abs().max() < 0.0001, \
                    err_msg.format(iou, i, deg, (scores - scores_ref).abs().max())

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_batched_softnms_rotated_random_degree_cuda(self):
        self.test_batched_softnms_rotated_random_degree_cpu(device="cuda")

    def test_nms_rotated_0_degree_cpu(self):
        N = 1000
        boxes, scores = self._create_tensors(N)
        rotated_boxes = torch.zeros(N, 5)
        rotated_boxes[:, 0] = (boxes[:, 0] + boxes[:, 2]) / 2.0
        rotated_boxes[:, 1] = (boxes[:, 1] + boxes[:, 3]) / 2.0
        rotated_boxes[:, 2] = boxes[:, 2] - boxes[:, 0]
        rotated_boxes[:, 3] = boxes[:, 3] - boxes[:, 1]
        err_msg = "Rotated NMS incompatible between CPU and reference implementation for IoU={}"
        for iou in [0.5]:
            keep_ref = self.reference_horizontal_nms(boxes, scores, iou)
            keep = nms_rotated(rotated_boxes, scores, iou)
            self.assertLessEqual(nms_edit_distance(keep, keep_ref), 1, err_msg.format(iou))

    def test_nms_rotated_90_degrees_cpu(self):
        N = 1000
        boxes, scores = self._create_tensors(N)
        rotated_boxes = torch.zeros(N, 5)
        rotated_boxes[:, 0] = (boxes[:, 0] + boxes[:, 2]) / 2.0
        rotated_boxes[:, 1] = (boxes[:, 1] + boxes[:, 3]) / 2.0
        # Note for rotated_boxes[:, 2] and rotated_boxes[:, 3]:
        # widths and heights are intentionally swapped here for 90 degrees case
        # so that the reference horizontal nms could be used
        rotated_boxes[:, 2] = boxes[:, 3] - boxes[:, 1]
        rotated_boxes[:, 3] = boxes[:, 2] - boxes[:, 0]

        rotated_boxes[:, 4] = torch.ones(N) * 90
        err_msg = "Rotated NMS incompatible between CPU and reference implementation for IoU={}"
        for iou in [0.2, 0.5, 0.8]:
            keep_ref = self.reference_horizontal_nms(boxes, scores, iou)
            keep = nms_rotated(rotated_boxes, scores, iou)
            self.assertLessEqual(nms_edit_distance(keep, keep_ref), 1, err_msg.format(iou))

    def test_nms_rotated_180_degrees_cpu(self):
        N = 1000
        boxes, scores = self._create_tensors(N)
        rotated_boxes = torch.zeros(N, 5)
        rotated_boxes[:, 0] = (boxes[:, 0] + boxes[:, 2]) / 2.0
        rotated_boxes[:, 1] = (boxes[:, 1] + boxes[:, 3]) / 2.0
        rotated_boxes[:, 2] = boxes[:, 2] - boxes[:, 0]
        rotated_boxes[:, 3] = boxes[:, 3] - boxes[:, 1]
        rotated_boxes[:, 4] = torch.ones(N) * 180
        err_msg = "Rotated NMS incompatible between CPU and reference implementation for IoU={}"
        for iou in [0.2, 0.5, 0.8]:
            keep_ref = self.reference_horizontal_nms(boxes, scores, iou)
            keep = nms_rotated(rotated_boxes, scores, iou)
            self.assertLessEqual(nms_edit_distance(keep, keep_ref), 1, err_msg.format(iou))

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_nms_rotated_0_degree_cuda(self):
        N = 1000
        boxes, scores = self._create_tensors(N)
        rotated_boxes = torch.zeros(N, 5)
        rotated_boxes[:, 0] = (boxes[:, 0] + boxes[:, 2]) / 2.0
        rotated_boxes[:, 1] = (boxes[:, 1] + boxes[:, 3]) / 2.0
        rotated_boxes[:, 2] = boxes[:, 2] - boxes[:, 0]
        rotated_boxes[:, 3] = boxes[:, 3] - boxes[:, 1]
        err_msg = "Rotated NMS incompatible between CPU and CUDA for IoU={}"

        for iou in [0.2, 0.5, 0.8]:
            r_cpu = nms_rotated(rotated_boxes, scores, iou)
            r_cuda = nms_rotated(rotated_boxes.cuda(), scores.cuda(), iou)

            self.assertLessEqual(nms_edit_distance(r_cpu, r_cuda.cpu()), 1, err_msg.format(iou))

    def test_batched_softnms_rotated_90_degree_cpu(self):
        # torch.manual_seed(0)
        N = 2000
        num_classes = 50
        boxes, scores = self._create_tensors(N)
        idxs = torch.randint(0, num_classes, (N,))
        rotated_boxes = torch.zeros(N, 5)
        rotated_boxes[:, 0] = (boxes[:, 0] + boxes[:, 2]) / 2.0
        rotated_boxes[:, 1] = (boxes[:, 1] + boxes[:, 3]) / 2.0
        rotated_boxes[:, 2] = boxes[:, 3] - boxes[:, 1]
        rotated_boxes[:, 3] = boxes[:, 2] - boxes[:, 0]
        rotated_boxes[:, 4] = torch.ones(N) * 90
        err_msg = "Rotated NMS with 0 degree is incompatible with horizontal NMS for IoU={}"
        pass_times = 0
        for iou in [0.2, 0.5, 0.8]:
            backup = boxes.clone()
            scores_ref = scores.clone()
            keep_ref = batched_softnms(boxes, scores_ref, idxs, iou)
            assert torch.allclose(boxes, backup), "boxes modified by batched_nms"
            backup = rotated_boxes.clone()
            keep = batched_softnms_rotated(rotated_boxes, scores, idxs, iou)
            assert torch.allclose(
                rotated_boxes, backup
            ), "rotated_boxes modified by batched_nms_rotated"

            if nms_edit_distance(keep, keep_ref) <= 1:
                pass_times += 1

            assert torch.equal(keep, keep_ref) and (scores - scores_ref).abs().max() < 0.0001, \
                   err_msg.format(iou)

        assert pass_times > 0, \
            "Rotated NMS with 0 degree is incompatible with horizontal NMS for IoU={}".format(
                [0.2, 0.5, 0.8])

    def test_batched_softnms_rotated_180_degree_cpu(self):
        # torch.manual_seed(0)
        N = 2000
        num_classes = 50
        boxes, scores = self._create_tensors(N)
        idxs = torch.randint(0, num_classes, (N,))
        rotated_boxes = torch.zeros(N, 5)
        rotated_boxes[:, 0] = (boxes[:, 0] + boxes[:, 2]) / 2.0
        rotated_boxes[:, 1] = (boxes[:, 1] + boxes[:, 3]) / 2.0
        rotated_boxes[:, 2] = boxes[:, 2] - boxes[:, 0]
        rotated_boxes[:, 3] = boxes[:, 3] - boxes[:, 1]
        rotated_boxes[:, 4] = torch.ones(N) * 180
        err_msg = "Rotated NMS with 0 degree is incompatible with horizontal NMS for IoU={}"
        pass_times = 0
        for iou in [0.2, 0.5, 0.8]:
            backup = boxes.clone()
            scores_ref = scores.clone()
            keep_ref = batched_softnms(boxes, scores_ref, idxs, iou)
            assert torch.allclose(boxes, backup), "boxes modified by batched_nms"
            backup = rotated_boxes.clone()
            keep = batched_softnms_rotated(rotated_boxes, scores, idxs, iou)
            assert torch.allclose(
                rotated_boxes, backup
            ), "rotated_boxes modified by batched_nms_rotated"

            if nms_edit_distance(keep, keep_ref) <= 1:
                pass_times += 1

            assert torch.equal(keep, keep_ref) and (scores - scores_ref).abs().max() < 0.0001, \
                   err_msg.format(iou)

        assert pass_times > 0, \
            "Rotated NMS with 0 degree is incompatible with horizontal NMS for IoU={}".format(
                [0.2, 0.5, 0.8])


if __name__ == "__main__":
    unittest.main()
```

### tests/structures/test_imagelist.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

import unittest
from typing import List, Tuple
import torch

from cvpods.structures import ImageList
from cvpods.utils.env import TORCH_VERSION


class TestImageList(unittest.TestCase):
    def test_imagelist_padding_shape(self):
        ret = ImageList.from_tensors([torch.ones((3, 15, 20), dtype=torch.float32)], 4).tensor
        self.assertEqual(list(ret.shape), [1, 3, 16, 20], str(ret.shape))

        ret = ImageList.from_tensors(
            [
                torch.ones((3, 25, 20), dtype=torch.float32),
                torch.ones((3, 10, 10), dtype=torch.float32),
            ], 4
        ).tensor
        self.assertEqual(list(ret.shape), [2, 3, 28, 20], str(ret.shape))

    # TODO: Make cvpods.structures.ImageList scriptable
    @unittest.skip("Don't support scriptable cvpods.structures.ImageList")
    @unittest.skipIf(TORCH_VERSION < (1, 6), "Insufficient pytorch version")
    def test_imagelist_scriptability(self):
        image_nums = 2
        image_tensor = torch.randn((image_nums, 10, 20), dtype=torch.float32)
        image_shape = [(10, 20)] * image_nums

        def f(image_tensor, image_shape: List[Tuple[int, int]]):
            return ImageList(image_tensor, image_shape)

        ret = f(image_tensor, image_shape)
        ret_script = torch.jit.script(f)(image_tensor, image_shape)

        self.assertEqual(len(ret), len(ret_script))
        for i in range(image_nums):
            self.assertTrue(torch.equal(ret[i], ret_script[i]))


if __name__ == "__main__":
    unittest.main()
```

### tests/structures/test_masks.py

```python
import unittest
import torch

from cvpods.structures.masks import BitMasks, PolygonMasks, polygons_to_bitmask


class TestBitMask(unittest.TestCase):
    # TODO: fix this
    @unittest.skip("WIP")
    def test_get_bounding_box(self):
        masks = torch.tensor(
            [
                [
                    [False, False, False, True],
                    [False, False, True, True],
                    [False, True, True, False],
                    [False, True, True, False],
                ],
                [
                    [False, False, False, False],
                    [False, False, True, False],
                    [False, True, True, False],
                    [False, True, True, False],
                ],
                torch.zeros(4, 4),
            ]
        )
        bitmask = BitMasks(masks)
        box_true = torch.tensor([[1, 0, 4, 4], [1, 1, 3, 4], [0, 0, 0, 0]], dtype=torch.float32)
        box = bitmask.get_bounding_boxes()
        self.assertTrue(torch.all(box.tensor == box_true).item(),
                        "box: {}\nbox_true: {}".format(box, box_true))

        for box in box_true:
            poly = box[[0, 1, 2, 1, 2, 3, 0, 3]].numpy()
            mask = polygons_to_bitmask([poly], 4, 4)
            reconstruct_box = BitMasks(mask[None, :, :]).get_bounding_boxes()[0].tensor
            self.assertTrue(torch.all(box == reconstruct_box).item())

            reconstruct_box = PolygonMasks([[poly]]).get_bounding_boxes()[0].tensor
            self.assertTrue(torch.all(box == reconstruct_box).item())


if __name__ == "__main__":
    unittest.main()
```

### tests/structures/test_rotated_boxes.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
from __future__ import absolute_import, division, print_function, unicode_literals
import logging
import math
import random
import unittest
import torch
from cvpods.utils import benchmark

from cvpods.layers.rotated_boxes import pairwise_iou_rotated
from cvpods.structures.boxes import Boxes
from cvpods.structures.rotated_boxes import RotatedBoxes, pairwise_iou

logger = logging.getLogger(__name__)


class TestRotatedBoxesLayer(unittest.TestCase):
    def test_iou_0_dim_cpu(self):
        boxes1 = torch.rand(0, 5, dtype=torch.float32)
        boxes2 = torch.rand(10, 5, dtype=torch.float32)
        expected_ious = torch.zeros(0, 10, dtype=torch.float32)
        ious = pairwise_iou_rotated(boxes1, boxes2)
        self.assertTrue(torch.allclose(ious, expected_ious))

        boxes1 = torch.rand(10, 5, dtype=torch.float32)
        boxes2 = torch.rand(0, 5, dtype=torch.float32)
        expected_ious = torch.zeros(10, 0, dtype=torch.float32)
        ious = pairwise_iou_rotated(boxes1, boxes2)
        self.assertTrue(torch.allclose(ious, expected_ious))

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_iou_0_dim_cuda(self):
        boxes1 = torch.rand(0, 5, dtype=torch.float32)
        boxes2 = torch.rand(10, 5, dtype=torch.float32)
        expected_ious = torch.zeros(0, 10, dtype=torch.float32)
        ious_cuda = pairwise_iou_rotated(boxes1.cuda(), boxes2.cuda())
        self.assertTrue(torch.allclose(ious_cuda.cpu(), expected_ious))

        boxes1 = torch.rand(10, 5, dtype=torch.float32)
        boxes2 = torch.rand(0, 5, dtype=torch.float32)
        expected_ious = torch.zeros(10, 0, dtype=torch.float32)
        ious_cuda = pairwise_iou_rotated(boxes1.cuda(), boxes2.cuda())
        self.assertTrue(torch.allclose(ious_cuda.cpu(), expected_ious))

    def test_iou_half_overlap_cpu(self):
        boxes1 = torch.tensor([[0.5, 0.5, 1.0, 1.0, 0.0]], dtype=torch.float32)
        boxes2 = torch.tensor([[0.25, 0.5, 0.5, 1.0, 0.0]], dtype=torch.float32)
        expected_ious = torch.tensor([[0.5]], dtype=torch.float32)
        ious = pairwise_iou_rotated(boxes1, boxes2)
        self.assertTrue(torch.allclose(ious, expected_ious))

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_iou_half_overlap_cuda(self):
        boxes1 = torch.tensor([[0.5, 0.5, 1.0, 1.0, 0.0]], dtype=torch.float32)
        boxes2 = torch.tensor([[0.25, 0.5, 0.5, 1.0, 0.0]], dtype=torch.float32)
        expected_ious = torch.tensor([[0.5]], dtype=torch.float32)
        ious_cuda = pairwise_iou_rotated(boxes1.cuda(), boxes2.cuda())
        self.assertTrue(torch.allclose(ious_cuda.cpu(), expected_ious))

    def test_iou_precision(self):
        for device in ["cpu"] + ["cuda"] if torch.cuda.is_available() else []:
            boxes1 = torch.tensor([[565, 565, 10, 10.0, 0]], dtype=torch.float32, device=device)
            boxes2 = torch.tensor([[565, 565, 10, 8.3, 0]], dtype=torch.float32, device=device)
            iou = 8.3 / 10.0
            expected_ious = torch.tensor([[iou]], dtype=torch.float32)
            ious = pairwise_iou_rotated(boxes1, boxes2)
            self.assertTrue(torch.allclose(ious.cpu(), expected_ious))

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_iou_too_many_boxes_cuda(self):
        s1, s2 = 5, 1289035
        boxes1 = torch.zeros(s1, 5)
        boxes2 = torch.zeros(s2, 5)
        ious_cuda = pairwise_iou_rotated(boxes1.cuda(), boxes2.cuda())
        self.assertTupleEqual(tuple(ious_cuda.shape), (s1, s2))

    # TODO: fix this
    @unittest.skip("Test failure")
    def test_iou_extreme(self):
        # Cause floating point issues in cuda kernels (#1266)
        # See: https://github.com/facebookresearch/detectron2/issues/1266
        for device in ["cpu"] + ["cuda"] if torch.cuda.is_available() else []:
            boxes1 = torch.tensor([[160.0, 153.0, 230.0, 23.0, -37.0]], device=device)
            boxes2 = torch.tensor(
                [
                    [
                        -1.117407639806935e17,
                        1.3858420478349148e18,
                        1000.0000610351562,
                        1000.0000610351562,
                        1612.0,
                    ]
                ],
                device=device,
            )
            ious = pairwise_iou_rotated(boxes1, boxes2)
            self.assertTrue(ious.min() >= 0, ious)


class TestRotatedBoxesStructure(unittest.TestCase):
    def test_clip_area_0_degree(self):
        for _ in range(50):
            num_boxes = 100
            boxes_5d = torch.zeros(num_boxes, 5)
            boxes_5d[:, 0] = torch.FloatTensor(num_boxes).uniform_(-100, 500)
            boxes_5d[:, 1] = torch.FloatTensor(num_boxes).uniform_(-100, 500)
            boxes_5d[:, 2] = torch.FloatTensor(num_boxes).uniform_(0, 500)
            boxes_5d[:, 3] = torch.FloatTensor(num_boxes).uniform_(0, 500)
            # Convert from (x_ctr, y_ctr, w, h, 0) to  (x1, y1, x2, y2)
            boxes_4d = torch.zeros(num_boxes, 4)
            boxes_4d[:, 0] = boxes_5d[:, 0] - boxes_5d[:, 2] / 2.0
            boxes_4d[:, 1] = boxes_5d[:, 1] - boxes_5d[:, 3] / 2.0
            boxes_4d[:, 2] = boxes_5d[:, 0] + boxes_5d[:, 2] / 2.0
            boxes_4d[:, 3] = boxes_5d[:, 1] + boxes_5d[:, 3] / 2.0

            image_size = (500, 600)
            test_boxes_4d = Boxes(boxes_4d)
            test_boxes_5d = RotatedBoxes(boxes_5d)
            # Before clip
            areas_4d = test_boxes_4d.area()
            areas_5d = test_boxes_5d.area()
            self.assertTrue(torch.allclose(areas_4d, areas_5d, atol=1e-1, rtol=1e-5))
            # After clip
            test_boxes_4d.clip(image_size)
            test_boxes_5d.clip(image_size)
            areas_4d = test_boxes_4d.area()
            areas_5d = test_boxes_5d.area()
            self.assertTrue(torch.allclose(areas_4d, areas_5d, atol=1e-1, rtol=1e-5))

    def test_clip_area_arbitrary_angle(self):
        num_boxes = 100
        boxes_5d = torch.zeros(num_boxes, 5)
        boxes_5d[:, 0] = torch.FloatTensor(num_boxes).uniform_(-100, 500)
        boxes_5d[:, 1] = torch.FloatTensor(num_boxes).uniform_(-100, 500)
        boxes_5d[:, 2] = torch.FloatTensor(num_boxes).uniform_(0, 500)
        boxes_5d[:, 3] = torch.FloatTensor(num_boxes).uniform_(0, 500)
        boxes_5d[:, 4] = torch.FloatTensor(num_boxes).uniform_(-1800, 1800)
        clip_angle_threshold = random.uniform(0, 180)

        image_size = (500, 600)
        test_boxes_5d = RotatedBoxes(boxes_5d)
        # Before clip
        areas_before = test_boxes_5d.area()
        # After clip
        test_boxes_5d.clip(image_size, clip_angle_threshold)
        areas_diff = test_boxes_5d.area() - areas_before

        # the areas should only decrease after clipping
        self.assertTrue(torch.all(areas_diff <= 0))
        # whenever the box is clipped (thus the area shrinks),
        # the angle for the box must be within the clip_angle_threshold
        # Note that the clip function will normalize the angle range
        # to be within (-180, 180]
        self.assertTrue(
            torch.all(torch.abs(boxes_5d[:, 4][torch.where(areas_diff < 0)]) < clip_angle_threshold)
        )

    def test_normalize_angles(self):
        # torch.manual_seed(0)
        for _ in range(50):
            num_boxes = 100
            boxes_5d = torch.zeros(num_boxes, 5)
            boxes_5d[:, 0] = torch.FloatTensor(num_boxes).uniform_(-100, 500)
            boxes_5d[:, 1] = torch.FloatTensor(num_boxes).uniform_(-100, 500)
            boxes_5d[:, 2] = torch.FloatTensor(num_boxes).uniform_(0, 500)
            boxes_5d[:, 3] = torch.FloatTensor(num_boxes).uniform_(0, 500)
            boxes_5d[:, 4] = torch.FloatTensor(num_boxes).uniform_(-1800, 1800)
            rotated_boxes = RotatedBoxes(boxes_5d)
            normalized_boxes = rotated_boxes.clone()
            normalized_boxes.normalize_angles()
            self.assertTrue(torch.all(normalized_boxes.tensor[:, 4] >= -180))
            self.assertTrue(torch.all(normalized_boxes.tensor[:, 4] < 180))
            # x, y, w, h should not change
            self.assertTrue(torch.allclose(boxes_5d[:, :4], normalized_boxes.tensor[:, :4]))
            # the cos/sin values of the angles should stay the same

            self.assertTrue(
                torch.allclose(
                    torch.cos(boxes_5d[:, 4] * math.pi / 180),
                    torch.cos(normalized_boxes.tensor[:, 4] * math.pi / 180),
                    atol=1e-5,
                )
            )

            self.assertTrue(
                torch.allclose(
                    torch.sin(boxes_5d[:, 4] * math.pi / 180),
                    torch.sin(normalized_boxes.tensor[:, 4] * math.pi / 180),
                    atol=1e-5,
                )
            )

    def test_pairwise_iou_0_degree(self):
        for device in ["cpu"] + ["cuda"] if torch.cuda.is_available() else []:
            boxes1 = torch.tensor(
                [[0.5, 0.5, 1.0, 1.0, 0.0], [0.5, 0.5, 1.0, 1.0, 0.0]],
                dtype=torch.float32,
                device=device,
            )
            boxes2 = torch.tensor(
                [
                    [0.5, 0.5, 1.0, 1.0, 0.0],
                    [0.25, 0.5, 0.5, 1.0, 0.0],
                    [0.5, 0.25, 1.0, 0.5, 0.0],
                    [0.25, 0.25, 0.5, 0.5, 0.0],
                    [0.75, 0.75, 0.5, 0.5, 0.0],
                    [1.0, 1.0, 1.0, 1.0, 0.0],
                ],
                dtype=torch.float32,
                device=device,
            )
            expected_ious = torch.tensor(
                [
                    [1.0, 0.5, 0.5, 0.25, 0.25, 0.25 / (2 - 0.25)],
                    [1.0, 0.5, 0.5, 0.25, 0.25, 0.25 / (2 - 0.25)],
                ],
                dtype=torch.float32,
                device=device,
            )
            ious = pairwise_iou(RotatedBoxes(boxes1), RotatedBoxes(boxes2))
            self.assertTrue(torch.allclose(ious, expected_ious))

    def test_pairwise_iou_45_degrees(self):
        for device in ["cpu"] + ["cuda"] if torch.cuda.is_available() else []:
            boxes1 = torch.tensor(
                [
                    [1, 1, math.sqrt(2), math.sqrt(2), 45],
                    [1, 1, 2 * math.sqrt(2), 2 * math.sqrt(2), -45],
                ],
                dtype=torch.float32,
                device=device,
            )
            boxes2 = torch.tensor([[1, 1, 2, 2, 0]], dtype=torch.float32, device=device)
            expected_ious = torch.tensor([[0.5], [0.5]], dtype=torch.float32, device=device)
            ious = pairwise_iou(RotatedBoxes(boxes1), RotatedBoxes(boxes2))
            self.assertTrue(torch.allclose(ious, expected_ious))

    def test_pairwise_iou_orthogonal(self):
        for device in ["cpu"] + ["cuda"] if torch.cuda.is_available() else []:
            boxes1 = torch.tensor([[5, 5, 10, 6, 55]], dtype=torch.float32, device=device)
            boxes2 = torch.tensor([[5, 5, 10, 6, -35]], dtype=torch.float32, device=device)
            iou = (6.0 * 6.0) / (6.0 * 6.0 + 4.0 * 6.0 + 4.0 * 6.0)
            expected_ious = torch.tensor([[iou]], dtype=torch.float32, device=device)
            ious = pairwise_iou(RotatedBoxes(boxes1), RotatedBoxes(boxes2))
            self.assertTrue(torch.allclose(ious, expected_ious))

    def test_pairwise_iou_large_close_boxes(self):
        for device in ["cpu"] + ["cuda"] if torch.cuda.is_available() else []:
            boxes1 = torch.tensor(
                [[299.500000, 417.370422, 600.000000, 364.259186, 27.1828]],
                dtype=torch.float32,
                device=device,
            )
            boxes2 = torch.tensor(
                [[299.500000, 417.370422, 600.000000, 364.259155, 27.1828]],
                dtype=torch.float32,
                device=device,
            )
            iou = 364.259155 / 364.259186
            expected_ious = torch.tensor([[iou]], dtype=torch.float32, device=device)
            ious = pairwise_iou(RotatedBoxes(boxes1), RotatedBoxes(boxes2))
            self.assertTrue(torch.allclose(ious, expected_ious))

    def test_pairwise_iou_many_boxes(self):
        for device in ["cpu"] + ["cuda"] if torch.cuda.is_available() else []:
            num_boxes1 = 100
            num_boxes2 = 200
            boxes1 = torch.stack(
                [
                    torch.tensor(
                        [5 + 20 * i, 5 + 20 * i, 10, 10, 0], dtype=torch.float32, device=device
                    )
                    for i in range(num_boxes1)
                ]
            )
            boxes2 = torch.stack(
                [
                    torch.tensor(
                        [5 + 20 * i, 5 + 20 * i, 10, 1 + 9 * i / num_boxes2, 0],
                        dtype=torch.float32,
                        device=device,
                    )
                    for i in range(num_boxes2)
                ]
            )
            expected_ious = torch.zeros(num_boxes1, num_boxes2, dtype=torch.float32, device=device)
            for i in range(min(num_boxes1, num_boxes2)):
                expected_ious[i][i] = (1 + 9 * i / num_boxes2) / 10.0
            ious = pairwise_iou(RotatedBoxes(boxes1), RotatedBoxes(boxes2))
            self.assertTrue(torch.allclose(ious, expected_ious))

    def test_pairwise_iou_issue1207_simplified(self):
        for device in ["cpu"] + ["cuda"] if torch.cuda.is_available() else []:
            # Simplified test case of D2-issue-1207
            # See: https://github.com/facebookresearch/detectron2/issues/1207
            boxes1 = torch.tensor([[3, 3, 8, 2, -45.0]], device=device)
            boxes2 = torch.tensor([[6, 0, 8, 2, -45.0]], device=device)
            iou = 0.0
            expected_ious = torch.tensor([[iou]], dtype=torch.float32, device=device)

            ious = pairwise_iou(RotatedBoxes(boxes1), RotatedBoxes(boxes2))
            self.assertTrue(torch.allclose(ious, expected_ious))

    def test_pairwise_iou_issue1207(self):
        for device in ["cpu"] + ["cuda"] if torch.cuda.is_available() else []:
            # The original test case in D2-issue-1207
            # See: https://github.com/facebookresearch/detectron2/issues/1207
            boxes1 = torch.tensor([[160.0, 153.0, 230.0, 23.0, -37.0]], device=device)
            boxes2 = torch.tensor([[190.0, 127.0, 80.0, 21.0, -46.0]], device=device)

            iou = 0.0
            expected_ious = torch.tensor([[iou]], dtype=torch.float32, device=device)

            ious = pairwise_iou(RotatedBoxes(boxes1), RotatedBoxes(boxes2))
            self.assertTrue(torch.allclose(ious, expected_ious))

    def test_empty_cat(self):
        x = RotatedBoxes.cat([])
        self.assertTrue(x.tensor.shape, (0, 5))


def benchmark_rotated_iou():
    num_boxes1 = 200
    num_boxes2 = 500
    boxes1 = torch.stack(
        [
            torch.tensor([5 + 20 * i, 5 + 20 * i, 10, 10, 0], dtype=torch.float32)
            for i in range(num_boxes1)
        ]
    )
    boxes2 = torch.stack(
        [
            torch.tensor(
                [5 + 20 * i, 5 + 20 * i, 10, 1 + 9 * i / num_boxes2, 0], dtype=torch.float32
            )
            for i in range(num_boxes2)
        ]
    )

    def func(dev, n=1):
        b1 = boxes1.to(device=dev)
        b2 = boxes2.to(device=dev)

        def bench():
            for _ in range(n):
                pairwise_iou_rotated(b1, b2)
            if dev.type == "cuda":
                torch.cuda.synchronize()

        return bench

    # only run it once per timed loop, since it's slow
    args = [{"dev": torch.device("cpu"), "n": 1}]
    if torch.cuda.is_available():
        args.append({"dev": torch.device("cuda"), "n": 10})

    benchmark(func, "rotated_iou", args, warmup_iters=3)


if __name__ == "__main__":
    unittest.main()
    benchmark_rotated_iou()
```

### tests/structures/test_boxes.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import json
import math
import numpy as np
import unittest
import torch

from cvpods.structures import Boxes, BoxMode, pairwise_iou
from cvpods.utils.env import TORCH_VERSION


class TestBoxMode(unittest.TestCase):
    def _convert_xy_to_wh(self, x):
        return BoxMode.convert(x, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)

    def _convert_xywha_to_xyxy(self, x):
        return BoxMode.convert(x, BoxMode.XYWHA_ABS, BoxMode.XYXY_ABS)

    def _convert_xywh_to_xywha(self, x):
        return BoxMode.convert(x, BoxMode.XYWH_ABS, BoxMode.XYWHA_ABS)

    def test_box_convert_list(self):
        for tp in [list, tuple]:
            box = tp([5, 5, 10, 10])
            output = self._convert_xy_to_wh(box)
            self.assertIsInstance(output, tp)
            self.assertEqual(output, tp([5, 5, 5, 5]))

            with self.assertRaises(Exception):
                self._convert_xy_to_wh([box])

    def test_box_convert_array(self):
        box = np.asarray([[5, 5, 10, 10], [1, 1, 2, 3]])
        output = self._convert_xy_to_wh(box)
        self.assertEqual(output.dtype, box.dtype)
        self.assertEqual(output.shape, box.shape)
        self.assertTrue((output[0] == [5, 5, 5, 5]).all())
        self.assertTrue((output[1] == [1, 1, 1, 2]).all())

    def test_box_convert_cpu_tensor(self):
        box = torch.tensor([[5, 5, 10, 10], [1, 1, 2, 3]])
        output = self._convert_xy_to_wh(box)
        self.assertEqual(output.dtype, box.dtype)
        self.assertEqual(output.shape, box.shape)
        output = output.numpy()
        self.assertTrue((output[0] == [5, 5, 5, 5]).all())
        self.assertTrue((output[1] == [1, 1, 1, 2]).all())

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_box_convert_cuda_tensor(self):
        box = torch.tensor([[5, 5, 10, 10], [1, 1, 2, 3]]).cuda()
        output = self._convert_xy_to_wh(box)
        self.assertEqual(output.dtype, box.dtype)
        self.assertEqual(output.shape, box.shape)
        self.assertEqual(output.device, box.device)
        output = output.cpu().numpy()
        self.assertTrue((output[0] == [5, 5, 5, 5]).all())
        self.assertTrue((output[1] == [1, 1, 1, 2]).all())

    def test_box_convert_xywha_to_xyxy_list(self):
        for tp in [list, tuple]:
            box = tp([50, 50, 30, 20, 0])
            output = self._convert_xywha_to_xyxy(box)
            self.assertIsInstance(output, tp)
            self.assertEqual(output, tp([35, 40, 65, 60]))

            with self.assertRaises(Exception):
                self._convert_xywha_to_xyxy([box])

    def test_box_convert_xywha_to_xyxy_array(self):
        for dtype in [np.float64, np.float32]:
            box = np.asarray(
                [
                    [50, 50, 30, 20, 0],
                    [50, 50, 30, 20, 90],
                    [1, 1, math.sqrt(2), math.sqrt(2), -45],
                ],
                dtype=dtype,
            )
            output = self._convert_xywha_to_xyxy(box)
            self.assertEqual(output.dtype, box.dtype)
            expected = np.asarray([[35, 40, 65, 60], [40, 35, 60, 65], [0, 0, 2, 2]], dtype=dtype)
            self.assertTrue(np.allclose(output, expected, atol=1e-6), "output={}".format(output))

    def test_box_convert_xywha_to_xyxy_tensor(self):
        for dtype in [torch.float32, torch.float64]:
            box = torch.tensor(
                [
                    [50, 50, 30, 20, 0],
                    [50, 50, 30, 20, 90],
                    [1, 1, math.sqrt(2), math.sqrt(2), -45],
                ],
                dtype=dtype,
            )
            output = self._convert_xywha_to_xyxy(box)
            self.assertEqual(output.dtype, box.dtype)
            expected = torch.tensor([[35, 40, 65, 60], [40, 35, 60, 65], [0, 0, 2, 2]], dtype=dtype)

            self.assertTrue(torch.allclose(output, expected, atol=1e-6), "output={}".format(output))

    def test_box_convert_xywh_to_xywha_list(self):
        for tp in [list, tuple]:
            box = tp([50, 50, 30, 20])
            output = self._convert_xywh_to_xywha(box)
            self.assertIsInstance(output, tp)
            self.assertEqual(output, tp([65, 60, 30, 20, 0]))

            with self.assertRaises(Exception):
                self._convert_xywh_to_xywha([box])

    def test_box_convert_xywh_to_xywha_array(self):
        for dtype in [np.float64, np.float32]:
            box = np.asarray([[30, 40, 70, 60], [30, 40, 60, 70], [-1, -1, 2, 2]], dtype=dtype)
            output = self._convert_xywh_to_xywha(box)
            self.assertEqual(output.dtype, box.dtype)
            expected = np.asarray(
                [[65, 70, 70, 60, 0], [60, 75, 60, 70, 0], [0, 0, 2, 2, 0]], dtype=dtype
            )
            self.assertTrue(np.allclose(output, expected, atol=1e-6), "output={}".format(output))

    def test_box_convert_xywh_to_xywha_tensor(self):
        for dtype in [torch.float32, torch.float64]:
            box = torch.tensor([[30, 40, 70, 60], [30, 40, 60, 70], [-1, -1, 2, 2]], dtype=dtype)
            output = self._convert_xywh_to_xywha(box)
            self.assertEqual(output.dtype, box.dtype)
            expected = torch.tensor(
                [[65, 70, 70, 60, 0], [60, 75, 60, 70, 0], [0, 0, 2, 2, 0]], dtype=dtype
            )

            self.assertTrue(torch.allclose(output, expected, atol=1e-6), "output={}".format(output))

    def test_json_serializable(self):
        payload = {"box_mode": BoxMode.XYWH_REL}
        try:
            json.dumps(payload)
        except Exception:
            self.fail("JSON serialization failed")

    def test_json_deserializable(self):
        payload = '{"box_mode": 2}'
        obj = json.loads(payload)
        try:
            obj["box_mode"] = BoxMode(obj["box_mode"])
        except Exception:
            self.fail("JSON deserialization failed")


class TestBoxIOU(unittest.TestCase):
    def test_pairwise_iou(self):
        boxes1 = torch.tensor([[0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 1.0, 1.0]])

        boxes2 = torch.tensor(
            [
                [0.0, 0.0, 1.0, 1.0],
                [0.0, 0.0, 0.5, 1.0],
                [0.0, 0.0, 1.0, 0.5],
                [0.0, 0.0, 0.5, 0.5],
                [0.5, 0.5, 1.0, 1.0],
                [0.5, 0.5, 1.5, 1.5],
            ]
        )

        expected_ious = torch.tensor(
            [
                [1.0, 0.5, 0.5, 0.25, 0.25, 0.25 / (2 - 0.25)],
                [1.0, 0.5, 0.5, 0.25, 0.25, 0.25 / (2 - 0.25)],
            ]
        )

        ious = pairwise_iou(Boxes(boxes1), Boxes(boxes2))

        self.assertTrue(torch.allclose(ious, expected_ious))


class TestBoxes(unittest.TestCase):
    def test_empty_cat(self):
        x = Boxes.cat([])
        self.assertTrue(x.tensor.shape, (0, 4))

    # require https://github.com/pytorch/pytorch/pull/39336
    # TODO: Make cvpods.structures.Boxes scriptable
    @unittest.skip("Don't support scriptable cvpods.structures.Boxes")
    @unittest.skipIf(TORCH_VERSION < (1, 6), "Insufficient pytorch version")
    def test_scriptability(self):
        def func(x):
            boxes = Boxes(x)
            return boxes.area()

        f = torch.jit.script(func)
        f(torch.rand((3, 4)))


if __name__ == "__main__":
    unittest.main()
```

### tests/structures/test_instances.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import unittest
import torch

# from cvpods.export.torchscript import patch_instancess  # TODO: to be implemented
from cvpods.structures import Instances
from cvpods.utils.env import TORCH_VERSION


class TestInstances(unittest.TestCase):
    def test_int_indexing(self):
        attr1 = torch.tensor([[0.0, 0.0, 1.0], [0.0, 0.0, 0.5], [0.0, 0.0, 1.0], [0.0, 0.5, 0.5]])
        attr2 = torch.tensor([0.1, 0.2, 0.3, 0.4])
        instances = Instances((100, 100))
        instances.attr1 = attr1
        instances.attr2 = attr2
        for i in range(-len(instances), len(instances)):
            inst = instances[i]
            self.assertEqual((inst.attr1 == attr1[i]).all(), True)
            self.assertEqual((inst.attr2 == attr2[i]).all(), True)

        self.assertRaises(IndexError, lambda: instances[len(instances)])
        self.assertRaises(IndexError, lambda: instances[-len(instances) - 1])

    # TODO: make cvpods.structures.Instances scriptable
    @unittest.skip("Don't support scriptable cvpods.structures.Instances")
    @unittest.skipIf(TORCH_VERSION < (1, 7), "Insufficient pytorch version")
    def test_script_new_fields(self):
        class f(torch.nn.Module):
            def forward(self, x: Instances):
                proposal_boxes = x.proposal_boxes  # noqa F841
                objectness_logits = x.objectness_logits  # noqa F841
                return x

        class g(torch.nn.Module):
            def forward(self, x: Instances):
                mask = x.mask  # noqa F841
                return x

        class g2(torch.nn.Module):
            def forward(self, x: Instances):
                proposal_boxes = x.proposal_boxes  # noqa F841
                return x

        fields = {"proposal_boxes": "Boxes", "objectness_logits": "Tensor"}
        with patch_instances(fields):  # noqa
            torch.jit.script(f())

        # can't script anymore after exiting the context
        with self.assertRaises(Exception):
            torch.jit.script(g2())

        new_fields = {"mask": "Tensor"}
        with patch_instances(new_fields):  # noqa
            torch.jit.script(g())
            with self.assertRaises(Exception):
                torch.jit.script(g2())

    # TODO: make cvpods.structures.Instances scriptable
    @unittest.skip("Don't support scriptable cvpods.structures.Instances")
    @unittest.skipIf(TORCH_VERSION < (1, 7), "Insufficient pytorch version")
    def test_script_access_fields(self):
        class f(torch.nn.Module):
            def forward(self, x: Instances):
                proposal_boxes = x.proposal_boxes
                objectness_logits = x.objectness_logits
                return proposal_boxes.tensor + objectness_logits

        fields = {"proposal_boxes": "Boxes", "objectness_logits": "Tensor"}
        with patch_instances(fields):  # noqa
            torch.jit.script(f())


if __name__ == "__main__":
    unittest.main()
```

### tests/utils/test_events.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import json
import os
import tempfile
import unittest

from cvpods.utils import EventStorage, JSONWriter


class TestEventWriter(unittest.TestCase):
    def testScalar(self):
        with tempfile.TemporaryDirectory(
            prefix="cvpods_tests"
        ) as dir, EventStorage() as storage:
            json_file = os.path.join(dir, "test.json")
            writer = JSONWriter(json_file)
            for k in range(60):
                storage.put_scalar("key", k, smoothing_hint=False)
                if (k + 1) % 20 == 0:
                    writer.write()
                storage.step()
            writer.close()
            with open(json_file) as f:
                data = [json.loads(line) for line in f]
                self.assertTrue([int(k["key"]) for k in data] == [19, 39, 59])

    @unittest.skip("EventStorage doesn't support mismatched period writing")
    def testScalarMismatchedPeriod(self):
        with tempfile.TemporaryDirectory(
            prefix="cvpods_tests"
        ) as dir, EventStorage() as storage:
            json_file = os.path.join(dir, "test.json")

            writer = JSONWriter(json_file)
            for k in range(60):
                if k % 17 == 0:  # write in a differnt period
                    storage.put_scalar("key2", k, smoothing_hint=False)
                storage.put_scalar("key", k, smoothing_hint=False)
                if (k + 1) % 20 == 0:
                    writer.write()
                storage.step()
            writer.close()
            with open(json_file) as f:
                data = [json.loads(line) for line in f]
                print([int(k.get("key2", 0)) for k in data])
                print([int(k.get("key", 0)) for k in data])
                print([int(k.get("iteration", 0)) for k in data])
                self.assertTrue([int(k.get("key2", 0)) for k in data] == [17, 0, 34, 0, 51, 0])
                self.assertTrue([int(k.get("key", 0)) for k in data] == [0, 19, 0, 39, 0, 59])
                self.assertTrue([int(k["iteration"]) for k in data] == [17, 19, 34, 39, 51, 59])
```

### tests/modeling/test_matcher.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import unittest
from typing import List
import torch

from cvpods.configs import RCNNConfig
from cvpods.modeling.matcher import Matcher
from cvpods.utils.env import TORCH_VERSION


class TestMatcher(unittest.TestCase):
    # need https://github.com/pytorch/pytorch/pull/38378
    # TODO: make cvpods.modeling.matcher.Matche scriptable
    @unittest.skip("Don't support scriptable cvpods.modeling.matcher.Matcher")
    @unittest.skipIf(TORCH_VERSION < (1, 6), "Insufficient pytorch version")
    def test_scriptability(self):
        cfg = RCNNConfig()
        anchor_matcher = Matcher(
            cfg.MODEL.RPN.IOU_THRESHOLDS, cfg.MODEL.RPN.IOU_LABELS, allow_low_quality_matches=True
        )
        match_quality_matrix = torch.tensor(
            [[0.15, 0.45, 0.2, 0.6], [0.3, 0.65, 0.05, 0.1], [0.05, 0.4, 0.25, 0.4]]
        )
        expected_matches = torch.tensor([1, 1, 2, 0])
        expected_match_labels = torch.tensor([-1, 1, 0, 1], dtype=torch.int8)

        matches, match_labels = anchor_matcher(match_quality_matrix)
        self.assertTrue(torch.allclose(matches, expected_matches))
        self.assertTrue(torch.allclose(match_labels, expected_match_labels))

        # nonzero_tuple must be import explicitly to let jit know what it is.
        # https://github.com/pytorch/pytorch/issues/38964
        from detectron2.layers import nonzero_tuple  # noqa F401

        def f(thresholds: List[float], labels: List[int]):
            return Matcher(thresholds, labels, allow_low_quality_matches=True)

        scripted_anchor_matcher = torch.jit.script(f)(
            cfg.MODEL.RPN.IOU_THRESHOLDS, cfg.MODEL.RPN.IOU_LABELS
        )
        matches, match_labels = scripted_anchor_matcher(match_quality_matrix)
        self.assertTrue(torch.allclose(matches, expected_matches))
        self.assertTrue(torch.allclose(match_labels, expected_match_labels))


if __name__ == "__main__":
    unittest.main()
```

### tests/modeling/test_roi_pooler.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import logging
import unittest
import torch

from cvpods.modeling.poolers import ROIPooler
from cvpods.structures import Boxes, RotatedBoxes
from cvpods.utils.env import TORCH_VERSION

logger = logging.getLogger(__name__)


class TestROIPooler(unittest.TestCase):
    def _rand_boxes(self, num_boxes, x_max, y_max):
        coords = torch.rand(num_boxes, 4)
        coords[:, 0] *= x_max
        coords[:, 1] *= y_max
        coords[:, 2] *= x_max
        coords[:, 3] *= y_max
        boxes = torch.zeros(num_boxes, 4)
        boxes[:, 0] = torch.min(coords[:, 0], coords[:, 2])
        boxes[:, 1] = torch.min(coords[:, 1], coords[:, 3])
        boxes[:, 2] = torch.max(coords[:, 0], coords[:, 2])
        boxes[:, 3] = torch.max(coords[:, 1], coords[:, 3])
        return boxes

    def _test_roialignv2_roialignrotated_match(self, device):
        pooler_resolution = 14
        canonical_level = 4
        canonical_scale_factor = 2 ** canonical_level
        pooler_scales = (1.0 / canonical_scale_factor,)
        sampling_ratio = 0

        N, C, H, W = 2, 4, 10, 8
        N_rois = 10
        std = 11
        mean = 0
        feature = (torch.rand(N, C, H, W) - 0.5) * 2 * std + mean

        features = [feature.to(device)]

        rois = []
        rois_rotated = []
        for _ in range(N):
            boxes = self._rand_boxes(
                num_boxes=N_rois, x_max=W * canonical_scale_factor, y_max=H * canonical_scale_factor
            )

            rotated_boxes = torch.zeros(N_rois, 5)
            rotated_boxes[:, 0] = (boxes[:, 0] + boxes[:, 2]) / 2.0
            rotated_boxes[:, 1] = (boxes[:, 1] + boxes[:, 3]) / 2.0
            rotated_boxes[:, 2] = boxes[:, 2] - boxes[:, 0]
            rotated_boxes[:, 3] = boxes[:, 3] - boxes[:, 1]
            rois.append(Boxes(boxes).to(device))
            rois_rotated.append(RotatedBoxes(rotated_boxes).to(device))

        roialignv2_pooler = ROIPooler(
            output_size=pooler_resolution,
            scales=pooler_scales,
            sampling_ratio=sampling_ratio,
            pooler_type="ROIAlignV2",
        )

        roialignv2_out = roialignv2_pooler(features, rois)

        roialignrotated_pooler = ROIPooler(
            output_size=pooler_resolution,
            scales=pooler_scales,
            sampling_ratio=sampling_ratio,
            pooler_type="ROIAlignRotated",
        )

        roialignrotated_out = roialignrotated_pooler(features, rois_rotated)

        self.assertTrue(torch.allclose(roialignv2_out, roialignrotated_out, atol=1e-4))

    def test_roialignv2_roialignrotated_match_cpu(self):
        self._test_roialignv2_roialignrotated_match(device="cpu")

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_roialignv2_roialignrotated_match_cuda(self):
        self._test_roialignv2_roialignrotated_match(device="cuda")

    # TODO: make cvpods.modeling.poolers.ROIPooler scriptable
    @unittest.skip("Don't support scriptable cvpods.modeling.poolers.ROIPooler")
    @unittest.skipIf(TORCH_VERSION < (1, 7), "Insufficient pytorch version")
    def _test_scriptability(self, device):
        pooler_resolution = 14
        canonical_level = 4
        canonical_scale_factor = 2 ** canonical_level
        pooler_scales = (1.0 / canonical_scale_factor,)
        sampling_ratio = 0

        N, C, H, W = 2, 4, 10, 8
        N_rois = 10
        std = 11
        mean = 0
        feature = (torch.rand(N, C, H, W) - 0.5) * 2 * std + mean

        features = [feature.to(device)]

        rois = []
        for _ in range(N):
            boxes = self._rand_boxes(
                num_boxes=N_rois, x_max=W * canonical_scale_factor, y_max=H * canonical_scale_factor
            )

            rois.append(Boxes(boxes).to(device))

        roialignv2_pooler = ROIPooler(
            output_size=pooler_resolution,
            scales=pooler_scales,
            sampling_ratio=sampling_ratio,
            pooler_type="ROIAlignV2",
        )

        roialignv2_out = roialignv2_pooler(features, rois)
        scripted_roialignv2_out = torch.jit.script(roialignv2_pooler)(features, rois)
        self.assertTrue(torch.equal(roialignv2_out, scripted_roialignv2_out))

    @unittest.skipIf(TORCH_VERSION < (1, 7), "Insufficient pytorch version")
    def test_scriptability_cpu(self):
        self._test_scriptability(device="cpu")

    @unittest.skipIf(TORCH_VERSION < (1, 7), "Insufficient pytorch version")
    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_scriptability_gpu(self):
        self._test_scriptability(device="cuda")

    def test_no_images(self):
        N, C, H, W = 0, 32, 32, 32
        feature = torch.rand(N, C, H, W) - 0.5
        features = [feature]
        pooler = ROIPooler(
            output_size=14, scales=(1.0,), sampling_ratio=0.0, pooler_type="ROIAlignV2"
        )
        output = pooler.forward(features, [])
        self.assertEqual(output.shape, (0, C, 14, 14))


if __name__ == "__main__":
    unittest.main()
```

### tests/modeling/test_anchor_generator.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import logging
import unittest
import torch

from cvpods.configs import BaseDetectionConfig
from cvpods.layers import ShapeSpec
from cvpods.modeling.anchor_generator import DefaultAnchorGenerator, RotatedAnchorGenerator
from cvpods.utils.env import TORCH_VERSION  # noqa

logger = logging.getLogger(__name__)


class TestAnchorGenerator(unittest.TestCase):
    def test_default_anchor_generator(self):
        cfg = BaseDetectionConfig()
        cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[32, 64]]
        cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.25, 1, 4]]

        anchor_generator = DefaultAnchorGenerator(cfg, [ShapeSpec(stride=4)])

        # only the last two dimensions of features matter here
        num_images = 2
        features = {"stage3": torch.rand(num_images, 96, 1, 2)}
        anchors = anchor_generator([features["stage3"]])
        expected_anchor_tensor = torch.tensor(
            [
                [-32.0, -8.0, 32.0, 8.0],
                [-16.0, -16.0, 16.0, 16.0],
                [-8.0, -32.0, 8.0, 32.0],
                [-64.0, -16.0, 64.0, 16.0],
                [-32.0, -32.0, 32.0, 32.0],
                [-16.0, -64.0, 16.0, 64.0],
                [-28.0, -8.0, 36.0, 8.0],  # -28.0 == -32.0 + STRIDE (4)
                [-12.0, -16.0, 20.0, 16.0],
                [-4.0, -32.0, 12.0, 32.0],
                [-60.0, -16.0, 68.0, 16.0],
                [-28.0, -32.0, 36.0, 32.0],
                [-12.0, -64.0, 20.0, 64.0],
            ]
        )

        for i in range(num_images):
            assert torch.allclose(anchors[i][0].tensor, expected_anchor_tensor)

        # TODO: make cvpods.modeling.anchor_generator.DefaultAnchorGenerator scriptable
        # if TORCH_VERSION >= (1, 6):
        #     anchors = torch.jit.script(anchor_generator)([features["stage3"]])
        #     assert torch.allclose(anchors[0].tensor, expected_anchor_tensor)

    def test_default_anchor_generator_centered(self):
        cfg = BaseDetectionConfig()
        cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[32, 64]]
        cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.25, 1, 4]]
        cfg.MODEL.ANCHOR_GENERATOR.OFFSET = 0.5

        anchor_generator = DefaultAnchorGenerator(cfg, [ShapeSpec(stride=4)])

        # only the last two dimensions of features matter here
        num_images = 2
        features = {"stage3": torch.rand(num_images, 96, 1, 2)}
        anchors = anchor_generator([features["stage3"]])
        expected_anchor_tensor = torch.tensor(
            [
                [-30.0, -6.0, 34.0, 10.0],
                [-14.0, -14.0, 18.0, 18.0],
                [-6.0, -30.0, 10.0, 34.0],
                [-62.0, -14.0, 66.0, 18.0],
                [-30.0, -30.0, 34.0, 34.0],
                [-14.0, -62.0, 18.0, 66.0],
                [-26.0, -6.0, 38.0, 10.0],
                [-10.0, -14.0, 22.0, 18.0],
                [-2.0, -30.0, 14.0, 34.0],
                [-58.0, -14.0, 70.0, 18.0],
                [-26.0, -30.0, 38.0, 34.0],
                [-10.0, -62.0, 22.0, 66.0],
            ]
        )

        for i in range(num_images):
            assert torch.allclose(anchors[i][0].tensor, expected_anchor_tensor)

    def test_rrpn_anchor_generator(self):
        cfg = BaseDetectionConfig()
        cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[32, 64]]
        cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.25, 1, 4]]
        cfg.MODEL.ANCHOR_GENERATOR.ANGLES = [[0, 45]]
        anchor_generator = RotatedAnchorGenerator(cfg, [ShapeSpec(stride=4)])

        # only the last two dimensions of features matter here
        num_images = 2
        features = {"stage3": torch.rand(num_images, 96, 1, 2)}
        anchors = anchor_generator([features["stage3"]])
        expected_anchor_tensor = torch.tensor(
            [
                [0.0, 0.0, 64.0, 16.0, 0.0],
                [0.0, 0.0, 64.0, 16.0, 45.0],
                [0.0, 0.0, 32.0, 32.0, 0.0],
                [0.0, 0.0, 32.0, 32.0, 45.0],
                [0.0, 0.0, 16.0, 64.0, 0.0],
                [0.0, 0.0, 16.0, 64.0, 45.0],
                [0.0, 0.0, 128.0, 32.0, 0.0],
                [0.0, 0.0, 128.0, 32.0, 45.0],
                [0.0, 0.0, 64.0, 64.0, 0.0],
                [0.0, 0.0, 64.0, 64.0, 45.0],
                [0.0, 0.0, 32.0, 128.0, 0.0],
                [0.0, 0.0, 32.0, 128.0, 45.0],
                [4.0, 0.0, 64.0, 16.0, 0.0],  # 4.0 == 0.0 + STRIDE (4)
                [4.0, 0.0, 64.0, 16.0, 45.0],
                [4.0, 0.0, 32.0, 32.0, 0.0],
                [4.0, 0.0, 32.0, 32.0, 45.0],
                [4.0, 0.0, 16.0, 64.0, 0.0],
                [4.0, 0.0, 16.0, 64.0, 45.0],
                [4.0, 0.0, 128.0, 32.0, 0.0],
                [4.0, 0.0, 128.0, 32.0, 45.0],
                [4.0, 0.0, 64.0, 64.0, 0.0],
                [4.0, 0.0, 64.0, 64.0, 45.0],
                [4.0, 0.0, 32.0, 128.0, 0.0],
                [4.0, 0.0, 32.0, 128.0, 45.0],
            ]
        )

        for i in range(num_images):
            assert torch.allclose(anchors[i][0].tensor, expected_anchor_tensor)


if __name__ == "__main__":
    unittest.main()
```

### tests/modeling/test_roi_heads.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import logging
import unittest
import torch

from cvpods.configs import RCNNConfig
from cvpods.layers import ShapeSpec
from cvpods.modeling.backbone import Backbone
from cvpods.modeling.backbone.resnet import build_resnet_backbone
from cvpods.modeling.proposal_generator import RPN, RRPN
from cvpods.modeling.roi_heads import StandardROIHeads, RROIHeads
from cvpods.modeling.roi_heads.box_head import FastRCNNConvFCHead
from cvpods.structures import Boxes, ImageList, Instances, RotatedBoxes
from cvpods.utils import EventStorage

logger = logging.getLogger(__name__)


def build_backbone(cfg, input_shape=None):
    if input_shape is None:
        input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))
    backbone = build_resnet_backbone(cfg, input_shape)
    assert isinstance(backbone, Backbone)
    return backbone


class ROIHeadsTest(unittest.TestCase):
    def test_roi_heads(self):
        torch.manual_seed(121)
        cfg = RCNNConfig()
        # PROPOSAL_GENERATOR: "RPN"
        # ROI_HEADS: "StandardROIHeads"
        # ROI_BOX_HEAD: "FastRCNNConvFCHead"
        cfg.MODEL.RESNETS.DEPTH = 50
        cfg.MODEL.ROI_BOX_HEAD.NUM_FC = 2
        cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE = "ROIAlignV2"
        cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS = (10, 10, 5, 5)

        def build_box_head(cfg, input_shape):
            return FastRCNNConvFCHead(cfg, input_shape)
        cfg.build_box_head = build_box_head

        backbone = build_backbone(cfg)
        num_images = 2
        images_tensor = torch.rand(num_images, 20, 30)
        image_sizes = [(10, 10), (20, 30)]
        images = ImageList(images_tensor, image_sizes)
        num_channels = 1024
        features = {"res4": torch.rand(num_images, num_channels, 1, 2)}

        image_shape = (15, 15)
        gt_boxes0 = torch.tensor([[1, 1, 3, 3], [2, 2, 6, 6]], dtype=torch.float32)
        gt_instance0 = Instances(image_shape)
        gt_instance0.gt_boxes = Boxes(gt_boxes0)
        gt_instance0.gt_classes = torch.tensor([2, 1])
        gt_boxes1 = torch.tensor([[1, 5, 2, 8], [7, 3, 10, 5]], dtype=torch.float32)
        gt_instance1 = Instances(image_shape)
        gt_instance1.gt_boxes = Boxes(gt_boxes1)
        gt_instance1.gt_classes = torch.tensor([1, 2])
        gt_instances = [gt_instance0, gt_instance1]

        proposal_generator = RPN(cfg, backbone.output_shape())
        roi_heads = StandardROIHeads(cfg, backbone.output_shape())

        with EventStorage():  # capture events in a new storage to discard them
            proposals, proposal_losses = proposal_generator(images, features, gt_instances)
            _, detector_losses = roi_heads(images, features, proposals, gt_instances)

        expected_losses = {
            "loss_cls": torch.tensor(4.4236516953),
            "loss_box_reg": torch.tensor(0.0091214813),
        }
        for name in expected_losses.keys():
            self.assertTrue(torch.allclose(detector_losses[name], expected_losses[name]))

    @unittest.skip("rotated_rcnn not supported")
    def test_rroi_heads(self):
        torch.manual_seed(121)
        cfg = RCNNConfig()
        cfg.MODEL.ANCHOR_GENERATOR.NAME = "RotatedAnchorGenerator"
        # PROPOSAL_GENERATOR: "RRPN"
        # ROI_HEADS: "RROIHeads"
        # ROI_BOX_HEAD.NAME: "FastRCNNConvFCHead"

        def build_box_head(cfg, input_shape):
            return FastRCNNConvFCHead(cfg, input_shape)
        cfg.build_box_head = build_box_head

        cfg.MODEL.RESNETS.DEPTH = 50
        cfg.MODEL.ROI_BOX_HEAD.NUM_FC = 2
        cfg.MODEL.RPN.BBOX_REG_WEIGHTS = (1, 1, 1, 1, 1)
        cfg.MODEL.RPN.HEAD_NAME = "StandardRPNHead"
        cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE = "ROIAlignRotated"
        cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS = (10, 10, 5, 5, 1)
        backbone = build_backbone(cfg)
        num_images = 2
        images_tensor = torch.rand(num_images, 20, 30)
        image_sizes = [(10, 10), (20, 30)]
        images = ImageList(images_tensor, image_sizes)
        num_channels = 1024
        features = {"res4": torch.rand(num_images, num_channels, 1, 2)}

        image_shape = (15, 15)
        gt_boxes0 = torch.tensor([[2, 2, 2, 2, 30], [4, 4, 4, 4, 0]], dtype=torch.float32)
        gt_instance0 = Instances(image_shape)
        gt_instance0.gt_boxes = RotatedBoxes(gt_boxes0)
        gt_instance0.gt_classes = torch.tensor([2, 1])
        gt_boxes1 = torch.tensor([[1.5, 5.5, 1, 3, 0], [8.5, 4, 3, 2, -50]], dtype=torch.float32)
        gt_instance1 = Instances(image_shape)
        gt_instance1.gt_boxes = RotatedBoxes(gt_boxes1)
        gt_instance1.gt_classes = torch.tensor([1, 2])
        gt_instances = [gt_instance0, gt_instance1]

        # currently using DefaultAnchorGenerator in RRPN
        proposal_generator = RRPN(cfg, backbone.output_shape())
        roi_heads = RROIHeads(cfg, backbone.output_shape())

        with EventStorage():  # capture events in a new storage to discard them
            proposals, proposal_losses = proposal_generator(images, features, gt_instances)
            _, detector_losses = roi_heads(images, features, proposals, gt_instances)

        expected_losses = {
            "loss_cls": torch.tensor(4.381618499755859),
            "loss_box_reg": torch.tensor(0.0011829272843897343),
        }
        for name in expected_losses.keys():
            err_msg = "detector_losses[{}] = {}, expected losses = {}".format(
                name, detector_losses[name], expected_losses[name]
            )
            self.assertTrue(torch.allclose(detector_losses[name], expected_losses[name]), err_msg)


if __name__ == "__main__":
    unittest.main()
```

### tests/modeling/test_rpn.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import logging
import unittest
import torch

from cvpods.configs import RCNNConfig
# TODO: to be implemented
# from cvpods.export.torchscript import export_torchscript_with_instances
from cvpods.layers import ShapeSpec
from cvpods.modeling.backbone import Backbone
from cvpods.modeling.backbone.resnet import build_resnet_backbone
from cvpods.modeling.proposal_generator import RPN, RRPN
from cvpods.modeling.proposal_generator.rpn_outputs import find_top_rpn_proposals
from cvpods.structures import Boxes, ImageList, Instances, RotatedBoxes
from cvpods.utils import EventStorage
from cvpods.utils.env import TORCH_VERSION


logger = logging.getLogger(__name__)


def build_backbone(cfg, input_shape=None):
    if input_shape is None:
        input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))
    backbone = build_resnet_backbone(cfg, input_shape)
    assert isinstance(backbone, Backbone)
    return backbone


class RPNTest(unittest.TestCase):
    def test_rpn(self):
        torch.manual_seed(121)
        cfg = RCNNConfig()
        # PROPOSAL_GENERATOR: "RPN"
        # ANCHOR_GENERATOR: "DefaultAnchorGenerator"
        cfg.MODEL.RESNETS.DEPTH = 50
        cfg.MODEL.RPN.BBOX_REG_WEIGHTS = (1, 1, 1, 1)
        backbone = build_backbone(cfg)
        proposal_generator = RPN(cfg, backbone.output_shape())
        num_images = 2
        images_tensor = torch.rand(num_images, 20, 30)
        image_sizes = [(10, 10), (20, 30)]
        images = ImageList(images_tensor, image_sizes)
        image_shape = (15, 15)
        num_channels = 1024
        features = {"res4": torch.rand(num_images, num_channels, 1, 2)}
        gt_boxes = torch.tensor([[1, 1, 3, 3], [2, 2, 6, 6]], dtype=torch.float32)
        gt_instances = Instances(image_shape)
        gt_instances.gt_boxes = Boxes(gt_boxes)
        with EventStorage():  # capture events in a new storage to discard them
            proposals, proposal_losses = proposal_generator(
                images, features, [gt_instances[0], gt_instances[1]]
            )

        expected_losses = {
            "loss_rpn_cls": torch.tensor(0.0804563984),
            "loss_rpn_loc": torch.tensor(0.0990132466),
        }
        for name in expected_losses.keys():
            err_msg = "proposal_losses[{}] = {}, expected losses = {}".format(
                name, proposal_losses[name], expected_losses[name]
            )
            self.assertTrue(torch.allclose(proposal_losses[name], expected_losses[name]), err_msg)

        expected_proposal_boxes = [
            Boxes(torch.tensor([[0, 0, 10, 10], [7.3365392685, 0, 10, 10]])),
            Boxes(
                torch.tensor(
                    [
                        [0, 0, 30, 20],
                        [0, 0, 16.7862777710, 13.1362524033],
                        [0, 0, 30, 13.3173446655],
                        [0, 0, 10.8602609634, 20],
                        [7.7165775299, 0, 27.3875980377, 20],
                    ]
                )
            ),
        ]

        expected_objectness_logits = [
            torch.tensor([0.1225359365, -0.0133192837]),
            torch.tensor([0.1415634006, 0.0989848152, 0.0565387346, -0.0072308783, -0.0428492837]),
        ]

        for proposal, expected_proposal_box, im_size, expected_objectness_logit in zip(
            proposals, expected_proposal_boxes, image_sizes, expected_objectness_logits
        ):
            self.assertEqual(len(proposal), len(expected_proposal_box))
            self.assertEqual(proposal.image_size, im_size)
            self.assertTrue(
                torch.allclose(proposal.proposal_boxes.tensor, expected_proposal_box.tensor)
            )
            self.assertTrue(torch.allclose(proposal.objectness_logits, expected_objectness_logit))

    # TODO: make cvpods.modeling.proposal_generator.RPN scriptable
    @unittest.skip("Don't support scriptable cvpods.modeling.proposal_generator.RPN")
    @unittest.skipIf(TORCH_VERSION < (1, 7), "Insufficient pytorch version")
    def test_rpn_scriptability(self):
        cfg = RCNNConfig()
        proposal_generator = RPN(cfg, {"res4": ShapeSpec(channels=1024, stride=16)}).eval()
        num_images = 2
        images_tensor = torch.rand(num_images, 30, 40)
        image_sizes = [(32, 32), (30, 40)]
        images = ImageList(images_tensor, image_sizes)
        features = {"res4": torch.rand(num_images, 1024, 1, 2)}

        fields = {"proposal_boxes": "Boxes", "objectness_logits": "Tensor"}
        proposal_generator_ts = export_torchscript_with_instances(proposal_generator, fields)  # noqa

        proposals, _ = proposal_generator(images, features)
        proposals_ts, _ = proposal_generator_ts(images, features)

        for proposal, proposal_ts in zip(proposals, proposals_ts):
            self.assertEqual(proposal.image_size, proposal_ts.image_size)
            self.assertTrue(
                torch.equal(proposal.proposal_boxes.tensor, proposal_ts.proposal_boxes.tensor)
            )
            self.assertTrue(torch.equal(proposal.objectness_logits, proposal_ts.objectness_logits))

    @unittest.skip("rotated_rcnn not supported")
    def test_rrpn(self):
        torch.manual_seed(121)
        cfg = RCNNConfig()
        # PROPOSAL_GENERATOR: "RRPN"
        # ANCHOR_GENERATOR: "RotatedAnchorGenerator"
        cfg.MODEL.RESNETS.DEPTH = 50
        cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[32, 64]]
        cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.25, 1]]
        cfg.MODEL.ANCHOR_GENERATOR.ANGLES = [[0, 60]]
        cfg.MODEL.RPN.BBOX_REG_WEIGHTS = (1, 1, 1, 1, 1)
        # cfg.MODEL.RPN.HEAD_NAME = "StandardRPNHead"
        backbone = build_backbone(cfg)
        # currently using DefaultAnchorGenerator in RRPN
        proposal_generator = RRPN(cfg, backbone.output_shape())
        num_images = 2
        images_tensor = torch.rand(num_images, 20, 30)
        image_sizes = [(10, 10), (20, 30)]
        images = ImageList(images_tensor, image_sizes)
        image_shape = (15, 15)
        num_channels = 1024
        features = {"res4": torch.rand(num_images, num_channels, 1, 2)}
        gt_boxes = torch.tensor([[2, 2, 2, 2, 0], [4, 4, 4, 4, 0]], dtype=torch.float32)
        gt_instances = Instances(image_shape)
        gt_instances.gt_boxes = RotatedBoxes(gt_boxes)
        with EventStorage():  # capture events in a new storage to discard them
            proposals, proposal_losses = proposal_generator(
                images, features, [gt_instances[0], gt_instances[1]]
            )

        expected_losses = {
            "loss_rpn_cls": torch.tensor(0.043263837695121765),
            "loss_rpn_loc": torch.tensor(0.14432406425476074),
        }
        for name in expected_losses.keys():
            err_msg = "proposal_losses[{}] = {}, expected losses = {}".format(
                name, proposal_losses[name], expected_losses[name]
            )
            self.assertTrue(torch.allclose(proposal_losses[name], expected_losses[name]), err_msg)

        expected_proposal_boxes = [
            RotatedBoxes(
                torch.tensor(
                    [
                        [0.60189795, 1.24095452, 61.98131943, 18.03621292, -4.07244873],
                        [15.64940453, 1.69624567, 59.59749603, 16.34339333, 2.62692475],
                        [-3.02982378, -2.69752932, 67.90952301, 59.62455750, 59.97010040],
                        [16.71863365, 1.98309708, 35.61507797, 32.81484985, 62.92267227],
                        [0.49432933, -7.92979717, 67.77606201, 62.93098450, -1.85656738],
                        [8.00880814, 1.36017394, 121.81007385, 32.74150467, 50.44297409],
                        [16.44299889, -4.82221127, 63.39775848, 61.22503662, 54.12270737],
                        [5.00000000, 5.00000000, 10.00000000, 10.00000000, -0.76943970],
                        [17.64130402, -0.98095351, 61.40377808, 16.28918839, 55.53118134],
                        [0.13016054, 4.60568953, 35.80157471, 32.30180359, 62.52872086],
                        [-4.26460743, 0.39604485, 124.30079651, 31.84611320, -1.58203125],
                        [7.52815342, -0.91636634, 62.39784622, 15.45565224, 60.79549789],
                    ]
                )
            ),
            RotatedBoxes(
                torch.tensor(
                    [
                        [0.07734215, 0.81635046, 65.33510590, 17.34688377, -1.51821899],
                        [-3.41833067, -3.11320257, 64.17595673, 60.55617905, 58.27033234],
                        [20.67383385, -6.16561556, 63.60531998, 62.52315903, 54.85546494],
                        [15.00000000, 10.00000000, 30.00000000, 20.00000000, -0.18218994],
                        [9.22646523, -6.84775209, 62.09895706, 65.46472931, -2.74307251],
                        [15.00000000, 4.93451595, 30.00000000, 9.86903191, -0.60272217],
                        [8.88342094, 2.65560246, 120.95362854, 32.45022202, 55.75970078],
                        [16.39088631, 2.33887148, 34.78761292, 35.61492920, 60.81977463],
                        [9.78298569, 10.00000000, 19.56597137, 20.00000000, -0.86660767],
                        [1.28576660, 5.49873352, 34.93610382, 33.22600174, 60.51599884],
                        [17.58912468, -1.63270092, 62.96052551, 16.45713997, 52.91245270],
                        [5.64749718, -1.90428460, 62.37649155, 16.19474792, 61.09543991],
                        [0.82255805, 2.34931135, 118.83985901, 32.83671188, 56.50753784],
                        [-5.33874989, 1.64404404, 125.28501892, 33.35424042, -2.80731201],
                    ]
                )
            ),
        ]

        expected_objectness_logits = [
            torch.tensor(
                [
                    0.10111768,
                    0.09112845,
                    0.08466332,
                    0.07589971,
                    0.06650183,
                    0.06350251,
                    0.04299347,
                    0.01864817,
                    0.00986163,
                    0.00078543,
                    -0.04573630,
                    -0.04799230,
                ]
            ),
            torch.tensor(
                [
                    0.11373727,
                    0.09377633,
                    0.05281663,
                    0.05143715,
                    0.04040275,
                    0.03250912,
                    0.01307789,
                    0.01177734,
                    0.00038105,
                    -0.00540255,
                    -0.01194804,
                    -0.01461012,
                    -0.03061717,
                    -0.03599222,
                ]
            ),
        ]

        torch.set_printoptions(precision=8, sci_mode=False)

        for proposal, expected_proposal_box, im_size, expected_objectness_logit in zip(
            proposals, expected_proposal_boxes, image_sizes, expected_objectness_logits
        ):
            self.assertEqual(len(proposal), len(expected_proposal_box))
            self.assertEqual(proposal.image_size, im_size)
            # It seems that there's some randomness in the result across different machines:
            # This test can be run on a local machine for 100 times with exactly the same result,
            # However, a different machine might produce slightly different results,
            # thus the atol here.
            err_msg = "computed proposal boxes = {}, expected {}".format(
                proposal.proposal_boxes.tensor, expected_proposal_box.tensor
            )
            self.assertTrue(
                torch.allclose(
                    proposal.proposal_boxes.tensor, expected_proposal_box.tensor, atol=1e-5
                ),
                err_msg,
            )

            err_msg = "computed objectness logits = {}, expected {}".format(
                proposal.objectness_logits, expected_objectness_logit
            )
            self.assertTrue(
                torch.allclose(proposal.objectness_logits, expected_objectness_logit, atol=1e-5),
                err_msg,
            )

    def test_rpn_proposals_inf(self):
        N, Hi, Wi, A = 3, 3, 3, 3
        images = ImageList.from_tensors([torch.rand(3, 20, 30) for i in range(N)])
        proposals = [torch.rand(N, Hi * Wi * A, 4)]
        pred_logits = [torch.rand(N, Hi * Wi * A)]
        pred_logits[0][1][3:5].fill_(float("inf"))
        find_top_rpn_proposals(
            proposals, pred_logits, images,
            0.5, "normal", 1000, 1000,
            0, False
        )


if __name__ == "__main__":
    unittest.main()
```

### tests/modeling/test_fast_rcnn.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import logging
import unittest
import torch

from cvpods.configs import RCNNConfig
from cvpods.modeling.box_regression import Box2BoxTransform, Box2BoxTransformRotated
from cvpods.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers, FastRCNNOutputs
from cvpods.modeling.roi_heads.rotated_fast_rcnn import RotatedFastRCNNOutputs
from cvpods.structures import Boxes, Instances, RotatedBoxes
from cvpods.utils import EventStorage

logger = logging.getLogger(__name__)


class FastRCNNTest(unittest.TestCase):
    def test_fast_rcnn(self):
        torch.manual_seed(132)
        cfg = RCNNConfig()
        cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS = (10, 10, 5, 5)
        box2box_transform = Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)

        box_head_output_size = 8
        num_classes = 5
        cls_agnostic_bbox_reg = False

        box_predictor = FastRCNNOutputLayers(
            box_head_output_size, num_classes, cls_agnostic_bbox_reg, box_dim=4
        )
        feature_pooled = torch.rand(2, box_head_output_size)
        pred_class_logits, pred_proposal_deltas = box_predictor(feature_pooled)
        image_shape = (10, 10)
        proposal_boxes = torch.tensor([[0.8, 1.1, 3.2, 2.8], [2.3, 2.5, 7, 8]], dtype=torch.float32)
        gt_boxes = torch.tensor([[1, 1, 3, 3], [2, 2, 6, 6]], dtype=torch.float32)
        result = Instances(image_shape)
        result.proposal_boxes = Boxes(proposal_boxes)
        result.gt_boxes = Boxes(gt_boxes)
        result.gt_classes = torch.tensor([1, 2])
        proposals = []
        proposals.append(result)
        smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA

        outputs = FastRCNNOutputs(
            box2box_transform, pred_class_logits, pred_proposal_deltas, proposals, smooth_l1_beta
        )
        with EventStorage():  # capture events in a new storage to discard them
            losses = outputs.losses()

        expected_losses = {
            "loss_cls": torch.tensor(1.7951188087),
            "loss_box_reg": torch.tensor(4.0357131958),
        }
        for name in expected_losses.keys():
            assert torch.allclose(losses[name], expected_losses[name])

    def test_fast_rcnn_empty_batch(self, device="cpu"):
        cfg = RCNNConfig()
        cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS = (10, 10, 5, 5)
        box2box_transform = Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)

        logits = torch.randn(0, 100, requires_grad=True, device=device)
        deltas = torch.randn(0, 4, requires_grad=True, device=device)

        smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA
        outputs = FastRCNNOutputs(
            box2box_transform, logits, deltas, [], smooth_l1_beta
        )
        with EventStorage():  # capture events in a new storage to discard them
            losses = outputs.losses()

        for value in losses.values():
            self.assertTrue(torch.allclose(value, torch.zeros_like(value)))

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_fast_rcnn_empty_batch_cuda(self):
        self.test_fast_rcnn_empty_batch(device=torch.device("cuda"))

    def test_fast_rcnn_rotated(self):
        torch.manual_seed(132)
        cfg = RCNNConfig()
        cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS = (10, 10, 5, 5, 1)
        box2box_transform = Box2BoxTransformRotated(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)

        box_head_output_size = 8
        num_classes = 5
        cls_agnostic_bbox_reg = False

        box_predictor = FastRCNNOutputLayers(
            box_head_output_size, num_classes, cls_agnostic_bbox_reg, box_dim=5
        )
        feature_pooled = torch.rand(2, box_head_output_size)
        pred_class_logits, pred_proposal_deltas = box_predictor(feature_pooled)
        image_shape = (10, 10)
        proposal_boxes = torch.tensor(
            [[2, 1.95, 2.4, 1.7, 0], [4.65, 5.25, 4.7, 5.5, 0]], dtype=torch.float32
        )
        gt_boxes = torch.tensor([[2, 2, 2, 2, 0], [4, 4, 4, 4, 0]], dtype=torch.float32)
        result = Instances(image_shape)
        result.proposal_boxes = RotatedBoxes(proposal_boxes)
        result.gt_boxes = RotatedBoxes(gt_boxes)
        result.gt_classes = torch.tensor([1, 2])
        proposals = []
        proposals.append(result)
        smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA

        outputs = RotatedFastRCNNOutputs(
            box2box_transform, pred_class_logits, pred_proposal_deltas, proposals, smooth_l1_beta
        )
        with EventStorage():  # capture events in a new storage to discard them
            losses = outputs.losses()

        # Note: the expected losses are slightly different even if
        # the boxes are essentially the same as in the FastRCNNOutput test, because
        # bbox_pred in FastRCNNOutputLayers have different Linear layers/initialization
        # between the two cases.
        expected_losses = {
            "loss_cls": torch.tensor(1.7920907736),
            "loss_box_reg": torch.tensor(4.0410838127),
        }
        for name in expected_losses.keys():
            assert torch.allclose(losses[name], expected_losses[name])


if __name__ == "__main__":
    unittest.main()
```

### tests/modeling/test_box2box_transform.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import logging
import unittest
import torch

from cvpods.modeling.box_regression import Box2BoxTransform, Box2BoxTransformRotated

logger = logging.getLogger(__name__)


def random_boxes(mean_box, stdev, N):
    return torch.rand(N, 4) * stdev + torch.tensor(mean_box, dtype=torch.float)


class TestBox2BoxTransform(unittest.TestCase):
    def test_reconstruction(self):
        weights = (5, 5, 10, 10)
        b2b_tfm = Box2BoxTransform(weights=weights)
        src_boxes = random_boxes([10, 10, 20, 20], 1, 10)
        dst_boxes = random_boxes([10, 10, 20, 20], 1, 10)

        devices = [torch.device("cpu")]
        if torch.cuda.is_available():
            devices.append(torch.device("cuda"))
        for device in devices:
            src_boxes = src_boxes.to(device=device)
            dst_boxes = dst_boxes.to(device=device)
            deltas = b2b_tfm.get_deltas(src_boxes, dst_boxes)
            dst_boxes_reconstructed = b2b_tfm.apply_deltas(deltas, src_boxes)
            assert torch.allclose(dst_boxes, dst_boxes_reconstructed)


def random_rotated_boxes(mean_box, std_length, std_angle, N):
    return torch.cat(
        [torch.rand(N, 4) * std_length, torch.rand(N, 1) * std_angle], dim=1
    ) + torch.tensor(mean_box, dtype=torch.float)


class TestBox2BoxTransformRotated(unittest.TestCase):
    def test_reconstruction(self):
        weights = (5, 5, 10, 10, 1)
        b2b_transform = Box2BoxTransformRotated(weights=weights)
        src_boxes = random_rotated_boxes([10, 10, 20, 20, -30], 5, 60.0, 10)
        dst_boxes = random_rotated_boxes([10, 10, 20, 20, -30], 5, 60.0, 10)

        devices = [torch.device("cpu")]
        if torch.cuda.is_available():
            devices.append(torch.device("cuda"))
        for device in devices:
            src_boxes = src_boxes.to(device=device)
            dst_boxes = dst_boxes.to(device=device)
            deltas = b2b_transform.get_deltas(src_boxes, dst_boxes)
            dst_boxes_reconstructed = b2b_transform.apply_deltas(deltas, src_boxes)
            assert torch.allclose(dst_boxes[:, :4], dst_boxes_reconstructed[:, :4], atol=1e-5)
            # angle difference has to be normalized
            assert torch.allclose(
                (dst_boxes[:, 4] - dst_boxes_reconstructed[:, 4] + 180.0) % 360.0 - 180.0,
                torch.zeros_like(dst_boxes[:, 4]),
                atol=1e-4,
            )


if __name__ == "__main__":
    unittest.main()
```

### tests/configs/test_config.py

```python
#!/usr/bin/env python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

# pylint: disable=W0613

import os
import tempfile
import unittest
import torch

# from cvpods.configs import configurable, downgrade_config, get_cfg, upgrade_config
from cvpods.layers import ShapeSpec

_V0_CFG = """
MODEL:
  RPN_HEAD:
    NAME: "TEST"
VERSION: 0
"""

_V1_CFG = """
MODEL:
  WEIGHT: "/path/to/weight"
"""

# flake8: noqa
# TODO: fix
@unittest.skip("Tests don't compatible cvpods.configs")
class TestConfigVersioning(unittest.TestCase):
    def test_upgrade_downgrade_consistency(self):
        cfg = get_cfg()
        # check that custom is preserved
        cfg.USER_CUSTOM = 1

        down = downgrade_config(cfg, to_version=0)
        up = upgrade_config(down)
        self.assertTrue(up == cfg)

    def _merge_cfg_str(self, cfg, merge_str):
        f = tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False)
        try:
            f.write(merge_str)
            f.close()
            cfg.merge_from_file(f.name)
        finally:
            os.remove(f.name)
        return cfg

    def test_auto_upgrade(self):
        cfg = get_cfg()
        latest_ver = cfg.VERSION
        cfg.USER_CUSTOM = 1

        self._merge_cfg_str(cfg, _V0_CFG)

        self.assertEqual(cfg.MODEL.RPN.HEAD_NAME, "TEST")
        self.assertEqual(cfg.VERSION, latest_ver)

    def test_guess_v1(self):
        cfg = get_cfg()
        latest_ver = cfg.VERSION
        self._merge_cfg_str(cfg, _V1_CFG)
        self.assertEqual(cfg.VERSION, latest_ver)

def configurable(func):
    pass

class _TestClassA(torch.nn.Module):
    @configurable
    def __init__(self, arg1, arg2, arg3=3):
        super().__init__()
        self.arg1 = arg1
        self.arg2 = arg2
        self.arg3 = arg3
        assert arg1 == 1
        assert arg2 == 2
        assert arg3 == 3

    @classmethod
    def from_config(cls, cfg):
        args = {"arg1": cfg.ARG1, "arg2": cfg.ARG2}
        return args


class _TestClassB(_TestClassA):
    @configurable
    def __init__(self, input_shape, arg1, arg2, arg3=3):
        """
        Doc of _TestClassB
        """
        assert input_shape == "shape"
        super().__init__(arg1, arg2, arg3)

    @classmethod
    def from_config(cls, cfg, input_shape):  # test extra positional arg in from_config
        args = {"arg1": cfg.ARG1, "arg2": cfg.ARG2}
        args["input_shape"] = input_shape
        return args


class _LegacySubClass(_TestClassB):
    # an old subclass written in cfg style
    def __init__(self, cfg, input_shape, arg4=4):
        super().__init__(cfg, input_shape)
        assert self.arg1 == 1
        assert self.arg2 == 2
        assert self.arg3 == 3


class _NewSubClassNewInit(_TestClassB):
    # test new subclass with a new __init__
    @configurable
    def __init__(self, input_shape, arg4=4, **kwargs):
        super().__init__(input_shape, **kwargs)
        assert self.arg1 == 1
        assert self.arg2 == 2
        assert self.arg3 == 3


class _LegacySubClassNotCfg(_TestClassB):
    # an old subclass written in cfg style, but argument is not called "cfg"
    def __init__(self, config, input_shape):
        super().__init__(config, input_shape)
        assert self.arg1 == 1
        assert self.arg2 == 2
        assert self.arg3 == 3


class _TestClassC(_TestClassB):
    @classmethod
    def from_config(cls, cfg, input_shape, **kwargs):  # test extra kwarg overwrite
        args = {"arg1": cfg.ARG1, "arg2": cfg.ARG2}
        args["input_shape"] = input_shape
        args.update(kwargs)
        return args


class _TestClassD(_TestClassA):
    @configurable
    def __init__(self, input_shape: ShapeSpec, arg1: int, arg2, arg3=3):
        assert input_shape == "shape"
        super().__init__(arg1, arg2, arg3)

    # _TestClassA.from_config does not have input_shape args.
    # Test whether input_shape will be forwarded to __init__


# flake8: noqa
# TODO: fix
@unittest.skip("Tests don't compatible cvpods.configs")
class TestConfigurable(unittest.TestCase):
    def testInitWithArgs(self):
        _ = _TestClassA(arg1=1, arg2=2, arg3=3)
        _ = _TestClassB("shape", arg1=1, arg2=2)
        _ = _TestClassC("shape", arg1=1, arg2=2)
        _ = _TestClassD("shape", arg1=1, arg2=2, arg3=3)

    def testPatchedAttr(self):
        self.assertTrue("Doc" in _TestClassB.__init__.__doc__)
        self.assertEqual(_TestClassD.__init__.__annotations__["arg1"], int)

    def testInitWithCfg(self):
        cfg = get_cfg()
        cfg.ARG1 = 1
        cfg.ARG2 = 2
        cfg.ARG3 = 3
        _ = _TestClassA(cfg)
        _ = _TestClassB(cfg, input_shape="shape")
        _ = _TestClassC(cfg, input_shape="shape")
        _ = _TestClassD(cfg, input_shape="shape")
        _ = _LegacySubClass(cfg, input_shape="shape")
        _ = _NewSubClassNewInit(cfg, input_shape="shape")
        _ = _LegacySubClassNotCfg(cfg, input_shape="shape")
        with self.assertRaises(TypeError):
            # disallow forwarding positional args to __init__ since it's prone to errors
            _ = _TestClassD(cfg, "shape")

        # call with kwargs instead
        _ = _TestClassA(cfg=cfg)
        _ = _TestClassB(cfg=cfg, input_shape="shape")
        _ = _TestClassC(cfg=cfg, input_shape="shape")
        _ = _TestClassD(cfg=cfg, input_shape="shape")
        _ = _LegacySubClass(cfg=cfg, input_shape="shape")
        _ = _NewSubClassNewInit(cfg=cfg, input_shape="shape")
        _ = _LegacySubClassNotCfg(config=cfg, input_shape="shape")

    def testInitWithCfgOverwrite(self):
        cfg = get_cfg()
        cfg.ARG1 = 1
        cfg.ARG2 = 999  # wrong config
        with self.assertRaises(AssertionError):
            _ = _TestClassA(cfg, arg3=3)

        # overwrite arg2 with correct config later:
        _ = _TestClassA(cfg, arg2=2, arg3=3)
        _ = _TestClassB(cfg, input_shape="shape", arg2=2, arg3=3)
        _ = _TestClassC(cfg, input_shape="shape", arg2=2, arg3=3)
        _ = _TestClassD(cfg, input_shape="shape", arg2=2, arg3=3)

        # call with kwargs cfg=cfg instead
        _ = _TestClassA(cfg=cfg, arg2=2, arg3=3)
        _ = _TestClassB(cfg=cfg, input_shape="shape", arg2=2, arg3=3)
        _ = _TestClassC(cfg=cfg, input_shape="shape", arg2=2, arg3=3)
        _ = _TestClassD(cfg=cfg, input_shape="shape", arg2=2, arg3=3)

    def testInitWithCfgWrongArgs(self):
        cfg = get_cfg()
        cfg.ARG1 = 1
        cfg.ARG2 = 2
        with self.assertRaises(TypeError):
            _ = _TestClassB(cfg, "shape", not_exist=1)
        with self.assertRaises(TypeError):
            _ = _TestClassC(cfg, "shape", not_exist=1)
        with self.assertRaises(TypeError):
            _ = _TestClassD(cfg, "shape", not_exist=1)

    def testBadClass(self):
        class _BadClass1:
            @configurable
            def __init__(self, a=1, b=2):
                pass

        class _BadClass2:
            @configurable
            def __init__(self, a=1, b=2):
                pass

            def from_config(self, cfg):  # noqa
                pass

        class _BadClass3:
            @configurable
            def __init__(self, a=1, b=2):
                pass

            # bad name: must be cfg
            @classmethod
            def from_config(cls, config):  # noqa
                pass

        with self.assertRaises(AttributeError):
            _ = _BadClass1(a=1)

        with self.assertRaises(TypeError):
            _ = _BadClass2(a=1)

        with self.assertRaises(TypeError):
            _ = _BadClass3(get_cfg())
```

### tests/data/test_coco.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import json
import numpy as np
import os
import tempfile
import unittest
import pycocotools.mask as mask_util

from cvpods.structures import BoxMode


def make_mask():
    """
    Makes a donut shaped binary mask.
    """
    H = 100
    W = 100
    mask = np.zeros([H, W], dtype=np.uint8)
    for x in range(W):
        for y in range(H):
            d = np.linalg.norm(np.array([W, H]) / 2 - np.array([x, y]))
            if d > 10 and d < 20:
                mask[y, x] = 1
    return mask


def uncompressed_rle(mask):
    lst = mask.flatten(order="F").tolist()
    counts = []
    p = False
    cnt = 0
    for i in lst:
        if i == p:
            cnt += 1
        else:
            counts.append(cnt)
            p = i
            cnt = 1
    counts.append(cnt)
    return {"counts": counts, "size": [mask.shape[0], mask.shape[1]]}


def make_dataset_dicts(mask, compressed: bool = True):
    """
    Returns a list of dicts that represents a single COCO data point for
    object detection. The single instance given by `mask` is represented by
    RLE, either compressed or uncompressed.
    """
    record = {}
    record["file_name"] = "test"
    record["image_id"] = 0
    record["height"] = mask.shape[0]
    record["width"] = mask.shape[1]

    y, x = np.nonzero(mask)
    if compressed:
        segmentation = mask_util.encode(np.asarray(mask, order="F"))
    else:
        segmentation = uncompressed_rle(mask)
    min_x = np.min(x)
    max_x = np.max(x)
    min_y = np.min(y)
    max_y = np.max(y)
    obj = {
        "bbox": [min_x, min_y, max_x, max_y],
        "bbox_mode": BoxMode.XYXY_ABS,
        "category_id": 0,
        "iscrowd": 0,
        "segmentation": segmentation,
    }
    record["annotations"] = [obj]
    return [record]


class TestRLEToJson(unittest.TestCase):
    @unittest.skip("DatasetCatalog, MetadataCatalog, load_coco_json not implemented")
    def test(self):
        from cvpods.data import DatasetCatalog, MetadataCatalog  # noqa
        from cvpods.data.datasets.coco import convert_to_coco_dict, load_coco_json  # noqa
        # Make a dummy dataset.
        mask = make_mask()
        DatasetCatalog.register("test_dataset", lambda: make_dataset_dicts(mask))
        MetadataCatalog.get("test_dataset").set(thing_classes=["test_label"])

        # Dump to json.
        json_dict = convert_to_coco_dict("test_dataset")
        with tempfile.TemporaryDirectory() as tmpdir:
            json_file_name = os.path.join(tmpdir, "test.json")
            with open(json_file_name, "w") as f:
                json.dump(json_dict, f)
            # Load from json.
            dicts = load_coco_json(json_file_name, "")

        # Check the loaded mask matches the original.
        anno = dicts[0]["annotations"][0]
        loaded_mask = mask_util.decode(anno["segmentation"])
        self.assertTrue(np.array_equal(loaded_mask, mask))

    def test_uncompressed_RLE(self):
        mask = make_mask()
        rle = mask_util.encode(np.asarray(mask, order="F"))
        uncompressed = uncompressed_rle(mask)
        compressed = mask_util.frPyObjects(uncompressed, *rle["size"])
        self.assertEqual(rle, compressed)
```

### tests/data/test_rotation_transform.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import numpy as np
import unittest

from cvpods.data.transforms.transform import RotationTransform


class TestRotationTransform(unittest.TestCase):
    def assertEqualsArrays(self, a1, a2):
        self.assertTrue(np.allclose(a1, a2))

    def randomData(self, h=5, w=5):
        image = np.random.rand(h, w)
        coords = np.array([[i, j] for j in range(h + 1) for i in range(w + 1)], dtype=float)
        return image, coords, h, w

    def test180(self):
        image, coords, h, w = self.randomData(6, 6)
        rot = RotationTransform(h, w, 180, expand=False, center=None)
        self.assertEqualsArrays(rot.apply_image(image), image[::-1, ::-1])
        rotated_coords = [[w - c[0], h - c[1]] for c in coords]
        self.assertEqualsArrays(rot.apply_coords(coords), rotated_coords)

    def test45_coords(self):
        _, coords, h, w = self.randomData(4, 6)
        rot = RotationTransform(h, w, 45, expand=False, center=None)
        rotated_coords = [
            [(x + y - (h + w) / 2) / np.sqrt(2) + w / 2, h / 2 + (y + (w - h) / 2 - x) / np.sqrt(2)]
            for (x, y) in coords
        ]
        self.assertEqualsArrays(rot.apply_coords(coords), rotated_coords)

    def test90(self):
        image, coords, h, w = self.randomData()
        rot = RotationTransform(h, w, 90, expand=False, center=None)
        self.assertEqualsArrays(rot.apply_image(image), image.T[::-1])
        rotated_coords = [[c[1], w - c[0]] for c in coords]
        self.assertEqualsArrays(rot.apply_coords(coords), rotated_coords)

    def test90_expand(self):  # non-square image
        image, coords, h, w = self.randomData(h=5, w=8)
        rot = RotationTransform(h, w, 90, expand=True, center=None)
        self.assertEqualsArrays(rot.apply_image(image), image.T[::-1])
        rotated_coords = [[c[1], w - c[0]] for c in coords]
        self.assertEqualsArrays(rot.apply_coords(coords), rotated_coords)

    def test_center_expand(self):
        # center has no effect if expand=True because it only affects shifting
        image, coords, h, w = self.randomData(h=5, w=8)
        angle = np.random.randint(360)
        rot1 = RotationTransform(h, w, angle, expand=True, center=None)
        rot2 = RotationTransform(h, w, angle, expand=True, center=(0, 0))
        rot3 = RotationTransform(h, w, angle, expand=True, center=(h, w))
        rot4 = RotationTransform(h, w, angle, expand=True, center=(2, 5))
        for r1 in [rot1, rot2, rot3, rot4]:
            for r2 in [rot1, rot2, rot3, rot4]:
                self.assertEqualsArrays(r1.apply_image(image), r2.apply_image(image))
                self.assertEqualsArrays(r1.apply_coords(coords), r2.apply_coords(coords))

    def test_inverse_transform(self):
        image, coords, h, w = self.randomData(h=5, w=8)
        rot = RotationTransform(h, w, 90, expand=True, center=None)
        rot_image = rot.apply_image(image)
        self.assertEqualsArrays(rot.inverse().apply_image(rot_image), image)
        rot = RotationTransform(h, w, 65, expand=True, center=None)
        rotated_coords = rot.apply_coords(coords)
        self.assertEqualsArrays(rot.inverse().apply_coords(rotated_coords), coords)


if __name__ == "__main__":
    unittest.main()
```

### tests/data/test_transforms.py

```python
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

# pylint: disable=W0613

import logging
import numpy as np
import unittest
from unittest import mock
from PIL import Image, ImageOps

from cvpods.data import detection_utils
from cvpods.data import transforms as T
from cvpods.utils import setup_logger

logger = logging.getLogger(__name__)


class TestTransforms(unittest.TestCase):
    def setUp(self):
        setup_logger()

    # flake8: noqa
    @unittest.skip("cvpods.data.transforms/transform_gen currently doesen't support rotated boxes")
    def test_apply_rotated_boxes(self):
        np.random.seed(125)
        cfg = get_cfg()
        is_train = True
        transform_gen = detection_utils.build_transform_gen(cfg, is_train)
        image = np.random.rand(200, 300)
        image, transforms = T.apply_transform_gens(transform_gen, image)
        image_shape = image.shape[:2]  # h, w
        assert image_shape == (800, 1200)
        annotation = {"bbox": [179, 97, 62, 40, -56]}

        boxes = np.array([annotation["bbox"]], dtype=np.float64)  # boxes.shape = (1, 5)
        transformed_bbox = transforms.apply_rotated_box(boxes)[0]

        expected_bbox = np.array([484, 388, 248, 160, 56], dtype=np.float64)
        err_msg = "transformed_bbox = {}, expected {}".format(transformed_bbox, expected_bbox)
        assert np.allclose(transformed_bbox, expected_bbox), err_msg

    @unittest.skip("cvpods.data.transforms/transform_gen currently doesen't support rotated boxes")
    def test_apply_rotated_boxes_unequal_scaling_factor(self):
        np.random.seed(125)
        h, w = 400, 200
        newh, neww = 800, 800
        image = np.random.rand(h, w)
        transform_gen = []
        transform_gen.append(T.Resize(shape=(newh, neww)))
        image, transforms = T.apply_transform_gens(transform_gen, image)
        image_shape = image.shape[:2]  # h, w
        assert image_shape == (newh, neww)

        boxes = np.array(
            [
                [150, 100, 40, 20, 0],
                [150, 100, 40, 20, 30],
                [150, 100, 40, 20, 90],
                [150, 100, 40, 20, -90],
            ],
            dtype=np.float64,
        )
        transformed_boxes = transforms.apply_rotated_box(boxes)

        expected_bboxes = np.array(
            [
                [600, 200, 160, 40, 0],
                [600, 200, 144.22205102, 52.91502622, 49.10660535],
                [600, 200, 80, 80, 90],
                [600, 200, 80, 80, -90],
            ],
            dtype=np.float64,
        )
        err_msg = "transformed_boxes = {}, expected {}".format(transformed_boxes, expected_bboxes)
        assert np.allclose(transformed_boxes, expected_bboxes), err_msg

    def test_print_transform_gen(self):
        t = T.RandomCrop("relative", (100, 100))
        self.assertTrue(str(t) == "RandomCrop(crop_type='relative', crop_size=(100, 100))")

        t = T.RandomFlip(prob=0.5)
        self.assertTrue(str(t) == "RandomFlip(prob=0.5)")

        t = T.RandomFlip()
        self.assertTrue(str(t) == "RandomFlip()")

    @unittest.skip("RandomApply not implemented")
    def test_random_apply_prob_out_of_range_check(self):
        test_probabilities = {0.0: True, 0.5: True, 1.0: True, -0.01: False, 1.01: False}

        for given_probability, is_valid in test_probabilities.items():
            if not is_valid:
                self.assertRaises(AssertionError, T.RandomApply, None, prob=given_probability)
            else:
                T.RandomApply(T.NoOpTransform(), prob=given_probability)

    @unittest.skip("RandomApply not implemented")
    def test_random_apply_wrapping_aug_probability_occured_evaluation(self):
        transform_mock = mock.MagicMock(name="MockTransform", spec=T.Augmentation)
        image_mock = mock.MagicMock(name="MockImage")
        random_apply = T.RandomApply(transform_mock, prob=0.001)

        with mock.patch.object(random_apply, "_rand_range", return_value=0.0001):
            transform = random_apply.get_transform(image_mock)
        transform_mock.get_transform.assert_called_once_with(image_mock)
        self.assertIsNot(transform, transform_mock)

    @unittest.skip("RandomApply not implemented")
    def test_random_apply_wrapping_std_transform_probability_occured_evaluation(self):
        transform_mock = mock.MagicMock(name="MockTransform", spec=T.Transform)
        image_mock = mock.MagicMock(name="MockImage")
        random_apply = T.RandomApply(transform_mock, prob=0.001)

        with mock.patch.object(random_apply, "_rand_range", return_value=0.0001):
            transform = random_apply.get_transform(image_mock)
        self.assertIs(transform, transform_mock)

    @unittest.skip("RandomApply not implemented")
    def test_random_apply_probability_not_occured_evaluation(self):
        transform_mock = mock.MagicMock(name="MockTransform", spec=T.Augmentation)
        image_mock = mock.MagicMock(name="MockImage")
        random_apply = T.RandomApply(transform_mock, prob=0.001)

        with mock.patch.object(random_apply, "_rand_range", return_value=0.9):
            transform = random_apply.get_transform(image_mock)
        transform_mock.get_transform.assert_not_called()
        self.assertIsInstance(transform, T.NoOpTransform)

    @unittest.skip("StandardAugInput not implemented")
    def test_augmentation_input_args(self):
        input_shape = (100, 100)
        output_shape = (50, 50)

        # define two augmentations with different args
        class TG1(T.Augmentation):
            input_args = ("image", "sem_seg")

            def get_transform(self, image, sem_seg):
                return T.ResizeTransform(
                    input_shape[0], input_shape[1], output_shape[0], output_shape[1]
                )

        class TG2(T.Augmentation):
            def get_transform(self, image):
                assert image.shape[:2] == output_shape  # check that TG1 is applied
                return T.HFlipTransform(output_shape[1])

        image = np.random.rand(*input_shape).astype("float32")
        sem_seg = (np.random.rand(*input_shape) < 0.5).astype("uint8")
        inputs = T.StandardAugInput(image, sem_seg=sem_seg)  # provide two args
        tfms = inputs.apply_augmentations([TG1(), TG2()])
        self.assertIsInstance(tfms[0], T.ResizeTransform)
        self.assertIsInstance(tfms[1], T.HFlipTransform)
        self.assertTrue(inputs.image.shape[:2] == output_shape)
        self.assertTrue(inputs.sem_seg.shape[:2] == output_shape)

        class TG3(T.Augmentation):
            input_args = ("image", "nonexist")

            def get_transform(self, image, nonexist):
                pass

        with self.assertRaises(AttributeError):
            inputs.apply_augmentations([TG3()])


    @unittest.skip("ColorTransform deprecated")
    def test_color_transforms(self):
        rand_img = np.random.random((100, 100, 3)) * 255
        rand_img = rand_img.astype("uint8")

        # Test no-op
        noop_transform = T.ColorTransform(lambda img: img)
        self.assertTrue(np.array_equal(rand_img, noop_transform.apply_image(rand_img)))

        # Test a ImageOps operation
        magnitude = np.random.randint(0, 256)
        solarize_transform = T.PILColorTransform(lambda img: ImageOps.solarize(img, magnitude))
        expected_img = ImageOps.solarize(Image.fromarray(rand_img), magnitude)
        self.assertTrue(np.array_equal(expected_img, solarize_transform.apply_image(rand_img)))
```

### tests/data/test_coco_evaluation.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import contextlib
import copy
import io
import json
import numpy as np
import os
import tempfile
import unittest
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval


class TestCOCOeval(unittest.TestCase):
    @unittest.skip("COCOeval_opt not implemented")
    def test(self):
        from cvpods.evaluation.fast_eval_api import COCOeval_opt
        # A small set of images/categories from COCO val
        # fmt: off
        detections = [{"image_id": 139, "category_id": 1, "bbox": [417.3332824707031, 159.27003479003906, 47.66064453125, 143.00193786621094], "score": 0.9949821829795837, "segmentation": {"size": [426, 640], "counts": "Tc`52W=3N0N4aNN^E7]:4XE1g:8kDMT;U100000001O1gE[Nk8h1dFiNY9Z1aFkN]9g2J3NdN`FlN`9S1cFRN07]9g1bFoM6;X9c1cFoM=8R9g1bFQN>3U9Y30O01OO1O001N2O1N1O4L4L5UNoE3V:CVF6Q:@YF9l9@ZF<k9[O`F=];HYnX2"}}, {"image_id": 139, "category_id": 1, "bbox": [383.5909118652344, 172.0777587890625, 17.959075927734375, 36.94813537597656], "score": 0.7685421705245972, "segmentation": {"size": [426, 640], "counts": "lZP5m0Z<300O100O100000001O00]OlC0T<OnCOT<OnCNX<JnC2bQT3"}}, {"image_id": 139, "category_id": 1, "bbox": [457.8359069824219, 158.88027954101562, 9.89764404296875, 8.771820068359375], "score": 0.07092753797769547, "segmentation": {"size": [426, 640], "counts": "bSo54T=2N2O1001O006ImiW2"}}] # noqa
        gt_annotations = {"categories": [{"supercategory": "person", "id": 1, "name": "person"}, {"supercategory": "furniture", "id": 65, "name": "bed"}], "images": [{"license": 4, "file_name": "000000000285.jpg", "coco_url": "http://images.cocodataset.org/val2017/000000000285.jpg", "height": 640, "width": 586, "date_captured": "2013-11-18 13:09:47", "flickr_url": "http://farm8.staticflickr.com/7434/9138147604_c6225224b8_z.jpg", "id": 285}, {"license": 2, "file_name": "000000000139.jpg", "coco_url": "http://images.cocodataset.org/val2017/000000000139.jpg", "height": 426, "width": 640, "date_captured": "2013-11-21 01:34:01", "flickr_url": "http://farm9.staticflickr.com/8035/8024364858_9c41dc1666_z.jpg", "id": 139}], "annotations": [{"segmentation": [[428.19, 219.47, 430.94, 209.57, 430.39, 210.12, 421.32, 216.17, 412.8, 217.27, 413.9, 214.24, 422.42, 211.22, 429.29, 201.6, 430.67, 181.8, 430.12, 175.2, 427.09, 168.06, 426.27, 164.21, 430.94, 159.26, 440.29, 157.61, 446.06, 163.93, 448.53, 168.06, 448.53, 173.01, 449.08, 174.93, 454.03, 185.1, 455.41, 188.4, 458.43, 195.0, 460.08, 210.94, 462.28, 226.61, 460.91, 233.76, 454.31, 234.04, 460.08, 256.85, 462.56, 268.13, 465.58, 290.67, 465.85, 293.14, 463.38, 295.62, 452.66, 295.34, 448.26, 294.52, 443.59, 282.7, 446.06, 235.14, 446.34, 230.19, 438.09, 232.39, 438.09, 221.67, 434.24, 221.12, 427.09, 219.74]], "area": 2913.1103999999987, "iscrowd": 0, "image_id": 139, "bbox": [412.8, 157.61, 53.05, 138.01], "category_id": 1, "id": 230831}, {"segmentation": [[384.98, 206.58, 384.43, 199.98, 385.25, 193.66, 385.25, 190.08, 387.18, 185.13, 387.18, 182.93, 386.08, 181.01, 385.25, 178.81, 385.25, 175.79, 388.0, 172.76, 394.88, 172.21, 398.72, 173.31, 399.27, 176.06, 399.55, 183.48, 397.9, 185.68, 395.15, 188.98, 396.8, 193.38, 398.45, 194.48, 399.0, 205.75, 395.43, 207.95, 388.83, 206.03]], "area": 435.1449499999997, "iscrowd": 0, "image_id": 139, "bbox": [384.43, 172.21, 15.12, 35.74], "category_id": 1, "id": 233201}]} # noqa
        # fmt: on

        # Test a small dataset for typical COCO format
        experiments = {"full": (detections, gt_annotations, {})}

        # Test what happens if the list of detections or ground truth annotations is empty
        experiments["empty_dt"] = ([], gt_annotations, {})
        gt = copy.deepcopy(gt_annotations)
        gt["annotations"] = []
        experiments["empty_gt"] = (detections, gt, {})

        # Test changing parameter settings
        experiments["no_categories"] = (detections, gt_annotations, {"useCats": 0})
        experiments["no_ious"] = (detections, gt_annotations, {"iouThrs": []})
        experiments["no_rec_thrs"] = (detections, gt_annotations, {"recThrs": []})
        experiments["no_max_dets"] = (detections, gt_annotations, {"maxDets": []})
        experiments["one_max_det"] = (detections, gt_annotations, {"maxDets": [1]})
        experiments["no_area"] = (detections, gt_annotations, {"areaRng": [], "areaRngLbl": []})

        # Test what happens if one omits different fields from the annotation structure
        annotation_fields = [
            "id",
            "image_id",
            "category_id",
            "score",
            "area",
            "iscrowd",
            "ignore",
            "bbox",
            "segmentation",
        ]
        for a in annotation_fields:
            gt = copy.deepcopy(gt_annotations)
            for g in gt["annotations"]:
                if a in g:
                    del g[a]
            dt = copy.deepcopy(detections)
            for d in dt:
                if a in d:
                    del d[a]
            experiments["omit_gt_" + a] = (detections, gt, {})
            experiments["omit_dt_" + a] = (dt, gt_annotations, {})

        # Compare precision/recall for original COCO PythonAPI to custom optimized one
        for name, (dt, gt, params) in experiments.items():
            # Dump to json.
            try:
                with tempfile.TemporaryDirectory() as tmpdir:
                    json_file_name = os.path.join(tmpdir, "gt_" + name + ".json")
                    with open(json_file_name, "w") as f:
                        json.dump(gt, f)
                    with contextlib.redirect_stdout(io.StringIO()):
                        coco_api = COCO(json_file_name)
            except Exception:
                pass

            for iou_type in ["bbox", "segm", "keypoints"]:
                # Run original COCOeval PythonAPI
                api_exception = None
                try:
                    with contextlib.redirect_stdout(io.StringIO()):
                        coco_dt = coco_api.loadRes(dt)
                        coco_eval = COCOeval(coco_api, coco_dt, iou_type)
                        for p, v in params.items():
                            setattr(coco_eval.params, p, v)
                        coco_eval.evaluate()
                        coco_eval.accumulate()
                        coco_eval.summarize()
                except Exception as ex:
                    api_exception = ex

                # Run optimized COCOeval_opt API
                opt_exception = None
                try:
                    with contextlib.redirect_stdout(io.StringIO()):
                        coco_dt = coco_api.loadRes(dt)
                        coco_eval_opt = COCOeval_opt(coco_api, coco_dt, iou_type)
                        for p, v in params.items():
                            setattr(coco_eval_opt.params, p, v)
                        coco_eval_opt.evaluate()
                        coco_eval_opt.accumulate()
                        coco_eval_opt.summarize()
                except Exception as ex:
                    opt_exception = ex

                if api_exception is not None and opt_exception is not None:
                    # Original API and optimized API should throw the same exception if annotation
                    # format is bad
                    api_error = "" if api_exception is None else type(api_exception).__name__
                    opt_error = "" if opt_exception is None else type(opt_exception).__name__
                    msg = "%s: comparing COCO APIs, '%s' != '%s'" % (name, api_error, opt_error)
                    self.assertTrue(api_error == opt_error, msg=msg)
                else:
                    # Original API and optimized API should produce the same precision/recalls
                    for k in ["precision", "recall"]:
                        diff = np.abs(coco_eval.eval[k] - coco_eval_opt.eval[k])
                        abs_diff = np.max(diff) if diff.size > 0 else 0.0
                        msg = "%s: comparing COCO APIs, %s differs by %f" % (name, k, abs_diff)
                        self.assertTrue(abs_diff < 1e-4, msg=msg)
```

### tests/engine/test_engine.py

```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved


import unittest
import torch
from torch import nn


from cvpods.engine import SimpleRunner
from torch.utils.data import Dataset


class SimpleDataset(Dataset):
    def __init__(self, length=100):
        self.data_list = torch.rand(length, 3, 3)

    def __getitem__(self, index):
        return self.data_list[index]


class SimpleModel(nn.Sequential):
    def forward(self, x):
        return {"loss": x.sum() + sum([x.mean() for x in self.parameters()])}


class TestTrainer(unittest.TestCase):
    def test_simple_trainer(self, device="cpu"):
        device = torch.device(device)
        model = SimpleModel(nn.Linear(10, 10)).to(device)

        class DataLoader:
            def __len__(self):
                return 10000

            def __iter__(self):
                while True:
                    yield torch.rand(3, 3).to(device)

        trainer = SimpleRunner(model, DataLoader(), torch.optim.SGD(model.parameters(), 0.1))
        trainer.max_epoch = None
        trainer.train(0, 0, 10)
        return trainer

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_simple_trainer_cuda(self):
        self.test_simple_trainer(device="cuda")
```

## docs/index.rst

```
.. cvpods documentation master file, created by
   sphinx-quickstart on Sat Sep 21 13:46:45 2019.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

Welcome to cvpods's documentation!
======================================

.. toctree::
   :maxdepth: 2

   modules/index
```

## docs/conf.py

```python
# -*- coding: utf-8 -*-
# Copyright (c) Facebook, Inc. and its affiliates.

# flake8: noqa
# pylint: disable=W0613

# Configuration file for the Sphinx documentation builder.
#
# This file does only contain a selection of the most common options. For a
# full list see the documentation:
# http://www.sphinx-doc.org/en/master/config

# -- Path setup --------------------------------------------------------------

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import os
import sys
from unittest import mock
from sphinx.domains import Domain
from typing import Dict, List, Tuple

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
import sphinx_rtd_theme

# to support markdown
from recommonmark.parser import CommonMarkParser

sys.path.insert(0, os.path.abspath("../"))
os.environ["DOC_BUILDING"] = "True"
DEPLOY = os.environ.get("READTHEDOCS") == "True"


# -- Project information -----------------------------------------------------

# fmt: off
try:
    import torch  # noqa
except ImportError:
    for m in [
        "torch", "torchvision", "torch.nn", "torch.nn.parallel", "torch.distributed", "torch.multiprocessing", "torch.autograd",
        "torch.autograd.function", "torch.nn.modules", "torch.nn.modules.utils", "torch.utils", "torch.utils.data", "torch.onnx",
        "torchvision", "torchvision.ops",
    ]:
        sys.modules[m] = mock.Mock(name=m)
    sys.modules['torch'].__version__ = "1.5"  # fake version

for m in [
    "cv2", "scipy", "portalocker", "cvpods._C",
    "pycocotools", "pycocotools.mask", "pycocotools.coco", "pycocotools.cocoeval",
    "google", "google.protobuf", "google.protobuf.internal", "onnx",
    "caffe2", "caffe2.proto", "caffe2.python", "caffe2.python.utils", "caffe2.python.onnx", "caffe2.python.onnx.backend",
]:
    sys.modules[m] = mock.Mock(name=m)
# fmt: on
sys.modules["cv2"].__version__ = "3.4"

import cvpods  # isort: skip


project = "cvpods"
copyright = "2019-2021, cvpods development team"
author = "cvpods development team"

# The short X.Y version
version = cvpods.__version__
# The full version, including alpha/beta/rc tags
release = version


# -- General configuration ---------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#
needs_sphinx = "3.0"

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    "recommonmark",
    "sphinx.ext.autodoc",
    "sphinx.ext.napoleon",
    "sphinx.ext.intersphinx",
    "sphinx.ext.todo",
    "sphinx.ext.coverage",
    "sphinx.ext.mathjax",
    "sphinx.ext.viewcode",
    "sphinx.ext.githubpages",
]

# -- Configurations for plugins ------------
napoleon_google_docstring = True
napoleon_include_init_with_doc = True
napoleon_include_special_with_doc = True
napoleon_numpy_docstring = False
napoleon_use_rtype = False
autodoc_inherit_docstrings = False
autodoc_member_order = "bysource"

if DEPLOY:
    intersphinx_timeout = 10
else:
    # skip this when building locally
    intersphinx_timeout = 0.1
intersphinx_mapping = {
    "python": ("https://docs.python.org/3.6", None),
    "numpy": ("https://docs.scipy.org/doc/numpy/", None),
    "torch": ("https://pytorch.org/docs/master/", None),
}
# -------------------------


# Add any paths that contain templates here, relative to this directory.
templates_path = ["_templates"]

source_suffix = [".rst", ".md"]

# The master toctree document.
master_doc = "index"

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#
# This is also used if you do content translation via gettext catalogs.
# Usually you set "language" from the command line for these cases.
language = None

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path.
exclude_patterns = ["_build", "Thumbs.db", ".DS_Store", "build", "README.md", "tutorials/README.md"]

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = "sphinx"


# -- Options for HTML output -------------------------------------------------

html_theme = "sphinx_rtd_theme"
html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#
# html_theme_options = {}

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ["_static"]
html_css_files = ["css/custom.css"]

# Custom sidebar templates, must be a dictionary that maps document names
# to template names.
#
# The default sidebars (for documents that don't match any pattern) are
# defined by theme itself.  Builtin themes are using these templates by
# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',
# 'searchbox.html']``.
#
# html_sidebars = {}


# -- Options for HTMLHelp output ---------------------------------------------

# Output file base name for HTML help builder.
htmlhelp_basename = "cvpodsdoc"


# -- Options for LaTeX output ------------------------------------------------

latex_elements = {
    # The paper size ('letterpaper' or 'a4paper').
    #
    # 'papersize': 'letterpaper',
    # The font size ('10pt', '11pt' or '12pt').
    #
    # 'pointsize': '10pt',
    # Additional stuff for the LaTeX preamble.
    #
    # 'preamble': '',
    # Latex figure (float) alignment
    #
    # 'figure_align': 'htbp',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title,
#  author, documentclass [howto, manual, or own class]).
latex_documents = [
    (master_doc, "cvpods.tex", "cvpods Documentation", "cvpods development team", "manual")
]


# -- Options for manual page output ------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [(master_doc, "cvpods", "cvpods Documentation", [author], 1)]


# -- Options for Texinfo output ----------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    (
        master_doc,
        "cvpods",
        "cvpods Documentation",
        author,
        "cvpods",
        "One line description of project.",
        "Miscellaneous",
    )
]


# -- Options for todo extension ----------------------------------------------

# If true, `todo` and `todoList` produce output, else they produce nothing.
todo_include_todos = True


def autodoc_skip_member(app, what, name, obj, skip, options):
    # we hide something deliberately
    if getattr(obj, "__HIDE_SPHINX_DOC__", False):
        return True

    # Hide some that are deprecated or not intended to be used
    HIDDEN = {
        "ResNetBlockBase",
        "GroupedBatchSampler",
        "build_transform_gen",
        "export_caffe2_model",
        "export_onnx_model",
        "apply_transform_gens",
        "TransformGen",
        "apply_augmentations",
        "StandardAugInput",
        "build_batch_data_loader",
        "draw_panoptic_seg_predictions",
    }
    try:
        if name in HIDDEN or (
            hasattr(obj, "__doc__") and obj.__doc__.lower().strip().startswith("deprecated")
        ):
            print("Skipping deprecated object: {}".format(name))
            return True
    except:
        pass
    return skip


_PAPER_DATA = {
    "resnet": ("1512.03385", "Deep Residual Learning for Image Recognition"),
    "fpn": ("1612.03144", "Feature Pyramid Networks for Object Detection"),
    "mask r-cnn": ("1703.06870", "Mask R-CNN"),
    "faster r-cnn": (
        "1506.01497",
        "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
    ),
    "deformconv": ("1703.06211", "Deformable Convolutional Networks"),
    "deformconv2": ("1811.11168", "Deformable ConvNets v2: More Deformable, Better Results"),
    "panopticfpn": ("1901.02446", "Panoptic Feature Pyramid Networks"),
    "retinanet": ("1708.02002", "Focal Loss for Dense Object Detection"),
    "cascade r-cnn": ("1712.00726", "Cascade R-CNN: Delving into High Quality Object Detection"),
    "lvis": ("1908.03195", "LVIS: A Dataset for Large Vocabulary Instance Segmentation"),
    "rrpn": ("1703.01086", "Arbitrary-Oriented Scene Text Detection via Rotation Proposals"),
    "imagenet in 1h": ("1706.02677", "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"),
    "xception": ("1610.02357", "Xception: Deep Learning with Depthwise Separable Convolutions"),
    "mobilenet": (
        "1704.04861",
        "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
    ),
}


def paper_ref_role(
    typ: str,
    rawtext: str,
    text: str,
    lineno: int,
    inliner,
    options: Dict = {},
    content: List[str] = [],
):
    """
    Parse :paper:`xxx`. Similar to the "extlinks" sphinx extension.
    """
    from docutils import nodes, utils
    from sphinx.util.nodes import split_explicit_title

    text = utils.unescape(text)
    has_explicit_title, title, link = split_explicit_title(text)
    link = link.lower()
    if link not in _PAPER_DATA:
        inliner.reporter.warning("Cannot find paper " + link)
        paper_url, paper_title = "#", link
    else:
        paper_url, paper_title = _PAPER_DATA[link]
        if "/" not in paper_url:
            paper_url = "https://arxiv.org/abs/" + paper_url
    if not has_explicit_title:
        title = paper_title
    pnode = nodes.reference(title, title, internal=False, refuri=paper_url)
    return [pnode], []


def setup(app):
    from recommonmark.transform import AutoStructify

    # app.connect("autodoc-skip-member", utodoc_skip_member)
    app.add_role("paper", paper_ref_role)
    app.add_config_value(
        "recommonmark_config",
        {"enable_math": True, "enable_inline_math": True, "enable_eval_rst": True},
        True,
    )
    app.add_transform(AutoStructify)
```

#### docs/_static/css/custom.css

```css
/*
 * Copyright (c) Facebook, Inc. and its affiliates.
 * some extra css to make markdown look similar between github/sphinx
 */

/*
 * Below is for install.md:
 */
.rst-content code {
  white-space: pre;
  border: 0px;
}

.rst-content th {
  border: 1px solid #e1e4e5;
}

.rst-content th p {
  /* otherwise will be default 24px for regular paragraph */
  margin-bottom: 0px;
}

div.section > details {
  padding-bottom: 1em;
}
```

### docs/modules/checkpoint.rst

```
cvpods.checkpoint package
=============================

.. automodule:: cvpods.checkpoint
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.checkpoint.checkpoint module
--------------------------------------

.. automodule:: cvpods.checkpoint.checkpoint
    :members:
    :undoc-members:
    :show-inheritance:

```

### docs/modules/evaluation.rst

```
cvpods.evaluation package
=============================

.. automodule:: cvpods.evaluation
    :members:
    :undoc-members:
    :show-inheritance:
```

### docs/modules/index.rst

```
API Documentation
==================

.. toctree::

    checkpoint
    configs
    data
    engine
    evaluation
    layers
    modeling
    solver
    structures
    utils
```

### docs/modules/solver.rst

```
cvpods.solver package
=========================

.. automodule:: cvpods.solver
    :members:
    :undoc-members:
    :show-inheritance:
```

### docs/modules/layers.rst

```
cvpods.layers package
=========================

.. automodule:: cvpods.layers
    :members:
    :undoc-members:
    :show-inheritance:
```

### docs/modules/engine.rst

```
cvpods.engine package
=========================


.. automodule:: cvpods.engine
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.engine.hooks module
---------------------------------

.. automodule:: cvpods.engine.hooks
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.engine.launch module
---------------------------------

.. automodule:: cvpods.engine.launch
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.engine.setup module
---------------------------------

.. automodule:: cvpods.engine.setup
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.engine.base_runner module
---------------------------------

.. automodule:: cvpods.engine.base_runner
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.engine.runner module
---------------------------------

.. automodule:: cvpods.engine.runner
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.engine.predictor module
----------------------------------

.. automodule:: cvpods.engine.predictor
    :members:
    :undoc-members:
    :show-inheritance:
```

### docs/modules/modeling.rst

```
cvpods.modeling package
===========================

.. automodule:: cvpods.modeling
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.modeilng.anchor\_generator module
-------------------------------------------

.. automodule:: cvpods.modeling.anchor_generator
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.modeilng.backbone module
---------------------------------------

.. automodule:: cvpods.modeling.backbone
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.modeilng.basenet module
---------------------------------------

.. automodule:: cvpods.modeling.basenet
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.modeilng.box\_regression module
---------------------------------------

.. automodule:: cvpods.modeling.box_regression
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.modeilng.losses module
-----------------------------------

.. automodule:: cvpods.modeling.losses
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.modeilng.matcher module
---------------------------------------

.. automodule:: cvpods.modeling.matcher
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.modeilng.meta\_arch module
---------------------------------------

.. automodule:: cvpods.modeling.meta_arch
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.modeilng.nn\_utils module
---------------------------------------

.. automodule:: cvpods.modeling.nn_utils
    :members:
    :undoc-members:
    :show-inheritance:


cvpods.modeilng.poolers module
---------------------------------------

.. automodule:: cvpods.modeling.poolers
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.modeilng.postprocessing module
-----------------------------------------

.. automodule:: cvpods.modeling.postprocessing
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.modeilng.proposal\_generator module
----------------------------------------------

.. automodule:: cvpods.modeling.proposal_generator
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.modeilng.roi\_heads module
---------------------------------------

.. automodule:: cvpods.modeling.roi_heads
    :members:
    :undoc-members:
    :show-inheritance:


cvpods.modeilng.sampling module
------------------------------------

.. automodule:: cvpods.modeling.sampling
    :members:
    :undoc-members:
    :show-inheritance:


cvpods.modeilng.test\_time\_augmentation module
----------------------------------------------------

.. automodule:: cvpods.modeling.test_time_augmentation
    :members:
    :undoc-members:
    :show-inheritance:
```

### docs/modules/configs.rst

```
cvpods.configs package
=========================

.. automodule:: cvpods.configs.base_config
    :members:
    :undoc-members:
    :show-inheritance:
    :inherited-members:

cvpods.configs.config\_helper module
----------------------------------------

.. automodule:: cvpods.configs.config_helper
    :members:
    :undoc-members:
    :show-inheritance:

```

### docs/modules/data.rst

```
cvpods.data package
=======================

.. automodule:: cvpods.data
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.data.catalog module
--------------------------------

cvpods.data.detection\_utils module
---------------------------------------

.. automodule:: cvpods.data.detection_utils
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.data.datasets module
---------------------------------------

.. automodule:: cvpods.data.datasets
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.data.samplers module
---------------------------------------

.. automodule:: cvpods.data.samplers
    :members:
    :undoc-members:
    :show-inheritance:


cvpods.data.transforms module
---------------------------------------

.. automodule:: cvpods.data.transforms
    :members:
    :undoc-members:
    :show-inheritance:
```

### docs/modules/structures.rst

```
cvpods.structures package
=============================

.. automodule:: cvpods.structures
    :members:
    :undoc-members:
    :show-inheritance:
```

### docs/modules/utils.rst

```
cvpods.utils package
========================

cvpods.utils.benchmark.benchmark module
------------------------------------------

.. automodule:: cvpods.utils.benchmark.benchmark
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.utils.benchmark.timer module
---------------------------------------

.. automodule:: cvpods.utils.benchmark.timer
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.utils.distributed.comm module
----------------------------------------

.. automodule:: cvpods.utils.distributed.comm
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.utils.env.collect\_env module
----------------------------------------

.. automodule:: cvpods.utils.env.collect_env
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.utils.env.env module
--------------------------------

.. automodule:: cvpods.utils.env.env
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.utils.file.download module
-------------------------------------

.. automodule:: cvpods.utils.file.download
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.utils.file.file\_io module
-------------------------------------

.. automodule:: cvpods.utils.file.file_io
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.utils.imports module
--------------------------------

.. automodule:: cvpods.utils.imports
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.utils.memory module
------------------------------------

.. automodule:: cvpods.utils.memory
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.utils.visualizer.colormap module
-------------------------------------------

.. automodule:: cvpods.utils.visualizer.colormap
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.utils.visualizer.video\_visualizer module
-----------------------------------------------------

.. automodule:: cvpods.utils.visualizer.video_visualizer
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.utils.visualizer.visualizer module
----------------------------------------------

.. automodule:: cvpods.utils.visualizer.visualizer
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.utils.file.serialize module
----------------------------------

.. automodule:: cvpods.utils.file.serialize
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.utils.dump.events module
--------------------------------------

.. automodule:: cvpods.utils.dump.events
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.utils.dump.history\_buffer module
----------------------------------------------

.. automodule:: cvpods.utils.dump.history_buffer
    :members:
    :undoc-members:
    :show-inheritance:

cvpods.utils.dump.logger module
-------------------------------------

.. automodule:: cvpods.utils.dump.logger
    :members:
    :undoc-members:
    :show-inheritance:

```

