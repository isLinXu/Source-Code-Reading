# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import ast
import json
import platform
import zipfile
from collections import OrderedDict, namedtuple
from pathlib import Path

import cv2
import numpy as np
import torch
import torch.nn as nn
from PIL import Image

from ultralytics.utils import ARM64, IS_JETSON, IS_RASPBERRYPI, LINUX, LOGGER, PYTHON_VERSION, ROOT, yaml_load
from ultralytics.utils.checks import check_requirements, check_suffix, check_version, check_yaml, is_rockchip
from ultralytics.utils.downloads import attempt_download_asset, is_url


# def check_class_names(names):
#     """
#     Check class names.

#     Map imagenet class codes to human-readable names if required. Convert lists to dicts.
#     """
#     if isinstance(names, list):  # names is a list
#         names = dict(enumerate(names))  # convert to dict
#     if isinstance(names, dict):
#         # Convert 1) string keys to int, i.e. '0' to 0, and non-string values to strings, i.e. True to 'True'
#         names = {int(k): str(v) for k, v in names.items()}
#         n = len(names)
#         if max(names.keys()) >= n:
#             raise KeyError(
#                 f"{n}-class dataset requires class indices 0-{n - 1}, but you have invalid class indices "
#                 f"{min(names.keys())}-{max(names.keys())} defined in your dataset YAML."
#             )
#         if isinstance(names[0], str) and names[0].startswith("n0"):  # imagenet class codes, i.e. 'n01440764'
#             names_map = yaml_load(ROOT / "cfg/datasets/ImageNet.yaml")["map"]  # human-readable names
#             names = {k: names_map[v] for k, v in names.items()}
#     return names


# def default_class_names(data=None):
#     """Applies default class names to an input YAML file or returns numerical class names."""
#     if data:
#         try:
#             return yaml_load(check_yaml(data))["names"]
#         except Exception:
#             pass
#     return {i: f"class{i}" for i in range(999)}  # return default if above errors


# class AutoBackend(nn.Module):
#     """
#     Handles dynamic backend selection for running inference using Ultralytics YOLO models.

#     The AutoBackend class is designed to provide an abstraction layer for various inference engines. It supports a wide
#     range of formats, each with specific naming conventions as outlined below:

#         Supported Formats and Naming Conventions:
#             | Format                | File Suffix       |
#             | --------------------- | ----------------- |
#             | PyTorch               | *.pt              |
#             | TorchScript           | *.torchscript     |
#             | ONNX Runtime          | *.onnx            |
#             | ONNX OpenCV DNN       | *.onnx (dnn=True) |
#             | OpenVINO              | *openvino_model/  |
#             | CoreML                | *.mlpackage       |
#             | TensorRT              | *.engine          |
#             | TensorFlow SavedModel | *_saved_model/    |
#             | TensorFlow GraphDef   | *.pb              |
#             | TensorFlow Lite       | *.tflite          |
#             | TensorFlow Edge TPU   | *_edgetpu.tflite  |
#             | PaddlePaddle          | *_paddle_model/   |
#             | MNN                   | *.mnn             |
#             | NCNN                  | *_ncnn_model/     |
#             | IMX                   | *_imx_model/      |
#             | RKNN                  | *_rknn_model/     |

#     This class offers dynamic backend switching capabilities based on the input model format, making it easier to deploy
#     models across various platforms.
#     """

#     @torch.no_grad()
#     def __init__(
#         self,
#         weights="yolo11n.pt",
#         device=torch.device("cpu"),
#         dnn=False,
#         data=None,
#         fp16=False,
#         batch=1,
#         fuse=True,
#         verbose=True,
#     ):
#         """
#         Initialize the AutoBackend for inference.

#         Args:
#             weights (str | torch.nn.Module): Path to the model weights file or a module instance. Defaults to 'yolo11n.pt'.
#             device (torch.device): Device to run the model on. Defaults to CPU.
#             dnn (bool): Use OpenCV DNN module for ONNX inference. Defaults to False.
#             data (str | Path | optional): Path to the additional data.yaml file containing class names. Optional.
#             fp16 (bool): Enable half-precision inference. Supported only on specific backends. Defaults to False.
#             batch (int): Batch-size to assume for inference.
#             fuse (bool): Fuse Conv2D + BatchNorm layers for optimization. Defaults to True.
#             verbose (bool): Enable verbose logging. Defaults to True.
#         """
#         super().__init__()
#         w = str(weights[0] if isinstance(weights, list) else weights)
#         nn_module = isinstance(weights, torch.nn.Module)
#         (
#             pt,
#             jit,
#             onnx,
#             xml,
#             engine,
#             coreml,
#             saved_model,
#             pb,
#             tflite,
#             edgetpu,
#             tfjs,
#             paddle,
#             mnn,
#             ncnn,
#             imx,
#             rknn,
#             triton,
#         ) = self._model_type(w)
#         fp16 &= pt or jit or onnx or xml or engine or nn_module or triton  # FP16
#         nhwc = coreml or saved_model or pb or tflite or edgetpu or rknn  # BHWC formats (vs torch BCWH)
#         stride = 32  # default stride
#         end2end = False  # default end2end
#         model, metadata, task = None, None, None

#         # Set device
#         cuda = torch.cuda.is_available() and device.type != "cpu"  # use CUDA
#         if cuda and not any([nn_module, pt, jit, engine, onnx, paddle]):  # GPU dataloader formats
#             device = torch.device("cpu")
#             cuda = False

#         # Download if not local
#         if not (pt or triton or nn_module):
#             w = attempt_download_asset(w)

#         # In-memory PyTorch model
#         if nn_module:
#             model = weights.to(device)
#             if fuse:
#                 model = model.fuse(verbose=verbose)
#             if hasattr(model, "kpt_shape"):
#                 kpt_shape = model.kpt_shape  # pose-only
#             stride = max(int(model.stride.max()), 32)  # model stride
#             names = model.module.names if hasattr(model, "module") else model.names  # get class names
#             model.half() if fp16 else model.float()
#             self.model = model  # explicitly assign for to(), cpu(), cuda(), half()
#             pt = True

#         # PyTorch
#         elif pt:
#             from ultralytics.nn.tasks import attempt_load_weights

#             model = attempt_load_weights(
#                 weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse
#             )
#             if hasattr(model, "kpt_shape"):
#                 kpt_shape = model.kpt_shape  # pose-only
#             stride = max(int(model.stride.max()), 32)  # model stride
#             names = model.module.names if hasattr(model, "module") else model.names  # get class names
#             model.half() if fp16 else model.float()
#             self.model = model  # explicitly assign for to(), cpu(), cuda(), half()

#         # TorchScript
#         elif jit:
#             LOGGER.info(f"Loading {w} for TorchScript inference...")
#             extra_files = {"config.txt": ""}  # model metadata
#             model = torch.jit.load(w, _extra_files=extra_files, map_location=device)
#             model.half() if fp16 else model.float()
#             if extra_files["config.txt"]:  # load metadata dict
#                 metadata = json.loads(extra_files["config.txt"], object_hook=lambda x: dict(x.items()))

#         # ONNX OpenCV DNN
#         elif dnn:
#             LOGGER.info(f"Loading {w} for ONNX OpenCV DNN inference...")
#             check_requirements("opencv-python>=4.5.4")
#             net = cv2.dnn.readNetFromONNX(w)

#         # ONNX Runtime and IMX
#         elif onnx or imx:
#             LOGGER.info(f"Loading {w} for ONNX Runtime inference...")
#             check_requirements(("onnx", "onnxruntime-gpu" if cuda else "onnxruntime"))
#             if IS_RASPBERRYPI or IS_JETSON:
#                 # Fix 'numpy.linalg._umath_linalg' has no attribute '_ilp64' for TF SavedModel on RPi and Jetson
#                 check_requirements("numpy==1.23.5")
#             import onnxruntime

#             providers = ["CPUExecutionProvider"]
#             if cuda and "CUDAExecutionProvider" in onnxruntime.get_available_providers():
#                 providers.insert(0, "CUDAExecutionProvider")
#             elif cuda:  # Only log warning if CUDA was requested but unavailable
#                 LOGGER.warning("WARNING âš ï¸ Failed to start ONNX Runtime with CUDA. Using CPU...")
#                 device = torch.device("cpu")
#                 cuda = False
#             LOGGER.info(f"Using ONNX Runtime {providers[0]}")
#             if onnx:
#                 session = onnxruntime.InferenceSession(w, providers=providers)
#             else:
#                 check_requirements(
#                     ["model-compression-toolkit==2.1.1", "sony-custom-layers[torch]==0.2.0", "onnxruntime-extensions"]
#                 )
#                 w = next(Path(w).glob("*.onnx"))
#                 LOGGER.info(f"Loading {w} for ONNX IMX inference...")
#                 import mct_quantizers as mctq
#                 from sony_custom_layers.pytorch.object_detection import nms_ort  # noqa

#                 session = onnxruntime.InferenceSession(
#                     w, mctq.get_ort_session_options(), providers=["CPUExecutionProvider"]
#                 )
#                 task = "detect"

#             output_names = [x.name for x in session.get_outputs()]
#             metadata = session.get_modelmeta().custom_metadata_map
#             dynamic = isinstance(session.get_outputs()[0].shape[0], str)
#             fp16 = True if "float16" in session.get_inputs()[0].type else False
#             if not dynamic:
#                 io = session.io_binding()
#                 bindings = []
#                 for output in session.get_outputs():
#                     out_fp16 = "float16" in output.type
#                     y_tensor = torch.empty(output.shape, dtype=torch.float16 if out_fp16 else torch.float32).to(device)
#                     io.bind_output(
#                         name=output.name,
#                         device_type=device.type,
#                         device_id=device.index if cuda else 0,
#                         element_type=np.float16 if out_fp16 else np.float32,
#                         shape=tuple(y_tensor.shape),
#                         buffer_ptr=y_tensor.data_ptr(),
#                     )
#                     bindings.append(y_tensor)

#         # OpenVINO
#         elif xml:
#             LOGGER.info(f"Loading {w} for OpenVINO inference...")
#             check_requirements("openvino>=2024.0.0")
#             import openvino as ov

#             core = ov.Core()
#             w = Path(w)
#             if not w.is_file():  # if not *.xml
#                 w = next(w.glob("*.xml"))  # get *.xml file from *_openvino_model dir
#             ov_model = core.read_model(model=str(w), weights=w.with_suffix(".bin"))
#             if ov_model.get_parameters()[0].get_layout().empty:
#                 ov_model.get_parameters()[0].set_layout(ov.Layout("NCHW"))

#             # OpenVINO inference modes are 'LATENCY', 'THROUGHPUT' (not recommended), or 'CUMULATIVE_THROUGHPUT'
#             inference_mode = "CUMULATIVE_THROUGHPUT" if batch > 1 else "LATENCY"
#             LOGGER.info(f"Using OpenVINO {inference_mode} mode for batch={batch} inference...")
#             ov_compiled_model = core.compile_model(
#                 ov_model,
#                 device_name="AUTO",  # AUTO selects best available device, do not modify
#                 config={"PERFORMANCE_HINT": inference_mode},
#             )
#             input_name = ov_compiled_model.input().get_any_name()
#             metadata = w.parent / "metadata.yaml"

#         # TensorRT
#         elif engine:
#             LOGGER.info(f"Loading {w} for TensorRT inference...")

#             if IS_JETSON and PYTHON_VERSION <= "3.8.0":
#                 # fix error: `np.bool` was a deprecated alias for the builtin `bool` for JetPack 4 with Python <= 3.8.0
#                 check_requirements("numpy==1.23.5")

#             try:
#                 import tensorrt as trt  # noqa https://developer.nvidia.com/nvidia-tensorrt-download
#             except ImportError:
#                 if LINUX:
#                     check_requirements("tensorrt>7.0.0,!=10.1.0")
#                 import tensorrt as trt  # noqa
#             check_version(trt.__version__, ">=7.0.0", hard=True)
#             check_version(trt.__version__, "!=10.1.0", msg="https://github.com/ultralytics/ultralytics/pull/14239")
#             if device.type == "cpu":
#                 device = torch.device("cuda:0")
#             Binding = namedtuple("Binding", ("name", "dtype", "shape", "data", "ptr"))
#             logger = trt.Logger(trt.Logger.INFO)
#             # Read file
#             with open(w, "rb") as f, trt.Runtime(logger) as runtime:
#                 try:
#                     meta_len = int.from_bytes(f.read(4), byteorder="little")  # read metadata length
#                     metadata = json.loads(f.read(meta_len).decode("utf-8"))  # read metadata
#                 except UnicodeDecodeError:
#                     f.seek(0)  # engine file may lack embedded Ultralytics metadata
#                 dla = metadata.get("dla", None)
#                 if dla is not None:
#                     runtime.DLA_core = int(dla)
#                 model = runtime.deserialize_cuda_engine(f.read())  # read engine

#             # Model context
#             try:
#                 context = model.create_execution_context()
#             except Exception as e:  # model is None
#                 LOGGER.error(f"ERROR: TensorRT model exported with a different version than {trt.__version__}\n")
#                 raise e

#             bindings = OrderedDict()
#             output_names = []
#             fp16 = False  # default updated below
#             dynamic = False
#             is_trt10 = not hasattr(model, "num_bindings")
#             num = range(model.num_io_tensors) if is_trt10 else range(model.num_bindings)
#             for i in num:
#                 if is_trt10:
#                     name = model.get_tensor_name(i)
#                     dtype = trt.nptype(model.get_tensor_dtype(name))
#                     is_input = model.get_tensor_mode(name) == trt.TensorIOMode.INPUT
#                     if is_input:
#                         if -1 in tuple(model.get_tensor_shape(name)):
#                             dynamic = True
#                             context.set_input_shape(name, tuple(model.get_tensor_profile_shape(name, 0)[1]))
#                         if dtype == np.float16:
#                             fp16 = True
#                     else:
#                         output_names.append(name)
#                     shape = tuple(context.get_tensor_shape(name))
#                 else:  # TensorRT < 10.0
#                     name = model.get_binding_name(i)
#                     dtype = trt.nptype(model.get_binding_dtype(i))
#                     is_input = model.binding_is_input(i)
#                     if model.binding_is_input(i):
#                         if -1 in tuple(model.get_binding_shape(i)):  # dynamic
#                             dynamic = True
#                             context.set_binding_shape(i, tuple(model.get_profile_shape(0, i)[1]))
#                         if dtype == np.float16:
#                             fp16 = True
#                     else:
#                         output_names.append(name)
#                     shape = tuple(context.get_binding_shape(i))
#                 im = torch.from_numpy(np.empty(shape, dtype=dtype)).to(device)
#                 bindings[name] = Binding(name, dtype, shape, im, int(im.data_ptr()))
#             binding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())
#             batch_size = bindings["images"].shape[0]  # if dynamic, this is instead max batch size

#         # CoreML
#         elif coreml:
#             LOGGER.info(f"Loading {w} for CoreML inference...")
#             import coremltools as ct

#             model = ct.models.MLModel(w)
#             metadata = dict(model.user_defined_metadata)

#         # TF SavedModel
#         elif saved_model:
#             LOGGER.info(f"Loading {w} for TensorFlow SavedModel inference...")
#             import tensorflow as tf

#             keras = False  # assume TF1 saved_model
#             model = tf.keras.models.load_model(w) if keras else tf.saved_model.load(w)
#             metadata = Path(w) / "metadata.yaml"

#         # TF GraphDef
#         elif pb:  # https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt
#             LOGGER.info(f"Loading {w} for TensorFlow GraphDef inference...")
#             import tensorflow as tf

#             from ultralytics.engine.exporter import gd_outputs

#             def wrap_frozen_graph(gd, inputs, outputs):
#                 """Wrap frozen graphs for deployment."""
#                 x = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=""), [])  # wrapped
#                 ge = x.graph.as_graph_element
#                 return x.prune(tf.nest.map_structure(ge, inputs), tf.nest.map_structure(ge, outputs))

#             gd = tf.Graph().as_graph_def()  # TF GraphDef
#             with open(w, "rb") as f:
#                 gd.ParseFromString(f.read())
#             frozen_func = wrap_frozen_graph(gd, inputs="x:0", outputs=gd_outputs(gd))
#             try:  # find metadata in SavedModel alongside GraphDef
#                 metadata = next(Path(w).resolve().parent.rglob(f"{Path(w).stem}_saved_model*/metadata.yaml"))
#             except StopIteration:
#                 pass

#         # TFLite or TFLite Edge TPU
#         elif tflite or edgetpu:  # https://www.tensorflow.org/lite/guide/python#install_tensorflow_lite_for_python
#             try:  # https://coral.ai/docs/edgetpu/tflite-python/#update-existing-tf-lite-code-for-the-edge-tpu
#                 from tflite_runtime.interpreter import Interpreter, load_delegate
#             except ImportError:
#                 import tensorflow as tf

#                 Interpreter, load_delegate = tf.lite.Interpreter, tf.lite.experimental.load_delegate
#             if edgetpu:  # TF Edge TPU https://coral.ai/software/#edgetpu-runtime
#                 device = device[3:] if str(device).startswith("tpu") else ":0"
#                 LOGGER.info(f"Loading {w} on device {device[1:]} for TensorFlow Lite Edge TPU inference...")
#                 delegate = {"Linux": "libedgetpu.so.1", "Darwin": "libedgetpu.1.dylib", "Windows": "edgetpu.dll"}[
#                     platform.system()
#                 ]
#                 interpreter = Interpreter(
#                     model_path=w,
#                     experimental_delegates=[load_delegate(delegate, options={"device": device})],
#                 )
#                 device = "cpu"  # Required, otherwise PyTorch will try to use the wrong device
#             else:  # TFLite
#                 LOGGER.info(f"Loading {w} for TensorFlow Lite inference...")
#                 interpreter = Interpreter(model_path=w)  # load TFLite model
#             interpreter.allocate_tensors()  # allocate
#             input_details = interpreter.get_input_details()  # inputs
#             output_details = interpreter.get_output_details()  # outputs
#             # Load metadata
#             try:
#                 with zipfile.ZipFile(w, "r") as model:
#                     meta_file = model.namelist()[0]
#                     metadata = ast.literal_eval(model.read(meta_file).decode("utf-8"))
#             except zipfile.BadZipFile:
#                 pass

#         # TF.js
#         elif tfjs:
#             raise NotImplementedError("YOLOv8 TF.js inference is not currently supported.")

#         # PaddlePaddle
#         elif paddle:
#             LOGGER.info(f"Loading {w} for PaddlePaddle inference...")
#             check_requirements("paddlepaddle-gpu" if cuda else "paddlepaddle")
#             import paddle.inference as pdi  # noqa

#             w = Path(w)
#             if not w.is_file():  # if not *.pdmodel
#                 w = next(w.rglob("*.pdmodel"))  # get *.pdmodel file from *_paddle_model dir
#             config = pdi.Config(str(w), str(w.with_suffix(".pdiparams")))
#             if cuda:
#                 config.enable_use_gpu(memory_pool_init_size_mb=2048, device_id=0)
#             predictor = pdi.create_predictor(config)
#             input_handle = predictor.get_input_handle(predictor.get_input_names()[0])
#             output_names = predictor.get_output_names()
#             metadata = w.parents[1] / "metadata.yaml"

#         # MNN
#         elif mnn:
#             LOGGER.info(f"Loading {w} for MNN inference...")
#             check_requirements("MNN")  # requires MNN
#             import os

#             import MNN

#             config = {"precision": "low", "backend": "CPU", "numThread": (os.cpu_count() + 1) // 2}
#             rt = MNN.nn.create_runtime_manager((config,))
#             net = MNN.nn.load_module_from_file(w, [], [], runtime_manager=rt, rearrange=True)

#             def torch_to_mnn(x):
#                 return MNN.expr.const(x.data_ptr(), x.shape)

#             metadata = json.loads(net.get_info()["bizCode"])

#         # NCNN
#         elif ncnn:
#             LOGGER.info(f"Loading {w} for NCNN inference...")
#             check_requirements("git+https://github.com/Tencent/ncnn.git" if ARM64 else "ncnn")  # requires NCNN
#             import ncnn as pyncnn

#             net = pyncnn.Net()
#             net.opt.use_vulkan_compute = cuda
#             w = Path(w)
#             if not w.is_file():  # if not *.param
#                 w = next(w.glob("*.param"))  # get *.param file from *_ncnn_model dir
#             net.load_param(str(w))
#             net.load_model(str(w.with_suffix(".bin")))
#             metadata = w.parent / "metadata.yaml"

#         # NVIDIA Triton Inference Server
#         elif triton:
#             check_requirements("tritonclient[all]")
#             from ultralytics.utils.triton import TritonRemoteModel

#             model = TritonRemoteModel(w)
#             metadata = model.metadata

def check_class_names(names):
    """
    Check class names.  # æ£€æŸ¥ç±»å

    Map imagenet class codes to human-readable names if required. Convert lists to dicts.  # å¦‚æœéœ€è¦ï¼Œå°†imagenetç±»ä»£ç æ˜ å°„åˆ°å¯è¯»çš„åç§°ã€‚å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸ã€‚
    """
    if isinstance(names, list):  # names is a list  # å¦‚æœnamesæ˜¯ä¸€ä¸ªåˆ—è¡¨
        names = dict(enumerate(names))  # convert to dict  # è½¬æ¢ä¸ºå­—å…¸
    if isinstance(names, dict):  # å¦‚æœnamesæ˜¯ä¸€ä¸ªå­—å…¸
        # Convert 1) string keys to int, i.e. '0' to 0, and non-string values to strings, i.e. True to 'True'  # å°†1) å­—ç¬¦ä¸²é”®è½¬æ¢ä¸ºæ•´æ•°ï¼Œä¾‹å¦‚'0'è½¬æ¢ä¸º0ï¼Œéå­—ç¬¦ä¸²å€¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚Trueè½¬æ¢ä¸º'True'
        names = {int(k): str(v) for k, v in names.items()}  # å°†é”®è½¬æ¢ä¸ºæ•´æ•°ï¼Œå€¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        n = len(names)  # è·å–å­—å…¸çš„é•¿åº¦
        if max(names.keys()) >= n:  # å¦‚æœæœ€å¤§é”®å€¼å¤§äºç­‰äºå­—å…¸é•¿åº¦
            raise KeyError(  # å¼•å‘KeyErrorå¼‚å¸¸
                f"{n}-class dataset requires class indices 0-{n - 1}, but you have invalid class indices "  # "{n}-ç±»æ•°æ®é›†éœ€è¦ç±»ç´¢å¼•0-{n - 1}ï¼Œä½†æ‚¨æœ‰æ— æ•ˆçš„ç±»ç´¢å¼•"
                f"{min(names.keys())}-{max(names.keys())} defined in your dataset YAML."  # "{min(names.keys())}-{max(names.keys())} åœ¨æ‚¨çš„æ•°æ®é›†YAMLä¸­å®šä¹‰ã€‚"
            )
        if isinstance(names[0], str) and names[0].startswith("n0"):  # imagenet class codes, i.e. 'n01440764'  # imagenetç±»ä»£ç ï¼Œä¾‹å¦‚'n01440764'
            names_map = yaml_load(ROOT / "cfg/datasets/ImageNet.yaml")["map"]  # human-readable names  # å¯è¯»åç§°
            names = {k: names_map[v] for k, v in names.items()}  # å°†ç±»ä»£ç æ˜ å°„åˆ°å¯è¯»åç§°
    return names  # è¿”å›å¤„ç†åçš„ç±»å

def default_class_names(data=None):
    """Applies default class names to an input YAML file or returns numerical class names.  # å°†é»˜è®¤ç±»ååº”ç”¨äºè¾“å…¥YAMLæ–‡ä»¶æˆ–è¿”å›æ•°å­—ç±»åã€‚"""
    if data:  # å¦‚æœæä¾›äº†æ•°æ®
        try:
            return yaml_load(check_yaml(data))["names"]  # å°è¯•åŠ è½½YAMLæ–‡ä»¶å¹¶è¿”å›ç±»å
        except Exception:  # å¦‚æœå‘ç”Ÿå¼‚å¸¸
            pass  # å¿½ç•¥å¼‚å¸¸
    return {i: f"class{i}" for i in range(999)}  # return default if above errors  # å¦‚æœå‘ç”Ÿä¸Šè¿°é”™è¯¯ï¼Œåˆ™è¿”å›é»˜è®¤ç±»å

class AutoBackend(nn.Module):
    """
    Handles dynamic backend selection for running inference using Ultralytics YOLO models.  # å¤„ç†ä½¿ç”¨Ultralytics YOLOæ¨¡å‹è¿›è¡Œæ¨ç†çš„åŠ¨æ€åç«¯é€‰æ‹©ã€‚

    The AutoBackend class is designed to provide an abstraction layer for various inference engines. It supports a wide  # AutoBackendç±»æ—¨åœ¨ä¸ºå„ç§æ¨ç†å¼•æ“æä¾›æŠ½è±¡å±‚ã€‚å®ƒæ”¯æŒå¹¿æ³›çš„
    range of formats, each with specific naming conventions as outlined below:  # æ ¼å¼ï¼Œæ¯ç§æ ¼å¼éƒ½æœ‰ç‰¹å®šçš„å‘½åçº¦å®šï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

        Supported Formats and Naming Conventions:  # æ”¯æŒçš„æ ¼å¼å’Œå‘½åçº¦å®šï¼š
            | Format                | File Suffix       |  # | æ ¼å¼                | æ–‡ä»¶åç¼€       |
            | --------------------- | ----------------- |  # | --------------------- | ----------------- |
            | PyTorch               | *.pt              |  # | PyTorch               | *.pt              |
            | TorchScript           | *.torchscript     |  # | TorchScript           | *.torchscript     |
            | ONNX Runtime          | *.onnx            |  # | ONNX Runtime          | *.onnx            |
            | ONNX OpenCV DNN       | *.onnx (dnn=True) |  # | ONNX OpenCV DNN       | *.onnx (dnn=True) |
            | OpenVINO              | *openvino_model/  |  # | OpenVINO              | *openvino_model/  |
            | CoreML                | *.mlpackage       |  # | CoreML                | *.mlpackage       |
            | TensorRT              | *.engine          |  # | TensorRT              | *.engine          |
            | TensorFlow SavedModel | *_saved_model/    |  # | TensorFlow SavedModel | *_saved_model/    |
            | TensorFlow GraphDef   | *.pb              |  # | TensorFlow GraphDef   | *.pb              |
            | TensorFlow Lite       | *.tflite          |  # | TensorFlow Lite       | *.tflite          |
            | TensorFlow Edge TPU   | *_edgetpu.tflite  |  # | TensorFlow Edge TPU   | *_edgetpu.tflite  |
            | PaddlePaddle          | *_paddle_model/   |  # | PaddlePaddle          | *_paddle_model/   |
            | MNN                   | *.mnn             |  # | MNN                   | *.mnn             |
            | NCNN                  | *_ncnn_model/     |  # | NCNN                  | *_ncnn_model/     |
            | IMX                   | *_imx_model/      |  # | IMX                   | *_imx_model/      |
            | RKNN                  | *_rknn_model/     |  # | RKNN                  | *_rknn_model/     |

    This class offers dynamic backend switching capabilities based on the input model format, making it easier to deploy  # æ­¤ç±»æä¾›åŸºäºè¾“å…¥æ¨¡å‹æ ¼å¼çš„åŠ¨æ€åç«¯åˆ‡æ¢åŠŸèƒ½ï¼Œä½¿å¾—åœ¨ä¸åŒå¹³å°ä¸Šéƒ¨ç½²
    models across various platforms.  # æ¨¡å‹å˜å¾—æ›´åŠ å®¹æ˜“ã€‚
    """

    @torch.no_grad()  # ä¸è®¡ç®—æ¢¯åº¦
    def __init__(  # æ„é€ å‡½æ•°
        self,
        weights="yolo11n.pt",  # æƒé‡æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º'yolo11n.pt'
        device=torch.device("cpu"),  # è¿è¡Œæ¨¡å‹çš„è®¾å¤‡ï¼Œé»˜è®¤ä¸ºCPU
        dnn=False,  # æ˜¯å¦ä½¿ç”¨OpenCV DNNæ¨¡å—è¿›è¡ŒONNXæ¨ç†ï¼Œé»˜è®¤ä¸ºFalse
        data=None,  # é¢å¤–çš„æ•°æ®.yamlæ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«ç±»åï¼Œå¯é€‰
        fp16=False,  # æ˜¯å¦å¯ç”¨åŠç²¾åº¦æ¨ç†ï¼Œä»…åœ¨ç‰¹å®šåç«¯æ”¯æŒï¼Œé»˜è®¤ä¸ºFalse
        batch=1,  # å‡è®¾çš„æ¨ç†æ‰¹æ¬¡å¤§å°
        fuse=True,  # æ˜¯å¦èåˆConv2D + BatchNormå±‚ä»¥ä¼˜åŒ–ï¼Œé»˜è®¤ä¸ºTrue
        verbose=True,  # æ˜¯å¦å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼Œé»˜è®¤ä¸ºTrue
    ):
        """
        Initialize the AutoBackend for inference.  # åˆå§‹åŒ–AutoBackendè¿›è¡Œæ¨ç†ã€‚

        Args:  # å‚æ•°ï¼š
            weights (str | torch.nn.Module): Path to the model weights file or a module instance. Defaults to 'yolo11n.pt'.  # æƒé‡ï¼ˆstr | torch.nn.Moduleï¼‰ï¼šæ¨¡å‹æƒé‡æ–‡ä»¶çš„è·¯å¾„æˆ–æ¨¡å—å®ä¾‹ã€‚é»˜è®¤ä¸º'yolo11n.pt'ã€‚
            device (torch.device): Device to run the model on. Defaults to CPU.  # è®¾å¤‡ï¼ˆtorch.deviceï¼‰ï¼šè¿è¡Œæ¨¡å‹çš„è®¾å¤‡ã€‚é»˜è®¤ä¸ºCPUã€‚
            dnn (bool): Use OpenCV DNN module for ONNX inference. Defaults to False.  # dnnï¼ˆboolï¼‰ï¼šä½¿ç”¨OpenCV DNNæ¨¡å—è¿›è¡ŒONNXæ¨ç†ã€‚é»˜è®¤ä¸ºFalseã€‚
            data (str | Path | optional): Path to the additional data.yaml file containing class names. Optional.  # dataï¼ˆstr | Path | å¯é€‰ï¼‰ï¼šåŒ…å«ç±»åçš„é¢å¤–data.yamlæ–‡ä»¶çš„è·¯å¾„ã€‚å¯é€‰ã€‚
            fp16 (bool): Enable half-precision inference. Supported only on specific backends. Defaults to False.  # fp16ï¼ˆboolï¼‰ï¼šå¯ç”¨åŠç²¾åº¦æ¨ç†ã€‚ä»…åœ¨ç‰¹å®šåç«¯æ”¯æŒã€‚é»˜è®¤ä¸ºFalseã€‚
            batch (int): Batch-size to assume for inference.  # batchï¼ˆintï¼‰ï¼šå‡è®¾çš„æ¨ç†æ‰¹æ¬¡å¤§å°ã€‚
            fuse (bool): Fuse Conv2D + BatchNorm layers for optimization. Defaults to True.  # fuseï¼ˆboolï¼‰ï¼šèåˆConv2D + BatchNormå±‚ä»¥ä¼˜åŒ–ã€‚é»˜è®¤ä¸ºTrueã€‚
            verbose (bool): Enable verbose logging. Defaults to True.  # verboseï¼ˆboolï¼‰ï¼šå¯ç”¨è¯¦ç»†æ—¥å¿—ã€‚é»˜è®¤ä¸ºTrueã€‚
        """
        super().__init__()  # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        w = str(weights[0] if isinstance(weights, list) else weights)  # å¦‚æœweightsæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œå¦åˆ™ç›´æ¥ä½¿ç”¨weights
        nn_module = isinstance(weights, torch.nn.Module)  # æ£€æŸ¥weightsæ˜¯å¦ä¸ºtorch.nn.Moduleç±»å‹
        (
            pt,  # PyTorchæ¨¡å‹
            jit,  # TorchScriptæ¨¡å‹
            onnx,  # ONNXæ¨¡å‹
            xml,  # OpenVINOæ¨¡å‹
            engine,  # TensorRTæ¨¡å‹
            coreml,  # CoreMLæ¨¡å‹
            saved_model,  # TensorFlow SavedModelæ¨¡å‹
            pb,  # TensorFlow GraphDefæ¨¡å‹
            tflite,  # TensorFlow Liteæ¨¡å‹
            edgetpu,  # TensorFlow Edge TPUæ¨¡å‹
            tfjs,  # TensorFlow.jsæ¨¡å‹
            paddle,  # PaddlePaddleæ¨¡å‹
            mnn,  # MNNæ¨¡å‹
            ncnn,  # NCNNæ¨¡å‹
            imx,  # IMXæ¨¡å‹
            rknn,  # RKNNæ¨¡å‹
            triton,  # NVIDIA Tritonæ¨¡å‹
        ) = self._model_type(w)  # ç¡®å®šæ¨¡å‹ç±»å‹
        fp16 &= pt or jit or onnx or xml or engine or nn_module or triton  # FP16
        nhwc = coreml or saved_model or pb or tflite or edgetpu or rknn  # BHWCæ ¼å¼ï¼ˆä¸torch BCWHç›¸å¯¹ï¼‰
        stride = 32  # default stride  # é»˜è®¤æ­¥å¹…
        end2end = False  # default end2end  # é»˜è®¤end2end
        model, metadata, task = None, None, None  # åˆå§‹åŒ–æ¨¡å‹ã€å…ƒæ•°æ®å’Œä»»åŠ¡

        # Set device  # è®¾ç½®è®¾å¤‡
        cuda = torch.cuda.is_available() and device.type != "cpu"  # use CUDA  # ä½¿ç”¨CUDA
        if cuda and not any([nn_module, pt, jit, engine, onnx, paddle]):  # GPU dataloader formats  # GPUæ•°æ®åŠ è½½å™¨æ ¼å¼
            device = torch.device("cpu")  # å¦‚æœä¸æ”¯æŒCUDAï¼Œåˆ™ä½¿ç”¨CPU
            cuda = False  # è®¾ç½®cudaä¸ºFalse

        # Download if not local  # å¦‚æœä¸æ˜¯æœ¬åœ°æ–‡ä»¶ï¼Œåˆ™ä¸‹è½½
        if not (pt or triton or nn_module):  # å¦‚æœä¸æ˜¯PyTorchã€Tritonæˆ–nnæ¨¡å—
            w = attempt_download_asset(w)  # å°è¯•ä¸‹è½½èµ„äº§

        # In-memory PyTorch model  # å†…å­˜ä¸­çš„PyTorchæ¨¡å‹
        if nn_module:  # å¦‚æœweightsæ˜¯nnæ¨¡å—
            model = weights.to(device)  # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            if fuse:  # å¦‚æœéœ€è¦èåˆ
                model = model.fuse(verbose=verbose)  # èåˆæ¨¡å‹
            if hasattr(model, "kpt_shape"):  # å¦‚æœæ¨¡å‹æœ‰å…³é”®ç‚¹å½¢çŠ¶å±æ€§
                kpt_shape = model.kpt_shape  # ä»…ç”¨äºå§¿æ€æ£€æµ‹
            stride = max(int(model.stride.max()), 32)  # è·å–æ¨¡å‹æ­¥å¹…
            names = model.module.names if hasattr(model, "module") else model.names  # è·å–ç±»å
            model.half() if fp16 else model.float()  # æ ¹æ®fp16è®¾ç½®æ¨¡å‹ä¸ºåŠç²¾åº¦æˆ–å•ç²¾åº¦
            self.model = model  # æ˜¾å¼èµ‹å€¼ç»™self.modelï¼Œä»¥ä¾¿åç»­è°ƒç”¨to()ã€cpu()ã€cuda()ã€half()
            pt = True  # è®¾ç½®ptä¸ºTrue

        # PyTorch  # PyTorchæ¨¡å‹
        elif pt:  # å¦‚æœæ˜¯PyTorchæ¨¡å‹
            from ultralytics.nn.tasks import attempt_load_weights  # ä»UltralyticsåŠ è½½æƒé‡

            model = attempt_load_weights(  # å°è¯•åŠ è½½æƒé‡
                weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse
            )
            if hasattr(model, "kpt_shape"):  # å¦‚æœæ¨¡å‹æœ‰å…³é”®ç‚¹å½¢çŠ¶å±æ€§
                kpt_shape = model.kpt_shape  # ä»…ç”¨äºå§¿æ€æ£€æµ‹
            stride = max(int(model.stride.max()), 32)  # è·å–æ¨¡å‹æ­¥å¹…
            names = model.module.names if hasattr(model, "module") else model.names  # è·å–ç±»å
            model.half() if fp16 else model.float()  # æ ¹æ®fp16è®¾ç½®æ¨¡å‹ä¸ºåŠç²¾åº¦æˆ–å•ç²¾åº¦
            self.model = model  # æ˜¾å¼èµ‹å€¼ç»™self.modelï¼Œä»¥ä¾¿åç»­è°ƒç”¨to()ã€cpu()ã€cuda()ã€half()

        # TorchScript  # TorchScriptæ¨¡å‹
        elif jit:  # å¦‚æœæ˜¯TorchScriptæ¨¡å‹
            LOGGER.info(f"Loading {w} for TorchScript inference...")  # è®°å½•åŠ è½½ä¿¡æ¯
            extra_files = {"config.txt": ""}  # model metadata  # æ¨¡å‹å…ƒæ•°æ®
            model = torch.jit.load(w, _extra_files=extra_files, map_location=device)  # åŠ è½½TorchScriptæ¨¡å‹

        # ONNX OpenCV DNN  # ONNX OpenCV DNNæ¨¡å‹
        elif dnn:  # å¦‚æœæ˜¯DNNæ¨¡å‹
            LOGGER.info(f"Loading {w} for ONNX OpenCV DNN inference...")  # è®°å½•åŠ è½½ä¿¡æ¯
            check_requirements("opencv-python>=4.5.4")  # æ£€æŸ¥OpenCVè¦æ±‚
            net = cv2.dnn.readNetFromONNX(w)  # ä»ONNXåŠ è½½DNNç½‘ç»œ

        # ONNX Runtime and IMX  # ONNX Runtimeå’ŒIMXæ¨¡å‹
        elif onnx or imx:  # å¦‚æœæ˜¯ONNXæˆ–IMXæ¨¡å‹
            LOGGER.info(f"Loading {w} for ONNX Runtime inference...")  # è®°å½•åŠ è½½ä¿¡æ¯
            check_requirements(("onnx", "onnxruntime-gpu" if cuda else "onnxruntime"))  # æ£€æŸ¥ONNXè¦æ±‚
            if IS_RASPBERRYPI or IS_JETSON:  # å¦‚æœæ˜¯æ ‘è“æ´¾æˆ–Jetson
                # Fix 'numpy.linalg._umath_linalg' has no attribute '_ilp64' for TF SavedModel on RPi and Jetson  # ä¿®å¤æ ‘è“æ´¾å’ŒJetsonä¸ŠTF SavedModelçš„é”™è¯¯
                check_requirements("numpy==1.23.5")  # æ£€æŸ¥numpyç‰ˆæœ¬
            import onnxruntime  # å¯¼å…¥ONNX Runtime

            providers = ["CPUExecutionProvider"]  # è®¾ç½®æ‰§è¡Œæä¾›è€…ä¸ºCPU
            if cuda and "CUDAExecutionProvider" in onnxruntime.get_available_providers():  # å¦‚æœæ”¯æŒCUDA
                providers.insert(0, "CUDAExecutionProvider")  # å°†CUDAæä¾›è€…æ’å…¥åˆ°åˆ—è¡¨ä¸­
            elif cuda:  # å¦‚æœè¯·æ±‚äº†CUDAä½†ä¸å¯ç”¨
                LOGGER.warning("WARNING âš ï¸ Failed to start ONNX Runtime with CUDA. Using CPU...")  # è®°å½•è­¦å‘Šä¿¡æ¯
                device = torch.device("cpu")  # ä½¿ç”¨CPU
                cuda = False  # è®¾ç½®cudaä¸ºFalse
            LOGGER.info(f"Using ONNX Runtime {providers[0]}")  # è®°å½•ä½¿ç”¨çš„ONNX Runtimeæä¾›è€…
            if onnx:  # å¦‚æœæ˜¯ONNXæ¨¡å‹
                session = onnxruntime.InferenceSession(w, providers=providers)  # åˆ›å»ºONNXæ¨ç†ä¼šè¯
            else:  # å¦‚æœæ˜¯IMXæ¨¡å‹
                check_requirements(  # æ£€æŸ¥IMXè¦æ±‚
                    ["model-compression-toolkit==2.1.1", "sony-custom-layers[torch]==0.2.0", "onnxruntime-extensions"]
                )
                w = next(Path(w).glob("*.onnx"))  # è·å–IMXæ¨¡å‹çš„ONNXæ–‡ä»¶
                LOGGER.info(f"Loading {w} for ONNX IMX inference...")  # è®°å½•åŠ è½½ä¿¡æ¯
                import mct_quantizers as mctq  # å¯¼å…¥é‡åŒ–å·¥å…·
                from sony_custom_layers.pytorch.object_detection import nms_ort  # noqa  # å¯¼å…¥è‡ªå®šä¹‰å±‚

                session = onnxruntime.InferenceSession(  # åˆ›å»ºONNXæ¨ç†ä¼šè¯
                    w, mctq.get_ort_session_options(), providers=["CPUExecutionProvider"]
                )
                task = "detect"  # è®¾ç½®ä»»åŠ¡ä¸ºæ£€æµ‹

            output_names = [x.name for x in session.get_outputs()]  # è·å–è¾“å‡ºåç§°
            metadata = session.get_modelmeta().custom_metadata_map  # è·å–æ¨¡å‹å…ƒæ•°æ®
            dynamic = isinstance(session.get_outputs()[0].shape[0], str)  # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶æ˜¯å¦åŠ¨æ€
            fp16 = True if "float16" in session.get_inputs()[0].type else False  # æ£€æŸ¥è¾“å…¥ç±»å‹æ˜¯å¦ä¸ºfloat16
            if not dynamic:  # å¦‚æœä¸æ˜¯åŠ¨æ€
                io = session.io_binding()  # è·å–IOç»‘å®š
                bindings = []  # åˆå§‹åŒ–ç»‘å®šåˆ—è¡¨
                for output in session.get_outputs():  # éå†è¾“å‡º
                    out_fp16 = "float16" in output.type  # æ£€æŸ¥è¾“å‡ºç±»å‹æ˜¯å¦ä¸ºfloat16
                    y_tensor = torch.empty(output.shape, dtype=torch.float16 if out_fp16 else torch.float32).to(device)  # åˆ›å»ºè¾“å‡ºå¼ é‡
                    io.bind_output(  # ç»‘å®šè¾“å‡º
                        name=output.name,
                        device_type=device.type,
                        device_id=device.index if cuda else 0,
                        element_type=np.float16 if out_fp16 else np.float32,
                        shape=tuple(y_tensor.shape),
                        buffer_ptr=y_tensor.data_ptr(),
                    )
                    bindings.append(y_tensor)  # æ·»åŠ åˆ°ç»‘å®šåˆ—è¡¨

        # OpenVINO  # OpenVINOæ¨¡å‹
        elif xml:  # å¦‚æœæ˜¯OpenVINOæ¨¡å‹
            LOGGER.info(f"Loading {w} for OpenVINO inference...")  # è®°å½•åŠ è½½ä¿¡æ¯
            check_requirements("openvino>=2024.0.0")  # æ£€æŸ¥OpenVINOè¦æ±‚
            import openvino as ov  # å¯¼å…¥OpenVINO

            core = ov.Core()  # åˆ›å»ºOpenVINOæ ¸å¿ƒ
            w = Path(w)  # å°†æƒé‡è·¯å¾„è½¬æ¢ä¸ºPathå¯¹è±¡
            if not w.is_file():  # å¦‚æœä¸æ˜¯*.xmlæ–‡ä»¶
                w = next(w.glob("*.xml"))  # è·å–*_openvino_modelç›®å½•ä¸­çš„*.xmlæ–‡ä»¶
            ov_model = core.read_model(model=str(w), weights=w.with_suffix(".bin"))  # è¯»å–OpenVINOæ¨¡å‹
            if ov_model.get_parameters()[0].get_layout().empty:  # å¦‚æœå¸ƒå±€ä¸ºç©º
                ov_model.get_parameters()[0].set_layout(ov.Layout("NCHW"))  # è®¾ç½®å¸ƒå±€ä¸ºNCHW

            # OpenVINO inference modes are 'LATENCY', 'THROUGHPUT' (not recommended), or 'CUMULATIVE_THROUGHPUT'  # OpenVINOæ¨ç†æ¨¡å¼ä¸º'LATENCY'ã€'THROUGHPUT'ï¼ˆä¸æ¨èï¼‰æˆ–'CUMULATIVE_THROUGHPUT'
            inference_mode = "CUMULATIVE_THROUGHPUT" if batch > 1 else "LATENCY"  # è®¾ç½®æ¨ç†æ¨¡å¼
            LOGGER.info(f"Using OpenVINO {inference_mode} mode for batch={batch} inference...")  # è®°å½•ä½¿ç”¨çš„æ¨ç†æ¨¡å¼
            ov_compiled_model = core.compile_model(  # ç¼–è¯‘OpenVINOæ¨¡å‹
                ov_model,
                device_name="AUTO",  # AUTOé€‰æ‹©æœ€ä½³å¯ç”¨è®¾å¤‡ï¼Œè¯·å‹¿ä¿®æ”¹
                config={"PERFORMANCE_HINT": inference_mode},  # è®¾ç½®æ€§èƒ½æç¤º
            )
            input_name = ov_compiled_model.input().get_any_name()  # è·å–è¾“å…¥åç§°
            metadata = w.parent / "metadata.yaml"  # è®¾ç½®å…ƒæ•°æ®è·¯å¾„

        # TensorRT  # TensorRTæ¨¡å‹
        elif engine:  # å¦‚æœæ˜¯TensorRTæ¨¡å‹
            LOGGER.info(f"Loading {w} for TensorRT inference...")  # è®°å½•åŠ è½½ä¿¡æ¯

            if IS_JETSON and PYTHON_VERSION <= "3.8.0":  # å¦‚æœæ˜¯Jetsonå¹¶ä¸”Pythonç‰ˆæœ¬å°äºç­‰äº3.8.0
                # fix error: `np.bool` was a deprecated alias for the builtin `bool` for JetPack 4 with Python <= 3.8.0  # ä¿®å¤é”™è¯¯
                check_requirements("numpy==1.23.5")  # æ£€æŸ¥numpyç‰ˆæœ¬

            try:
                import tensorrt as trt  # noqa https://developer.nvidia.com/nvidia-tensorrt-download  # å¯¼å…¥TensorRT
            except ImportError:  # å¦‚æœå¯¼å…¥å¤±è´¥
                if LINUX:  # å¦‚æœæ˜¯Linux
                    check_requirements("tensorrt>7.0.0,!=10.1.0")  # æ£€æŸ¥TensorRTè¦æ±‚
                import tensorrt as trt  # noqa  # é‡æ–°å¯¼å…¥TensorRT
            check_version(trt.__version__, ">=7.0.0", hard=True)  # æ£€æŸ¥TensorRTç‰ˆæœ¬
            check_version(trt.__version__, "!=10.1.0", msg="https://github.com/ultralytics/ultralytics/pull/14239")  # æ£€æŸ¥TensorRTç‰ˆæœ¬
            if device.type == "cpu":  # å¦‚æœè®¾å¤‡æ˜¯CPU
                device = torch.device("cuda:0")  # åˆ‡æ¢åˆ°CUDAè®¾å¤‡
            Binding = namedtuple("Binding", ("name", "dtype", "shape", "data", "ptr"))  # å®šä¹‰ç»‘å®šå…ƒç»„
            logger = trt.Logger(trt.Logger.INFO)  # åˆ›å»ºTensorRTæ—¥å¿—è®°å½•å™¨
            # Read file  # è¯»å–æ–‡ä»¶
            with open(w, "rb") as f, trt.Runtime(logger) as runtime:  # æ‰“å¼€æƒé‡æ–‡ä»¶
                try:
                    meta_len = int.from_bytes(f.read(4), byteorder="little")  # è¯»å–å…ƒæ•°æ®é•¿åº¦
                    metadata = json.loads(f.read(meta_len).decode("utf-8"))  # è¯»å–å…ƒæ•°æ®
                except UnicodeDecodeError:  # å¦‚æœè§£ç é”™è¯¯
                    f.seek(0)  # å¦‚æœå¼•æ“æ–‡ä»¶ç¼ºå°‘åµŒå…¥çš„Ultralyticså…ƒæ•°æ®
                dla = metadata.get("dla", None)  # è·å–DLAä¿¡æ¯
                if dla is not None:  # å¦‚æœDLAä¿¡æ¯å­˜åœ¨
                    runtime.DLA_core = int(dla)  # è®¾ç½®DLAæ ¸å¿ƒ
                model = runtime.deserialize_cuda_engine(f.read())  # ååºåˆ—åŒ–CUDAå¼•æ“

            # Model context  # æ¨¡å‹ä¸Šä¸‹æ–‡
            try:
                context = model.create_execution_context()  # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
            except Exception as e:  # å¦‚æœæ¨¡å‹ä¸ºNone
                LOGGER.error(f"ERROR: TensorRT model exported with a different version than {trt.__version__}\n")  # è®°å½•é”™è¯¯ä¿¡æ¯
                raise e  # å¼•å‘å¼‚å¸¸

            bindings = OrderedDict()  # åˆå§‹åŒ–ç»‘å®šå­—å…¸
            output_names = []  # åˆå§‹åŒ–è¾“å‡ºåç§°åˆ—è¡¨
            fp16 = False  # default updated below  # é»˜è®¤æ›´æ–°ä¸ºFalse
            dynamic = False  # é»˜è®¤åŠ¨æ€ä¸ºFalse
            is_trt10 = not hasattr(model, "num_bindings")  # æ£€æŸ¥TensorRTç‰ˆæœ¬
            num = range(model.num_io_tensors) if is_trt10 else range(model.num_bindings)  # è·å–ç»‘å®šæ•°é‡
            for i in num:  # éå†ç»‘å®š
                if is_trt10:  # å¦‚æœæ˜¯TensorRT 10
                    name = model.get_tensor_name(i)  # è·å–å¼ é‡åç§°
                    dtype = trt.nptype(model.get_tensor_dtype(name))  # è·å–å¼ é‡æ•°æ®ç±»å‹
                    is_input = model.get_tensor_mode(name) == trt.TensorIOMode.INPUT  # æ£€æŸ¥æ˜¯å¦ä¸ºè¾“å…¥
                    if is_input:  # å¦‚æœæ˜¯è¾“å…¥
                        if -1 in tuple(model.get_tensor_shape(name)):  # åŠ¨æ€ç»‘å®š
                            dynamic = True  # è®¾ç½®åŠ¨æ€ä¸ºTrue
                            context.set_input_shape(name, tuple(model.get_tensor_profile_shape(name, 0)[1]))  # è®¾ç½®è¾“å…¥å½¢çŠ¶
                        if dtype == np.float16:  # å¦‚æœæ•°æ®ç±»å‹ä¸ºfloat16
                            fp16 = True  # è®¾ç½®fp16ä¸ºTrue
                    else:
                        output_names.append(name)  # æ·»åŠ åˆ°è¾“å‡ºåç§°åˆ—è¡¨
                    shape = tuple(context.get_tensor_shape(name))  # è·å–å¼ é‡å½¢çŠ¶
                else:  # TensorRT < 10.0
                    name = model.get_binding_name(i)  # è·å–ç»‘å®šåç§°
                    dtype = trt.nptype(model.get_binding_dtype(i))  # è·å–ç»‘å®šæ•°æ®ç±»å‹
                    is_input = model.binding_is_input(i)  # æ£€æŸ¥æ˜¯å¦ä¸ºè¾“å…¥
                    if model.binding_is_input(i):  # å¦‚æœæ˜¯è¾“å…¥
                        if -1 in tuple(model.get_binding_shape(i)):  # åŠ¨æ€ç»‘å®š
                            dynamic = True  # è®¾ç½®åŠ¨æ€ä¸ºTrue
                            context.set_binding_shape(i, tuple(model.get_profile_shape(0, i)[1]))  # è®¾ç½®ç»‘å®šå½¢çŠ¶
                        if dtype == np.float16:  # å¦‚æœæ•°æ®ç±»å‹ä¸ºfloat16
                            fp16 = True  # è®¾ç½®fp16ä¸ºTrue
                    else:
                        output_names.append(name)  # æ·»åŠ åˆ°è¾“å‡ºåç§°åˆ—è¡¨
                    shape = tuple(context.get_binding_shape(i))  # è·å–ç»‘å®šå½¢çŠ¶
                im = torch.from_numpy(np.empty(shape, dtype=dtype)).to(device)  # åˆ›å»ºè¾“å…¥å¼ é‡
                bindings[name] = Binding(name, dtype, shape, im, int(im.data_ptr()))  # æ·»åŠ åˆ°ç»‘å®šå­—å…¸
            binding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())  # è·å–ç»‘å®šåœ°å€å­—å…¸
            batch_size = bindings["images"].shape[0]  # å¦‚æœæ˜¯åŠ¨æ€ï¼Œåˆ™è¿™æ˜¯æ‰¹å¤§å°

        # RKNN
        elif rknn:  # å¦‚æœæ˜¯ RKNN æ ¼å¼
            if not is_rockchip():  # å¦‚æœä¸æ˜¯ Rockchip è®¾å¤‡
                raise OSError("RKNN inference is only supported on Rockchip devices.")  # æŠ›å‡º OSErrorï¼Œæç¤º RKNN æ¨ç†ä»…æ”¯æŒ Rockchip è®¾å¤‡
            LOGGER.info(f"Loading {w} for RKNN inference...")  # è®°å½•åŠ è½½ RKNN æ¨ç†æ¨¡å‹çš„ä¿¡æ¯
            check_requirements("rknn-toolkit-lite2")  # æ£€æŸ¥ RKNN å·¥å…·åŒ…çš„è¦æ±‚
            from rknnlite.api import RKNNLite  # ä» rknnlite å¯¼å…¥ RKNNLite ç±»

            w = Path(w)  # å°†æƒé‡è·¯å¾„è½¬æ¢ä¸º Path å¯¹è±¡
            if not w.is_file():  # if not *.rknn  # å¦‚æœä¸æ˜¯ *.rknn æ–‡ä»¶
                w = next(w.rglob("*.rknn"))  # get *.rknn file from *_rknn_model dir  # ä» *_rknn_model ç›®å½•ä¸­è·å– *.rknn æ–‡ä»¶
            rknn_model = RKNNLite()  # åˆ›å»º RKNNLite å®ä¾‹
            rknn_model.load_rknn(w)  # åŠ è½½ RKNN æ¨¡å‹
            rknn_model.init_runtime()  # åˆå§‹åŒ–è¿è¡Œæ—¶
            metadata = Path(w).parent / "metadata.yaml"  # è®¾ç½®å…ƒæ•°æ®è·¯å¾„

        # Any other format (unsupported)  # å…¶ä»–æ ¼å¼ï¼ˆä¸æ”¯æŒï¼‰
        else:  # å¦‚æœä¸æ˜¯å·²çŸ¥æ ¼å¼
            from ultralytics.engine.exporter import export_formats  # ä» ultralytics å¯¼å…¥ export_formats

            raise TypeError(  # æŠ›å‡º TypeError
                f"model='{w}' is not a supported model format. Ultralytics supports: {export_formats()['Format']}\n"  # "{w} ä¸æ˜¯æ”¯æŒçš„æ¨¡å‹æ ¼å¼ã€‚Ultralytics æ”¯æŒçš„æ ¼å¼ä¸ºï¼š"
                f"See https://docs.ultralytics.com/modes/predict for help."  # "è¯·å‚é˜… https://docs.ultralytics.com/modes/predict è·å–å¸®åŠ©ã€‚"
            )

        # Load external metadata YAML  # åŠ è½½å¤–éƒ¨å…ƒæ•°æ® YAML
        if isinstance(metadata, (str, Path)) and Path(metadata).exists():  # æ£€æŸ¥ metadata æ˜¯å¦ä¸ºå­—ç¬¦ä¸²æˆ– Path ä¸”å­˜åœ¨
            metadata = yaml_load(metadata)  # åŠ è½½å…ƒæ•°æ®
        if metadata and isinstance(metadata, dict):  # å¦‚æœ metadata å­˜åœ¨ä¸”ä¸ºå­—å…¸
            for k, v in metadata.items():  # éå†å…ƒæ•°æ®é¡¹
                if k in {"stride", "batch"}:  # å¦‚æœé”®æ˜¯ "stride" æˆ– "batch"
                    metadata[k] = int(v)  # å°†å€¼è½¬æ¢ä¸ºæ•´æ•°
                elif k in {"imgsz", "names", "kpt_shape", "args"} and isinstance(v, str):  # å¦‚æœé”®æ˜¯ "imgsz"ã€"names"ã€"kpt_shape" æˆ– "args" ä¸”å€¼ä¸ºå­—ç¬¦ä¸²
                    metadata[k] = eval(v)  # è¯„ä¼°å­—ç¬¦ä¸²å¹¶æ›´æ–°å€¼
            stride = metadata["stride"]  # è·å–æ­¥å¹…
            task = metadata["task"]  # è·å–ä»»åŠ¡
            batch = metadata["batch"]  # è·å–æ‰¹æ¬¡å¤§å°
            imgsz = metadata["imgsz"]  # è·å–å›¾åƒå¤§å°
            names = metadata["names"]  # è·å–ç±»å
            kpt_shape = metadata.get("kpt_shape")  # è·å–å…³é”®ç‚¹å½¢çŠ¶
            end2end = metadata.get("args", {}).get("nms", False)  # è·å–æ˜¯å¦å¯ç”¨ NMS çš„å‚æ•°
        elif not (pt or triton or nn_module):  # å¦‚æœä¸æ˜¯ PyTorchã€Triton æˆ– nn_module
            LOGGER.warning(f"WARNING âš ï¸ Metadata not found for 'model={weights}'")  # è®°å½•è­¦å‘Šä¿¡æ¯ï¼Œæç¤ºæœªæ‰¾åˆ°æ¨¡å‹çš„å…ƒæ•°æ®

        # Check names  # æ£€æŸ¥ç±»å
        if "names" not in locals():  # names missing  # å¦‚æœç±»åæœªå®šä¹‰
            names = default_class_names(data)  # ä½¿ç”¨é»˜è®¤ç±»å
        names = check_class_names(names)  # æ£€æŸ¥ç±»å

        # Disable gradients  # ç¦ç”¨æ¢¯åº¦
        if pt:  # å¦‚æœæ˜¯ PyTorch æ¨¡å‹
            for p in model.parameters():  # éå†æ¨¡å‹å‚æ•°
                p.requires_grad = False  # ç¦ç”¨æ¢¯åº¦è®¡ç®—

        self.__dict__.update(locals())  # å°†æ‰€æœ‰å±€éƒ¨å˜é‡èµ‹å€¼ç»™å®ä¾‹å­—å…¸

    def forward(self, im, augment=False, visualize=False, embed=None):  # å®šä¹‰å‰å‘æ¨ç†æ–¹æ³•
        """
        Runs inference on the YOLOv8 MultiBackend model.  # åœ¨ YOLOv8 MultiBackend æ¨¡å‹ä¸Šè¿è¡Œæ¨ç†

        Args:  # å‚æ•°ï¼š
            im (torch.Tensor): The image tensor to perform inference on.  # imï¼ˆtorch.Tensorï¼‰ï¼šç”¨äºæ¨ç†çš„å›¾åƒå¼ é‡ã€‚
            augment (bool): whether to perform data augmentation during inference, defaults to False  # augmentï¼ˆboolï¼‰ï¼šæ˜¯å¦åœ¨æ¨ç†æœŸé—´æ‰§è¡Œæ•°æ®å¢å¼ºï¼Œé»˜è®¤ä¸º False
            visualize (bool): whether to visualize the output predictions, defaults to False  # visualizeï¼ˆboolï¼‰ï¼šæ˜¯å¦å¯è§†åŒ–è¾“å‡ºé¢„æµ‹ï¼Œé»˜è®¤ä¸º False
            embed (list, optional): A list of feature vectors/embeddings to return.  # embedï¼ˆåˆ—è¡¨ï¼Œå¯é€‰ï¼‰ï¼šè¦è¿”å›çš„ç‰¹å¾å‘é‡/åµŒå…¥åˆ—è¡¨ã€‚

        Returns:  # è¿”å›ï¼š
            (tuple): Tuple containing the raw output tensor, and processed output for visualization (if visualize=True)  # ï¼ˆå…ƒç»„ï¼‰ï¼šåŒ…å«åŸå§‹è¾“å‡ºå¼ é‡å’Œå¯è§†åŒ–å¤„ç†è¾“å‡ºçš„å…ƒç»„ï¼ˆå¦‚æœ visualize=Trueï¼‰
        """
        b, ch, h, w = im.shape  # batch, channel, height, width  # è·å–è¾“å…¥å›¾åƒçš„æ‰¹æ¬¡ã€é€šé“ã€é«˜åº¦å’Œå®½åº¦
        if self.fp16 and im.dtype != torch.float16:  # å¦‚æœå¯ç”¨äº†åŠç²¾åº¦ä¸”è¾“å…¥å›¾åƒä¸æ˜¯ float16 ç±»å‹
            im = im.half()  # è½¬æ¢ä¸º FP16
        if self.nhwc:  # å¦‚æœä½¿ç”¨ NHWC æ ¼å¼
            im = im.permute(0, 2, 3, 1)  # torch BCHW è½¬æ¢ä¸º numpy BHWC å½¢çŠ¶ï¼ˆ1, 320, 192, 3ï¼‰

        # PyTorch  # PyTorch æ¨ç†
        if self.pt or self.nn_module:  # å¦‚æœæ˜¯ PyTorch æ¨¡å‹æˆ– nn.Module
            y = self.model(im, augment=augment, visualize=visualize, embed=embed)  # æ‰§è¡Œæ¨ç†

        # TorchScript  # TorchScript æ¨ç†
        elif self.jit:  # å¦‚æœæ˜¯ TorchScript æ¨¡å‹
            y = self.model(im)  # æ‰§è¡Œæ¨ç†

        # ONNX OpenCV DNN  # ONNX OpenCV DNN æ¨ç†
        elif self.dnn:  # å¦‚æœæ˜¯ DNN æ¨¡å‹
            im = im.cpu().numpy()  # torch è½¬æ¢ä¸º numpy
            self.net.setInput(im)  # è®¾ç½®è¾“å…¥
            y = self.net.forward()  # æ‰§è¡Œå‰å‘æ¨ç†

        # ONNX Runtime  # ONNX Runtime æ¨ç†
        elif self.onnx or self.imx:  # å¦‚æœæ˜¯ ONNX æˆ– IMX æ¨¡å‹
            if self.dynamic:  # å¦‚æœæ˜¯åŠ¨æ€æ¨¡å‹
                im = im.cpu().numpy()  # torch è½¬æ¢ä¸º numpy
                y = self.session.run(self.output_names, {self.session.get_inputs()[0].name: im})  # æ‰§è¡Œæ¨ç†
            else:  # å¦‚æœä¸æ˜¯åŠ¨æ€æ¨¡å‹
                if not self.cuda:  # å¦‚æœä¸ä½¿ç”¨ CUDA
                    im = im.cpu()  # è½¬æ¢ä¸º CPU
                self.io.bind_input(  # ç»‘å®šè¾“å…¥
                    name="images",
                    device_type=im.device.type,
                    device_id=im.device.index if im.device.type == "cuda" else 0,
                    element_type=np.float16 if self.fp16 else np.float32,
                    shape=tuple(im.shape),
                    buffer_ptr=im.data_ptr(),
                )
                self.session.run_with_iobinding(self.io)  # ä½¿ç”¨ I/O ç»‘å®šæ‰§è¡Œæ¨ç†
                y = self.bindings  # è·å–ç»‘å®šè¾“å‡º
            if self.imx:  # å¦‚æœæ˜¯ IMX æ¨¡å‹
                # boxes, conf, cls  # ç›’å­ã€ç½®ä¿¡åº¦ã€ç±»åˆ«
                y = np.concatenate([y[0], y[1][:, :, None], y[2][:, :, None]], axis=-1)  # åˆå¹¶è¾“å‡º

        # OpenVINO  # OpenVINO æ¨ç†
        elif self.xml:  # å¦‚æœæ˜¯ OpenVINO æ¨¡å‹
            im = im.cpu().numpy()  # FP32  # è½¬æ¢ä¸º FP32

            if self.inference_mode in {"THROUGHPUT", "CUMULATIVE_THROUGHPUT"}:  # ä¼˜åŒ–å¤§æ‰¹æ¬¡æ¨ç†
                n = im.shape[0]  # batch ä¸­å›¾åƒçš„æ•°é‡
                results = [None] * n  # é¢„åˆ†é…ä¸å›¾åƒæ•°é‡ç›¸åŒçš„ç»“æœåˆ—è¡¨

                def callback(request, userdata):  # å›è°ƒå‡½æ•°
                    """Places result in preallocated list using userdata index."""  # ä½¿ç”¨ç”¨æˆ·æ•°æ®ç´¢å¼•å°†ç»“æœæ”¾å…¥é¢„åˆ†é…åˆ—è¡¨
                    results[userdata] = request.results  # å°†ç»“æœå­˜å‚¨åœ¨æŒ‡å®šä½ç½®

                # Create AsyncInferQueue, set the callback and start asynchronous inference for each input image  # åˆ›å»ºå¼‚æ­¥æ¨ç†é˜Ÿåˆ—ï¼Œè®¾ç½®å›è°ƒå¹¶ä¸ºæ¯ä¸ªè¾“å…¥å›¾åƒå¼€å§‹å¼‚æ­¥æ¨ç†
                async_queue = self.ov.runtime.AsyncInferQueue(self.ov_compiled_model)  # åˆ›å»ºå¼‚æ­¥æ¨ç†é˜Ÿåˆ—
                async_queue.set_callback(callback)  # è®¾ç½®å›è°ƒå‡½æ•°
                for i in range(n):  # éå†æ¯ä¸ªå›¾åƒ
                    # Start async inference with userdata=i to specify the position in results list  # ä½¿ç”¨ userdata=i å¼€å§‹å¼‚æ­¥æ¨ç†ï¼Œä»¥æŒ‡å®šç»“æœåˆ—è¡¨ä¸­çš„ä½ç½®
                    async_queue.start_async(inputs={self.input_name: im[i : i + 1]}, userdata=i)  # ä¿æŒå›¾åƒä¸º BCHW
                async_queue.wait_all()  # ç­‰å¾…æ‰€æœ‰æ¨ç†è¯·æ±‚å®Œæˆ
                y = np.concatenate([list(r.values())[0] for r in results])  # åˆå¹¶ç»“æœ

            else:  # inference_mode = "LATENCY"ï¼Œä¼˜åŒ–ä»¥æœ€å¿«é€Ÿåº¦è¿”å›ç»“æœï¼Œæ‰¹æ¬¡å¤§å°ä¸º 1
                y = list(self.ov_compiled_model(im).values())  # æ‰§è¡Œæ¨ç†å¹¶è·å–ç»“æœ

        # TensorRT  # TensorRT æ¨ç†
        elif self.engine:  # å¦‚æœæ˜¯ TensorRT æ¨¡å‹
            if self.dynamic and im.shape != self.bindings["images"].shape:  # å¦‚æœæ˜¯åŠ¨æ€æ¨¡å‹ä¸”è¾“å…¥å½¢çŠ¶ä¸åŒ¹é…
                if self.is_trt10:  # å¦‚æœæ˜¯ TensorRT 10 ç‰ˆæœ¬
                    self.context.set_input_shape("images", im.shape)  # è®¾ç½®è¾“å…¥å½¢çŠ¶
                    self.bindings["images"] = self.bindings["images"]._replace(shape=im.shape)  # æ›´æ–°ç»‘å®šå½¢çŠ¶
                    for name in self.output_names:  # éå†è¾“å‡ºåç§°
                        self.bindings[name].data.resize_(tuple(self.context.get_tensor_shape(name)))  # è°ƒæ•´è¾“å‡ºå½¢çŠ¶
                else:  # TensorRT < 10.0
                    i = self.model.get_binding_index("images")  # è·å–è¾“å…¥ç»‘å®šç´¢å¼•
                    self.context.set_binding_shape(i, im.shape)  # è®¾ç½®ç»‘å®šå½¢çŠ¶
                    self.bindings["images"] = self.bindings["images"]._replace(shape=im.shape)  # æ›´æ–°ç»‘å®šå½¢çŠ¶
                    for name in self.output_names:  # éå†è¾“å‡ºåç§°
                        i = self.model.get_binding_index(name)  # è·å–è¾“å‡ºç»‘å®šç´¢å¼•
                        self.bindings[name].data.resize_(tuple(self.context.get_binding_shape(i)))  # è°ƒæ•´è¾“å‡ºå½¢çŠ¶

            s = self.bindings["images"].shape  # è·å–è¾“å…¥ç»‘å®šçš„å½¢çŠ¶
            assert im.shape == s, f"input size {im.shape} {'>' if self.dynamic else 'not equal to'} max model size {s}"  # æ–­è¨€è¾“å…¥å½¢çŠ¶ä¸æ¨¡å‹æœ€å¤§å½¢çŠ¶åŒ¹é…
            self.binding_addrs["images"] = int(im.data_ptr())  # è·å–è¾“å…¥æ•°æ®æŒ‡é’ˆ
            self.context.execute_v2(list(self.binding_addrs.values()))  # æ‰§è¡Œæ¨ç†
            y = [self.bindings[x].data for x in sorted(self.output_names)]  # è·å–è¾“å‡ºæ•°æ®

        # CoreML  # CoreML æ¨ç†
        elif self.coreml:  # å¦‚æœæ˜¯ CoreML æ¨¡å‹
            im = im[0].cpu().numpy()  # è·å–è¾“å…¥å›¾åƒå¹¶è½¬æ¢ä¸º numpy
            im_pil = Image.fromarray((im * 255).astype("uint8"))  # è½¬æ¢ä¸º PIL å›¾åƒ
            # im = im.resize((192, 320), Image.BILINEAR)  # å¯é€‰ï¼šè°ƒæ•´å›¾åƒå¤§å°
            y = self.model.predict({"image": im_pil})  # æ‰§è¡Œæ¨ç†ï¼Œè¿”å›åæ ‡
            if "confidence" in y:  # å¦‚æœè¿”å›ç»“æœä¸­åŒ…å«ç½®ä¿¡åº¦
                raise TypeError(  # æŠ›å‡ºç±»å‹é”™è¯¯
                    "Ultralytics only supports inference of non-pipelined CoreML models exported with "  # "Ultralytics ä»…æ”¯æŒæœªç®¡é“åŒ–çš„ CoreML æ¨¡å‹æ¨ç†ï¼Œå¯¼å‡ºæ—¶éœ€ä½¿ç”¨"
                    f"'nms=False', but 'model={w}' has an NMS pipeline created by an 'nms=True' export."  # "'nms=True' å¯¼å‡ºçš„æ¨¡å‹ã€‚"
                )
                # TODO: CoreML NMS inference handling  # TODO: CoreML NMS æ¨ç†å¤„ç†
                # from ultralytics.utils.ops import xywh2xyxy
                # box = xywh2xyxy(y['coordinates'] * [[w, h, w, h]])  # xyxy pixels
                # conf, cls = y['confidence'].max(1), y['confidence'].argmax(1).astype(np.float32)
                # y = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)
            y = list(y.values())  # å°†ç»“æœè½¬æ¢ä¸ºåˆ—è¡¨
            if len(y) == 2 and len(y[1].shape) != 4:  # segmentation model  # å¦‚æœæ˜¯åˆ†å‰²æ¨¡å‹
                y = list(reversed(y))  # reversed for segmentation models (pred, proto)  # åè½¬ç»“æœé¡ºåº

        # PaddlePaddle  # PaddlePaddle æ¨ç†
        elif self.paddle:  # å¦‚æœæ˜¯ PaddlePaddle æ¨¡å‹
            im = im.cpu().numpy().astype(np.float32)  # è½¬æ¢ä¸º numpy å¹¶è®¾ç½®æ•°æ®ç±»å‹
            self.input_handle.copy_from_cpu(im)  # ä» CPU å¤åˆ¶è¾“å…¥æ•°æ®
            self.predictor.run()  # æ‰§è¡Œæ¨ç†
            y = [self.predictor.get_output_handle(x).copy_to_cpu() for x in self.output_names]  # è·å–è¾“å‡ºæ•°æ®å¹¶å¤åˆ¶åˆ° CPU

        # MNN  # MNN æ¨ç†
        elif self.mnn:  # å¦‚æœæ˜¯ MNN æ¨¡å‹
            input_var = self.torch_to_mnn(im)  # å°† PyTorch å¼ é‡è½¬æ¢ä¸º MNN å¼ é‡
            output_var = self.net.onForward([input_var])  # æ‰§è¡Œå‰å‘æ¨ç†
            y = [x.read() for x in output_var]  # è¯»å–è¾“å‡ºæ•°æ®

        # NCNN  # NCNN æ¨ç†
        elif self.ncnn:  # å¦‚æœæ˜¯ NCNN æ¨¡å‹
            mat_in = self.pyncnn.Mat(im[0].cpu().numpy())  # å°†è¾“å…¥æ•°æ®è½¬æ¢ä¸º NCNN æ ¼å¼
            with self.net.create_extractor() as ex:  # åˆ›å»ºæå–å™¨
                ex.input(self.net.input_names()[0], mat_in)  # è®¾ç½®è¾“å…¥
                # WARNING: 'output_names' sorted as a temporary fix for https://github.com/pnnx/pnnx/issues/130  # è­¦å‘Šï¼š'output_names' æ’åºæ˜¯ä¸´æ—¶ä¿®å¤
                y = [np.array(ex.extract(x)[1])[None] for x in sorted(self.net.output_names())]  # è·å–è¾“å‡ºæ•°æ®

        # NVIDIA Triton Inference Server  # NVIDIA Triton æ¨ç†æœåŠ¡å™¨
        elif self.triton:  # å¦‚æœæ˜¯ Triton æ¨¡å‹
            im = im.cpu().numpy()  # torch è½¬æ¢ä¸º numpy
            y = self.model(im)  # æ‰§è¡Œæ¨ç†

        # RKNN  # RKNN æ¨ç†
        elif self.rknn:  # å¦‚æœæ˜¯ RKNN æ¨¡å‹
            im = (im.cpu().numpy() * 255).astype("uint8")  # è½¬æ¢ä¸º numpy å¹¶è®¾ç½®æ•°æ®ç±»å‹
            im = im if isinstance(im, (list, tuple)) else [im]  # å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼Œåˆ™è½¬æ¢ä¸ºåˆ—è¡¨
            y = self.rknn_model.inference(inputs=im)  # æ‰§è¡Œæ¨ç†

        # TensorFlow (SavedModel, GraphDef, Lite, Edge TPU)  # TensorFlowï¼ˆSavedModelã€GraphDefã€Liteã€Edge TPUï¼‰
        else:  # å¦‚æœä¸æ˜¯å·²çŸ¥æ ¼å¼
            im = im.cpu().numpy()  # è½¬æ¢ä¸º numpy
            if self.saved_model:  # SavedModel
                y = self.model(im, training=False) if self.keras else self.model(im)  # æ‰§è¡Œæ¨ç†
                if not isinstance(y, list):  # å¦‚æœè¿”å›ç»“æœä¸æ˜¯åˆ—è¡¨
                    y = [y]  # è½¬æ¢ä¸ºåˆ—è¡¨
            elif self.pb:  # GraphDef
                y = self.frozen_func(x=self.tf.constant(im))  # æ‰§è¡Œæ¨ç†
            else:  # Lite or Edge TPU
                details = self.input_details[0]  # è·å–è¾“å…¥è¯¦æƒ…
                is_int = details["dtype"] in {np.int8, np.int16}  # æ£€æŸ¥æ˜¯å¦ä¸º TFLite é‡åŒ– int8 æˆ– int16 æ¨¡å‹
                if is_int:  # å¦‚æœæ˜¯é‡åŒ–æ¨¡å‹
                    scale, zero_point = details["quantization"]  # è·å–é‡åŒ–å‚æ•°
                    im = (im / scale + zero_point).astype(details["dtype"])  # åé‡åŒ–
                self.interpreter.set_tensor(details["index"], im)  # è®¾ç½®è¾“å…¥å¼ é‡
                self.interpreter.invoke()  # æ‰§è¡Œæ¨ç†
                y = []  # åˆå§‹åŒ–è¾“å‡º
                for output in self.output_details:  # éå†è¾“å‡ºè¯¦æƒ…
                    x = self.interpreter.get_tensor(output["index"])  # è·å–è¾“å‡ºå¼ é‡
                    if is_int:  # å¦‚æœæ˜¯é‡åŒ–æ¨¡å‹
                        scale, zero_point = output["quantization"]  # è·å–é‡åŒ–å‚æ•°
                        x = (x.astype(np.float32) - zero_point) * scale  # åé‡åŒ–
                    if x.ndim == 3:  # å¦‚æœä»»åŠ¡ä¸æ˜¯åˆ†ç±»ï¼Œä¸”ä¸åŒ…æ‹¬æ©ç ï¼ˆndim=4ï¼‰
                        # Denormalize xywh by image size. See https://github.com/ultralytics/ultralytics/pull/1695  # æ ¹æ®å›¾åƒå¤§å°åå½’ä¸€åŒ– xywh
                        # xywh are normalized in TFLite/EdgeTPU to mitigate quantization error of integer models  # xywh åœ¨ TFLite/EdgeTPU ä¸­å½’ä¸€åŒ–ï¼Œä»¥å‡å°‘æ•´æ•°æ¨¡å‹çš„é‡åŒ–è¯¯å·®
                        if x.shape[-1] == 6 or self.end2end:  # end-to-end model  # å¦‚æœæ˜¯ç«¯åˆ°ç«¯æ¨¡å‹
                            x[:, :, [0, 2]] *= w  # åå½’ä¸€åŒ–å®½åº¦
                            x[:, :, [1, 3]] *= h  # åå½’ä¸€åŒ–é«˜åº¦
                            if self.task == "pose":  # å¦‚æœä»»åŠ¡æ˜¯å§¿æ€æ£€æµ‹
                                x[:, :, 6::3] *= w  # åå½’ä¸€åŒ–å…³é”®ç‚¹ x åæ ‡
                                x[:, :, 7::3] *= h  # åå½’ä¸€åŒ–å…³é”®ç‚¹ y åæ ‡
                        else:  # å¦‚æœä¸æ˜¯ç«¯åˆ°ç«¯æ¨¡å‹
                            x[:, [0, 2]] *= w  # åå½’ä¸€åŒ–å®½åº¦
                            x[:, [1, 3]] *= h  # åå½’ä¸€åŒ–é«˜åº¦
                            if self.task == "pose":  # å¦‚æœä»»åŠ¡æ˜¯å§¿æ€æ£€æµ‹
                                x[:, 5::3] *= w  # åå½’ä¸€åŒ–å…³é”®ç‚¹ x åæ ‡
                                x[:, 6::3] *= h  # åå½’ä¸€åŒ–å…³é”®ç‚¹ y åæ ‡
                    y.append(x)  # æ·»åŠ è¾“å‡º
            # TF segment fixes: export is reversed vs ONNX export and protos are transposed  # TensorFlow åˆ†å‰²ä¿®å¤ï¼šå¯¼å‡ºé¡ºåºä¸ ONNX å¯¼å‡ºç›¸åï¼ŒåŸå‹è¢«è½¬ç½®
            if len(y) == 2:  # segment with (det, proto) output order reversed  # å¦‚æœæ˜¯åˆ†å‰²æ¨¡å‹ï¼Œè¾“å‡ºé¡ºåºä¸ºï¼ˆæ£€æµ‹ï¼ŒåŸå‹ï¼‰
                if len(y[1].shape) != 4:  # å¦‚æœåŸå‹çš„å½¢çŠ¶ä¸æ˜¯ 4 ç»´
                    y = list(reversed(y))  # åè½¬è¾“å‡ºé¡ºåº
                if y[1].shape[-1] == 6:  # end-to-end model  # å¦‚æœæ˜¯ç«¯åˆ°ç«¯æ¨¡å‹
                    y = [y[1]]  # ä»…è¿”å›åŸå‹
                else:
                    y[1] = np.transpose(y[1], (0, 3, 1, 2))  # è½¬ç½®åŸå‹
            y = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]  # ç¡®ä¿è¾“å‡ºä¸º numpy æ•°ç»„

        # for x in y:  # è°ƒè¯•è¾“å‡ºå½¢çŠ¶
        #     print(type(x), len(x)) if isinstance(x, (list, tuple)) else print(type(x), x.shape)  # debug shapes
        if isinstance(y, (list, tuple)):  # å¦‚æœè¾“å‡ºæ˜¯åˆ—è¡¨æˆ–å…ƒç»„
            if len(self.names) == 999 and (self.task == "segment" or len(y) == 2):  # segments and names not defined  # å¦‚æœç±»åæœªå®šä¹‰
                nc = y[0].shape[1] - y[1].shape[1] - 4  # è®¡ç®—ç±»åˆ«æ•°é‡
                self.names = {i: f"class{i}" for i in range(nc)}  # åˆ›å»ºç±»åå­—å…¸
            return self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]  # è¿”å›è¾“å‡º
        else:
            return self.from_numpy(y)  # è¿”å›è¾“å‡º

    def from_numpy(self, x):  # å°† numpy æ•°ç»„è½¬æ¢ä¸ºå¼ é‡
        """
        Convert a numpy array to a tensor.  # å°† numpy æ•°ç»„è½¬æ¢ä¸ºå¼ é‡

        Args:  # å‚æ•°ï¼š
            x (np.ndarray): The array to be converted.  # xï¼ˆnp.ndarrayï¼‰ï¼šè¦è½¬æ¢çš„æ•°ç»„ã€‚

        Returns:  # è¿”å›ï¼š
            (torch.Tensor): The converted tensor  # ï¼ˆtorch.Tensorï¼‰ï¼šè½¬æ¢åçš„å¼ é‡
        """
        return torch.tensor(x).to(self.device) if isinstance(x, np.ndarray) else x  # è½¬æ¢ä¸ºå¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡

    def warmup(self, imgsz=(1, 3, 640, 640)):  # é¢„çƒ­æ¨¡å‹
        """
        Warm up the model by running one forward pass with a dummy input.  # é€šè¿‡ä½¿ç”¨è™šæ‹Ÿè¾“å…¥è¿è¡Œä¸€æ¬¡å‰å‘ä¼ é€’æ¥é¢„çƒ­æ¨¡å‹

        Args:  # å‚æ•°ï¼š
            imgsz (tuple): The shape of the dummy input tensor in the format (batch_size, channels, height, width)  # imgszï¼ˆå…ƒç»„ï¼‰ï¼šè™šæ‹Ÿè¾“å…¥å¼ é‡çš„å½¢çŠ¶ï¼Œæ ¼å¼ä¸ºï¼ˆæ‰¹æ¬¡å¤§å°ã€é€šé“æ•°ã€é«˜åº¦ã€å®½åº¦ï¼‰
        """
        import torchvision  # noqa (import here so torchvision import time not recorded in postprocess time)  # å¯¼å…¥ torchvision

        warmup_types = self.pt, self.jit, self.onnx, self.engine, self.saved_model, self.pb, self.triton, self.nn_module  # é¢„çƒ­ç±»å‹
        if any(warmup_types) and (self.device.type != "cpu" or self.triton):  # å¦‚æœéœ€è¦é¢„çƒ­ä¸”è®¾å¤‡ä¸æ˜¯ CPU æˆ–ä½¿ç”¨ Triton
            im = torch.empty(*imgsz, dtype=torch.half if self.fp16 else torch.float, device=self.device)  # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
            for _ in range(2 if self.jit else 1):  # æ ¹æ®æ˜¯å¦ä¸º JIT è¿›è¡Œé¢„çƒ­
                self.forward(im)  # è¿è¡Œå‰å‘æ¨ç†

    @staticmethod
    def _model_type(p="path/to/model.pt"):  # ç¡®å®šæ¨¡å‹ç±»å‹
        """
        Takes a path to a model file and returns the model type. Possibles types are pt, jit, onnx, xml, engine, coreml,  # æ¥å—æ¨¡å‹æ–‡ä»¶è·¯å¾„å¹¶è¿”å›æ¨¡å‹ç±»å‹ã€‚å¯èƒ½çš„ç±»å‹æœ‰ ptã€jitã€onnxã€xmlã€engineã€coremlã€
        saved_model, pb, tflite, edgetpu, tfjs, ncnn or paddle.  # saved_modelã€pbã€tfliteã€edgetpuã€tfjsã€ncnn æˆ– paddleã€‚

        Args:  # å‚æ•°ï¼š
            p (str): path to the model file. Defaults to path/to/model.pt  # pï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼šæ¨¡å‹æ–‡ä»¶çš„è·¯å¾„ã€‚é»˜è®¤ä¸º path/to/model.pt

        Examples:  # ç¤ºä¾‹ï¼š
            >>> model = AutoBackend(weights="path/to/model.onnx")  # åˆ›å»º AutoBackend å®ä¾‹
            >>> model_type = model._model_type()  # returns "onnx"  # è¿”å› "onnx"
        """
        from ultralytics.engine.exporter import export_formats  # å¯¼å…¥å¯¼å‡ºæ ¼å¼

        sf = export_formats()["Suffix"]  # export suffixes  # è·å–å¯¼å‡ºåç¼€
        if not is_url(p) and not isinstance(p, str):  # æ£€æŸ¥è·¯å¾„æ˜¯å¦ä¸º URL æˆ–å­—ç¬¦ä¸²
            check_suffix(p, sf)  # æ£€æŸ¥åç¼€
        name = Path(p).name  # è·å–æ–‡ä»¶å
        types = [s in name for s in sf]  # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«åç¼€
        types[5] |= name.endswith(".mlmodel")  # retain support for older Apple CoreML *.mlmodel formats  # ä¿ç•™å¯¹æ—§ç‰ˆ Apple CoreML *.mlmodel æ ¼å¼çš„æ”¯æŒ
        types[8] &= not types[9]  # tflite &= not edgetpu  # tflite ä»…å½“ä¸æ˜¯ edgetpu æ—¶æœ‰æ•ˆ
        if any(types):  # å¦‚æœæœ‰åŒ¹é…çš„ç±»å‹
            triton = False  # è®¾ç½® Triton ä¸º False
        else:  # å¦‚æœæ²¡æœ‰åŒ¹é…çš„ç±»å‹
            from urllib.parse import urlsplit  # å¯¼å…¥ URL è§£ææ¨¡å—

            url = urlsplit(p)  # è§£æ URL
            triton = bool(url.netloc) and bool(url.path) and url.scheme in {"http", "grpc"}  # æ£€æŸ¥æ˜¯å¦ä¸º Triton URL

        return types + [triton]  # è¿”å›ç±»å‹åˆ—è¡¨