# Ultralytics üöÄ AGPL-3.0 License - https://ultralytics.com/license

import ast
import json
import platform
import zipfile
from collections import OrderedDict, namedtuple
from pathlib import Path

import cv2
import numpy as np
import torch
import torch.nn as nn
from PIL import Image

from ultralytics.utils import ARM64, IS_JETSON, IS_RASPBERRYPI, LINUX, LOGGER, PYTHON_VERSION, ROOT, yaml_load
from ultralytics.utils.checks import check_requirements, check_suffix, check_version, check_yaml, is_rockchip
from ultralytics.utils.downloads import attempt_download_asset, is_url


# def check_class_names(names):
#     """
#     Check class names.

#     Map imagenet class codes to human-readable names if required. Convert lists to dicts.
#     """
#     if isinstance(names, list):  # names is a list
#         names = dict(enumerate(names))  # convert to dict
#     if isinstance(names, dict):
#         # Convert 1) string keys to int, i.e. '0' to 0, and non-string values to strings, i.e. True to 'True'
#         names = {int(k): str(v) for k, v in names.items()}
#         n = len(names)
#         if max(names.keys()) >= n:
#             raise KeyError(
#                 f"{n}-class dataset requires class indices 0-{n - 1}, but you have invalid class indices "
#                 f"{min(names.keys())}-{max(names.keys())} defined in your dataset YAML."
#             )
#         if isinstance(names[0], str) and names[0].startswith("n0"):  # imagenet class codes, i.e. 'n01440764'
#             names_map = yaml_load(ROOT / "cfg/datasets/ImageNet.yaml")["map"]  # human-readable names
#             names = {k: names_map[v] for k, v in names.items()}
#     return names


# def default_class_names(data=None):
#     """Applies default class names to an input YAML file or returns numerical class names."""
#     if data:
#         try:
#             return yaml_load(check_yaml(data))["names"]
#         except Exception:
#             pass
#     return {i: f"class{i}" for i in range(999)}  # return default if above errors


# class AutoBackend(nn.Module):
#     """
#     Handles dynamic backend selection for running inference using Ultralytics YOLO models.

#     The AutoBackend class is designed to provide an abstraction layer for various inference engines. It supports a wide
#     range of formats, each with specific naming conventions as outlined below:

#         Supported Formats and Naming Conventions:
#             | Format                | File Suffix       |
#             | --------------------- | ----------------- |
#             | PyTorch               | *.pt              |
#             | TorchScript           | *.torchscript     |
#             | ONNX Runtime          | *.onnx            |
#             | ONNX OpenCV DNN       | *.onnx (dnn=True) |
#             | OpenVINO              | *openvino_model/  |
#             | CoreML                | *.mlpackage       |
#             | TensorRT              | *.engine          |
#             | TensorFlow SavedModel | *_saved_model/    |
#             | TensorFlow GraphDef   | *.pb              |
#             | TensorFlow Lite       | *.tflite          |
#             | TensorFlow Edge TPU   | *_edgetpu.tflite  |
#             | PaddlePaddle          | *_paddle_model/   |
#             | MNN                   | *.mnn             |
#             | NCNN                  | *_ncnn_model/     |
#             | IMX                   | *_imx_model/      |
#             | RKNN                  | *_rknn_model/     |

#     This class offers dynamic backend switching capabilities based on the input model format, making it easier to deploy
#     models across various platforms.
#     """

#     @torch.no_grad()
#     def __init__(
#         self,
#         weights="yolo11n.pt",
#         device=torch.device("cpu"),
#         dnn=False,
#         data=None,
#         fp16=False,
#         batch=1,
#         fuse=True,
#         verbose=True,
#     ):
#         """
#         Initialize the AutoBackend for inference.

#         Args:
#             weights (str | torch.nn.Module): Path to the model weights file or a module instance. Defaults to 'yolo11n.pt'.
#             device (torch.device): Device to run the model on. Defaults to CPU.
#             dnn (bool): Use OpenCV DNN module for ONNX inference. Defaults to False.
#             data (str | Path | optional): Path to the additional data.yaml file containing class names. Optional.
#             fp16 (bool): Enable half-precision inference. Supported only on specific backends. Defaults to False.
#             batch (int): Batch-size to assume for inference.
#             fuse (bool): Fuse Conv2D + BatchNorm layers for optimization. Defaults to True.
#             verbose (bool): Enable verbose logging. Defaults to True.
#         """
#         super().__init__()
#         w = str(weights[0] if isinstance(weights, list) else weights)
#         nn_module = isinstance(weights, torch.nn.Module)
#         (
#             pt,
#             jit,
#             onnx,
#             xml,
#             engine,
#             coreml,
#             saved_model,
#             pb,
#             tflite,
#             edgetpu,
#             tfjs,
#             paddle,
#             mnn,
#             ncnn,
#             imx,
#             rknn,
#             triton,
#         ) = self._model_type(w)
#         fp16 &= pt or jit or onnx or xml or engine or nn_module or triton  # FP16
#         nhwc = coreml or saved_model or pb or tflite or edgetpu or rknn  # BHWC formats (vs torch BCWH)
#         stride = 32  # default stride
#         end2end = False  # default end2end
#         model, metadata, task = None, None, None

#         # Set device
#         cuda = torch.cuda.is_available() and device.type != "cpu"  # use CUDA
#         if cuda and not any([nn_module, pt, jit, engine, onnx, paddle]):  # GPU dataloader formats
#             device = torch.device("cpu")
#             cuda = False

#         # Download if not local
#         if not (pt or triton or nn_module):
#             w = attempt_download_asset(w)

#         # In-memory PyTorch model
#         if nn_module:
#             model = weights.to(device)
#             if fuse:
#                 model = model.fuse(verbose=verbose)
#             if hasattr(model, "kpt_shape"):
#                 kpt_shape = model.kpt_shape  # pose-only
#             stride = max(int(model.stride.max()), 32)  # model stride
#             names = model.module.names if hasattr(model, "module") else model.names  # get class names
#             model.half() if fp16 else model.float()
#             self.model = model  # explicitly assign for to(), cpu(), cuda(), half()
#             pt = True

#         # PyTorch
#         elif pt:
#             from ultralytics.nn.tasks import attempt_load_weights

#             model = attempt_load_weights(
#                 weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse
#             )
#             if hasattr(model, "kpt_shape"):
#                 kpt_shape = model.kpt_shape  # pose-only
#             stride = max(int(model.stride.max()), 32)  # model stride
#             names = model.module.names if hasattr(model, "module") else model.names  # get class names
#             model.half() if fp16 else model.float()
#             self.model = model  # explicitly assign for to(), cpu(), cuda(), half()

#         # TorchScript
#         elif jit:
#             LOGGER.info(f"Loading {w} for TorchScript inference...")
#             extra_files = {"config.txt": ""}  # model metadata
#             model = torch.jit.load(w, _extra_files=extra_files, map_location=device)
#             model.half() if fp16 else model.float()
#             if extra_files["config.txt"]:  # load metadata dict
#                 metadata = json.loads(extra_files["config.txt"], object_hook=lambda x: dict(x.items()))

#         # ONNX OpenCV DNN
#         elif dnn:
#             LOGGER.info(f"Loading {w} for ONNX OpenCV DNN inference...")
#             check_requirements("opencv-python>=4.5.4")
#             net = cv2.dnn.readNetFromONNX(w)

#         # ONNX Runtime and IMX
#         elif onnx or imx:
#             LOGGER.info(f"Loading {w} for ONNX Runtime inference...")
#             check_requirements(("onnx", "onnxruntime-gpu" if cuda else "onnxruntime"))
#             if IS_RASPBERRYPI or IS_JETSON:
#                 # Fix 'numpy.linalg._umath_linalg' has no attribute '_ilp64' for TF SavedModel on RPi and Jetson
#                 check_requirements("numpy==1.23.5")
#             import onnxruntime

#             providers = ["CPUExecutionProvider"]
#             if cuda and "CUDAExecutionProvider" in onnxruntime.get_available_providers():
#                 providers.insert(0, "CUDAExecutionProvider")
#             elif cuda:  # Only log warning if CUDA was requested but unavailable
#                 LOGGER.warning("WARNING ‚ö†Ô∏è Failed to start ONNX Runtime with CUDA. Using CPU...")
#                 device = torch.device("cpu")
#                 cuda = False
#             LOGGER.info(f"Using ONNX Runtime {providers[0]}")
#             if onnx:
#                 session = onnxruntime.InferenceSession(w, providers=providers)
#             else:
#                 check_requirements(
#                     ["model-compression-toolkit==2.1.1", "sony-custom-layers[torch]==0.2.0", "onnxruntime-extensions"]
#                 )
#                 w = next(Path(w).glob("*.onnx"))
#                 LOGGER.info(f"Loading {w} for ONNX IMX inference...")
#                 import mct_quantizers as mctq
#                 from sony_custom_layers.pytorch.object_detection import nms_ort  # noqa

#                 session = onnxruntime.InferenceSession(
#                     w, mctq.get_ort_session_options(), providers=["CPUExecutionProvider"]
#                 )
#                 task = "detect"

#             output_names = [x.name for x in session.get_outputs()]
#             metadata = session.get_modelmeta().custom_metadata_map
#             dynamic = isinstance(session.get_outputs()[0].shape[0], str)
#             fp16 = True if "float16" in session.get_inputs()[0].type else False
#             if not dynamic:
#                 io = session.io_binding()
#                 bindings = []
#                 for output in session.get_outputs():
#                     out_fp16 = "float16" in output.type
#                     y_tensor = torch.empty(output.shape, dtype=torch.float16 if out_fp16 else torch.float32).to(device)
#                     io.bind_output(
#                         name=output.name,
#                         device_type=device.type,
#                         device_id=device.index if cuda else 0,
#                         element_type=np.float16 if out_fp16 else np.float32,
#                         shape=tuple(y_tensor.shape),
#                         buffer_ptr=y_tensor.data_ptr(),
#                     )
#                     bindings.append(y_tensor)

#         # OpenVINO
#         elif xml:
#             LOGGER.info(f"Loading {w} for OpenVINO inference...")
#             check_requirements("openvino>=2024.0.0")
#             import openvino as ov

#             core = ov.Core()
#             w = Path(w)
#             if not w.is_file():  # if not *.xml
#                 w = next(w.glob("*.xml"))  # get *.xml file from *_openvino_model dir
#             ov_model = core.read_model(model=str(w), weights=w.with_suffix(".bin"))
#             if ov_model.get_parameters()[0].get_layout().empty:
#                 ov_model.get_parameters()[0].set_layout(ov.Layout("NCHW"))

#             # OpenVINO inference modes are 'LATENCY', 'THROUGHPUT' (not recommended), or 'CUMULATIVE_THROUGHPUT'
#             inference_mode = "CUMULATIVE_THROUGHPUT" if batch > 1 else "LATENCY"
#             LOGGER.info(f"Using OpenVINO {inference_mode} mode for batch={batch} inference...")
#             ov_compiled_model = core.compile_model(
#                 ov_model,
#                 device_name="AUTO",  # AUTO selects best available device, do not modify
#                 config={"PERFORMANCE_HINT": inference_mode},
#             )
#             input_name = ov_compiled_model.input().get_any_name()
#             metadata = w.parent / "metadata.yaml"

#         # TensorRT
#         elif engine:
#             LOGGER.info(f"Loading {w} for TensorRT inference...")

#             if IS_JETSON and PYTHON_VERSION <= "3.8.0":
#                 # fix error: `np.bool` was a deprecated alias for the builtin `bool` for JetPack 4 with Python <= 3.8.0
#                 check_requirements("numpy==1.23.5")

#             try:
#                 import tensorrt as trt  # noqa https://developer.nvidia.com/nvidia-tensorrt-download
#             except ImportError:
#                 if LINUX:
#                     check_requirements("tensorrt>7.0.0,!=10.1.0")
#                 import tensorrt as trt  # noqa
#             check_version(trt.__version__, ">=7.0.0", hard=True)
#             check_version(trt.__version__, "!=10.1.0", msg="https://github.com/ultralytics/ultralytics/pull/14239")
#             if device.type == "cpu":
#                 device = torch.device("cuda:0")
#             Binding = namedtuple("Binding", ("name", "dtype", "shape", "data", "ptr"))
#             logger = trt.Logger(trt.Logger.INFO)
#             # Read file
#             with open(w, "rb") as f, trt.Runtime(logger) as runtime:
#                 try:
#                     meta_len = int.from_bytes(f.read(4), byteorder="little")  # read metadata length
#                     metadata = json.loads(f.read(meta_len).decode("utf-8"))  # read metadata
#                 except UnicodeDecodeError:
#                     f.seek(0)  # engine file may lack embedded Ultralytics metadata
#                 dla = metadata.get("dla", None)
#                 if dla is not None:
#                     runtime.DLA_core = int(dla)
#                 model = runtime.deserialize_cuda_engine(f.read())  # read engine

#             # Model context
#             try:
#                 context = model.create_execution_context()
#             except Exception as e:  # model is None
#                 LOGGER.error(f"ERROR: TensorRT model exported with a different version than {trt.__version__}\n")
#                 raise e

#             bindings = OrderedDict()
#             output_names = []
#             fp16 = False  # default updated below
#             dynamic = False
#             is_trt10 = not hasattr(model, "num_bindings")
#             num = range(model.num_io_tensors) if is_trt10 else range(model.num_bindings)
#             for i in num:
#                 if is_trt10:
#                     name = model.get_tensor_name(i)
#                     dtype = trt.nptype(model.get_tensor_dtype(name))
#                     is_input = model.get_tensor_mode(name) == trt.TensorIOMode.INPUT
#                     if is_input:
#                         if -1 in tuple(model.get_tensor_shape(name)):
#                             dynamic = True
#                             context.set_input_shape(name, tuple(model.get_tensor_profile_shape(name, 0)[1]))
#                         if dtype == np.float16:
#                             fp16 = True
#                     else:
#                         output_names.append(name)
#                     shape = tuple(context.get_tensor_shape(name))
#                 else:  # TensorRT < 10.0
#                     name = model.get_binding_name(i)
#                     dtype = trt.nptype(model.get_binding_dtype(i))
#                     is_input = model.binding_is_input(i)
#                     if model.binding_is_input(i):
#                         if -1 in tuple(model.get_binding_shape(i)):  # dynamic
#                             dynamic = True
#                             context.set_binding_shape(i, tuple(model.get_profile_shape(0, i)[1]))
#                         if dtype == np.float16:
#                             fp16 = True
#                     else:
#                         output_names.append(name)
#                     shape = tuple(context.get_binding_shape(i))
#                 im = torch.from_numpy(np.empty(shape, dtype=dtype)).to(device)
#                 bindings[name] = Binding(name, dtype, shape, im, int(im.data_ptr()))
#             binding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())
#             batch_size = bindings["images"].shape[0]  # if dynamic, this is instead max batch size

#         # CoreML
#         elif coreml:
#             LOGGER.info(f"Loading {w} for CoreML inference...")
#             import coremltools as ct

#             model = ct.models.MLModel(w)
#             metadata = dict(model.user_defined_metadata)

#         # TF SavedModel
#         elif saved_model:
#             LOGGER.info(f"Loading {w} for TensorFlow SavedModel inference...")
#             import tensorflow as tf

#             keras = False  # assume TF1 saved_model
#             model = tf.keras.models.load_model(w) if keras else tf.saved_model.load(w)
#             metadata = Path(w) / "metadata.yaml"

#         # TF GraphDef
#         elif pb:  # https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt
#             LOGGER.info(f"Loading {w} for TensorFlow GraphDef inference...")
#             import tensorflow as tf

#             from ultralytics.engine.exporter import gd_outputs

#             def wrap_frozen_graph(gd, inputs, outputs):
#                 """Wrap frozen graphs for deployment."""
#                 x = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=""), [])  # wrapped
#                 ge = x.graph.as_graph_element
#                 return x.prune(tf.nest.map_structure(ge, inputs), tf.nest.map_structure(ge, outputs))

#             gd = tf.Graph().as_graph_def()  # TF GraphDef
#             with open(w, "rb") as f:
#                 gd.ParseFromString(f.read())
#             frozen_func = wrap_frozen_graph(gd, inputs="x:0", outputs=gd_outputs(gd))
#             try:  # find metadata in SavedModel alongside GraphDef
#                 metadata = next(Path(w).resolve().parent.rglob(f"{Path(w).stem}_saved_model*/metadata.yaml"))
#             except StopIteration:
#                 pass

#         # TFLite or TFLite Edge TPU
#         elif tflite or edgetpu:  # https://www.tensorflow.org/lite/guide/python#install_tensorflow_lite_for_python
#             try:  # https://coral.ai/docs/edgetpu/tflite-python/#update-existing-tf-lite-code-for-the-edge-tpu
#                 from tflite_runtime.interpreter import Interpreter, load_delegate
#             except ImportError:
#                 import tensorflow as tf

#                 Interpreter, load_delegate = tf.lite.Interpreter, tf.lite.experimental.load_delegate
#             if edgetpu:  # TF Edge TPU https://coral.ai/software/#edgetpu-runtime
#                 device = device[3:] if str(device).startswith("tpu") else ":0"
#                 LOGGER.info(f"Loading {w} on device {device[1:]} for TensorFlow Lite Edge TPU inference...")
#                 delegate = {"Linux": "libedgetpu.so.1", "Darwin": "libedgetpu.1.dylib", "Windows": "edgetpu.dll"}[
#                     platform.system()
#                 ]
#                 interpreter = Interpreter(
#                     model_path=w,
#                     experimental_delegates=[load_delegate(delegate, options={"device": device})],
#                 )
#                 device = "cpu"  # Required, otherwise PyTorch will try to use the wrong device
#             else:  # TFLite
#                 LOGGER.info(f"Loading {w} for TensorFlow Lite inference...")
#                 interpreter = Interpreter(model_path=w)  # load TFLite model
#             interpreter.allocate_tensors()  # allocate
#             input_details = interpreter.get_input_details()  # inputs
#             output_details = interpreter.get_output_details()  # outputs
#             # Load metadata
#             try:
#                 with zipfile.ZipFile(w, "r") as model:
#                     meta_file = model.namelist()[0]
#                     metadata = ast.literal_eval(model.read(meta_file).decode("utf-8"))
#             except zipfile.BadZipFile:
#                 pass

#         # TF.js
#         elif tfjs:
#             raise NotImplementedError("YOLOv8 TF.js inference is not currently supported.")

#         # PaddlePaddle
#         elif paddle:
#             LOGGER.info(f"Loading {w} for PaddlePaddle inference...")
#             check_requirements("paddlepaddle-gpu" if cuda else "paddlepaddle")
#             import paddle.inference as pdi  # noqa

#             w = Path(w)
#             if not w.is_file():  # if not *.pdmodel
#                 w = next(w.rglob("*.pdmodel"))  # get *.pdmodel file from *_paddle_model dir
#             config = pdi.Config(str(w), str(w.with_suffix(".pdiparams")))
#             if cuda:
#                 config.enable_use_gpu(memory_pool_init_size_mb=2048, device_id=0)
#             predictor = pdi.create_predictor(config)
#             input_handle = predictor.get_input_handle(predictor.get_input_names()[0])
#             output_names = predictor.get_output_names()
#             metadata = w.parents[1] / "metadata.yaml"

#         # MNN
#         elif mnn:
#             LOGGER.info(f"Loading {w} for MNN inference...")
#             check_requirements("MNN")  # requires MNN
#             import os

#             import MNN

#             config = {"precision": "low", "backend": "CPU", "numThread": (os.cpu_count() + 1) // 2}
#             rt = MNN.nn.create_runtime_manager((config,))
#             net = MNN.nn.load_module_from_file(w, [], [], runtime_manager=rt, rearrange=True)

#             def torch_to_mnn(x):
#                 return MNN.expr.const(x.data_ptr(), x.shape)

#             metadata = json.loads(net.get_info()["bizCode"])

#         # NCNN
#         elif ncnn:
#             LOGGER.info(f"Loading {w} for NCNN inference...")
#             check_requirements("git+https://github.com/Tencent/ncnn.git" if ARM64 else "ncnn")  # requires NCNN
#             import ncnn as pyncnn

#             net = pyncnn.Net()
#             net.opt.use_vulkan_compute = cuda
#             w = Path(w)
#             if not w.is_file():  # if not *.param
#                 w = next(w.glob("*.param"))  # get *.param file from *_ncnn_model dir
#             net.load_param(str(w))
#             net.load_model(str(w.with_suffix(".bin")))
#             metadata = w.parent / "metadata.yaml"

#         # NVIDIA Triton Inference Server
#         elif triton:
#             check_requirements("tritonclient[all]")
#             from ultralytics.utils.triton import TritonRemoteModel

#             model = TritonRemoteModel(w)
#             metadata = model.metadata

def check_class_names(names):
    """
    Check class names.  # Ê£ÄÊü•Á±ªÂêç

    Map imagenet class codes to human-readable names if required. Convert lists to dicts.  # Â¶ÇÊûúÈúÄË¶ÅÔºåÂ∞ÜimagenetÁ±ª‰ª£Á†ÅÊò†Â∞ÑÂà∞ÂèØËØªÁöÑÂêçÁß∞„ÄÇÂ∞ÜÂàóË°®ËΩ¨Êç¢‰∏∫Â≠óÂÖ∏„ÄÇ
    """
    if isinstance(names, list):  # names is a list  # Â¶ÇÊûúnamesÊòØ‰∏Ä‰∏™ÂàóË°®
        names = dict(enumerate(names))  # convert to dict  # ËΩ¨Êç¢‰∏∫Â≠óÂÖ∏
    if isinstance(names, dict):  # Â¶ÇÊûúnamesÊòØ‰∏Ä‰∏™Â≠óÂÖ∏
        # Convert 1) string keys to int, i.e. '0' to 0, and non-string values to strings, i.e. True to 'True'  # Â∞Ü1) Â≠óÁ¨¶‰∏≤ÈîÆËΩ¨Êç¢‰∏∫Êï¥Êï∞Ôºå‰æãÂ¶Ç'0'ËΩ¨Êç¢‰∏∫0ÔºåÈùûÂ≠óÁ¨¶‰∏≤ÂÄºËΩ¨Êç¢‰∏∫Â≠óÁ¨¶‰∏≤Ôºå‰æãÂ¶ÇTrueËΩ¨Êç¢‰∏∫'True'
        names = {int(k): str(v) for k, v in names.items()}  # Â∞ÜÈîÆËΩ¨Êç¢‰∏∫Êï¥Êï∞ÔºåÂÄºËΩ¨Êç¢‰∏∫Â≠óÁ¨¶‰∏≤
        n = len(names)  # Ëé∑ÂèñÂ≠óÂÖ∏ÁöÑÈïøÂ∫¶
        if max(names.keys()) >= n:  # Â¶ÇÊûúÊúÄÂ§ßÈîÆÂÄºÂ§ß‰∫éÁ≠â‰∫éÂ≠óÂÖ∏ÈïøÂ∫¶
            raise KeyError(  # ÂºïÂèëKeyErrorÂºÇÂ∏∏
                f"{n}-class dataset requires class indices 0-{n - 1}, but you have invalid class indices "  # "{n}-Á±ªÊï∞ÊçÆÈõÜÈúÄË¶ÅÁ±ªÁ¥¢Âºï0-{n - 1}Ôºå‰ΩÜÊÇ®ÊúâÊó†ÊïàÁöÑÁ±ªÁ¥¢Âºï"
                f"{min(names.keys())}-{max(names.keys())} defined in your dataset YAML."  # "{min(names.keys())}-{max(names.keys())} Âú®ÊÇ®ÁöÑÊï∞ÊçÆÈõÜYAML‰∏≠ÂÆö‰πâ„ÄÇ"
            )
        if isinstance(names[0], str) and names[0].startswith("n0"):  # imagenet class codes, i.e. 'n01440764'  # imagenetÁ±ª‰ª£Á†ÅÔºå‰æãÂ¶Ç'n01440764'
            names_map = yaml_load(ROOT / "cfg/datasets/ImageNet.yaml")["map"]  # human-readable names  # ÂèØËØªÂêçÁß∞
            names = {k: names_map[v] for k, v in names.items()}  # Â∞ÜÁ±ª‰ª£Á†ÅÊò†Â∞ÑÂà∞ÂèØËØªÂêçÁß∞
    return names  # ËøîÂõûÂ§ÑÁêÜÂêéÁöÑÁ±ªÂêç

def default_class_names(data=None):
    """Applies default class names to an input YAML file or returns numerical class names.  # Â∞ÜÈªòËÆ§Á±ªÂêçÂ∫îÁî®‰∫éËæìÂÖ•YAMLÊñá‰ª∂ÊàñËøîÂõûÊï∞Â≠óÁ±ªÂêç„ÄÇ"""
    if data:  # Â¶ÇÊûúÊèê‰æõ‰∫ÜÊï∞ÊçÆ
        try:
            return yaml_load(check_yaml(data))["names"]  # Â∞ùËØïÂä†ËΩΩYAMLÊñá‰ª∂Âπ∂ËøîÂõûÁ±ªÂêç
        except Exception:  # Â¶ÇÊûúÂèëÁîüÂºÇÂ∏∏
            pass  # ÂøΩÁï•ÂºÇÂ∏∏
    return {i: f"class{i}" for i in range(999)}  # return default if above errors  # Â¶ÇÊûúÂèëÁîü‰∏äËø∞ÈîôËØØÔºåÂàôËøîÂõûÈªòËÆ§Á±ªÂêç

class AutoBackend(nn.Module):
    """
    Handles dynamic backend selection for running inference using Ultralytics YOLO models.  # Â§ÑÁêÜ‰ΩøÁî®Ultralytics YOLOÊ®°ÂûãËøõË°åÊé®ÁêÜÁöÑÂä®ÊÄÅÂêéÁ´ØÈÄâÊã©„ÄÇ

    The AutoBackend class is designed to provide an abstraction layer for various inference engines. It supports a wide  # AutoBackendÁ±ªÊó®Âú®‰∏∫ÂêÑÁßçÊé®ÁêÜÂºïÊìéÊèê‰æõÊäΩË±°Â±Ç„ÄÇÂÆÉÊîØÊåÅÂπøÊ≥õÁöÑ
    range of formats, each with specific naming conventions as outlined below:  # Ê†ºÂºèÔºåÊØèÁßçÊ†ºÂºèÈÉΩÊúâÁâπÂÆöÁöÑÂëΩÂêçÁ∫¶ÂÆöÔºåÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö

        Supported Formats and Naming Conventions:  # ÊîØÊåÅÁöÑÊ†ºÂºèÂíåÂëΩÂêçÁ∫¶ÂÆöÔºö
            | Format                | File Suffix       |  # | Ê†ºÂºè                | Êñá‰ª∂ÂêéÁºÄ       |
            | --------------------- | ----------------- |  # | --------------------- | ----------------- |
            | PyTorch               | *.pt              |  # | PyTorch               | *.pt              |
            | TorchScript           | *.torchscript     |  # | TorchScript           | *.torchscript     |
            | ONNX Runtime          | *.onnx            |  # | ONNX Runtime          | *.onnx            |
            | ONNX OpenCV DNN       | *.onnx (dnn=True) |  # | ONNX OpenCV DNN       | *.onnx (dnn=True) |
            | OpenVINO              | *openvino_model/  |  # | OpenVINO              | *openvino_model/  |
            | CoreML                | *.mlpackage       |  # | CoreML                | *.mlpackage       |
            | TensorRT              | *.engine          |  # | TensorRT              | *.engine          |
            | TensorFlow SavedModel | *_saved_model/    |  # | TensorFlow SavedModel | *_saved_model/    |
            | TensorFlow GraphDef   | *.pb              |  # | TensorFlow GraphDef   | *.pb              |
            | TensorFlow Lite       | *.tflite          |  # | TensorFlow Lite       | *.tflite          |
            | TensorFlow Edge TPU   | *_edgetpu.tflite  |  # | TensorFlow Edge TPU   | *_edgetpu.tflite  |
            | PaddlePaddle          | *_paddle_model/   |  # | PaddlePaddle          | *_paddle_model/   |
            | MNN                   | *.mnn             |  # | MNN                   | *.mnn             |
            | NCNN                  | *_ncnn_model/     |  # | NCNN                  | *_ncnn_model/     |
            | IMX                   | *_imx_model/      |  # | IMX                   | *_imx_model/      |
            | RKNN                  | *_rknn_model/     |  # | RKNN                  | *_rknn_model/     |

    This class offers dynamic backend switching capabilities based on the input model format, making it easier to deploy  # Ê≠§Á±ªÊèê‰æõÂü∫‰∫éËæìÂÖ•Ê®°ÂûãÊ†ºÂºèÁöÑÂä®ÊÄÅÂêéÁ´ØÂàáÊç¢ÂäüËÉΩÔºå‰ΩøÂæóÂú®‰∏çÂêåÂπ≥Âè∞‰∏äÈÉ®ÁΩ≤
    models across various platforms.  # Ê®°ÂûãÂèòÂæóÊõ¥Âä†ÂÆπÊòì„ÄÇ
    """

    @torch.no_grad()  # ‰∏çËÆ°ÁÆóÊ¢ØÂ∫¶
    def __init__(  # ÊûÑÈÄ†ÂáΩÊï∞
        self,
        weights="yolo11n.pt",  # ÊùÉÈáçÊñá‰ª∂Ë∑ØÂæÑÔºåÈªòËÆ§‰∏∫'yolo11n.pt'
        device=torch.device("cpu"),  # ËøêË°åÊ®°ÂûãÁöÑËÆæÂ§áÔºåÈªòËÆ§‰∏∫CPU
        dnn=False,  # ÊòØÂê¶‰ΩøÁî®OpenCV DNNÊ®°ÂùóËøõË°åONNXÊé®ÁêÜÔºåÈªòËÆ§‰∏∫False
        data=None,  # È¢ùÂ§ñÁöÑÊï∞ÊçÆ.yamlÊñá‰ª∂Ë∑ØÂæÑÔºåÂåÖÂê´Á±ªÂêçÔºåÂèØÈÄâ
        fp16=False,  # ÊòØÂê¶ÂêØÁî®ÂçäÁ≤æÂ∫¶Êé®ÁêÜÔºå‰ªÖÂú®ÁâπÂÆöÂêéÁ´ØÊîØÊåÅÔºåÈªòËÆ§‰∏∫False
        batch=1,  # ÂÅáËÆæÁöÑÊé®ÁêÜÊâπÊ¨°Â§ßÂ∞è
        fuse=True,  # ÊòØÂê¶ËûçÂêàConv2D + BatchNormÂ±Ç‰ª•‰ºòÂåñÔºåÈªòËÆ§‰∏∫True
        verbose=True,  # ÊòØÂê¶ÂêØÁî®ËØ¶ÁªÜÊó•ÂøóÔºåÈªòËÆ§‰∏∫True
    ):
        """
        Initialize the AutoBackend for inference.  # ÂàùÂßãÂåñAutoBackendËøõË°åÊé®ÁêÜ„ÄÇ

        Args:  # ÂèÇÊï∞Ôºö
            weights (str | torch.nn.Module): Path to the model weights file or a module instance. Defaults to 'yolo11n.pt'.  # ÊùÉÈáçÔºàstr | torch.nn.ModuleÔºâÔºöÊ®°ÂûãÊùÉÈáçÊñá‰ª∂ÁöÑË∑ØÂæÑÊàñÊ®°ÂùóÂÆû‰æã„ÄÇÈªòËÆ§‰∏∫'yolo11n.pt'„ÄÇ
            device (torch.device): Device to run the model on. Defaults to CPU.  # ËÆæÂ§áÔºàtorch.deviceÔºâÔºöËøêË°åÊ®°ÂûãÁöÑËÆæÂ§á„ÄÇÈªòËÆ§‰∏∫CPU„ÄÇ
            dnn (bool): Use OpenCV DNN module for ONNX inference. Defaults to False.  # dnnÔºàboolÔºâÔºö‰ΩøÁî®OpenCV DNNÊ®°ÂùóËøõË°åONNXÊé®ÁêÜ„ÄÇÈªòËÆ§‰∏∫False„ÄÇ
            data (str | Path | optional): Path to the additional data.yaml file containing class names. Optional.  # dataÔºàstr | Path | ÂèØÈÄâÔºâÔºöÂåÖÂê´Á±ªÂêçÁöÑÈ¢ùÂ§ñdata.yamlÊñá‰ª∂ÁöÑË∑ØÂæÑ„ÄÇÂèØÈÄâ„ÄÇ
            fp16 (bool): Enable half-precision inference. Supported only on specific backends. Defaults to False.  # fp16ÔºàboolÔºâÔºöÂêØÁî®ÂçäÁ≤æÂ∫¶Êé®ÁêÜ„ÄÇ‰ªÖÂú®ÁâπÂÆöÂêéÁ´ØÊîØÊåÅ„ÄÇÈªòËÆ§‰∏∫False„ÄÇ
            batch (int): Batch-size to assume for inference.  # batchÔºàintÔºâÔºöÂÅáËÆæÁöÑÊé®ÁêÜÊâπÊ¨°Â§ßÂ∞è„ÄÇ
            fuse (bool): Fuse Conv2D + BatchNorm layers for optimization. Defaults to True.  # fuseÔºàboolÔºâÔºöËûçÂêàConv2D + BatchNormÂ±Ç‰ª•‰ºòÂåñ„ÄÇÈªòËÆ§‰∏∫True„ÄÇ
            verbose (bool): Enable verbose logging. Defaults to True.  # verboseÔºàboolÔºâÔºöÂêØÁî®ËØ¶ÁªÜÊó•Âøó„ÄÇÈªòËÆ§‰∏∫True„ÄÇ
        """
        super().__init__()  # Ë∞ÉÁî®Áà∂Á±ªÊûÑÈÄ†ÂáΩÊï∞
        w = str(weights[0] if isinstance(weights, list) else weights)  # Â¶ÇÊûúweightsÊòØÂàóË°®ÔºåÂèñÁ¨¨‰∏Ä‰∏™ÂÖÉÁ¥†ÔºåÂê¶ÂàôÁõ¥Êé•‰ΩøÁî®weights
        nn_module = isinstance(weights, torch.nn.Module)  # Ê£ÄÊü•weightsÊòØÂê¶‰∏∫torch.nn.ModuleÁ±ªÂûã
        (
            pt,  # PyTorchÊ®°Âûã
            jit,  # TorchScriptÊ®°Âûã
            onnx,  # ONNXÊ®°Âûã
            xml,  # OpenVINOÊ®°Âûã
            engine,  # TensorRTÊ®°Âûã
            coreml,  # CoreMLÊ®°Âûã
            saved_model,  # TensorFlow SavedModelÊ®°Âûã
            pb,  # TensorFlow GraphDefÊ®°Âûã
            tflite,  # TensorFlow LiteÊ®°Âûã
            edgetpu,  # TensorFlow Edge TPUÊ®°Âûã
            tfjs,  # TensorFlow.jsÊ®°Âûã
            paddle,  # PaddlePaddleÊ®°Âûã
            mnn,  # MNNÊ®°Âûã
            ncnn,  # NCNNÊ®°Âûã
            imx,  # IMXÊ®°Âûã
            rknn,  # RKNNÊ®°Âûã
            triton,  # NVIDIA TritonÊ®°Âûã
        ) = self._model_type(w)  # Á°ÆÂÆöÊ®°ÂûãÁ±ªÂûã
        fp16 &= pt or jit or onnx or xml or engine or nn_module or triton  # FP16
        nhwc = coreml or saved_model or pb or tflite or edgetpu or rknn  # BHWCÊ†ºÂºèÔºà‰∏étorch BCWHÁõ∏ÂØπÔºâ
        stride = 32  # default stride  # ÈªòËÆ§Ê≠•ÂπÖ
        end2end = False  # default end2end  # ÈªòËÆ§end2end
        model, metadata, task = None, None, None  # ÂàùÂßãÂåñÊ®°Âûã„ÄÅÂÖÉÊï∞ÊçÆÂíå‰ªªÂä°

        # Set device  # ËÆæÁΩÆËÆæÂ§á
        cuda = torch.cuda.is_available() and device.type != "cpu"  # use CUDA  # ‰ΩøÁî®CUDA
        if cuda and not any([nn_module, pt, jit, engine, onnx, paddle]):  # GPU dataloader formats  # GPUÊï∞ÊçÆÂä†ËΩΩÂô®Ê†ºÂºè
            device = torch.device("cpu")  # Â¶ÇÊûú‰∏çÊîØÊåÅCUDAÔºåÂàô‰ΩøÁî®CPU
            cuda = False  # ËÆæÁΩÆcuda‰∏∫False

        # Download if not local  # Â¶ÇÊûú‰∏çÊòØÊú¨Âú∞Êñá‰ª∂ÔºåÂàô‰∏ãËΩΩ
        if not (pt or triton or nn_module):  # Â¶ÇÊûú‰∏çÊòØPyTorch„ÄÅTritonÊàñnnÊ®°Âùó
            w = attempt_download_asset(w)  # Â∞ùËØï‰∏ãËΩΩËµÑ‰∫ß

        # In-memory PyTorch model  # ÂÜÖÂ≠ò‰∏≠ÁöÑPyTorchÊ®°Âûã
        if nn_module:  # Â¶ÇÊûúweightsÊòØnnÊ®°Âùó
            model = weights.to(device)  # Â∞ÜÊ®°ÂûãÁßªÂä®Âà∞ÊåáÂÆöËÆæÂ§á
            if fuse:  # Â¶ÇÊûúÈúÄË¶ÅËûçÂêà
                model = model.fuse(verbose=verbose)  # ËûçÂêàÊ®°Âûã
            if hasattr(model, "kpt_shape"):  # Â¶ÇÊûúÊ®°ÂûãÊúâÂÖ≥ÈîÆÁÇπÂΩ¢Áä∂Â±ûÊÄß
                kpt_shape = model.kpt_shape  # ‰ªÖÁî®‰∫éÂßøÊÄÅÊ£ÄÊµã
            stride = max(int(model.stride.max()), 32)  # Ëé∑ÂèñÊ®°ÂûãÊ≠•ÂπÖ
            names = model.module.names if hasattr(model, "module") else model.names  # Ëé∑ÂèñÁ±ªÂêç
            model.half() if fp16 else model.float()  # Ê†πÊçÆfp16ËÆæÁΩÆÊ®°Âûã‰∏∫ÂçäÁ≤æÂ∫¶ÊàñÂçïÁ≤æÂ∫¶
            self.model = model  # ÊòæÂºèËµãÂÄºÁªôself.modelÔºå‰ª•‰æøÂêéÁª≠Ë∞ÉÁî®to()„ÄÅcpu()„ÄÅcuda()„ÄÅhalf()
            pt = True  # ËÆæÁΩÆpt‰∏∫True

        # PyTorch  # PyTorchÊ®°Âûã
        elif pt:  # Â¶ÇÊûúÊòØPyTorchÊ®°Âûã
            from ultralytics.nn.tasks import attempt_load_weights  # ‰ªéUltralyticsÂä†ËΩΩÊùÉÈáç

            model = attempt_load_weights(  # Â∞ùËØïÂä†ËΩΩÊùÉÈáç
                weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse
            )
            if hasattr(model, "kpt_shape"):  # Â¶ÇÊûúÊ®°ÂûãÊúâÂÖ≥ÈîÆÁÇπÂΩ¢Áä∂Â±ûÊÄß
                kpt_shape = model.kpt_shape  # ‰ªÖÁî®‰∫éÂßøÊÄÅÊ£ÄÊµã
            stride = max(int(model.stride.max()), 32)  # Ëé∑ÂèñÊ®°ÂûãÊ≠•ÂπÖ
            names = model.module.names if hasattr(model, "module") else model.names  # Ëé∑ÂèñÁ±ªÂêç
            model.half() if fp16 else model.float()  # Ê†πÊçÆfp16ËÆæÁΩÆÊ®°Âûã‰∏∫ÂçäÁ≤æÂ∫¶ÊàñÂçïÁ≤æÂ∫¶
            self.model = model  # ÊòæÂºèËµãÂÄºÁªôself.modelÔºå‰ª•‰æøÂêéÁª≠Ë∞ÉÁî®to()„ÄÅcpu()„ÄÅcuda()„ÄÅhalf()

        # TorchScript  # TorchScriptÊ®°Âûã
        elif jit:  # Â¶ÇÊûúÊòØTorchScriptÊ®°Âûã
            LOGGER.info(f"Loading {w} for TorchScript inference...")  # ËÆ∞ÂΩïÂä†ËΩΩ‰ø°ÊÅØ
            extra_files = {"config.txt": ""}  # model metadata  # Ê®°ÂûãÂÖÉÊï∞ÊçÆ
            model = torch.jit.load(w, _extra_files=extra_files, map_location=device)  # Âä†ËΩΩTorchScriptÊ®°Âûã

        # ONNX OpenCV DNN  # ONNX OpenCV DNNÊ®°Âûã
        elif dnn:  # Â¶ÇÊûúÊòØDNNÊ®°Âûã
            LOGGER.info(f"Loading {w} for ONNX OpenCV DNN inference...")  # ËÆ∞ÂΩïÂä†ËΩΩ‰ø°ÊÅØ
            check_requirements("opencv-python>=4.5.4")  # Ê£ÄÊü•OpenCVË¶ÅÊ±Ç
            net = cv2.dnn.readNetFromONNX(w)  # ‰ªéONNXÂä†ËΩΩDNNÁΩëÁªú

        # ONNX Runtime and IMX  # ONNX RuntimeÂíåIMXÊ®°Âûã
        elif onnx or imx:  # Â¶ÇÊûúÊòØONNXÊàñIMXÊ®°Âûã
            LOGGER.info(f"Loading {w} for ONNX Runtime inference...")  # ËÆ∞ÂΩïÂä†ËΩΩ‰ø°ÊÅØ
            check_requirements(("onnx", "onnxruntime-gpu" if cuda else "onnxruntime"))  # Ê£ÄÊü•ONNXË¶ÅÊ±Ç
            if IS_RASPBERRYPI or IS_JETSON:  # Â¶ÇÊûúÊòØÊ†ëËéìÊ¥æÊàñJetson
                # Fix 'numpy.linalg._umath_linalg' has no attribute '_ilp64' for TF SavedModel on RPi and Jetson  # ‰øÆÂ§çÊ†ëËéìÊ¥æÂíåJetson‰∏äTF SavedModelÁöÑÈîôËØØ
                check_requirements("numpy==1.23.5")  # Ê£ÄÊü•numpyÁâàÊú¨
            import onnxruntime  # ÂØºÂÖ•ONNX Runtime

            providers = ["CPUExecutionProvider"]  # ËÆæÁΩÆÊâßË°åÊèê‰æõËÄÖ‰∏∫CPU
            if cuda and "CUDAExecutionProvider" in onnxruntime.get_available_providers():  # Â¶ÇÊûúÊîØÊåÅCUDA
                providers.insert(0, "CUDAExecutionProvider")  # Â∞ÜCUDAÊèê‰æõËÄÖÊèíÂÖ•Âà∞ÂàóË°®‰∏≠
            elif cuda:  # Â¶ÇÊûúËØ∑Ê±Ç‰∫ÜCUDA‰ΩÜ‰∏çÂèØÁî®
                LOGGER.warning("WARNING ‚ö†Ô∏è Failed to start ONNX Runtime with CUDA. Using CPU...")  # ËÆ∞ÂΩïË≠¶Âëä‰ø°ÊÅØ
                device = torch.device("cpu")  # ‰ΩøÁî®CPU
                cuda = False  # ËÆæÁΩÆcuda‰∏∫False
            LOGGER.info(f"Using ONNX Runtime {providers[0]}")  # ËÆ∞ÂΩï‰ΩøÁî®ÁöÑONNX RuntimeÊèê‰æõËÄÖ
            if onnx:  # Â¶ÇÊûúÊòØONNXÊ®°Âûã
                session = onnxruntime.InferenceSession(w, providers=providers)  # ÂàõÂª∫ONNXÊé®ÁêÜ‰ºöËØù
            else:  # Â¶ÇÊûúÊòØIMXÊ®°Âûã
                check_requirements(  # Ê£ÄÊü•IMXË¶ÅÊ±Ç
                    ["model-compression-toolkit==2.1.1", "sony-custom-layers[torch]==0.2.0", "onnxruntime-extensions"]
                )
                w = next(Path(w).glob("*.onnx"))  # Ëé∑ÂèñIMXÊ®°ÂûãÁöÑONNXÊñá‰ª∂
                LOGGER.info(f"Loading {w} for ONNX IMX inference...")  # ËÆ∞ÂΩïÂä†ËΩΩ‰ø°ÊÅØ
                import mct_quantizers as mctq  # ÂØºÂÖ•ÈáèÂåñÂ∑•ÂÖ∑
                from sony_custom_layers.pytorch.object_detection import nms_ort  # noqa  # ÂØºÂÖ•Ëá™ÂÆö‰πâÂ±Ç

                session = onnxruntime.InferenceSession(  # ÂàõÂª∫ONNXÊé®ÁêÜ‰ºöËØù
                    w, mctq.get_ort_session_options(), providers=["CPUExecutionProvider"]
                )
                task = "detect"  # ËÆæÁΩÆ‰ªªÂä°‰∏∫Ê£ÄÊµã

            output_names = [x.name for x in session.get_outputs()]  # Ëé∑ÂèñËæìÂá∫ÂêçÁß∞
            metadata = session.get_modelmeta().custom_metadata_map  # Ëé∑ÂèñÊ®°ÂûãÂÖÉÊï∞ÊçÆ
            dynamic = isinstance(session.get_outputs()[0].shape[0], str)  # Ê£ÄÊü•ËæìÂá∫ÂΩ¢Áä∂ÊòØÂê¶Âä®ÊÄÅ
            fp16 = True if "float16" in session.get_inputs()[0].type else False  # Ê£ÄÊü•ËæìÂÖ•Á±ªÂûãÊòØÂê¶‰∏∫float16
            if not dynamic:  # Â¶ÇÊûú‰∏çÊòØÂä®ÊÄÅ
                io = session.io_binding()  # Ëé∑ÂèñIOÁªëÂÆö
                bindings = []  # ÂàùÂßãÂåñÁªëÂÆöÂàóË°®
                for output in session.get_outputs():  # ÈÅçÂéÜËæìÂá∫
                    out_fp16 = "float16" in output.type  # Ê£ÄÊü•ËæìÂá∫Á±ªÂûãÊòØÂê¶‰∏∫float16
                    y_tensor = torch.empty(output.shape, dtype=torch.float16 if out_fp16 else torch.float32).to(device)  # ÂàõÂª∫ËæìÂá∫Âº†Èáè
                    io.bind_output(  # ÁªëÂÆöËæìÂá∫
                        name=output.name,
                        device_type=device.type,
                        device_id=device.index if cuda else 0,
                        element_type=np.float16 if out_fp16 else np.float32,
                        shape=tuple(y_tensor.shape),
                        buffer_ptr=y_tensor.data_ptr(),
                    )
                    bindings.append(y_tensor)  # Ê∑ªÂä†Âà∞ÁªëÂÆöÂàóË°®

        # OpenVINO  # OpenVINOÊ®°Âûã
        elif xml:  # Â¶ÇÊûúÊòØOpenVINOÊ®°Âûã
            LOGGER.info(f"Loading {w} for OpenVINO inference...")  # ËÆ∞ÂΩïÂä†ËΩΩ‰ø°ÊÅØ
            check_requirements("openvino>=2024.0.0")  # Ê£ÄÊü•OpenVINOË¶ÅÊ±Ç
            import openvino as ov  # ÂØºÂÖ•OpenVINO

            core = ov.Core()  # ÂàõÂª∫OpenVINOÊ†∏ÂøÉ
            w = Path(w)  # Â∞ÜÊùÉÈáçË∑ØÂæÑËΩ¨Êç¢‰∏∫PathÂØπË±°
            if not w.is_file():  # Â¶ÇÊûú‰∏çÊòØ*.xmlÊñá‰ª∂
                w = next(w.glob("*.xml"))  # Ëé∑Âèñ*_openvino_modelÁõÆÂΩï‰∏≠ÁöÑ*.xmlÊñá‰ª∂
            ov_model = core.read_model(model=str(w), weights=w.with_suffix(".bin"))  # ËØªÂèñOpenVINOÊ®°Âûã
            if ov_model.get_parameters()[0].get_layout().empty:  # Â¶ÇÊûúÂ∏ÉÂ±Ä‰∏∫Á©∫
                ov_model.get_parameters()[0].set_layout(ov.Layout("NCHW"))  # ËÆæÁΩÆÂ∏ÉÂ±Ä‰∏∫NCHW

            # OpenVINO inference modes are 'LATENCY', 'THROUGHPUT' (not recommended), or 'CUMULATIVE_THROUGHPUT'  # OpenVINOÊé®ÁêÜÊ®°Âºè‰∏∫'LATENCY'„ÄÅ'THROUGHPUT'Ôºà‰∏çÊé®ËçêÔºâÊàñ'CUMULATIVE_THROUGHPUT'
            inference_mode = "CUMULATIVE_THROUGHPUT" if batch > 1 else "LATENCY"  # ËÆæÁΩÆÊé®ÁêÜÊ®°Âºè
            LOGGER.info(f"Using OpenVINO {inference_mode} mode for batch={batch} inference...")  # ËÆ∞ÂΩï‰ΩøÁî®ÁöÑÊé®ÁêÜÊ®°Âºè
            ov_compiled_model = core.compile_model(  # ÁºñËØëOpenVINOÊ®°Âûã
                ov_model,
                device_name="AUTO",  # AUTOÈÄâÊã©ÊúÄ‰Ω≥ÂèØÁî®ËÆæÂ§áÔºåËØ∑Âãø‰øÆÊîπ
                config={"PERFORMANCE_HINT": inference_mode},  # ËÆæÁΩÆÊÄßËÉΩÊèêÁ§∫
            )
            input_name = ov_compiled_model.input().get_any_name()  # Ëé∑ÂèñËæìÂÖ•ÂêçÁß∞
            metadata = w.parent / "metadata.yaml"  # ËÆæÁΩÆÂÖÉÊï∞ÊçÆË∑ØÂæÑ

        # TensorRT  # TensorRTÊ®°Âûã
        elif engine:  # Â¶ÇÊûúÊòØTensorRTÊ®°Âûã
            LOGGER.info(f"Loading {w} for TensorRT inference...")  # ËÆ∞ÂΩïÂä†ËΩΩ‰ø°ÊÅØ

            if IS_JETSON and PYTHON_VERSION <= "3.8.0":  # Â¶ÇÊûúÊòØJetsonÂπ∂‰∏îPythonÁâàÊú¨Â∞è‰∫éÁ≠â‰∫é3.8.0
                # fix error: `np.bool` was a deprecated alias for the builtin `bool` for JetPack 4 with Python <= 3.8.0  # ‰øÆÂ§çÈîôËØØ
                check_requirements("numpy==1.23.5")  # Ê£ÄÊü•numpyÁâàÊú¨

            try:
                import tensorrt as trt  # noqa https://developer.nvidia.com/nvidia-tensorrt-download  # ÂØºÂÖ•TensorRT
            except ImportError:  # Â¶ÇÊûúÂØºÂÖ•Â§±Ë¥•
                if LINUX:  # Â¶ÇÊûúÊòØLinux
                    check_requirements("tensorrt>7.0.0,!=10.1.0")  # Ê£ÄÊü•TensorRTË¶ÅÊ±Ç
                import tensorrt as trt  # noqa  # ÈáçÊñ∞ÂØºÂÖ•TensorRT
            check_version(trt.__version__, ">=7.0.0", hard=True)  # Ê£ÄÊü•TensorRTÁâàÊú¨
            check_version(trt.__version__, "!=10.1.0", msg="https://github.com/ultralytics/ultralytics/pull/14239")  # Ê£ÄÊü•TensorRTÁâàÊú¨
            if device.type == "cpu":  # Â¶ÇÊûúËÆæÂ§áÊòØCPU
                device = torch.device("cuda:0")  # ÂàáÊç¢Âà∞CUDAËÆæÂ§á
            Binding = namedtuple("Binding", ("name", "dtype", "shape", "data", "ptr"))  # ÂÆö‰πâÁªëÂÆöÂÖÉÁªÑ
            logger = trt.Logger(trt.Logger.INFO)  # ÂàõÂª∫TensorRTÊó•ÂøóËÆ∞ÂΩïÂô®
            # Read file  # ËØªÂèñÊñá‰ª∂
            with open(w, "rb") as f, trt.Runtime(logger) as runtime:  # ÊâìÂºÄÊùÉÈáçÊñá‰ª∂
                try:
                    meta_len = int.from_bytes(f.read(4), byteorder="little")  # ËØªÂèñÂÖÉÊï∞ÊçÆÈïøÂ∫¶
                    metadata = json.loads(f.read(meta_len).decode("utf-8"))  # ËØªÂèñÂÖÉÊï∞ÊçÆ
                except UnicodeDecodeError:  # Â¶ÇÊûúËß£Á†ÅÈîôËØØ
                    f.seek(0)  # Â¶ÇÊûúÂºïÊìéÊñá‰ª∂Áº∫Â∞ëÂµåÂÖ•ÁöÑUltralyticsÂÖÉÊï∞ÊçÆ
                dla = metadata.get("dla", None)  # Ëé∑ÂèñDLA‰ø°ÊÅØ
                if dla is not None:  # Â¶ÇÊûúDLA‰ø°ÊÅØÂ≠òÂú®
                    runtime.DLA_core = int(dla)  # ËÆæÁΩÆDLAÊ†∏ÂøÉ
                model = runtime.deserialize_cuda_engine(f.read())  # ÂèçÂ∫èÂàóÂåñCUDAÂºïÊìé

            # Model context  # Ê®°Âûã‰∏ä‰∏ãÊñá
            try:
                context = model.create_execution_context()  # ÂàõÂª∫ÊâßË°å‰∏ä‰∏ãÊñá
            except Exception as e:  # Â¶ÇÊûúÊ®°Âûã‰∏∫None
                LOGGER.error(f"ERROR: TensorRT model exported with a different version than {trt.__version__}\n")  # ËÆ∞ÂΩïÈîôËØØ‰ø°ÊÅØ
                raise e  # ÂºïÂèëÂºÇÂ∏∏

            bindings = OrderedDict()  # ÂàùÂßãÂåñÁªëÂÆöÂ≠óÂÖ∏
            output_names = []  # ÂàùÂßãÂåñËæìÂá∫ÂêçÁß∞ÂàóË°®
            fp16 = False  # default updated below  # ÈªòËÆ§Êõ¥Êñ∞‰∏∫False
            dynamic = False  # ÈªòËÆ§Âä®ÊÄÅ‰∏∫False
            is_trt10 = not hasattr(model, "num_bindings")  # Ê£ÄÊü•TensorRTÁâàÊú¨
            num = range(model.num_io_tensors) if is_trt10 else range(model.num_bindings)  # Ëé∑ÂèñÁªëÂÆöÊï∞Èáè
            for i in num:  # ÈÅçÂéÜÁªëÂÆö
                if is_trt10:  # Â¶ÇÊûúÊòØTensorRT 10
                    name = model.get_tensor_name(i)  # Ëé∑ÂèñÂº†ÈáèÂêçÁß∞
                    dtype = trt.nptype(model.get_tensor_dtype(name))  # Ëé∑ÂèñÂº†ÈáèÊï∞ÊçÆÁ±ªÂûã
                    is_input = model.get_tensor_mode(name) == trt.TensorIOMode.INPUT  # Ê£ÄÊü•ÊòØÂê¶‰∏∫ËæìÂÖ•
                    if is_input:  # Â¶ÇÊûúÊòØËæìÂÖ•
                        if -1 in tuple(model.get_tensor_shape(name)):  # Âä®ÊÄÅÁªëÂÆö
                            dynamic = True  # ËÆæÁΩÆÂä®ÊÄÅ‰∏∫True
                            context.set_input_shape(name, tuple(model.get_tensor_profile_shape(name, 0)[1]))  # ËÆæÁΩÆËæìÂÖ•ÂΩ¢Áä∂
                        if dtype == np.float16:  # Â¶ÇÊûúÊï∞ÊçÆÁ±ªÂûã‰∏∫float16
                            fp16 = True  # ËÆæÁΩÆfp16‰∏∫True
                    else:
                        output_names.append(name)  # Ê∑ªÂä†Âà∞ËæìÂá∫ÂêçÁß∞ÂàóË°®
                    shape = tuple(context.get_tensor_shape(name))  # Ëé∑ÂèñÂº†ÈáèÂΩ¢Áä∂
                else:  # TensorRT < 10.0
                    name = model.get_binding_name(i)  # Ëé∑ÂèñÁªëÂÆöÂêçÁß∞
                    dtype = trt.nptype(model.get_binding_dtype(i))  # Ëé∑ÂèñÁªëÂÆöÊï∞ÊçÆÁ±ªÂûã
                    is_input = model.binding_is_input(i)  # Ê£ÄÊü•ÊòØÂê¶‰∏∫ËæìÂÖ•
                    if model.binding_is_input(i):  # Â¶ÇÊûúÊòØËæìÂÖ•
                        if -1 in tuple(model.get_binding_shape(i)):  # Âä®ÊÄÅÁªëÂÆö
                            dynamic = True  # ËÆæÁΩÆÂä®ÊÄÅ‰∏∫True
                            context.set_binding_shape(i, tuple(model.get_profile_shape(0, i)[1]))  # ËÆæÁΩÆÁªëÂÆöÂΩ¢Áä∂
                        if dtype == np.float16:  # Â¶ÇÊûúÊï∞ÊçÆÁ±ªÂûã‰∏∫float16
                            fp16 = True  # ËÆæÁΩÆfp16‰∏∫True
                    else:
                        output_names.append(name)  # Ê∑ªÂä†Âà∞ËæìÂá∫ÂêçÁß∞ÂàóË°®
                    shape = tuple(context.get_binding_shape(i))  # Ëé∑ÂèñÁªëÂÆöÂΩ¢Áä∂
                im = torch.from_numpy(np.empty(shape, dtype=dtype)).to(device)  # ÂàõÂª∫ËæìÂÖ•Âº†Èáè
                bindings[name] = Binding(name, dtype, shape, im, int(im.data_ptr()))  # Ê∑ªÂä†Âà∞ÁªëÂÆöÂ≠óÂÖ∏
            binding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())  # Ëé∑ÂèñÁªëÂÆöÂú∞ÂùÄÂ≠óÂÖ∏
            batch_size = bindings["images"].shape[0]  # Â¶ÇÊûúÊòØÂä®ÊÄÅÔºåÂàôËøôÊòØÊâπÂ§ßÂ∞è

        # RKNN
        elif rknn:  # Â¶ÇÊûúÊòØ RKNN Ê†ºÂºè
            if not is_rockchip():  # Â¶ÇÊûú‰∏çÊòØ Rockchip ËÆæÂ§á
                raise OSError("RKNN inference is only supported on Rockchip devices.")  # ÊäõÂá∫ OSErrorÔºåÊèêÁ§∫ RKNN Êé®ÁêÜ‰ªÖÊîØÊåÅ Rockchip ËÆæÂ§á
            LOGGER.info(f"Loading {w} for RKNN inference...")  # ËÆ∞ÂΩïÂä†ËΩΩ RKNN Êé®ÁêÜÊ®°ÂûãÁöÑ‰ø°ÊÅØ
            check_requirements("rknn-toolkit-lite2")  # Ê£ÄÊü• RKNN Â∑•ÂÖ∑ÂåÖÁöÑË¶ÅÊ±Ç
            from rknnlite.api import RKNNLite  # ‰ªé rknnlite ÂØºÂÖ• RKNNLite Á±ª

            w = Path(w)  # Â∞ÜÊùÉÈáçË∑ØÂæÑËΩ¨Êç¢‰∏∫ Path ÂØπË±°
            if not w.is_file():  # if not *.rknn  # Â¶ÇÊûú‰∏çÊòØ *.rknn Êñá‰ª∂
                w = next(w.rglob("*.rknn"))  # get *.rknn file from *_rknn_model dir  # ‰ªé *_rknn_model ÁõÆÂΩï‰∏≠Ëé∑Âèñ *.rknn Êñá‰ª∂
            rknn_model = RKNNLite()  # ÂàõÂª∫ RKNNLite ÂÆû‰æã
            rknn_model.load_rknn(w)  # Âä†ËΩΩ RKNN Ê®°Âûã
            rknn_model.init_runtime()  # ÂàùÂßãÂåñËøêË°åÊó∂
            metadata = Path(w).parent / "metadata.yaml"  # ËÆæÁΩÆÂÖÉÊï∞ÊçÆË∑ØÂæÑ

        # Any other format (unsupported)  # ÂÖ∂‰ªñÊ†ºÂºèÔºà‰∏çÊîØÊåÅÔºâ
        else:  # Â¶ÇÊûú‰∏çÊòØÂ∑≤Áü•Ê†ºÂºè
            from ultralytics.engine.exporter import export_formats  # ‰ªé ultralytics ÂØºÂÖ• export_formats

            raise TypeError(  # ÊäõÂá∫ TypeError
                f"model='{w}' is not a supported model format. Ultralytics supports: {export_formats()['Format']}\n"  # "{w} ‰∏çÊòØÊîØÊåÅÁöÑÊ®°ÂûãÊ†ºÂºè„ÄÇUltralytics ÊîØÊåÅÁöÑÊ†ºÂºè‰∏∫Ôºö"
                f"See https://docs.ultralytics.com/modes/predict for help."  # "ËØ∑ÂèÇÈòÖ https://docs.ultralytics.com/modes/predict Ëé∑ÂèñÂ∏ÆÂä©„ÄÇ"
            )

        # Load external metadata YAML  # Âä†ËΩΩÂ§ñÈÉ®ÂÖÉÊï∞ÊçÆ YAML
        if isinstance(metadata, (str, Path)) and Path(metadata).exists():  # Ê£ÄÊü• metadata ÊòØÂê¶‰∏∫Â≠óÁ¨¶‰∏≤Êàñ Path ‰∏îÂ≠òÂú®
            metadata = yaml_load(metadata)  # Âä†ËΩΩÂÖÉÊï∞ÊçÆ
        if metadata and isinstance(metadata, dict):  # Â¶ÇÊûú metadata Â≠òÂú®‰∏î‰∏∫Â≠óÂÖ∏
            for k, v in metadata.items():  # ÈÅçÂéÜÂÖÉÊï∞ÊçÆÈ°π
                if k in {"stride", "batch"}:  # Â¶ÇÊûúÈîÆÊòØ "stride" Êàñ "batch"
                    metadata[k] = int(v)  # Â∞ÜÂÄºËΩ¨Êç¢‰∏∫Êï¥Êï∞
                elif k in {"imgsz", "names", "kpt_shape", "args"} and isinstance(v, str):  # Â¶ÇÊûúÈîÆÊòØ "imgsz"„ÄÅ"names"„ÄÅ"kpt_shape" Êàñ "args" ‰∏îÂÄº‰∏∫Â≠óÁ¨¶‰∏≤
                    metadata[k] = eval(v)  # ËØÑ‰º∞Â≠óÁ¨¶‰∏≤Âπ∂Êõ¥Êñ∞ÂÄº
            stride = metadata["stride"]  # Ëé∑ÂèñÊ≠•ÂπÖ
            task = metadata["task"]  # Ëé∑Âèñ‰ªªÂä°
            batch = metadata["batch"]  # Ëé∑ÂèñÊâπÊ¨°Â§ßÂ∞è
            imgsz = metadata["imgsz"]  # Ëé∑ÂèñÂõæÂÉèÂ§ßÂ∞è
            names = metadata["names"]  # Ëé∑ÂèñÁ±ªÂêç
            kpt_shape = metadata.get("kpt_shape")  # Ëé∑ÂèñÂÖ≥ÈîÆÁÇπÂΩ¢Áä∂
            end2end = metadata.get("args", {}).get("nms", False)  # Ëé∑ÂèñÊòØÂê¶ÂêØÁî® NMS ÁöÑÂèÇÊï∞
        elif not (pt or triton or nn_module):  # Â¶ÇÊûú‰∏çÊòØ PyTorch„ÄÅTriton Êàñ nn_module
            LOGGER.warning(f"WARNING ‚ö†Ô∏è Metadata not found for 'model={weights}'")  # ËÆ∞ÂΩïË≠¶Âëä‰ø°ÊÅØÔºåÊèêÁ§∫Êú™ÊâæÂà∞Ê®°ÂûãÁöÑÂÖÉÊï∞ÊçÆ

        # Check names  # Ê£ÄÊü•Á±ªÂêç
        if "names" not in locals():  # names missing  # Â¶ÇÊûúÁ±ªÂêçÊú™ÂÆö‰πâ
            names = default_class_names(data)  # ‰ΩøÁî®ÈªòËÆ§Á±ªÂêç
        names = check_class_names(names)  # Ê£ÄÊü•Á±ªÂêç

        # Disable gradients  # Á¶ÅÁî®Ê¢ØÂ∫¶
        if pt:  # Â¶ÇÊûúÊòØ PyTorch Ê®°Âûã
            for p in model.parameters():  # ÈÅçÂéÜÊ®°ÂûãÂèÇÊï∞
                p.requires_grad = False  # Á¶ÅÁî®Ê¢ØÂ∫¶ËÆ°ÁÆó

        self.__dict__.update(locals())  # Â∞ÜÊâÄÊúâÂ±ÄÈÉ®ÂèòÈáèËµãÂÄºÁªôÂÆû‰æãÂ≠óÂÖ∏

    def forward(self, im, augment=False, visualize=False, embed=None):  # ÂÆö‰πâÂâçÂêëÊé®ÁêÜÊñπÊ≥ï
        """
        Runs inference on the YOLOv8 MultiBackend model.  # Âú® YOLOv8 MultiBackend Ê®°Âûã‰∏äËøêË°åÊé®ÁêÜ

        Args:  # ÂèÇÊï∞Ôºö
            im (torch.Tensor): The image tensor to perform inference on.  # imÔºàtorch.TensorÔºâÔºöÁî®‰∫éÊé®ÁêÜÁöÑÂõæÂÉèÂº†Èáè„ÄÇ
            augment (bool): whether to perform data augmentation during inference, defaults to False  # augmentÔºàboolÔºâÔºöÊòØÂê¶Âú®Êé®ÁêÜÊúüÈó¥ÊâßË°åÊï∞ÊçÆÂ¢ûÂº∫ÔºåÈªòËÆ§‰∏∫ False
            visualize (bool): whether to visualize the output predictions, defaults to False  # visualizeÔºàboolÔºâÔºöÊòØÂê¶ÂèØËßÜÂåñËæìÂá∫È¢ÑÊµãÔºåÈªòËÆ§‰∏∫ False
            embed (list, optional): A list of feature vectors/embeddings to return.  # embedÔºàÂàóË°®ÔºåÂèØÈÄâÔºâÔºöË¶ÅËøîÂõûÁöÑÁâπÂæÅÂêëÈáè/ÂµåÂÖ•ÂàóË°®„ÄÇ

        Returns:  # ËøîÂõûÔºö
            (tuple): Tuple containing the raw output tensor, and processed output for visualization (if visualize=True)  # ÔºàÂÖÉÁªÑÔºâÔºöÂåÖÂê´ÂéüÂßãËæìÂá∫Âº†ÈáèÂíåÂèØËßÜÂåñÂ§ÑÁêÜËæìÂá∫ÁöÑÂÖÉÁªÑÔºàÂ¶ÇÊûú visualize=TrueÔºâ
        """
        b, ch, h, w = im.shape  # batch, channel, height, width  # Ëé∑ÂèñËæìÂÖ•ÂõæÂÉèÁöÑÊâπÊ¨°„ÄÅÈÄöÈÅì„ÄÅÈ´òÂ∫¶ÂíåÂÆΩÂ∫¶
        if self.fp16 and im.dtype != torch.float16:  # Â¶ÇÊûúÂêØÁî®‰∫ÜÂçäÁ≤æÂ∫¶‰∏îËæìÂÖ•ÂõæÂÉè‰∏çÊòØ float16 Á±ªÂûã
            im = im.half()  # ËΩ¨Êç¢‰∏∫ FP16
        if self.nhwc:  # Â¶ÇÊûú‰ΩøÁî® NHWC Ê†ºÂºè
            im = im.permute(0, 2, 3, 1)  # torch BCHW ËΩ¨Êç¢‰∏∫ numpy BHWC ÂΩ¢Áä∂Ôºà1, 320, 192, 3Ôºâ

        # PyTorch  # PyTorch Êé®ÁêÜ
        if self.pt or self.nn_module:  # Â¶ÇÊûúÊòØ PyTorch Ê®°ÂûãÊàñ nn.Module
            y = self.model(im, augment=augment, visualize=visualize, embed=embed)  # ÊâßË°åÊé®ÁêÜ

        # TorchScript  # TorchScript Êé®ÁêÜ
        elif self.jit:  # Â¶ÇÊûúÊòØ TorchScript Ê®°Âûã
            y = self.model(im)  # ÊâßË°åÊé®ÁêÜ

        # ONNX OpenCV DNN  # ONNX OpenCV DNN Êé®ÁêÜ
        elif self.dnn:  # Â¶ÇÊûúÊòØ DNN Ê®°Âûã
            im = im.cpu().numpy()  # torch ËΩ¨Êç¢‰∏∫ numpy
            self.net.setInput(im)  # ËÆæÁΩÆËæìÂÖ•
            y = self.net.forward()  # ÊâßË°åÂâçÂêëÊé®ÁêÜ

        # ONNX Runtime  # ONNX Runtime Êé®ÁêÜ
        elif self.onnx or self.imx:  # Â¶ÇÊûúÊòØ ONNX Êàñ IMX Ê®°Âûã
            if self.dynamic:  # Â¶ÇÊûúÊòØÂä®ÊÄÅÊ®°Âûã
                im = im.cpu().numpy()  # torch ËΩ¨Êç¢‰∏∫ numpy
                y = self.session.run(self.output_names, {self.session.get_inputs()[0].name: im})  # ÊâßË°åÊé®ÁêÜ
            else:  # Â¶ÇÊûú‰∏çÊòØÂä®ÊÄÅÊ®°Âûã
                if not self.cuda:  # Â¶ÇÊûú‰∏ç‰ΩøÁî® CUDA
                    im = im.cpu()  # ËΩ¨Êç¢‰∏∫ CPU
                self.io.bind_input(  # ÁªëÂÆöËæìÂÖ•
                    name="images",
                    device_type=im.device.type,
                    device_id=im.device.index if im.device.type == "cuda" else 0,
                    element_type=np.float16 if self.fp16 else np.float32,
                    shape=tuple(im.shape),
                    buffer_ptr=im.data_ptr(),
                )
                self.session.run_with_iobinding(self.io)  # ‰ΩøÁî® I/O ÁªëÂÆöÊâßË°åÊé®ÁêÜ
                y = self.bindings  # Ëé∑ÂèñÁªëÂÆöËæìÂá∫
            if self.imx:  # Â¶ÇÊûúÊòØ IMX Ê®°Âûã
                # boxes, conf, cls  # ÁõíÂ≠ê„ÄÅÁΩÆ‰ø°Â∫¶„ÄÅÁ±ªÂà´
                y = np.concatenate([y[0], y[1][:, :, None], y[2][:, :, None]], axis=-1)  # ÂêàÂπ∂ËæìÂá∫

        # OpenVINO  # OpenVINO Êé®ÁêÜ
        elif self.xml:  # Â¶ÇÊûúÊòØ OpenVINO Ê®°Âûã
            im = im.cpu().numpy()  # FP32  # ËΩ¨Êç¢‰∏∫ FP32

            if self.inference_mode in {"THROUGHPUT", "CUMULATIVE_THROUGHPUT"}:  # ‰ºòÂåñÂ§ßÊâπÊ¨°Êé®ÁêÜ
                n = im.shape[0]  # batch ‰∏≠ÂõæÂÉèÁöÑÊï∞Èáè
                results = [None] * n  # È¢ÑÂàÜÈÖç‰∏éÂõæÂÉèÊï∞ÈáèÁõ∏ÂêåÁöÑÁªìÊûúÂàóË°®

                def callback(request, userdata):  # ÂõûË∞ÉÂáΩÊï∞
                    """Places result in preallocated list using userdata index."""  # ‰ΩøÁî®Áî®Êà∑Êï∞ÊçÆÁ¥¢ÂºïÂ∞ÜÁªìÊûúÊîæÂÖ•È¢ÑÂàÜÈÖçÂàóË°®
                    results[userdata] = request.results  # Â∞ÜÁªìÊûúÂ≠òÂÇ®Âú®ÊåáÂÆö‰ΩçÁΩÆ

                # Create AsyncInferQueue, set the callback and start asynchronous inference for each input image  # ÂàõÂª∫ÂºÇÊ≠•Êé®ÁêÜÈòüÂàóÔºåËÆæÁΩÆÂõûË∞ÉÂπ∂‰∏∫ÊØè‰∏™ËæìÂÖ•ÂõæÂÉèÂºÄÂßãÂºÇÊ≠•Êé®ÁêÜ
                async_queue = self.ov.runtime.AsyncInferQueue(self.ov_compiled_model)  # ÂàõÂª∫ÂºÇÊ≠•Êé®ÁêÜÈòüÂàó
                async_queue.set_callback(callback)  # ËÆæÁΩÆÂõûË∞ÉÂáΩÊï∞
                for i in range(n):  # ÈÅçÂéÜÊØè‰∏™ÂõæÂÉè
                    # Start async inference with userdata=i to specify the position in results list  # ‰ΩøÁî® userdata=i ÂºÄÂßãÂºÇÊ≠•Êé®ÁêÜÔºå‰ª•ÊåáÂÆöÁªìÊûúÂàóË°®‰∏≠ÁöÑ‰ΩçÁΩÆ
                    async_queue.start_async(inputs={self.input_name: im[i : i + 1]}, userdata=i)  # ‰øùÊåÅÂõæÂÉè‰∏∫ BCHW
                async_queue.wait_all()  # Á≠âÂæÖÊâÄÊúâÊé®ÁêÜËØ∑Ê±ÇÂÆåÊàê
                y = np.concatenate([list(r.values())[0] for r in results])  # ÂêàÂπ∂ÁªìÊûú

            else:  # inference_mode = "LATENCY"Ôºå‰ºòÂåñ‰ª•ÊúÄÂø´ÈÄüÂ∫¶ËøîÂõûÁªìÊûúÔºåÊâπÊ¨°Â§ßÂ∞è‰∏∫ 1
                y = list(self.ov_compiled_model(im).values())  # ÊâßË°åÊé®ÁêÜÂπ∂Ëé∑ÂèñÁªìÊûú

        # TensorRT  # TensorRT Êé®ÁêÜ
        elif self.engine:  # Â¶ÇÊûúÊòØ TensorRT Ê®°Âûã
            if self.dynamic and im.shape != self.bindings["images"].shape:  # Â¶ÇÊûúÊòØÂä®ÊÄÅÊ®°Âûã‰∏îËæìÂÖ•ÂΩ¢Áä∂‰∏çÂåπÈÖç
                if self.is_trt10:  # Â¶ÇÊûúÊòØ TensorRT 10 ÁâàÊú¨
                    self.context.set_input_shape("images", im.shape)  # ËÆæÁΩÆËæìÂÖ•ÂΩ¢Áä∂
                    self.bindings["images"] = self.bindings["images"]._replace(shape=im.shape)  # Êõ¥Êñ∞ÁªëÂÆöÂΩ¢Áä∂
                    for name in self.output_names:  # ÈÅçÂéÜËæìÂá∫ÂêçÁß∞
                        self.bindings[name].data.resize_(tuple(self.context.get_tensor_shape(name)))  # Ë∞ÉÊï¥ËæìÂá∫ÂΩ¢Áä∂
                else:  # TensorRT < 10.0
                    i = self.model.get_binding_index("images")  # Ëé∑ÂèñËæìÂÖ•ÁªëÂÆöÁ¥¢Âºï
                    self.context.set_binding_shape(i, im.shape)  # ËÆæÁΩÆÁªëÂÆöÂΩ¢Áä∂
                    self.bindings["images"] = self.bindings["images"]._replace(shape=im.shape)  # Êõ¥Êñ∞ÁªëÂÆöÂΩ¢Áä∂
                    for name in self.output_names:  # ÈÅçÂéÜËæìÂá∫ÂêçÁß∞
                        i = self.model.get_binding_index(name)  # Ëé∑ÂèñËæìÂá∫ÁªëÂÆöÁ¥¢Âºï
                        self.bindings[name].data.resize_(tuple(self.context.get_binding_shape(i)))  # Ë∞ÉÊï¥ËæìÂá∫ÂΩ¢Áä∂

            s = self.bindings["images"].shape  # Ëé∑ÂèñËæìÂÖ•ÁªëÂÆöÁöÑÂΩ¢Áä∂
            assert im.shape == s, f"input size {im.shape} {'>' if self.dynamic else 'not equal to'} max model size {s}"  # Êñ≠Ë®ÄËæìÂÖ•ÂΩ¢Áä∂‰∏éÊ®°ÂûãÊúÄÂ§ßÂΩ¢Áä∂ÂåπÈÖç
            self.binding_addrs["images"] = int(im.data_ptr())  # Ëé∑ÂèñËæìÂÖ•Êï∞ÊçÆÊåáÈíà
            self.context.execute_v2(list(self.binding_addrs.values()))  # ÊâßË°åÊé®ÁêÜ
            y = [self.bindings[x].data for x in sorted(self.output_names)]  # Ëé∑ÂèñËæìÂá∫Êï∞ÊçÆ

        # CoreML  # CoreML Êé®ÁêÜ
        elif self.coreml:  # Â¶ÇÊûúÊòØ CoreML Ê®°Âûã
            im = im[0].cpu().numpy()  # Ëé∑ÂèñËæìÂÖ•ÂõæÂÉèÂπ∂ËΩ¨Êç¢‰∏∫ numpy
            im_pil = Image.fromarray((im * 255).astype("uint8"))  # ËΩ¨Êç¢‰∏∫ PIL ÂõæÂÉè
            # im = im.resize((192, 320), Image.BILINEAR)  # ÂèØÈÄâÔºöË∞ÉÊï¥ÂõæÂÉèÂ§ßÂ∞è
            y = self.model.predict({"image": im_pil})  # ÊâßË°åÊé®ÁêÜÔºåËøîÂõûÂùêÊ†á
            if "confidence" in y:  # Â¶ÇÊûúËøîÂõûÁªìÊûú‰∏≠ÂåÖÂê´ÁΩÆ‰ø°Â∫¶
                raise TypeError(  # ÊäõÂá∫Á±ªÂûãÈîôËØØ
                    "Ultralytics only supports inference of non-pipelined CoreML models exported with "  # "Ultralytics ‰ªÖÊîØÊåÅÊú™ÁÆ°ÈÅìÂåñÁöÑ CoreML Ê®°ÂûãÊé®ÁêÜÔºåÂØºÂá∫Êó∂ÈúÄ‰ΩøÁî®"
                    f"'nms=False', but 'model={w}' has an NMS pipeline created by an 'nms=True' export."  # "'nms=True' ÂØºÂá∫ÁöÑÊ®°Âûã„ÄÇ"
                )
                # TODO: CoreML NMS inference handling  # TODO: CoreML NMS Êé®ÁêÜÂ§ÑÁêÜ
                # from ultralytics.utils.ops import xywh2xyxy
                # box = xywh2xyxy(y['coordinates'] * [[w, h, w, h]])  # xyxy pixels
                # conf, cls = y['confidence'].max(1), y['confidence'].argmax(1).astype(np.float32)
                # y = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)
            y = list(y.values())  # Â∞ÜÁªìÊûúËΩ¨Êç¢‰∏∫ÂàóË°®
            if len(y) == 2 and len(y[1].shape) != 4:  # segmentation model  # Â¶ÇÊûúÊòØÂàÜÂâ≤Ê®°Âûã
                y = list(reversed(y))  # reversed for segmentation models (pred, proto)  # ÂèçËΩ¨ÁªìÊûúÈ°∫Â∫è

        # PaddlePaddle  # PaddlePaddle Êé®ÁêÜ
        elif self.paddle:  # Â¶ÇÊûúÊòØ PaddlePaddle Ê®°Âûã
            im = im.cpu().numpy().astype(np.float32)  # ËΩ¨Êç¢‰∏∫ numpy Âπ∂ËÆæÁΩÆÊï∞ÊçÆÁ±ªÂûã
            self.input_handle.copy_from_cpu(im)  # ‰ªé CPU Â§çÂà∂ËæìÂÖ•Êï∞ÊçÆ
            self.predictor.run()  # ÊâßË°åÊé®ÁêÜ
            y = [self.predictor.get_output_handle(x).copy_to_cpu() for x in self.output_names]  # Ëé∑ÂèñËæìÂá∫Êï∞ÊçÆÂπ∂Â§çÂà∂Âà∞ CPU

        # MNN  # MNN Êé®ÁêÜ
        elif self.mnn:  # Â¶ÇÊûúÊòØ MNN Ê®°Âûã
            input_var = self.torch_to_mnn(im)  # Â∞Ü PyTorch Âº†ÈáèËΩ¨Êç¢‰∏∫ MNN Âº†Èáè
            output_var = self.net.onForward([input_var])  # ÊâßË°åÂâçÂêëÊé®ÁêÜ
            y = [x.read() for x in output_var]  # ËØªÂèñËæìÂá∫Êï∞ÊçÆ

        # NCNN  # NCNN Êé®ÁêÜ
        elif self.ncnn:  # Â¶ÇÊûúÊòØ NCNN Ê®°Âûã
            mat_in = self.pyncnn.Mat(im[0].cpu().numpy())  # Â∞ÜËæìÂÖ•Êï∞ÊçÆËΩ¨Êç¢‰∏∫ NCNN Ê†ºÂºè
            with self.net.create_extractor() as ex:  # ÂàõÂª∫ÊèêÂèñÂô®
                ex.input(self.net.input_names()[0], mat_in)  # ËÆæÁΩÆËæìÂÖ•
                # WARNING: 'output_names' sorted as a temporary fix for https://github.com/pnnx/pnnx/issues/130  # Ë≠¶ÂëäÔºö'output_names' ÊéíÂ∫èÊòØ‰∏¥Êó∂‰øÆÂ§ç
                y = [np.array(ex.extract(x)[1])[None] for x in sorted(self.net.output_names())]  # Ëé∑ÂèñËæìÂá∫Êï∞ÊçÆ

        # NVIDIA Triton Inference Server  # NVIDIA Triton Êé®ÁêÜÊúçÂä°Âô®
        elif self.triton:  # Â¶ÇÊûúÊòØ Triton Ê®°Âûã
            im = im.cpu().numpy()  # torch ËΩ¨Êç¢‰∏∫ numpy
            y = self.model(im)  # ÊâßË°åÊé®ÁêÜ

        # RKNN  # RKNN Êé®ÁêÜ
        elif self.rknn:  # Â¶ÇÊûúÊòØ RKNN Ê®°Âûã
            im = (im.cpu().numpy() * 255).astype("uint8")  # ËΩ¨Êç¢‰∏∫ numpy Âπ∂ËÆæÁΩÆÊï∞ÊçÆÁ±ªÂûã
            im = im if isinstance(im, (list, tuple)) else [im]  # Â¶ÇÊûú‰∏çÊòØÂàóË°®ÔºåÂàôËΩ¨Êç¢‰∏∫ÂàóË°®
            y = self.rknn_model.inference(inputs=im)  # ÊâßË°åÊé®ÁêÜ

        # TensorFlow (SavedModel, GraphDef, Lite, Edge TPU)  # TensorFlowÔºàSavedModel„ÄÅGraphDef„ÄÅLite„ÄÅEdge TPUÔºâ
        else:  # Â¶ÇÊûú‰∏çÊòØÂ∑≤Áü•Ê†ºÂºè
            im = im.cpu().numpy()  # ËΩ¨Êç¢‰∏∫ numpy
            if self.saved_model:  # SavedModel
                y = self.model(im, training=False) if self.keras else self.model(im)  # ÊâßË°åÊé®ÁêÜ
                if not isinstance(y, list):  # Â¶ÇÊûúËøîÂõûÁªìÊûú‰∏çÊòØÂàóË°®
                    y = [y]  # ËΩ¨Êç¢‰∏∫ÂàóË°®
            elif self.pb:  # GraphDef
                y = self.frozen_func(x=self.tf.constant(im))  # ÊâßË°åÊé®ÁêÜ
            else:  # Lite or Edge TPU
                details = self.input_details[0]  # Ëé∑ÂèñËæìÂÖ•ËØ¶ÊÉÖ
                is_int = details["dtype"] in {np.int8, np.int16}  # Ê£ÄÊü•ÊòØÂê¶‰∏∫ TFLite ÈáèÂåñ int8 Êàñ int16 Ê®°Âûã
                if is_int:  # Â¶ÇÊûúÊòØÈáèÂåñÊ®°Âûã
                    scale, zero_point = details["quantization"]  # Ëé∑ÂèñÈáèÂåñÂèÇÊï∞
                    im = (im / scale + zero_point).astype(details["dtype"])  # ÂèçÈáèÂåñ
                self.interpreter.set_tensor(details["index"], im)  # ËÆæÁΩÆËæìÂÖ•Âº†Èáè
                self.interpreter.invoke()  # ÊâßË°åÊé®ÁêÜ
                y = []  # ÂàùÂßãÂåñËæìÂá∫
                for output in self.output_details:  # ÈÅçÂéÜËæìÂá∫ËØ¶ÊÉÖ
                    x = self.interpreter.get_tensor(output["index"])  # Ëé∑ÂèñËæìÂá∫Âº†Èáè
                    if is_int:  # Â¶ÇÊûúÊòØÈáèÂåñÊ®°Âûã
                        scale, zero_point = output["quantization"]  # Ëé∑ÂèñÈáèÂåñÂèÇÊï∞
                        x = (x.astype(np.float32) - zero_point) * scale  # ÂèçÈáèÂåñ
                    if x.ndim == 3:  # Â¶ÇÊûú‰ªªÂä°‰∏çÊòØÂàÜÁ±ªÔºå‰∏î‰∏çÂåÖÊã¨Êé©Á†ÅÔºàndim=4Ôºâ
                        # Denormalize xywh by image size. See https://github.com/ultralytics/ultralytics/pull/1695  # Ê†πÊçÆÂõæÂÉèÂ§ßÂ∞èÂèçÂΩí‰∏ÄÂåñ xywh
                        # xywh are normalized in TFLite/EdgeTPU to mitigate quantization error of integer models  # xywh Âú® TFLite/EdgeTPU ‰∏≠ÂΩí‰∏ÄÂåñÔºå‰ª•ÂáèÂ∞ëÊï¥Êï∞Ê®°ÂûãÁöÑÈáèÂåñËØØÂ∑Æ
                        if x.shape[-1] == 6 or self.end2end:  # end-to-end model  # Â¶ÇÊûúÊòØÁ´ØÂà∞Á´ØÊ®°Âûã
                            x[:, :, [0, 2]] *= w  # ÂèçÂΩí‰∏ÄÂåñÂÆΩÂ∫¶
                            x[:, :, [1, 3]] *= h  # ÂèçÂΩí‰∏ÄÂåñÈ´òÂ∫¶
                            if self.task == "pose":  # Â¶ÇÊûú‰ªªÂä°ÊòØÂßøÊÄÅÊ£ÄÊµã
                                x[:, :, 6::3] *= w  # ÂèçÂΩí‰∏ÄÂåñÂÖ≥ÈîÆÁÇπ x ÂùêÊ†á
                                x[:, :, 7::3] *= h  # ÂèçÂΩí‰∏ÄÂåñÂÖ≥ÈîÆÁÇπ y ÂùêÊ†á
                        else:  # Â¶ÇÊûú‰∏çÊòØÁ´ØÂà∞Á´ØÊ®°Âûã
                            x[:, [0, 2]] *= w  # ÂèçÂΩí‰∏ÄÂåñÂÆΩÂ∫¶
                            x[:, [1, 3]] *= h  # ÂèçÂΩí‰∏ÄÂåñÈ´òÂ∫¶
                            if self.task == "pose":  # Â¶ÇÊûú‰ªªÂä°ÊòØÂßøÊÄÅÊ£ÄÊµã
                                x[:, 5::3] *= w  # ÂèçÂΩí‰∏ÄÂåñÂÖ≥ÈîÆÁÇπ x ÂùêÊ†á
                                x[:, 6::3] *= h  # ÂèçÂΩí‰∏ÄÂåñÂÖ≥ÈîÆÁÇπ y ÂùêÊ†á
                    y.append(x)  # Ê∑ªÂä†ËæìÂá∫
            # TF segment fixes: export is reversed vs ONNX export and protos are transposed  # TensorFlow ÂàÜÂâ≤‰øÆÂ§çÔºöÂØºÂá∫È°∫Â∫è‰∏é ONNX ÂØºÂá∫Áõ∏ÂèçÔºåÂéüÂûãË¢´ËΩ¨ÁΩÆ
            if len(y) == 2:  # segment with (det, proto) output order reversed  # Â¶ÇÊûúÊòØÂàÜÂâ≤Ê®°ÂûãÔºåËæìÂá∫È°∫Â∫è‰∏∫ÔºàÊ£ÄÊµãÔºåÂéüÂûãÔºâ
                if len(y[1].shape) != 4:  # Â¶ÇÊûúÂéüÂûãÁöÑÂΩ¢Áä∂‰∏çÊòØ 4 Áª¥
                    y = list(reversed(y))  # ÂèçËΩ¨ËæìÂá∫È°∫Â∫è
                if y[1].shape[-1] == 6:  # end-to-end model  # Â¶ÇÊûúÊòØÁ´ØÂà∞Á´ØÊ®°Âûã
                    y = [y[1]]  # ‰ªÖËøîÂõûÂéüÂûã
                else:
                    y[1] = np.transpose(y[1], (0, 3, 1, 2))  # ËΩ¨ÁΩÆÂéüÂûã
            y = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]  # Á°Æ‰øùËæìÂá∫‰∏∫ numpy Êï∞ÁªÑ

        # for x in y:  # Ë∞ÉËØïËæìÂá∫ÂΩ¢Áä∂
        #     print(type(x), len(x)) if isinstance(x, (list, tuple)) else print(type(x), x.shape)  # debug shapes
        if isinstance(y, (list, tuple)):  # Â¶ÇÊûúËæìÂá∫ÊòØÂàóË°®ÊàñÂÖÉÁªÑ
            if len(self.names) == 999 and (self.task == "segment" or len(y) == 2):  # segments and names not defined  # Â¶ÇÊûúÁ±ªÂêçÊú™ÂÆö‰πâ
                nc = y[0].shape[1] - y[1].shape[1] - 4  # ËÆ°ÁÆóÁ±ªÂà´Êï∞Èáè
                self.names = {i: f"class{i}" for i in range(nc)}  # ÂàõÂª∫Á±ªÂêçÂ≠óÂÖ∏
            return self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]  # ËøîÂõûËæìÂá∫
        else:
            return self.from_numpy(y)  # ËøîÂõûËæìÂá∫

    def from_numpy(self, x):  # Â∞Ü numpy Êï∞ÁªÑËΩ¨Êç¢‰∏∫Âº†Èáè
        """
        Convert a numpy array to a tensor.  # Â∞Ü numpy Êï∞ÁªÑËΩ¨Êç¢‰∏∫Âº†Èáè

        Args:  # ÂèÇÊï∞Ôºö
            x (np.ndarray): The array to be converted.  # xÔºànp.ndarrayÔºâÔºöË¶ÅËΩ¨Êç¢ÁöÑÊï∞ÁªÑ„ÄÇ

        Returns:  # ËøîÂõûÔºö
            (torch.Tensor): The converted tensor  # Ôºàtorch.TensorÔºâÔºöËΩ¨Êç¢ÂêéÁöÑÂº†Èáè
        """
        return torch.tensor(x).to(self.device) if isinstance(x, np.ndarray) else x  # ËΩ¨Êç¢‰∏∫Âº†ÈáèÂπ∂ÁßªÂä®Âà∞ËÆæÂ§á

    def warmup(self, imgsz=(1, 3, 640, 640)):  # È¢ÑÁÉ≠Ê®°Âûã
        """
        Warm up the model by running one forward pass with a dummy input.  # ÈÄöËøá‰ΩøÁî®ËôöÊãüËæìÂÖ•ËøêË°å‰∏ÄÊ¨°ÂâçÂêë‰º†ÈÄíÊù•È¢ÑÁÉ≠Ê®°Âûã

        Args:  # ÂèÇÊï∞Ôºö
            imgsz (tuple): The shape of the dummy input tensor in the format (batch_size, channels, height, width)  # imgszÔºàÂÖÉÁªÑÔºâÔºöËôöÊãüËæìÂÖ•Âº†ÈáèÁöÑÂΩ¢Áä∂ÔºåÊ†ºÂºè‰∏∫ÔºàÊâπÊ¨°Â§ßÂ∞è„ÄÅÈÄöÈÅìÊï∞„ÄÅÈ´òÂ∫¶„ÄÅÂÆΩÂ∫¶Ôºâ
        """
        import torchvision  # noqa (import here so torchvision import time not recorded in postprocess time)  # ÂØºÂÖ• torchvision

        warmup_types = self.pt, self.jit, self.onnx, self.engine, self.saved_model, self.pb, self.triton, self.nn_module  # È¢ÑÁÉ≠Á±ªÂûã
        if any(warmup_types) and (self.device.type != "cpu" or self.triton):  # Â¶ÇÊûúÈúÄË¶ÅÈ¢ÑÁÉ≠‰∏îËÆæÂ§á‰∏çÊòØ CPU Êàñ‰ΩøÁî® Triton
            im = torch.empty(*imgsz, dtype=torch.half if self.fp16 else torch.float, device=self.device)  # ÂàõÂª∫ËôöÊãüËæìÂÖ•
            for _ in range(2 if self.jit else 1):  # Ê†πÊçÆÊòØÂê¶‰∏∫ JIT ËøõË°åÈ¢ÑÁÉ≠
                self.forward(im)  # ËøêË°åÂâçÂêëÊé®ÁêÜ

    @staticmethod
    def _model_type(p="path/to/model.pt"):  # Á°ÆÂÆöÊ®°ÂûãÁ±ªÂûã
        """
        Takes a path to a model file and returns the model type. Possibles types are pt, jit, onnx, xml, engine, coreml,  # Êé•ÂèóÊ®°ÂûãÊñá‰ª∂Ë∑ØÂæÑÂπ∂ËøîÂõûÊ®°ÂûãÁ±ªÂûã„ÄÇÂèØËÉΩÁöÑÁ±ªÂûãÊúâ pt„ÄÅjit„ÄÅonnx„ÄÅxml„ÄÅengine„ÄÅcoreml„ÄÅ
        saved_model, pb, tflite, edgetpu, tfjs, ncnn or paddle.  # saved_model„ÄÅpb„ÄÅtflite„ÄÅedgetpu„ÄÅtfjs„ÄÅncnn Êàñ paddle„ÄÇ

        Args:  # ÂèÇÊï∞Ôºö
            p (str): path to the model file. Defaults to path/to/model.pt  # pÔºàÂ≠óÁ¨¶‰∏≤ÔºâÔºöÊ®°ÂûãÊñá‰ª∂ÁöÑË∑ØÂæÑ„ÄÇÈªòËÆ§‰∏∫ path/to/model.pt

        Examples:  # Á§∫‰æãÔºö
            >>> model = AutoBackend(weights="path/to/model.onnx")  # ÂàõÂª∫ AutoBackend ÂÆû‰æã
            >>> model_type = model._model_type()  # returns "onnx"  # ËøîÂõû "onnx"
        """
        from ultralytics.engine.exporter import export_formats  # ÂØºÂÖ•ÂØºÂá∫Ê†ºÂºè

        sf = export_formats()["Suffix"]  # export suffixes  # Ëé∑ÂèñÂØºÂá∫ÂêéÁºÄ
        if not is_url(p) and not isinstance(p, str):  # Ê£ÄÊü•Ë∑ØÂæÑÊòØÂê¶‰∏∫ URL ÊàñÂ≠óÁ¨¶‰∏≤
            check_suffix(p, sf)  # Ê£ÄÊü•ÂêéÁºÄ
        name = Path(p).name  # Ëé∑ÂèñÊñá‰ª∂Âêç
        types = [s in name for s in sf]  # Ê£ÄÊü•Êñá‰ª∂ÂêçÊòØÂê¶ÂåÖÂê´ÂêéÁºÄ
        types[5] |= name.endswith(".mlmodel")  # retain support for older Apple CoreML *.mlmodel formats  # ‰øùÁïôÂØπÊóßÁâà Apple CoreML *.mlmodel Ê†ºÂºèÁöÑÊîØÊåÅ
        types[8] &= not types[9]  # tflite &= not edgetpu  # tflite ‰ªÖÂΩì‰∏çÊòØ edgetpu Êó∂ÊúâÊïà
        if any(types):  # Â¶ÇÊûúÊúâÂåπÈÖçÁöÑÁ±ªÂûã
            triton = False  # ËÆæÁΩÆ Triton ‰∏∫ False
        else:  # Â¶ÇÊûúÊ≤°ÊúâÂåπÈÖçÁöÑÁ±ªÂûã
            from urllib.parse import urlsplit  # ÂØºÂÖ• URL Ëß£ÊûêÊ®°Âùó

            url = urlsplit(p)  # Ëß£Êûê URL
            triton = bool(url.netloc) and bool(url.path) and url.scheme in {"http", "grpc"}  # Ê£ÄÊü•ÊòØÂê¶‰∏∫ Triton URL

        return types + [triton]  # ËøîÂõûÁ±ªÂûãÂàóË°®