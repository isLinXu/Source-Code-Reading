# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""Transformer modules."""
# å˜æ¢å™¨æ¨¡å—

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.init import constant_, xavier_uniform_

from .conv import Conv
from .utils import _get_clones, inverse_sigmoid, multi_scale_deformable_attn_pytorch

__all__ = (
    "TransformerEncoderLayer",
    "TransformerLayer",
    "TransformerBlock",
    "MLPBlock",
    "LayerNorm2d",
    "AIFI",
    "DeformableTransformerDecoder",
    "DeformableTransformerDecoderLayer",
    "MSDeformAttn",
    "MLP",
)

class RTDETRDecoder(nn.Module):
    """Defines a Real-Time Deformable Transformer Decoder for object detection."""
    # å®šä¹‰å®æ—¶å¯å˜å½¢å˜æ¢è§£ç å™¨ï¼Œç”¨äºç›®æ ‡æ£€æµ‹

    def __init__(
        self,
        nc=80,
        ch=(512, 1024, 2048),
        hd=256,  # hidden dim
        nq=300,  # num queries
        ndp=4,  # num decoder points
        nh=8,  # num head
        ndl=6,  # num decoder layers
        d_ffn=1024,  # dim of feedforward
        dropout=0.0,
        act=nn.ReLU(),
        eval_idx=-1,
        # Training args
        nd=100,  # num denoising
        label_noise_ratio=0.5,
        box_noise_scale=1.0,
        learnt_init_query=False,
    ):
        """Initialize the RTDETRDecoder module with the given parameters."""
        # ä½¿ç”¨ç»™å®šå‚æ•°åˆå§‹åŒ–RTDETRDecoderæ¨¡å—
        super().__init__()  # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        self.hidden_dim = hd  # éšè—å±‚ç»´åº¦
        self.nhead = nh  # å¤´æ•°
        self.nl = len(ch)  # å±‚æ•°
        self.nc = nc  # ç±»åˆ«æ•°é‡
        self.num_queries = nq  # æŸ¥è¯¢æ•°é‡
        self.num_decoder_layers = ndl  # è§£ç å™¨å±‚æ•°é‡

        # Backbone feature projection
        self.input_proj = nn.ModuleList(nn.Sequential(nn.Conv2d(x, hd, 1, bias=False), nn.BatchNorm2d(hd)) for x in ch)
        # åˆ›å»ºè¾“å…¥æŠ•å½±æ¨¡å—åˆ—è¡¨ï¼ŒåŒ…å«å·ç§¯å±‚å’Œæ‰¹é‡å½’ä¸€åŒ–å±‚
        # NOTE: simplified version but it's not consistent with .pt weights.
        # self.input_proj = nn.ModuleList(Conv(x, hd, act=False) for x in ch)

        # Transformer module
        decoder_layer = DeformableTransformerDecoderLayer(hd, nh, d_ffn, dropout, act, self.nl, ndp)
        # åˆ›å»ºå¯å˜å½¢å˜æ¢å™¨è§£ç å™¨å±‚
        self.decoder = DeformableTransformerDecoder(hd, decoder_layer, ndl)
        # åˆ›å»ºå¯å˜å½¢å˜æ¢å™¨è§£ç å™¨

        # Denoising part
        self.denoising_class_embed = nn.Embedding(nc, hd)  # åˆ›å»ºå»å™ªåˆ†ç±»åµŒå…¥
        self.num_denoising = nd  # å»å™ªæ•°é‡
        self.label_noise_ratio = label_noise_ratio  # æ ‡ç­¾å™ªå£°æ¯”ç‡
        self.box_noise_scale = box_noise_scale  # è¾¹æ¡†å™ªå£°æ¯”ä¾‹

        # Decoder embedding
        self.learnt_init_query = learnt_init_query  # æ˜¯å¦å­¦ä¹ åˆå§‹æŸ¥è¯¢
        if learnt_init_query:
            self.tgt_embed = nn.Embedding(nq, hd)  # åˆ›å»ºç›®æ ‡åµŒå…¥
        self.query_pos_head = MLP(4, 2 * hd, hd, num_layers=2)  # åˆ›å»ºæŸ¥è¯¢ä½ç½®å¤´

        # Encoder head
        self.enc_output = nn.Sequential(nn.Linear(hd, hd), nn.LayerNorm(hd))  # åˆ›å»ºç¼–ç å™¨è¾“å‡º
        self.enc_score_head = nn.Linear(hd, nc)  # åˆ›å»ºç¼–ç å™¨å¾—åˆ†å¤´
        self.enc_bbox_head = MLP(hd, hd, 4, num_layers=3)  # åˆ›å»ºç¼–ç å™¨è¾¹æ¡†å¤´

        # Decoder head
        self.dec_score_head = nn.ModuleList([nn.Linear(hd, nc) for _ in range(ndl)])  # åˆ›å»ºè§£ç å™¨å¾—åˆ†å¤´
        self.dec_bbox_head = nn.ModuleList([MLP(hd, hd, 4, num_layers=3) for _ in range(ndl)])  # åˆ›å»ºè§£ç å™¨è¾¹æ¡†å¤´

        self._reset_parameters()  # é‡ç½®å‚æ•°

    def forward(self, x, batch=None):
        """Runs the forward pass of the module, returning bounding box and classification scores for the input."""
        # æ‰§è¡Œæ¨¡å—çš„å‰å‘ä¼ æ’­ï¼Œè¿”å›è¾“å…¥çš„è¾¹æ¡†å’Œåˆ†ç±»å¾—åˆ†
        from ultralytics.models.utils.ops import get_cdn_group  # å¯¼å…¥å‡½æ•°

        # Input projection and embedding
        feats, shapes = self._get_encoder_input(x)  # è·å–ç¼–ç å™¨è¾“å…¥

        # Prepare denoising training
        dn_embed, dn_bbox, attn_mask, dn_meta = get_cdn_group(
            batch,
            self.nc,
            self.num_queries,
            self.denoising_class_embed.weight,
            self.num_denoising,
            self.label_noise_ratio,
            self.box_noise_scale,
            self.training,
        )
        # å‡†å¤‡å»å™ªè®­ç»ƒ

        embed, refer_bbox, enc_bboxes, enc_scores = self._get_decoder_input(feats, shapes, dn_embed, dn_bbox)
        # è·å–è§£ç å™¨è¾“å…¥

        # Decoder
        dec_bboxes, dec_scores = self.decoder(
            embed,
            refer_bbox,
            feats,
            shapes,
            self.dec_bbox_head,
            self.dec_score_head,
            self.query_pos_head,
            attn_mask=attn_mask,
        )
        # æ‰§è¡Œè§£ç å™¨

        x = dec_bboxes, dec_scores, enc_bboxes, enc_scores, dn_meta  # ç»„åˆè¾“å‡º
        if self.training:
            return x  # å¦‚æœæ˜¯è®­ç»ƒæ¨¡å¼ï¼Œè¿”å›è¾“å‡º
        # (bs, 300, 4+nc)
        y = torch.cat((dec_bboxes.squeeze(0), dec_scores.squeeze(0).sigmoid()), -1)  # æ‹¼æ¥è¾¹æ¡†å’Œå¾—åˆ†
        return y if self.export else (y, x)  # æ ¹æ®å¯¼å‡ºæ¨¡å¼è¿”å›ç»“æœ

    def _generate_anchors(self, shapes, grid_size=0.05, dtype=torch.float32, device="cpu", eps=1e-2):
        """Generates anchor bounding boxes for given shapes with specific grid size and validates them."""
        # ä¸ºç»™å®šå½¢çŠ¶ç”Ÿæˆé”šæ¡†è¾¹ç•Œæ¡†ï¼Œå¹¶è¿›è¡ŒéªŒè¯
        anchors = []  # åˆå§‹åŒ–é”šæ¡†åˆ—è¡¨
        for i, (h, w) in enumerate(shapes):  # éå†å½¢çŠ¶
            sy = torch.arange(end=h, dtype=dtype, device=device)  # åˆ›å»ºyåæ ‡
            sx = torch.arange(end=w, dtype=dtype, device=device)  # åˆ›å»ºxåæ ‡
            grid_y, grid_x = torch.meshgrid(sy, sx, indexing="ij") if TORCH_1_10 else torch.meshgrid(sy, sx)
            grid_xy = torch.stack([grid_x, grid_y], -1)  # (h, w, 2)

            valid_WH = torch.tensor([w, h], dtype=dtype, device=device)  # åˆ›å»ºæœ‰æ•ˆå®½é«˜å¼ é‡
            grid_xy = (grid_xy.unsqueeze(0) + 0.5) / valid_WH  # (1, h, w, 2)
            wh = torch.ones_like(grid_xy, dtype=dtype, device=device) * grid_size * (2.0**i)  # è®¡ç®—å®½é«˜
            anchors.append(torch.cat([grid_xy, wh], -1).view(-1, h * w, 4))  # (1, h*w, 4)

        anchors = torch.cat(anchors, 1)  # (1, h*w*nl, 4)
        valid_mask = ((anchors > eps) & (anchors < 1 - eps)).all(-1, keepdim=True)  # 1, h*w*nl, 1
        anchors = torch.log(anchors / (1 - anchors))  # è®¡ç®—é”šæ¡†çš„å¯¹æ•°
        anchors = anchors.masked_fill(~valid_mask, float("inf"))  # ç”¨æ— æ•ˆå€¼å¡«å……
        return anchors, valid_mask  # è¿”å›é”šæ¡†å’Œæœ‰æ•ˆæ©ç 

    def _get_encoder_input(self, x):
        """Processes and returns encoder inputs by getting projection features from input and concatenating them."""
        # é€šè¿‡è·å–è¾“å…¥çš„æŠ•å½±ç‰¹å¾å¤„ç†å¹¶è¿”å›ç¼–ç å™¨è¾“å…¥
        x = [self.input_proj[i](feat) for i, feat in enumerate(x)]  # è·å–æŠ•å½±ç‰¹å¾
        # Get encoder inputs
        feats = []  # åˆå§‹åŒ–ç‰¹å¾åˆ—è¡¨
        shapes = []  # åˆå§‹åŒ–å½¢çŠ¶åˆ—è¡¨
        for feat in x:  # éå†ç‰¹å¾
            h, w = feat.shape[2:]  # è·å–é«˜åº¦å’Œå®½åº¦
            # [b, c, h, w] -> [b, h*w, c]
            feats.append(feat.flatten(2).permute(0, 2, 1))  # å°†ç‰¹å¾å±•å¹³å¹¶è°ƒæ•´ç»´åº¦
            # [nl, 2]
            shapes.append([h, w])  # è®°å½•å½¢çŠ¶

        # [b, h*w, c]
        feats = torch.cat(feats, 1)  # æ‹¼æ¥ç‰¹å¾
        return feats, shapes  # è¿”å›ç‰¹å¾å’Œå½¢çŠ¶

    def _get_decoder_input(self, feats, shapes, dn_embed=None, dn_bbox=None):
        """Generates and prepares the input required for the decoder from the provided features and shapes."""
        # ä»æä¾›çš„ç‰¹å¾å’Œå½¢çŠ¶ç”Ÿæˆå¹¶å‡†å¤‡è§£ç å™¨æ‰€éœ€çš„è¾“å…¥
        bs = feats.shape[0]  # è·å–æ‰¹æ¬¡å¤§å°
        # Prepare input for decoder
        anchors, valid_mask = self._generate_anchors(shapes, dtype=feats.dtype, device=feats.device)  # ç”Ÿæˆé”šæ¡†
        features = self.enc_output(valid_mask * feats)  # bs, h*w, 256  # é€šè¿‡ç¼–ç å™¨è¾“å‡ºå¤„ç†ç‰¹å¾

        enc_outputs_scores = self.enc_score_head(features)  # (bs, h*w, nc)  # è·å–ç¼–ç å™¨è¾“å‡ºå¾—åˆ†

        # Query selection
        # (bs, num_queries)
        topk_ind = torch.topk(enc_outputs_scores.max(-1).values, self.num_queries, dim=1).indices.view(-1)  # è·å–top kç´¢å¼•
        # (bs, num_queries)
        batch_ind = torch.arange(end=bs, dtype=topk_ind.dtype).unsqueeze(-1).repeat(1, self.num_queries).view(-1)  # åˆ›å»ºæ‰¹æ¬¡ç´¢å¼•

        # (bs, num_queries, 256)
        top_k_features = features[batch_ind, topk_ind].view(bs, self.num_queries, -1)  # è·å–top kç‰¹å¾
        # (bs, num_queries, 4)
        top_k_anchors = anchors[:, topk_ind].view(bs, self.num_queries, -1)  # è·å–top ké”šæ¡†

        # Dynamic anchors + static content
        refer_bbox = self.enc_bbox_head(top_k_features) + top_k_anchors  # è®¡ç®—å‚è€ƒè¾¹æ¡†

        enc_bboxes = refer_bbox.sigmoid()  # å¯¹è¾¹æ¡†åº”ç”¨sigmoid
        if dn_bbox is not None:
            refer_bbox = torch.cat([dn_bbox, refer_bbox], 1)  # å¦‚æœæœ‰å»å™ªè¾¹æ¡†ï¼Œæ‹¼æ¥
        enc_scores = enc_outputs_scores[batch_ind, topk_ind].view(bs, self.num_queries, -1)  # è·å–ç¼–ç å™¨å¾—åˆ†

        embeddings = self.tgt_embed.weight.unsqueeze(0).repeat(bs, 1, 1) if self.learnt_init_query else top_k_features
        # å¦‚æœå­¦ä¹ åˆå§‹æŸ¥è¯¢ï¼Œé‡å¤ç›®æ ‡åµŒå…¥ï¼›å¦åˆ™ä½¿ç”¨top kç‰¹å¾
        if self.training:
            refer_bbox = refer_bbox.detach()  # åœ¨è®­ç»ƒæ—¶åˆ†ç¦»è¾¹æ¡†
            if not self.learnt_init_query:
                embeddings = embeddings.detach()  # åœ¨è®­ç»ƒæ—¶åˆ†ç¦»åµŒå…¥
        if dn_embed is not None:
            embeddings = torch.cat([dn_embed, embeddings], 1)  # å¦‚æœæœ‰å»å™ªåµŒå…¥ï¼Œæ‹¼æ¥

        return embeddings, refer_bbox, enc_bboxes, enc_scores  # è¿”å›åµŒå…¥ã€å‚è€ƒè¾¹æ¡†ã€ç¼–ç è¾¹æ¡†å’Œç¼–ç å¾—åˆ†

    # TODO
    def _reset_parameters(self):
        """Reset module parameters."""
        # é‡ç½®æ¨¡å—å‚æ•°
        bias_cls = bias_init_with_prob(0.01) / 80 * self.nc  # åˆå§‹åŒ–ç±»åç½®
        # NOTE: the weight initialization in `linear_init` would cause NaN when training with custom datasets.
        # linear_init(self.enc_score_head)
        constant_(self.sampling_offsets.weight.data, 0.0)  # è®¾ç½®é‡‡æ ·åç§»çš„æƒé‡
        thetas = torch.arange(self.n_heads, dtype=torch.float32) * (2.0 * math.pi / self.n_heads)  # è®¡ç®—theta
        grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)  # åˆ›å»ºç½‘æ ¼åˆå§‹åŒ–
        grid_init = (
            (grid_init / grid_init.abs().max(-1, keepdim=True)[0])  # å½’ä¸€åŒ–ç½‘æ ¼
            .view(self.n_heads, 1, 1, 2)
            .repeat(1, self.n_levels, self.n_points, 1)
        )
        for i in range(self.n_points):  # éå†æ¯ä¸ªç‚¹
            grid_init[:, :, i, :] *= i + 1  # è°ƒæ•´ç½‘æ ¼
        with torch.no_grad():
            self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))  # è®¾ç½®åç½®
        constant_(self.attention_weights.weight.data, 0.0)  # è®¾ç½®æ³¨æ„åŠ›æƒé‡
        constant_(self.attention_weights.bias.data, 0.0)  # è®¾ç½®æ³¨æ„åŠ›åç½®
        xavier_uniform_(self.value_proj.weight.data)  # ä½¿ç”¨Xavierå‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–å€¼æŠ•å½±æƒé‡
        constant_(self.value_proj.bias.data, 0.0)  # è®¾ç½®å€¼æŠ•å½±åç½®
        xavier_uniform_(self.output_proj.weight.data)  # ä½¿ç”¨Xavierå‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–è¾“å‡ºæŠ•å½±æƒé‡
        constant_(self.output_proj.bias.data, 0.0)  # è®¾ç½®è¾“å‡ºæŠ•å½±åç½®

    def forward(self, query, refer_bbox, value, value_shapes, value_mask=None):
        """
        Perform forward pass for multiscale deformable attention.
        # æ‰§è¡Œå¤šå°ºåº¦å¯å˜å½¢æ³¨æ„åŠ›çš„å‰å‘ä¼ æ’­

        Args:
            query (torch.Tensor): [bs, query_length, C]
            # æŸ¥è¯¢å¼ é‡ï¼Œå½¢çŠ¶ä¸º[bs, query_length, C]
            refer_bbox (torch.Tensor): [bs, query_length, n_levels, 2], range in [0, 1], top-left (0,0),
                bottom-right (1, 1), including padding area
            # å‚è€ƒè¾¹æ¡†ï¼Œå½¢çŠ¶ä¸º[bs, query_length, n_levels, 2]ï¼ŒèŒƒå›´åœ¨[0, 1]ï¼Œå·¦ä¸Šè§’ä¸º(0,0)ï¼Œå³ä¸‹è§’ä¸º(1, 1)ï¼ŒåŒ…æ‹¬å¡«å……åŒºåŸŸ
            value (torch.Tensor): [bs, value_length, C]
            # å€¼å¼ é‡ï¼Œå½¢çŠ¶ä¸º[bs, value_length, C]
            value_shapes (List): [n_levels, 2], [(H_0, W_0), (H_1, W_1), ..., (H_{L-1}, W_{L-1})]
            # å€¼å½¢çŠ¶åˆ—è¡¨ï¼Œå½¢çŠ¶ä¸º[n_levels, 2]
            value_mask (Tensor): [bs, value_length], True for non-padding elements, False for padding elements
            # å€¼æ©ç ï¼Œå½¢çŠ¶ä¸º[bs, value_length]ï¼Œéå¡«å……å…ƒç´ ä¸ºTrueï¼Œå¡«å……å…ƒç´ ä¸ºFalse

        Returns:
            output (Tensor): [bs, Length_{query}, C]
            # è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º[bs, Length_{query}, C]
        """
        bs, len_q = query.shape[:2]  # è·å–æ‰¹æ¬¡å¤§å°å’ŒæŸ¥è¯¢é•¿åº¦
        len_v = value.shape[1]  # è·å–å€¼çš„é•¿åº¦
        assert sum(s[0] * s[1] for s in value_shapes) == len_v  # ç¡®ä¿å€¼çš„é•¿åº¦ä¸å½¢çŠ¶åŒ¹é…

        value = self.value_proj(value)  # é€šè¿‡å€¼æŠ•å½±å¤„ç†å€¼
        if value_mask is not None:
            value = value.masked_fill(value_mask[..., None], float(0))  # ç”¨0å¡«å……æ©ç ä½ç½®
        value = value.view(bs, len_v, self.n_heads, self.d_model // self.n_heads)  # è°ƒæ•´å€¼çš„å½¢çŠ¶
        sampling_offsets = self.sampling_offsets(query).view(bs, len_q, self.n_heads, self.n_levels, self.n_points, 2)  # è®¡ç®—é‡‡æ ·åç§»
        attention_weights = self.attention_weights(query).view(bs, len_q, self.n_heads, self.n_levels * self.n_points)  # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_weights = F.softmax(attention_weights, -1).view(bs, len_q, self.n_heads, self.n_levels, self.n_points)  # åº”ç”¨softmaxè®¡ç®—æ³¨æ„åŠ›æƒé‡
        # N, Len_q, n_heads, n_levels, n_points, 2
        num_points = refer_bbox.shape[-1]  # è·å–å‚è€ƒè¾¹æ¡†çš„ç‚¹æ•°
        if num_points == 2:
            offset_normalizer = torch.as_tensor(value_shapes, dtype=query.dtype, device=query.device).flip(-1)  # åˆ›å»ºåç§»å½’ä¸€åŒ–å™¨
            add = sampling_offsets / offset_normalizer[None, None, None, :, None, :]  # è®¡ç®—åç§»
            sampling_locations = refer_bbox[:, :, None, :, None, :] + add  # è®¡ç®—é‡‡æ ·ä½ç½®
        elif num_points == 4:
            add = sampling_offsets / self.n_points * refer_bbox[:, :, None, :, None, 2:] * 0.5  # è®¡ç®—åç§»
            sampling_locations = refer_bbox[:, :, None, :, None, :2] + add  # è®¡ç®—é‡‡æ ·ä½ç½®
        else:
            raise ValueError(f"Last dim of reference_points must be 2 or 4, but got {num_points}.")  # æ£€æŸ¥ç‚¹æ•°æ˜¯å¦åˆæ³•
        output = multi_scale_deformable_attn_pytorch(value, value_shapes, sampling_locations, attention_weights)  # æ‰§è¡Œå¤šå°ºåº¦å¯å˜å½¢æ³¨æ„åŠ›
        return self.output_proj(output)  # é€šè¿‡è¾“å‡ºæŠ•å½±å¤„ç†è¾“å‡º


class DeformableTransformerDecoderLayer(nn.Module):
    """
    Deformable Transformer Decoder Layer inspired by PaddleDetection and Deformable-DETR implementations.
    # å¯å˜å½¢å˜æ¢å™¨è§£ç å™¨å±‚ï¼Œçµæ„Ÿæ¥è‡ªPaddleDetectionå’ŒDeformable-DETRå®ç°

    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/transformers/deformable_transformer.py
    https://github.com/fundamentalvision/Deformable-DETR/blob/main/models/deformable_transformer.py
    """

    def __init__(self, d_model=256, n_heads=8, d_ffn=1024, dropout=0.0, act=nn.ReLU(), n_levels=4, n_points=4):
        """Initialize the DeformableTransformerDecoderLayer with the given parameters."""
        # ä½¿ç”¨ç»™å®šå‚æ•°åˆå§‹åŒ–å¯å˜å½¢å˜æ¢å™¨è§£ç å™¨å±‚
        super().__init__()

        # Self attention
        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)  # åˆ›å»ºè‡ªæ³¨æ„åŠ›å±‚
        self.dropout1 = nn.Dropout(dropout)  # åˆ›å»ºdropoutå±‚
        self.norm1 = nn.LayerNorm(d_model)  # åˆ›å»ºå±‚å½’ä¸€åŒ–å±‚

        # Cross attention
        self.cross_attn = MSDeformAttn(d_model, n_levels, n_heads, n_points)  # åˆ›å»ºäº¤å‰æ³¨æ„åŠ›å±‚
        self.dropout2 = nn.Dropout(dropout)  # åˆ›å»ºdropoutå±‚
        self.norm2 = nn.LayerNorm(d_model)  # åˆ›å»ºå±‚å½’ä¸€åŒ–å±‚

        # FFN
        self.linear1 = nn.Linear(d_model, d_ffn)  # åˆ›å»ºå‰é¦ˆç½‘ç»œçš„ç¬¬ä¸€å±‚
        self.act = act  # æ¿€æ´»å‡½æ•°
        self.dropout3 = nn.Dropout(dropout)  # åˆ›å»ºdropoutå±‚
        self.linear2 = nn.Linear(d_ffn, d_model)  # åˆ›å»ºå‰é¦ˆç½‘ç»œçš„ç¬¬äºŒå±‚
        self.dropout4 = nn.Dropout(dropout)  # åˆ›å»ºdropoutå±‚
        self.norm3 = nn.LayerNorm(d_model)  # åˆ›å»ºå±‚å½’ä¸€åŒ–å±‚

    @staticmethod
    def with_pos_embed(tensor, pos):
        """Add positional embeddings to the input tensor, if provided."""
        # å¦‚æœæä¾›ä½ç½®åµŒå…¥ï¼Œåˆ™å°†å…¶æ·»åŠ åˆ°è¾“å…¥å¼ é‡
        return tensor if pos is None else tensor + pos

    def forward_ffn(self, tgt):
        """Perform forward pass through the Feed-Forward Network part of the layer."""
        # æ‰§è¡Œå‰é¦ˆç½‘ç»œéƒ¨åˆ†çš„å‰å‘ä¼ æ’­
        tgt2 = self.linear2(self.dropout3(self.act(self.linear1(tgt))))  # é€šè¿‡å‰é¦ˆç½‘ç»œå¤„ç†ç›®æ ‡
        tgt = tgt + self.dropout4(tgt2)  # æ·»åŠ dropout
        return self.norm3(tgt)  # è¿”å›å½’ä¸€åŒ–åçš„ç›®æ ‡

    def forward(self, embed, refer_bbox, feats, shapes, padding_mask=None, attn_mask=None, query_pos=None):
        """Perform the forward pass through the entire decoder layer."""
        # æ‰§è¡Œæ•´ä¸ªè§£ç å™¨å±‚çš„å‰å‘ä¼ æ’­
        # Self attention
        q = k = self.with_pos_embed(embed, query_pos)  # è·å–æŸ¥è¯¢å’Œé”®
        tgt = self.self_attn(q.transpose(0, 1), k.transpose(0, 1), embed.transpose(0, 1), attn_mask=attn_mask)[
            0
        ].transpose(0, 1)  # æ‰§è¡Œè‡ªæ³¨æ„åŠ›
        embed = embed + self.dropout1(tgt)  # æ·»åŠ dropout
        embed = self.norm1(embed)  # å½’ä¸€åŒ–

        # Cross attention
        tgt = self.cross_attn(
            self.with_pos_embed(embed, query_pos), refer_bbox.unsqueeze(2), feats, shapes, padding_mask
        )  # æ‰§è¡Œäº¤å‰æ³¨æ„åŠ›
        embed = embed + self.dropout2(tgt)  # æ·»åŠ dropout
        embed = self.norm2(embed)  # å½’ä¸€åŒ–

        # FFN
        return self.forward_ffn(embed)  # æ‰§è¡Œå‰é¦ˆç½‘ç»œçš„å‰å‘ä¼ æ’­


class DeformableTransformerDecoder(nn.Module):
    """
    Implementation of Deformable Transformer Decoder based on PaddleDetection.
    # åŸºäºPaddleDetectionå®ç°çš„å¯å˜å½¢å˜æ¢å™¨è§£ç å™¨

    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/transformers/deformable_transformer.py
    """

    def __init__(self, hidden_dim, decoder_layer, num_layers, eval_idx=-1):
        """Initialize the DeformableTransformerDecoder with the given parameters."""
        # ä½¿ç”¨ç»™å®šå‚æ•°åˆå§‹åŒ–å¯å˜å½¢å˜æ¢å™¨è§£ç å™¨
        super().__init__()
        self.layers = _get_clones(decoder_layer, num_layers)  # å…‹éš†è§£ç å™¨å±‚
        self.num_layers = num_layers  # è§£ç å™¨å±‚æ•°é‡
        self.hidden_dim = hidden_dim  # éšè—å±‚ç»´åº¦
        self.eval_idx = eval_idx if eval_idx >= 0 else num_layers + eval_idx  # è¯„ä¼°ç´¢å¼•

    def forward(
        self,
        embed,  # decoder embeddings
        refer_bbox,  # anchor
        feats,  # image features
        shapes,  # feature shapes
        bbox_head,
        score_head,
        pos_mlp,
        attn_mask=None,
        padding_mask=None,
    ):
        """Perform the forward pass through the entire decoder."""
        # æ‰§è¡Œæ•´ä¸ªè§£ç å™¨çš„å‰å‘ä¼ æ’­
        output = embed  # åˆå§‹åŒ–è¾“å‡º
        dec_bboxes = []  # åˆå§‹åŒ–è§£ç è¾¹æ¡†åˆ—è¡¨
        dec_cls = []  # åˆå§‹åŒ–è§£ç åˆ†ç±»åˆ—è¡¨
        last_refined_bbox = None  # åˆå§‹åŒ–æœ€åçš„ç²¾ç‚¼è¾¹æ¡†
        refer_bbox = refer_bbox.sigmoid()  # å¯¹å‚è€ƒè¾¹æ¡†åº”ç”¨sigmoid
        for i, layer in enumerate(self.layers):  # éå†è§£ç å™¨å±‚
            output = layer(output, refer_bbox, feats, shapes, padding_mask, attn_mask, pos_mlp(refer_bbox))  # æ‰§è¡Œå±‚çš„å‰å‘ä¼ æ’­

            bbox = bbox_head[i](output)  # è·å–è¾¹æ¡†
            refined_bbox = torch.sigmoid(bbox + inverse_sigmoid(refer_bbox))  # ç²¾ç‚¼è¾¹æ¡†

            if self.training:
                dec_cls.append(score_head[i](output))  # è·å–åˆ†ç±»å¾—åˆ†
                if i == 0:
                    dec_bboxes.append(refined_bbox)  # å¦‚æœæ˜¯ç¬¬ä¸€å±‚ï¼Œæ·»åŠ ç²¾ç‚¼è¾¹æ¡†
                else:
                    dec_bboxes.append(torch.sigmoid(bbox + inverse_sigmoid(last_refined_bbox)))  # å¦åˆ™ï¼Œä½¿ç”¨ä¸Šä¸€ä¸ªç²¾ç‚¼è¾¹æ¡†
            elif i == self.eval_idx:
                dec_cls.append(score_head[i](output))  # è·å–åˆ†ç±»å¾—åˆ†
                dec_bboxes.append(refined_bbox)  # æ·»åŠ ç²¾ç‚¼è¾¹æ¡†
                break

            last_refined_bbox = refined_bbox  # æ›´æ–°æœ€åçš„ç²¾ç‚¼è¾¹æ¡†
            refer_bbox = refined_bbox.detach() if self.training else refined_bbox  # æ›´æ–°å‚è€ƒè¾¹æ¡†

        return torch.stack(dec_bboxes), torch.stack(dec_cls)  # è¿”å›å †å çš„è§£ç è¾¹æ¡†å’Œåˆ†ç±»å¾—åˆ†