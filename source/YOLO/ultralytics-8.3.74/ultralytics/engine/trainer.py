# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""
Train a model on a dataset.

Usage:
    $ yolo mode=train model=yolo11n.pt data=coco8.yaml imgsz=640 epochs=100 batch=16
"""

import gc  # å¯¼å…¥gcæ¨¡å—ï¼Œç”¨äºåƒåœ¾å›æ”¶
import math  # å¯¼å…¥mathæ¨¡å—ï¼Œç”¨äºæ•°å­¦è®¡ç®—
import os  # å¯¼å…¥osæ¨¡å—ï¼Œç”¨äºä¸æ“ä½œç³»ç»Ÿäº¤äº’
import subprocess  # å¯¼å…¥subprocessæ¨¡å—ï¼Œç”¨äºæ‰§è¡Œå­è¿›ç¨‹
import time  # å¯¼å…¥timeæ¨¡å—ï¼Œç”¨äºæ—¶é—´ç›¸å…³æ“ä½œ
import warnings  # å¯¼å…¥warningsæ¨¡å—ï¼Œç”¨äºå‘å‡ºè­¦å‘Š
from copy import copy, deepcopy  # ä»copyæ¨¡å—å¯¼å…¥copyå’Œdeepcopyå‡½æ•°
from datetime import datetime, timedelta  # ä»datetimeæ¨¡å—å¯¼å…¥datetimeå’Œtimedeltaç±»
from pathlib import Path  # ä»pathlibæ¨¡å—å¯¼å…¥Pathç±»

import numpy as np  # å¯¼å…¥numpyåº“å¹¶å‘½åä¸ºnpï¼Œç”¨äºæ•°ç»„æ“ä½œ
import torch  # å¯¼å…¥PyTorchåº“
from torch import distributed as dist  # ä»PyTorchå¯¼å…¥åˆ†å¸ƒå¼æ¨¡å—å¹¶å‘½åä¸ºdist
from torch import nn, optim  # ä»PyTorchå¯¼å…¥ç¥ç»ç½‘ç»œæ¨¡å—å’Œä¼˜åŒ–æ¨¡å—

from ultralytics.cfg import get_cfg, get_save_dir  # ä»ultralytics.cfgæ¨¡å—å¯¼å…¥get_cfgå’Œget_save_dirå‡½æ•°
from ultralytics.data.utils import check_cls_dataset, check_det_dataset  # ä»ultralytics.data.utilsæ¨¡å—å¯¼å…¥æ•°æ®é›†æ£€æŸ¥å‡½æ•°
from ultralytics.nn.tasks import attempt_load_one_weight, attempt_load_weights  # ä»ultralytics.nn.tasksæ¨¡å—å¯¼å…¥æƒé‡åŠ è½½å‡½æ•°
from ultralytics.utils import (
    DEFAULT_CFG,  # é»˜è®¤é…ç½®
    LOCAL_RANK,  # æœ¬åœ°è¿›ç¨‹çš„æ’å
    LOGGER,  # æ—¥å¿—è®°å½•å™¨
    RANK,  # å…¨å±€è¿›ç¨‹çš„æ’å
    TQDM,  # è¿›åº¦æ¡æ˜¾ç¤º
    __version__,  # å½“å‰ç‰ˆæœ¬
    callbacks,  # å›è°ƒå‡½æ•°
    clean_url,  # æ¸…ç†URLçš„å‡½æ•°
    colorstr,  # é¢œè‰²å­—ç¬¦ä¸²å¤„ç†å‡½æ•°
    emojis,  # è¡¨æƒ…ç¬¦å·å¤„ç†
    yaml_save,  # ä¿å­˜YAMLæ–‡ä»¶çš„å‡½æ•°
)
from ultralytics.utils.autobatch import check_train_batch_size  # ä»ultralytics.utils.autobatchæ¨¡å—å¯¼å…¥æ£€æŸ¥è®­ç»ƒæ‰¹æ¬¡å¤§å°çš„å‡½æ•°
from ultralytics.utils.checks import check_amp, check_file, check_imgsz, check_model_file_from_stem, print_args  # ä»ultralytics.utils.checksæ¨¡å—å¯¼å…¥æ£€æŸ¥å‡½æ•°
from ultralytics.utils.dist import ddp_cleanup, generate_ddp_command  # ä»ultralytics.utils.distæ¨¡å—å¯¼å…¥DDPæ¸…ç†å’Œå‘½ä»¤ç”Ÿæˆå‡½æ•°
from ultralytics.utils.files import get_latest_run  # ä»ultralytics.utils.filesæ¨¡å—å¯¼å…¥è·å–æœ€æ–°è¿è¡Œçš„å‡½æ•°
from ultralytics.utils.torch_utils import (
    TORCH_2_4,  # PyTorchç‰ˆæœ¬æ£€æŸ¥
    EarlyStopping,  # æå‰åœæ­¢ç±»
    ModelEMA,  # æŒ‡æ•°ç§»åŠ¨å¹³å‡æ¨¡å‹
    autocast,  # è‡ªåŠ¨æ··åˆç²¾åº¦
    convert_optimizer_state_dict_to_fp16,  # å°†ä¼˜åŒ–å™¨çŠ¶æ€å­—å…¸è½¬æ¢ä¸ºfp16
    init_seeds,  # åˆå§‹åŒ–éšæœºç§å­
    one_cycle,  # ä¸€å‘¨æœŸå­¦ä¹ ç‡è°ƒåº¦
    select_device,  # é€‰æ‹©è®¾å¤‡çš„å‡½æ•°
    strip_optimizer,  # å»é™¤ä¼˜åŒ–å™¨çš„å‡½æ•°
    torch_distributed_zero_first,  # DDPçš„é›¶è¿›ç¨‹ä¼˜å…ˆ
    unset_deterministic,  # å–æ¶ˆç¡®å®šæ€§è®¾ç½®
)

class BaseTrainer:
    """
    A base class for creating trainers.  # åˆ›å»ºè®­ç»ƒå™¨çš„åŸºç±»

    Attributes:
        args (SimpleNamespace): Configuration for the trainer.  # è®­ç»ƒå™¨çš„é…ç½®
        validator (BaseValidator): Validator instance.  # éªŒè¯å™¨å®ä¾‹
        model (nn.Module): Model instance.  # æ¨¡å‹å®ä¾‹
        callbacks (defaultdict): Dictionary of callbacks.  # å›è°ƒå‡½æ•°å­—å…¸
        save_dir (Path): Directory to save results.  # ä¿å­˜ç»“æœçš„ç›®å½•
        wdir (Path): Directory to save weights.  # ä¿å­˜æƒé‡çš„ç›®å½•
        last (Path): Path to the last checkpoint.  # æœ€åæ£€æŸ¥ç‚¹çš„è·¯å¾„
        best (Path): Path to the best checkpoint.  # æœ€ä½³æ£€æŸ¥ç‚¹çš„è·¯å¾„
        save_period (int): Save checkpoint every x epochs (disabled if < 1).  # æ¯xä¸ªepochä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœ<1åˆ™ç¦ç”¨ï¼‰
        batch_size (int): Batch size for training.  # è®­ç»ƒçš„æ‰¹æ¬¡å¤§å°
        epochs (int): Number of epochs to train for.  # è®­ç»ƒçš„è½®æ•°
        start_epoch (int): Starting epoch for training.  # å¼€å§‹è®­ç»ƒçš„è½®æ•°
        device (torch.device): Device to use for training.  # ç”¨äºè®­ç»ƒçš„è®¾å¤‡
        amp (bool): Flag to enable AMP (Automatic Mixed Precision).  # å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦çš„æ ‡å¿—
        scaler (amp.GradScaler): Gradient scaler for AMP.  # è‡ªåŠ¨æ··åˆç²¾åº¦çš„æ¢¯åº¦ç¼©æ”¾å™¨
        data (str): Path to data.  # æ•°æ®çš„è·¯å¾„
        trainset (torch.utils.data.Dataset): Training dataset.  # è®­ç»ƒæ•°æ®é›†
        testset (torch.utils.data.Dataset): Testing dataset.  # æµ‹è¯•æ•°æ®é›†
        ema (nn.Module): EMA (Exponential Moving Average) of the model.  # æ¨¡å‹çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡
        resume (bool): Resume training from a checkpoint.  # ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
        lf (nn.Module): Loss function.  # æŸå¤±å‡½æ•°
        scheduler (torch.optim.lr_scheduler._LRScheduler): Learning rate scheduler.  # å­¦ä¹ ç‡è°ƒåº¦å™¨
        best_fitness (float): The best fitness value achieved.  # è¾¾åˆ°çš„æœ€ä½³é€‚åº”åº¦å€¼
        fitness (float): Current fitness value.  # å½“å‰é€‚åº”åº¦å€¼
        loss (float): Current loss value.  # å½“å‰æŸå¤±å€¼
        tloss (float): Total loss value.  # æ€»æŸå¤±å€¼
        loss_names (list): List of loss names.  # æŸå¤±åç§°åˆ—è¡¨
        csv (Path): Path to results CSV file.  # ç»“æœCSVæ–‡ä»¶çš„è·¯å¾„
    """

    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
        """
        Initializes the BaseTrainer class.  # åˆå§‹åŒ–BaseTrainerç±»

        Args:
            cfg (str, optional): Path to a configuration file. Defaults to DEFAULT_CFG.  # é…ç½®æ–‡ä»¶çš„è·¯å¾„ï¼Œé»˜è®¤ä¸ºDEFAULT_CFG
            overrides (dict, optional): Configuration overrides. Defaults to None.  # é…ç½®è¦†ç›–ï¼Œé»˜è®¤ä¸ºNone
        """
        self.args = get_cfg(cfg, overrides)  # è·å–é…ç½®
        self.check_resume(overrides)  # æ£€æŸ¥æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤
        self.device = select_device(self.args.device, self.args.batch)  # é€‰æ‹©è®¾å¤‡
        self.validator = None  # éªŒè¯å™¨åˆå§‹åŒ–ä¸ºNone
        self.metrics = None  # æŒ‡æ ‡åˆå§‹åŒ–ä¸ºNone
        self.plots = {}  # åˆå§‹åŒ–ç»˜å›¾å­—å…¸
        init_seeds(self.args.seed + 1 + RANK, deterministic=self.args.deterministic)  # åˆå§‹åŒ–éšæœºç§å­

        # Dirs  # ç›®å½•è®¾ç½®
        self.save_dir = get_save_dir(self.args)  # è·å–ä¿å­˜ç›®å½•
        self.args.name = self.save_dir.name  # update name for loggers  # æ›´æ–°æ—¥å¿—è®°å½•å™¨çš„åç§°
        self.wdir = self.save_dir / "weights"  # weights dir  # æƒé‡ç›®å½•
        if RANK in {-1, 0}:  # å¦‚æœæ˜¯ä¸»è¿›ç¨‹
            self.wdir.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºç›®å½•
            self.args.save_dir = str(self.save_dir)  # ä¿å­˜ç›®å½•
            yaml_save(self.save_dir / "args.yaml", vars(self.args))  # ä¿å­˜è¿è¡Œå‚æ•°
        self.last, self.best = self.wdir / "last.pt", self.wdir / "best.pt"  # æ£€æŸ¥ç‚¹è·¯å¾„
        self.save_period = self.args.save_period  # ä¿å­˜å‘¨æœŸ

        self.batch_size = self.args.batch  # æ‰¹æ¬¡å¤§å°
        self.epochs = self.args.epochs or 100  # in case users accidentally pass epochs=None with timed training  # å¦‚æœç”¨æˆ·æ„å¤–ä¼ é€’epochs=Noneï¼Œé»˜è®¤ä¸º100
        self.start_epoch = 0  # èµ·å§‹è½®æ•°
        if RANK == -1:  # å¦‚æœæ˜¯ä¸»è¿›ç¨‹
            print_args(vars(self.args))  # æ‰“å°å‚æ•°

        # Device  # è®¾å¤‡è®¾ç½®
        if self.device.type in {"cpu", "mps"}:  # å¦‚æœè®¾å¤‡æ˜¯CPUæˆ–MPS
            self.args.workers = 0  # faster CPU training as time dominated by inference, not dataloading  # æ›´å¿«çš„CPUè®­ç»ƒï¼Œå› ä¸ºæ—¶é—´ä¸»è¦ç”±æ¨ç†è€Œä¸æ˜¯æ•°æ®åŠ è½½ä¸»å¯¼

        # Model and Dataset  # æ¨¡å‹å’Œæ•°æ®é›†è®¾ç½®
        self.model = check_model_file_from_stem(self.args.model)  # add suffix, i.e. yolo11n -> yolo11n.pt  # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        with torch_distributed_zero_first(LOCAL_RANK):  # avoid auto-downloading dataset multiple times  # é¿å…å¤šæ¬¡è‡ªåŠ¨ä¸‹è½½æ•°æ®é›†
            self.trainset, self.testset = self.get_dataset()  # è·å–è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        self.ema = None  # æŒ‡æ•°ç§»åŠ¨å¹³å‡åˆå§‹åŒ–ä¸ºNone

        # Optimization utils init  # ä¼˜åŒ–å·¥å…·åˆå§‹åŒ–
        self.lf = None  # æŸå¤±å‡½æ•°åˆå§‹åŒ–ä¸ºNone
        self.scheduler = None  # å­¦ä¹ ç‡è°ƒåº¦å™¨åˆå§‹åŒ–ä¸ºNone

        # Epoch level metrics  # è½®æ¬¡çº§åˆ«çš„æŒ‡æ ‡
        self.best_fitness = None  # æœ€ä½³é€‚åº”åº¦åˆå§‹åŒ–ä¸ºNone
        self.fitness = None  # å½“å‰é€‚åº”åº¦åˆå§‹åŒ–ä¸ºNone
        self.loss = None  # å½“å‰æŸå¤±åˆå§‹åŒ–ä¸ºNone
        self.tloss = None  # æ€»æŸå¤±åˆå§‹åŒ–ä¸ºNone
        self.loss_names = ["Loss"]  # æŸå¤±åç§°åˆ—è¡¨
        self.csv = self.save_dir / "results.csv"  # ç»“æœCSVæ–‡ä»¶è·¯å¾„
        self.plot_idx = [0, 1, 2]  # ç»˜å›¾ç´¢å¼•

        # HUB  # HUBè®¾ç½®
        self.hub_session = None  # HUBä¼šè¯åˆå§‹åŒ–ä¸ºNone

        # Callbacks  # å›è°ƒå‡½æ•°è®¾ç½®
        self.callbacks = _callbacks or callbacks.get_default_callbacks()  # è·å–å›è°ƒå‡½æ•°
        if RANK in {-1, 0}:  # å¦‚æœæ˜¯ä¸»è¿›ç¨‹
            callbacks.add_integration_callbacks(self)  # æ·»åŠ é›†æˆå›è°ƒ

    def add_callback(self, event: str, callback):
        """Appends the given callback.  # æ·»åŠ ç»™å®šçš„å›è°ƒå‡½æ•°"""
        self.callbacks[event].append(callback)  # å°†å›è°ƒå‡½æ•°æ·»åŠ åˆ°æŒ‡å®šäº‹ä»¶

    def set_callback(self, event: str, callback):
        """Overrides the existing callbacks with the given callback.  # ç”¨ç»™å®šçš„å›è°ƒå‡½æ•°è¦†ç›–ç°æœ‰å›è°ƒå‡½æ•°"""
        self.callbacks[event] = [callback]  # è®¾ç½®æŒ‡å®šäº‹ä»¶çš„å›è°ƒå‡½æ•°

    def run_callbacks(self, event: str):
        """Run all existing callbacks associated with a particular event.  # è¿è¡Œä¸ç‰¹å®šäº‹ä»¶ç›¸å…³çš„æ‰€æœ‰ç°æœ‰å›è°ƒå‡½æ•°"""
        for callback in self.callbacks.get(event, []):  # éå†æŒ‡å®šäº‹ä»¶çš„å›è°ƒå‡½æ•°
            callback(self)  # æ‰§è¡Œå›è°ƒå‡½æ•°

    def train(self):
        """Allow device='', device=None on Multi-GPU systems to default to device=0.  # å…è®¸åœ¨å¤šGPUç³»ç»Ÿä¸Šå°†device=''æˆ–device=Noneé»˜è®¤ä¸ºdevice=0"""
        if isinstance(self.args.device, str) and len(self.args.device):  # i.e. device='0' or device='0,1,2,3'  # å¦‚æœè®¾å¤‡æ˜¯å­—ç¬¦ä¸²ä¸”é•¿åº¦å¤§äº0
            world_size = len(self.args.device.split(","))  # è·å–ä¸–ç•Œå¤§å°
        elif isinstance(self.args.device, (tuple, list)):  # i.e. device=[0, 1, 2, 3] (multi-GPU from CLI is list)  # å¦‚æœè®¾å¤‡æ˜¯å…ƒç»„æˆ–åˆ—è¡¨
            world_size = len(self.args.device)  # è·å–ä¸–ç•Œå¤§å°
        elif self.args.device in {"cpu", "mps"}:  # i.e. device='cpu' or 'mps'  # å¦‚æœè®¾å¤‡æ˜¯CPUæˆ–MPS
            world_size = 0  # ä¸–ç•Œå¤§å°ä¸º0
        elif torch.cuda.is_available():  # i.e. device=None or device='' or device=number  # å¦‚æœCUDAå¯ç”¨
            world_size = 1  # é»˜è®¤ä¸ºè®¾å¤‡0
        else:  # i.e. device=None or device=''  # å¦åˆ™
            world_size = 0  # ä¸–ç•Œå¤§å°ä¸º0

        # Run subprocess if DDP training, else train normally  # å¦‚æœæ˜¯DDPè®­ç»ƒåˆ™è¿è¡Œå­è¿›ç¨‹ï¼Œå¦åˆ™æ­£å¸¸è®­ç»ƒ
        if world_size > 1 and "LOCAL_RANK" not in os.environ:  # å¦‚æœä¸–ç•Œå¤§å°å¤§äº1ä¸”LOCAL_RANKä¸åœ¨ç¯å¢ƒå˜é‡ä¸­
            # Argument checks  # å‚æ•°æ£€æŸ¥
            if self.args.rect:  # å¦‚æœä½¿ç”¨çŸ©å½¢è®­ç»ƒ
                LOGGER.warning("WARNING âš ï¸ 'rect=True' is incompatible with Multi-GPU training, setting 'rect=False'")  # å‘å‡ºè­¦å‘Š
                self.args.rect = False  # å°†çŸ©å½¢è®¾ç½®ä¸ºFalse
            if self.args.batch < 1.0:  # å¦‚æœæ‰¹æ¬¡å°äº1.0
                LOGGER.warning(
                    "WARNING âš ï¸ 'batch<1' for AutoBatch is incompatible with Multi-GPU training, setting "
                    "default 'batch=16'"  # å‘å‡ºè­¦å‘Š
                )
                self.args.batch = 16  # å°†æ‰¹æ¬¡è®¾ç½®ä¸º16

            # Command  # å‘½ä»¤è®¾ç½®
            cmd, file = generate_ddp_command(world_size, self)  # ç”ŸæˆDDPå‘½ä»¤
            try:
                LOGGER.info(f"{colorstr('DDP:')} debug command {' '.join(cmd)}")  # è®°å½•DDPè°ƒè¯•å‘½ä»¤
                subprocess.run(cmd, check=True)  # è¿è¡Œå‘½ä»¤
            except Exception as e:  # æ•è·å¼‚å¸¸
                raise e  # æŠ›å‡ºå¼‚å¸¸
            finally:
                ddp_cleanup(self, str(file))  # æ¸…ç†DDP

        else:
            self._do_train(world_size)  # æ­£å¸¸è®­ç»ƒ

    def _setup_scheduler(self):
        """Initialize training learning rate scheduler.  # åˆå§‹åŒ–è®­ç»ƒå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if self.args.cos_lr:  # å¦‚æœä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡
            self.lf = one_cycle(1, self.args.lrf, self.epochs)  # cosine 1->hyp['lrf']  # è®¾ç½®ä½™å¼¦å­¦ä¹ ç‡
        else:
            self.lf = lambda x: max(1 - x / self.epochs, 0) * (1.0 - self.args.lrf) + self.args.lrf  # linear  # è®¾ç½®çº¿æ€§å­¦ä¹ ç‡
        self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=self.lf)  # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨

    def _setup_ddp(self, world_size):
        """Initializes and sets the DistributedDataParallel parameters for training.  # åˆå§‹åŒ–å¹¶è®¾ç½®åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œè®­ç»ƒå‚æ•°"""
        torch.cuda.set_device(RANK)  # è®¾ç½®å½“å‰CUDAè®¾å¤‡
        self.device = torch.device("cuda", RANK)  # è®¾ç½®è®¾å¤‡ä¸ºCUDA
        # LOGGER.info(f'DDP info: RANK {RANK}, WORLD_SIZE {world_size}, DEVICE {self.device}')  # è®°å½•DDPä¿¡æ¯
        os.environ["TORCH_NCCL_BLOCKING_WAIT"] = "1"  # set to enforce timeout  # è®¾ç½®ä»¥å¼ºåˆ¶è¶…æ—¶
        dist.init_process_group(
            backend="nccl" if dist.is_nccl_available() else "gloo",  # åˆå§‹åŒ–è¿›ç¨‹ç»„
            timeout=timedelta(seconds=10800),  # 3 hours  # è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º3å°æ—¶
            rank=RANK,  # å½“å‰è¿›ç¨‹çš„æ’å
            world_size=world_size,  # ä¸–ç•Œå¤§å°
        )

    def _setup_train(self, world_size):
        """Builds dataloaders and optimizer on correct rank process.  # åœ¨æ­£ç¡®çš„æ’åè¿›ç¨‹ä¸Šæ„å»ºæ•°æ®åŠ è½½å™¨å’Œä¼˜åŒ–å™¨"""
        # Model  # æ¨¡å‹è®¾ç½®
        self.run_callbacks("on_pretrain_routine_start")  # è¿è¡Œé¢„è®­ç»ƒä¾‹ç¨‹å¼€å§‹çš„å›è°ƒ
        ckpt = self.setup_model()  # è®¾ç½®æ¨¡å‹
        self.model = self.model.to(self.device)  # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
        self.set_model_attributes()  # è®¾ç½®æ¨¡å‹å±æ€§

        # Freeze layers  # å†»ç»“å±‚è®¾ç½®
        freeze_list = (
            self.args.freeze  # å†»ç»“å±‚åˆ—è¡¨
            if isinstance(self.args.freeze, list)  # å¦‚æœæ˜¯åˆ—è¡¨
            else range(self.args.freeze)  # å¦åˆ™ä½¿ç”¨èŒƒå›´
            if isinstance(self.args.freeze, int)
            else []
        )
        always_freeze_names = [".dfl"]  # always freeze these layers  # å§‹ç»ˆå†»ç»“è¿™äº›å±‚
        freeze_layer_names = [f"model.{x}." for x in freeze_list] + always_freeze_names  # å†»ç»“å±‚åç§°åˆ—è¡¨
        for k, v in self.model.named_parameters():  # éå†æ¨¡å‹å‚æ•°
            # v.register_hook(lambda x: torch.nan_to_num(x))  # NaN to 0 (commented for erratic training results)  # NaNè½¬ä¸º0ï¼ˆæ³¨é‡Šæ‰ä»¥é¿å…è®­ç»ƒç»“æœä¸ç¨³å®šï¼‰
            if any(x in k for x in freeze_layer_names):  # å¦‚æœå‚æ•°åç§°åœ¨å†»ç»“å±‚åç§°ä¸­
                LOGGER.info(f"Freezing layer '{k}'")  # è®°å½•å†»ç»“å±‚
                v.requires_grad = False  # ä¸è®¡ç®—æ¢¯åº¦
            elif not v.requires_grad and v.dtype.is_floating_point:  # ä»…æµ®ç‚¹å‹Tensorå¯ä»¥è®¡ç®—æ¢¯åº¦
                LOGGER.info(
                    f"WARNING âš ï¸ setting 'requires_grad=True' for frozen layer '{k}'. "
                    "See ultralytics.engine.trainer for customization of frozen layers."  # å‘å‡ºè­¦å‘Š
                )
                v.requires_grad = True  # è®¾ç½®ä¸ºè®¡ç®—æ¢¯åº¦

        # Check AMP  # æ£€æŸ¥è‡ªåŠ¨æ··åˆç²¾åº¦
        self.amp = torch.tensor(self.args.amp).to(self.device)  # Trueæˆ–False
        if self.amp and RANK in {-1, 0}:  # Single-GPU and DDP  # å•GPUå’ŒDDP
            callbacks_backup = callbacks.default_callbacks.copy()  # backup callbacks as check_amp() resets them  # å¤‡ä»½å›è°ƒå‡½æ•°
            self.amp = torch.tensor(check_amp(self.model), device=self.device)  # æ£€æŸ¥AMP
            callbacks.default_callbacks = callbacks_backup  # æ¢å¤å›è°ƒå‡½æ•°
        if RANK > -1 and world_size > 1:  # DDP
            dist.broadcast(self.amp, src=0)  # broadcast the tensor from rank 0 to all other ranks (returns None)  # ä»rank 0å¹¿æ’­å¼ é‡åˆ°æ‰€æœ‰å…¶ä»–rank
        self.amp = bool(self.amp)  # as boolean  # è½¬æ¢ä¸ºå¸ƒå°”å€¼
        self.scaler = (
            torch.amp.GradScaler("cuda", enabled=self.amp) if TORCH_2_4 else torch.cuda.amp.GradScaler(enabled=self.amp)  # åˆå§‹åŒ–æ¢¯åº¦ç¼©æ”¾å™¨
        )
        if world_size > 1:  # å¦‚æœä¸–ç•Œå¤§å°å¤§äº1
            self.model = nn.parallel.DistributedDataParallel(self.model, device_ids=[RANK], find_unused_parameters=True)  # è®¾ç½®åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ

        # Check imgsz  # æ£€æŸ¥å›¾åƒå¤§å°
        gs = max(int(self.model.stride.max() if hasattr(self.model, "stride") else 32), 32)  # grid size (max stride)  # ç½‘æ ¼å¤§å°ï¼ˆæœ€å¤§æ­¥å¹…ï¼‰
        self.args.imgsz = check_imgsz(self.args.imgsz, stride=gs, floor=gs, max_dim=1)  # æ£€æŸ¥å›¾åƒå¤§å°
        self.stride = gs  # for multiscale training  # ç”¨äºå¤šå°ºåº¦è®­ç»ƒ

        # Batch size  # æ‰¹æ¬¡å¤§å°è®¾ç½®
        if self.batch_size < 1 and RANK == -1:  # single-GPU only, estimate best batch size  # ä»…å•GPUï¼Œä¼°è®¡æœ€ä½³æ‰¹æ¬¡å¤§å°
            self.args.batch = self.batch_size = self.auto_batch()  # è‡ªåŠ¨æ‰¹æ¬¡å¤§å°

        # Dataloaders  # æ•°æ®åŠ è½½å™¨è®¾ç½®
        batch_size = self.batch_size // max(world_size, 1)  # è®¡ç®—æ¯ä¸ªè¿›ç¨‹çš„æ‰¹æ¬¡å¤§å°
        self.train_loader = self.get_dataloader(self.trainset, batch_size=batch_size, rank=LOCAL_RANK, mode="train")  # è·å–è®­ç»ƒæ•°æ®åŠ è½½å™¨
        if RANK in {-1, 0}:  # å¦‚æœæ˜¯ä¸»è¿›ç¨‹
            # Note: When training DOTA dataset, double batch size could get OOM on images with >2000 objects.  # æ³¨æ„ï¼šè®­ç»ƒDOTAæ•°æ®é›†æ—¶ï¼ŒåŒå€æ‰¹æ¬¡å¤§å°å¯èƒ½å¯¼è‡´å†…å­˜ä¸è¶³
            self.test_loader = self.get_dataloader(
                self.testset, batch_size=batch_size if self.args.task == "obb" else batch_size * 2, rank=-1, mode="val"  # è·å–æµ‹è¯•æ•°æ®åŠ è½½å™¨
            )
            self.validator = self.get_validator()  # è·å–éªŒè¯å™¨
            metric_keys = self.validator.metrics.keys + self.label_loss_items(prefix="val")  # è·å–æŒ‡æ ‡é”®
            self.metrics = dict(zip(metric_keys, [0] * len(metric_keys)))  # åˆå§‹åŒ–æŒ‡æ ‡å­—å…¸
            self.ema = ModelEMA(self.model)  # åˆå§‹åŒ–EMAæ¨¡å‹
            if self.args.plots:  # å¦‚æœéœ€è¦ç»˜å›¾
                self.plot_training_labels()  # ç»˜åˆ¶è®­ç»ƒæ ‡ç­¾

        # Optimizer  # ä¼˜åŒ–å™¨è®¾ç½®
        self.accumulate = max(round(self.args.nbs / self.batch_size), 1)  # accumulate loss before optimizing  # åœ¨ä¼˜åŒ–ä¹‹å‰ç´¯ç§¯æŸå¤±
        weight_decay = self.args.weight_decay * self.batch_size * self.accumulate / self.args.nbs  # scale weight_decay  # ç¼©æ”¾æƒé‡è¡°å‡
        iterations = math.ceil(len(self.train_loader.dataset) / max(self.batch_size, self.args.nbs)) * self.epochs  # è®¡ç®—è¿­ä»£æ¬¡æ•°
        self.optimizer = self.build_optimizer(
            model=self.model,  # æ¨¡å‹
            name=self.args.optimizer,  # ä¼˜åŒ–å™¨åç§°
            lr=self.args.lr0,  # åˆå§‹å­¦ä¹ ç‡
            momentum=self.args.momentum,  # åŠ¨é‡
            decay=weight_decay,  # æƒé‡è¡°å‡
            iterations=iterations,  # è¿­ä»£æ¬¡æ•°
        )
        # Scheduler  # å­¦ä¹ ç‡è°ƒåº¦å™¨è®¾ç½®
        self._setup_scheduler()  # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.stopper, self.stop = EarlyStopping(patience=self.args.patience), False  # åˆå§‹åŒ–æå‰åœæ­¢
        self.resume_training(ckpt)  # ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
        self.scheduler.last_epoch = self.start_epoch - 1  # do not move  # ä¸ç§»åŠ¨è°ƒåº¦å™¨çš„æœ€åè½®æ•°
        self.run_callbacks("on_pretrain_routine_end")  # è¿è¡Œé¢„è®­ç»ƒä¾‹ç¨‹ç»“æŸçš„å›è°ƒ

        
    def _do_train(self, world_size=1):
        """Train completed, evaluate and plot if specified by arguments."""
        # è®­ç»ƒå®Œæˆåï¼Œå¦‚æœå‚æ•°æŒ‡å®šï¼Œåˆ™è¿›è¡Œè¯„ä¼°å’Œç»˜å›¾ã€‚
        if world_size > 1:
            self._setup_ddp(world_size)  # å¦‚æœä¸–ç•Œå¤§å°å¤§äº1ï¼Œè®¾ç½®åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDDPï¼‰ã€‚
        self._setup_train(world_size)  # è®¾ç½®è®­ç»ƒç¯å¢ƒã€‚
    
        nb = len(self.train_loader)  # number of batches
        # æ‰¹æ¬¡æ•°é‡
        nw = max(round(self.args.warmup_epochs * nb), 100) if self.args.warmup_epochs > 0 else -1  # warmup iterations
        # è®¡ç®—é¢„çƒ­è¿­ä»£æ¬¡æ•°
        last_opt_step = -1  # ä¸Šä¸€æ¬¡ä¼˜åŒ–æ­¥éª¤
        self.epoch_time = None  # è®°å½•æ¯ä¸ªepochçš„æ—¶é—´
        self.epoch_time_start = time.time()  # è®°å½•å½“å‰epochå¼€å§‹æ—¶é—´
        self.train_time_start = time.time()  # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
        self.run_callbacks("on_train_start")  # è¿è¡Œè®­ç»ƒå¼€å§‹çš„å›è°ƒå‡½æ•°
        LOGGER.info(
            f"Image sizes {self.args.imgsz} train, {self.args.imgsz} val\n"
            f"Using {self.train_loader.num_workers * (world_size or 1)} dataloader workers\n"
            f"Logging results to {colorstr('bold', self.save_dir)}\n"
            f"Starting training for " + (f"{self.args.time} hours..." if self.args.time else f"{self.epochs} epochs...")
        )
        # è®°å½•å›¾åƒå¤§å°ã€æ•°æ®åŠ è½½å™¨å·¥ä½œçº¿ç¨‹æ•°é‡å’Œè®­ç»ƒå¼€å§‹ä¿¡æ¯
        if self.args.close_mosaic:
            base_idx = (self.epochs - self.args.close_mosaic) * nb  # è®¡ç®—åŸºç¡€ç´¢å¼•
            self.plot_idx.extend([base_idx, base_idx + 1, base_idx + 2])  # æ›´æ–°ç»˜å›¾ç´¢å¼•
        epoch = self.start_epoch  # ä»èµ·å§‹epochå¼€å§‹
        self.optimizer.zero_grad()  # zero any resumed gradients to ensure stability on train start
        # å°†ä»»ä½•æ¢å¤çš„æ¢¯åº¦å½’é›¶ï¼Œä»¥ç¡®ä¿è®­ç»ƒå¼€å§‹æ—¶çš„ç¨³å®šæ€§
        while True:
            self.epoch = epoch  # æ›´æ–°å½“å‰epoch
            self.run_callbacks("on_train_epoch_start")  # è¿è¡Œæ¯ä¸ªepochå¼€å§‹çš„å›è°ƒå‡½æ•°
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")  # suppress 'Detected lr_scheduler.step() before optimizer.step()'
                # æŠ‘åˆ¶è­¦å‘Šï¼šåœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹å‰æ£€æµ‹åˆ°å­¦ä¹ ç‡è°ƒåº¦å™¨æ­¥éª¤
                self.scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
    
            self.model.train()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
            if RANK != -1:
                self.train_loader.sampler.set_epoch(epoch)  # è®¾ç½®æ•°æ®åŠ è½½å™¨çš„é‡‡æ ·å™¨epoch
            pbar = enumerate(self.train_loader)  # éå†è®­ç»ƒæ•°æ®åŠ è½½å™¨
            # Update dataloader attributes (optional)
            if epoch == (self.epochs - self.args.close_mosaic):
                self._close_dataloader_mosaic()  # å…³é—­æ•°æ®åŠ è½½å™¨çš„é©¬èµ›å…‹
                self.train_loader.reset()  # é‡ç½®æ•°æ®åŠ è½½å™¨
    
            if RANK in {-1, 0}:
                LOGGER.info(self.progress_string())  # è®°å½•è¿›åº¦ä¿¡æ¯
                pbar = TQDM(enumerate(self.train_loader), total=nb)  # ä½¿ç”¨TQDMæ˜¾ç¤ºè¿›åº¦æ¡
            self.tloss = None  # åˆå§‹åŒ–æ€»æŸå¤±
            for i, batch in pbar:  # éå†æ¯ä¸ªæ‰¹æ¬¡
                self.run_callbacks("on_train_batch_start")  # è¿è¡Œæ¯ä¸ªæ‰¹æ¬¡å¼€å§‹çš„å›è°ƒå‡½æ•°
                # Warmup
                ni = i + nb * epoch  # å½“å‰è¿­ä»£æ¬¡æ•°
                if ni <= nw:
                    xi = [0, nw]  # x interp
                    self.accumulate = max(1, int(np.interp(ni, xi, [1, self.args.nbs / self.batch_size]).round()))
                    # è®¡ç®—ç´¯è®¡æ­¥éª¤
                    for j, x in enumerate(self.optimizer.param_groups):
                        # Bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0
                        x["lr"] = np.interp(
                            ni, xi, [self.args.warmup_bias_lr if j == 0 else 0.0, x["initial_lr"] * self.lf(epoch)]
                        )
                        # æ›´æ–°å­¦ä¹ ç‡
                        if "momentum" in x:
                            x["momentum"] = np.interp(ni, xi, [self.args.warmup_momentum, self.args.momentum])
                            # æ›´æ–°åŠ¨é‡
    
                # Forward
                with autocast(self.amp):  # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
                    batch = self.preprocess_batch(batch)  # é¢„å¤„ç†æ‰¹æ¬¡
                    self.loss, self.loss_items = self.model(batch)  # è®¡ç®—æŸå¤±
                    if RANK != -1:
                        self.loss *= world_size  # å¦‚æœæ˜¯åˆ†å¸ƒå¼è®­ç»ƒï¼ŒæŸå¤±ä¹˜ä»¥ä¸–ç•Œå¤§å°
                    self.tloss = (
                        (self.tloss * i + self.loss_items) / (i + 1) if self.tloss is not None else self.loss_items
                    )
                    # æ›´æ–°æ€»æŸå¤±
    
                # Backward
                self.scaler.scale(self.loss).backward()  # åå‘ä¼ æ’­
    
                # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html
                if ni - last_opt_step >= self.accumulate:
                    self.optimizer_step()  # æ‰§è¡Œä¼˜åŒ–æ­¥éª¤
                    last_opt_step = ni  # æ›´æ–°ä¸Šä¸€æ¬¡ä¼˜åŒ–æ­¥éª¤
    
                    # Timed stopping
                    if self.args.time:
                        self.stop = (time.time() - self.train_time_start) > (self.args.time * 3600)  # åˆ¤æ–­æ˜¯å¦è¶…æ—¶
                        if RANK != -1:  # if DDP training
                            broadcast_list = [self.stop if RANK == 0 else None]
                            dist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranks
                            # å°†åœæ­¢æ ‡å¿—å¹¿æ’­åˆ°æ‰€æœ‰è¿›ç¨‹
                            self.stop = broadcast_list[0]
                        if self.stop:  # training time exceeded
                            break  # è¶…è¿‡è®­ç»ƒæ—¶é—´ï¼Œé€€å‡ºå¾ªç¯
    
                # Log
                if RANK in {-1, 0}:  # è®°å½•æ—¥å¿—
                    loss_length = self.tloss.shape[0] if len(self.tloss.shape) else 1  # æ€»æŸå¤±çš„é•¿åº¦
                    pbar.set_description(
                        ("%11s" * 2 + "%11.4g" * (2 + loss_length))
                        % (
                            f"{epoch + 1}/{self.epochs}",  # å½“å‰epoch/æ€»epoch
                            f"{self._get_memory():.3g}G",  # (GB) GPUå†…å­˜ä½¿ç”¨
                            *(self.tloss if loss_length > 1 else torch.unsqueeze(self.tloss, 0)),  # æŸå¤±
                            batch["cls"].shape[0],  # æ‰¹æ¬¡å¤§å°ï¼Œä¾‹å¦‚8
                            batch["img"].shape[-1],  # å›¾åƒå¤§å°ï¼Œä¾‹å¦‚640
                        )
                    )
                    self.run_callbacks("on_batch_end")  # è¿è¡Œæ¯ä¸ªæ‰¹æ¬¡ç»“æŸçš„å›è°ƒå‡½æ•°
                    if self.args.plots and ni in self.plot_idx:
                        self.plot_training_samples(batch, ni)  # ç»˜åˆ¶è®­ç»ƒæ ·æœ¬
    
                self.run_callbacks("on_train_batch_end")  # è¿è¡Œæ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡ç»“æŸçš„å›è°ƒå‡½æ•°
    
            self.lr = {f"lr/pg{ir}": x["lr"] for ir, x in enumerate(self.optimizer.param_groups)}  # for loggers
            # è®°å½•å­¦ä¹ ç‡
            self.run_callbacks("on_train_epoch_end")  # è¿è¡Œæ¯ä¸ªepochç»“æŸçš„å›è°ƒå‡½æ•°
            if RANK in {-1, 0}:  # è¿›è¡ŒéªŒè¯
                final_epoch = epoch + 1 >= self.epochs  # åˆ¤æ–­æ˜¯å¦ä¸ºæœ€åä¸€ä¸ªepoch
                self.ema.update_attr(self.model, include=["yaml", "nc", "args", "names", "stride", "class_weights"])
                # æ›´æ–°æ¨¡å‹çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡
    
                # Validation
                if self.args.val or final_epoch or self.stopper.possible_stop or self.stop:
                    self.metrics, self.fitness = self.validate()  # è¿›è¡ŒéªŒè¯
                self.save_metrics(metrics={**self.label_loss_items(self.tloss), **self.metrics, **self.lr})  # ä¿å­˜æŒ‡æ ‡
                self.stop |= self.stopper(epoch + 1, self.fitness) or final_epoch  # åˆ¤æ–­æ˜¯å¦åœæ­¢è®­ç»ƒ
                if self.args.time:
                    self.stop |= (time.time() - self.train_time_start) > (self.args.time * 3600)  # è¶…è¿‡æ—¶é—´åœæ­¢
    
                # Save model
                if self.args.save or final_epoch:
                    self.save_model()  # ä¿å­˜æ¨¡å‹
                    self.run_callbacks("on_model_save")  # è¿è¡Œæ¨¡å‹ä¿å­˜çš„å›è°ƒå‡½æ•°
    
            # Scheduler
            t = time.time()  # è·å–å½“å‰æ—¶é—´
            self.epoch_time = t - self.epoch_time_start  # è®¡ç®—epochæ—¶é—´
            self.epoch_time_start = t  # æ›´æ–°epochå¼€å§‹æ—¶é—´
            if self.args.time:
                mean_epoch_time = (t - self.train_time_start) / (epoch - self.start_epoch + 1)  # è®¡ç®—å¹³å‡epochæ—¶é—´
                self.epochs = self.args.epochs = math.ceil(self.args.time * 3600 / mean_epoch_time)  # æ›´æ–°æ€»epochæ•°
                self._setup_scheduler()  # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
                self.scheduler.last_epoch = self.epoch  # ä¸ç§»åŠ¨
                self.stop |= epoch >= self.epochs  # è¶…è¿‡æ€»epochæ•°åœæ­¢
            self.run_callbacks("on_fit_epoch_end")  # è¿è¡Œæ¯ä¸ªfit epochç»“æŸçš„å›è°ƒå‡½æ•°
            self._clear_memory()  # æ¸…é™¤å†…å­˜
    
            # Early Stopping
            if RANK != -1:  # if DDP training
                broadcast_list = [self.stop if RANK == 0 else None]
                dist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranks
                # å°†åœæ­¢æ ‡å¿—å¹¿æ’­åˆ°æ‰€æœ‰è¿›ç¨‹
                self.stop = broadcast_list[0]
            if self.stop:
                break  # must break all DDP ranks
                # å¿…é¡»é€€å‡ºæ‰€æœ‰DDPè¿›ç¨‹
            epoch += 1  # å¢åŠ epochè®¡æ•°
    
        if RANK in {-1, 0}:  # å¦‚æœæ˜¯ä¸»è¿›ç¨‹
            # Do final val with best.pt
            seconds = time.time() - self.train_time_start  # è®¡ç®—è®­ç»ƒæ€»æ—¶é—´
            LOGGER.info(f"\n{epoch - self.start_epoch + 1} epochs completed in {seconds / 3600:.3f} hours.")
            # è®°å½•å®Œæˆçš„epochæ•°å’Œæ€»æ—¶é—´
            self.final_eval()  # æœ€ç»ˆè¯„ä¼°
            if self.args.plots:
                self.plot_metrics()  # ç»˜åˆ¶æŒ‡æ ‡
            self.run_callbacks("on_train_end")  # è¿è¡Œè®­ç»ƒç»“æŸçš„å›è°ƒå‡½æ•°
        self._clear_memory()  # æ¸…é™¤å†…å­˜
        unset_deterministic()  # å–æ¶ˆç¡®å®šæ€§è®¾ç½®
        self.run_callbacks("teardown")  # è¿è¡Œæ¸…ç†çš„å›è°ƒå‡½æ•°

    def auto_batch(self, max_num_obj=0):
        """Get batch size by calculating memory occupation of model."""
        # é€šè¿‡è®¡ç®—æ¨¡å‹çš„å†…å­˜å ç”¨æ¥è·å–æ‰¹æ¬¡å¤§å°ã€‚
        return check_train_batch_size(
            model=self.model,  # ä¼ å…¥æ¨¡å‹
            imgsz=self.args.imgsz,  # ä¼ å…¥å›¾åƒå¤§å°
            amp=self.amp,  # ä¼ å…¥è‡ªåŠ¨æ··åˆç²¾åº¦è®¾ç½®
            batch=self.batch_size,  # ä¼ å…¥å½“å‰æ‰¹æ¬¡å¤§å°
            max_num_obj=max_num_obj,  # ä¼ å…¥æœ€å¤§å¯¹è±¡æ•°
        )  # returns batch size

    def _get_memory(self):
        """Get accelerator memory utilization in GB."""
        # è·å–åŠ é€Ÿå™¨å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆå•ä½ï¼šGBï¼‰ã€‚
        if self.device.type == "mps":
            memory = torch.mps.driver_allocated_memory()  # å¦‚æœè®¾å¤‡æ˜¯MPSï¼Œè·å–åˆ†é…çš„å†…å­˜
        elif self.device.type == "cpu":
            memory = 0  # å¦‚æœè®¾å¤‡æ˜¯CPUï¼Œå†…å­˜ä¸º0
        else:
            memory = torch.cuda.memory_reserved()  # å¦åˆ™ï¼Œè·å–CUDAä¿ç•™çš„å†…å­˜
        return memory / 1e9  # å°†å†…å­˜è½¬æ¢ä¸ºGBå¹¶è¿”å›

    def _clear_memory(self):
        """Clear accelerator memory on different platforms."""
        # åœ¨ä¸åŒå¹³å°ä¸Šæ¸…é™¤åŠ é€Ÿå™¨å†…å­˜ã€‚
        gc.collect()  # åƒåœ¾å›æ”¶ï¼Œæ¸…ç†æœªä½¿ç”¨çš„å†…å­˜
        if self.device.type == "mps":
            torch.mps.empty_cache()  # å¦‚æœè®¾å¤‡æ˜¯MPSï¼Œæ¸…ç©ºç¼“å­˜
        elif self.device.type == "cpu":
            return  # å¦‚æœè®¾å¤‡æ˜¯CPUï¼Œä¸è¿›è¡Œæ“ä½œ
        else:
            torch.cuda.empty_cache()  # å¦åˆ™ï¼Œæ¸…ç©ºCUDAç¼“å­˜

    def read_results_csv(self):
        """Read results.csv into a dict using pandas."""
        # ä½¿ç”¨pandasè¯»å–results.csvå¹¶è½¬æ¢ä¸ºå­—å…¸ã€‚
        import pandas as pd  # scope for faster 'import ultralytics'

        return pd.read_csv(self.csv).to_dict(orient="list")  # è¯»å–CSVæ–‡ä»¶å¹¶è¿”å›å­—å…¸æ ¼å¼

    def save_model(self):
        """Save model training checkpoints with additional metadata."""
        # ä¿å­˜æ¨¡å‹è®­ç»ƒæ£€æŸ¥ç‚¹åŠé™„åŠ å…ƒæ•°æ®ã€‚
        import io

        # Serialize ckpt to a byte buffer once (faster than repeated torch.save() calls)
        # å°†æ£€æŸ¥ç‚¹åºåˆ—åŒ–åˆ°å­—èŠ‚ç¼“å†²åŒºä¸­ï¼ˆæ¯”é‡å¤è°ƒç”¨torch.save()æ›´å¿«ï¼‰
        buffer = io.BytesIO()  # åˆ›å»ºå­—èŠ‚ç¼“å†²åŒº
        torch.save(
            {
                "epoch": self.epoch,  # å½“å‰epoch
                "best_fitness": self.best_fitness,  # æœ€ä½³é€‚åº”åº¦
                "model": None,  # resume and final checkpoints derive from EMA
                # æ¢å¤å’Œæœ€ç»ˆæ£€æŸ¥ç‚¹æ¥è‡ªEMA
                "ema": deepcopy(self.ema.ema).half(),  # æ·±æ‹·è´EMAå¹¶è½¬æ¢ä¸ºåŠç²¾åº¦
                "updates": self.ema.updates,  # æ›´æ–°æ¬¡æ•°
                "optimizer": convert_optimizer_state_dict_to_fp16(deepcopy(self.optimizer.state_dict())),  # å°†ä¼˜åŒ–å™¨çŠ¶æ€å­—å…¸è½¬æ¢ä¸ºFP16
                "train_args": vars(self.args),  # å°†è®­ç»ƒå‚æ•°ä¿å­˜ä¸ºå­—å…¸
                "train_metrics": {**self.metrics, **{"fitness": self.fitness}},  # ä¿å­˜è®­ç»ƒæŒ‡æ ‡ï¼ŒåŒ…æ‹¬é€‚åº”åº¦
                "train_results": self.read_results_csv(),  # è¯»å–è®­ç»ƒç»“æœCSV
                "date": datetime.now().isoformat(),  # å½“å‰æ—¥æœŸæ—¶é—´
                "version": __version__,  # å½“å‰ç‰ˆæœ¬
                "license": "AGPL-3.0 (https://ultralytics.com/license)",  # è®¸å¯è¯ä¿¡æ¯
                "docs": "https://docs.ultralytics.com",  # æ–‡æ¡£é“¾æ¥
            },
            buffer,  # å°†æ•°æ®å†™å…¥ç¼“å†²åŒº
        )
        serialized_ckpt = buffer.getvalue()  # è·å–åºåˆ—åŒ–çš„å†…å®¹ä»¥ä¿å­˜

        # Save checkpoints
        # ä¿å­˜æ£€æŸ¥ç‚¹
        self.last.write_bytes(serialized_ckpt)  # ä¿å­˜last.pt
        if self.best_fitness == self.fitness:
            self.best.write_bytes(serialized_ckpt)  # ä¿å­˜best.pt
        if (self.save_period > 0) and (self.epoch % self.save_period == 0):
            (self.wdir / f"epoch{self.epoch}.pt").write_bytes(serialized_ckpt)  # ä¿å­˜å½“å‰epochï¼Œä¾‹å¦‚'epoch3.pt'
        # if self.args.close_mosaic and self.epoch == (self.epochs - self.args.close_mosaic - 1):
        #    (self.wdir / "last_mosaic.pt").write_bytes(serialized_ckpt)  # ä¿å­˜é©¬èµ›å…‹æ£€æŸ¥ç‚¹
    
    def get_dataset(self):
        """
        Get train, val path from data dict if it exists.
    
        Returns None if data format is not recognized.
        """
        # ä»æ•°æ®å­—å…¸ä¸­è·å–è®­ç»ƒå’ŒéªŒè¯è·¯å¾„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚
        try:
            if self.args.task == "classify":
                data = check_cls_dataset(self.args.data)  # æ£€æŸ¥åˆ†ç±»æ•°æ®é›†
            elif self.args.data.split(".")[-1] in {"yaml", "yml"} or self.args.task in {
                "detect",
                "segment",
                "pose",
                "obb",
            }:
                data = check_det_dataset(self.args.data)  # æ£€æŸ¥æ£€æµ‹æ•°æ®é›†
                if "yaml_file" in data:
                    self.args.data = data["yaml_file"]  # ç”¨äºéªŒè¯'yolo train data=url.zip'çš„ç”¨æ³•
        except Exception as e:
            raise RuntimeError(emojis(f"Dataset '{clean_url(self.args.data)}' error âŒ {e}")) from e
            # å¦‚æœå‘ç”Ÿå¼‚å¸¸ï¼ŒæŠ›å‡ºè¿è¡Œæ—¶é”™è¯¯å¹¶æ˜¾ç¤ºæ•°æ®é›†é”™è¯¯ä¿¡æ¯
        self.data = data  # å°†æ•°æ®é›†èµ‹å€¼ç»™å®ä¾‹å˜é‡
        return data["train"], data.get("val") or data.get("test")  # è¿”å›è®­ç»ƒå’ŒéªŒè¯ï¼ˆæˆ–æµ‹è¯•ï¼‰æ•°æ®è·¯å¾„
    
    def setup_model(self):
        """Load/create/download model for any task."""
        # åŠ è½½/åˆ›å»º/ä¸‹è½½æ¨¡å‹ä»¥é€‚åº”ä»»ä½•ä»»åŠ¡ã€‚
        if isinstance(self.model, torch.nn.Module):  # å¦‚æœæ¨¡å‹å·²åŠ è½½ï¼Œåˆ™æ— éœ€è®¾ç½®
            return
    
        cfg, weights = self.model, None  # åˆå§‹åŒ–é…ç½®å’Œæƒé‡
        ckpt = None  # åˆå§‹åŒ–æ£€æŸ¥ç‚¹
        if str(self.model).endswith(".pt"):
            weights, ckpt = attempt_load_one_weight(self.model)  # å°è¯•åŠ è½½æƒé‡
            cfg = weights.yaml  # è·å–é…ç½®
        elif isinstance(self.args.pretrained, (str, Path)):
            weights, _ = attempt_load_one_weight(self.args.pretrained)  # åŠ è½½é¢„è®­ç»ƒæƒé‡
        self.model = self.get_model(cfg=cfg, weights=weights, verbose=RANK == -1)  # è°ƒç”¨Model(cfg, weights)
        return ckpt  # è¿”å›æ£€æŸ¥ç‚¹
    
    def optimizer_step(self):
        """Perform a single step of the training optimizer with gradient clipping and EMA update."""
        # æ‰§è¡Œä¸€æ¬¡è®­ç»ƒä¼˜åŒ–å™¨æ­¥éª¤ï¼ŒåŒ…æ‹¬æ¢¯åº¦è£å‰ªå’ŒEMAæ›´æ–°ã€‚
        self.scaler.unscale_(self.optimizer)  # åç¼©æ”¾æ¢¯åº¦
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)  # è£å‰ªæ¢¯åº¦
        self.scaler.step(self.optimizer)  # æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤
        self.scaler.update()  # æ›´æ–°ç¼©æ”¾å™¨
        self.optimizer.zero_grad()  # å°†æ¢¯åº¦å½’é›¶
        if self.ema:
            self.ema.update(self.model)  # æ›´æ–°EMA
    
    def preprocess_batch(self, batch):
        """Allows custom preprocessing model inputs and ground truths depending on task type."""
        # æ ¹æ®ä»»åŠ¡ç±»å‹å…è®¸è‡ªå®šä¹‰é¢„å¤„ç†æ¨¡å‹è¾“å…¥å’ŒçœŸå®å€¼ã€‚
        return batch  # è¿”å›æœªä¿®æ”¹çš„æ‰¹æ¬¡
    
    def validate(self):
        """
        Runs validation on test set using self.validator.
    
        The returned dict is expected to contain "fitness" key.
        """
        # ä½¿ç”¨self.validatorå¯¹æµ‹è¯•é›†è¿›è¡ŒéªŒè¯ã€‚
        metrics = self.validator(self)  # è¿è¡ŒéªŒè¯å™¨
        fitness = metrics.pop("fitness", -self.loss.detach().cpu().numpy())  # å¦‚æœæœªæ‰¾åˆ°ï¼Œåˆ™ä½¿ç”¨æŸå¤±ä½œä¸ºé€‚åº”åº¦åº¦é‡
        if not self.best_fitness or self.best_fitness < fitness:
            self.best_fitness = fitness  # æ›´æ–°æœ€ä½³é€‚åº”åº¦
        return metrics, fitness  # è¿”å›æŒ‡æ ‡å’Œé€‚åº”åº¦
    
    def get_model(self, cfg=None, weights=None, verbose=True):
        """Get model and raise NotImplementedError for loading cfg files."""
        # è·å–æ¨¡å‹ï¼Œå¦‚æœåŠ è½½cfgæ–‡ä»¶ï¼Œåˆ™å¼•å‘NotImplementedErrorã€‚
        raise NotImplementedError("This task trainer doesn't support loading cfg files")
    
    def get_validator(self):
        """Returns a NotImplementedError when the get_validator function is called."""
        # å½“è°ƒç”¨get_validatorå‡½æ•°æ—¶è¿”å›NotImplementedErrorã€‚
        raise NotImplementedError("get_validator function not implemented in trainer")
    
    def get_dataloader(self, dataset_path, batch_size=16, rank=0, mode="train"):
        """Returns dataloader derived from torch.data.Dataloader."""
        # è¿”å›ä»torch.data.Dataloaderæ´¾ç”Ÿçš„æ•°æ®åŠ è½½å™¨ã€‚
        raise NotImplementedError("get_dataloader function not implemented in trainer")
    
    def build_dataset(self, img_path, mode="train", batch=None):
        """Build dataset."""
        # æ„å»ºæ•°æ®é›†ã€‚
        raise NotImplementedError("build_dataset function not implemented in trainer")
    
    def label_loss_items(self, loss_items=None, prefix="train"):
        """
        Returns a loss dict with labelled training loss items tensor.
    
        Note:
            This is not needed for classification but necessary for segmentation & detection
        """
        # è¿”å›å¸¦æœ‰æ ‡è®°çš„è®­ç»ƒæŸå¤±é¡¹å¼ é‡çš„æŸå¤±å­—å…¸ã€‚
        return {"loss": loss_items} if loss_items is not None else ["loss"]  # å¦‚æœæä¾›äº†æŸå¤±é¡¹ï¼Œåˆ™è¿”å›å­—å…¸ï¼Œå¦åˆ™è¿”å›æŸå¤±åˆ—è¡¨
    
    def set_model_attributes(self):
        """To set or update model parameters before training."""
        # åœ¨è®­ç»ƒä¹‹å‰è®¾ç½®æˆ–æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
        self.model.names = self.data["names"]  # è®¾ç½®æ¨¡å‹çš„ç±»åˆ«åç§°
    
    def build_targets(self, preds, targets):
        """Builds target tensors for training YOLO model."""
        # ä¸ºè®­ç»ƒYOLOæ¨¡å‹æ„å»ºç›®æ ‡å¼ é‡ã€‚
        pass  # å…·ä½“å®ç°å¾…å®š
    
    def progress_string(self):
        """Returns a string describing training progress."""
        # è¿”å›æè¿°è®­ç»ƒè¿›åº¦çš„å­—ç¬¦ä¸²ã€‚
        return ""  # è¿”å›ç©ºå­—ç¬¦ä¸²
    
    
    # TODO: may need to put these following functions into callback
    def plot_training_samples(self, batch, ni):
        """Plots training samples during YOLO training."""
        # åœ¨YOLOè®­ç»ƒæœŸé—´ç»˜åˆ¶è®­ç»ƒæ ·æœ¬ã€‚
        pass  # å…·ä½“å®ç°å¾…å®š
    
    def plot_training_labels(self):
        """Plots training labels for YOLO model."""
        # ç»˜åˆ¶YOLOæ¨¡å‹çš„è®­ç»ƒæ ‡ç­¾ã€‚
        pass  # å…·ä½“å®ç°å¾…å®š
    
    def save_metrics(self, metrics):
        """Saves training metrics to a CSV file."""
        # å°†è®­ç»ƒæŒ‡æ ‡ä¿å­˜åˆ°CSVæ–‡ä»¶ä¸­ã€‚
        keys, vals = list(metrics.keys()), list(metrics.values())  # è·å–æŒ‡æ ‡çš„é”®å’Œå€¼
        n = len(metrics) + 2  # åˆ—æ•°
        s = "" if self.csv.exists() else (("%s," * n % tuple(["epoch", "time"] + keys)).rstrip(",") + "\n")  # è¡¨å¤´
        t = time.time() - self.train_time_start  # è®¡ç®—è®­ç»ƒæ—¶é—´
        with open(self.csv, "a") as f:
            f.write(s + ("%.6g," * n % tuple([self.epoch + 1, t] + vals)).rstrip(",") + "\n")  # å†™å…¥CSVæ–‡ä»¶
    
    def plot_metrics(self):
        """Plot and display metrics visually."""
        # å¯è§†åŒ–ç»˜åˆ¶å’Œæ˜¾ç¤ºæŒ‡æ ‡ã€‚
        pass  # å…·ä½“å®ç°å¾…å®š
    
    def on_plot(self, name, data=None):
        """Registers plots (e.g. to be consumed in callbacks)."""
        # æ³¨å†Œç»˜å›¾ï¼ˆä¾‹å¦‚ï¼Œåœ¨å›è°ƒä¸­ä½¿ç”¨ï¼‰ã€‚
        path = Path(name)  # å°†åç§°è½¬æ¢ä¸ºè·¯å¾„å¯¹è±¡
        self.plots[path] = {"data": data, "timestamp": time.time()}  # å­˜å‚¨ç»˜å›¾æ•°æ®å’Œæ—¶é—´æˆ³
    
    def final_eval(self):
        """Performs final evaluation and validation for object detection YOLO model."""
        # å¯¹YOLOæ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°å’ŒéªŒè¯ã€‚
        ckpt = {}  # åˆå§‹åŒ–æ£€æŸ¥ç‚¹å­—å…¸
        for f in self.last, self.best:  # éå†æœ€åå’Œæœ€ä½³æ£€æŸ¥ç‚¹
            if f.exists():  # å¦‚æœæ£€æŸ¥ç‚¹å­˜åœ¨
                if f is self.last:
                    ckpt = strip_optimizer(f)  # ä»æœ€åæ£€æŸ¥ç‚¹ä¸­å»é™¤ä¼˜åŒ–å™¨ä¿¡æ¯
                elif f is self.best:
                    k = "train_results"  # ä»last.ptæ›´æ–°best.ptçš„è®­ç»ƒæŒ‡æ ‡
                    strip_optimizer(f, updates={k: ckpt[k]} if k in ckpt else None)  # æ›´æ–°æœ€ä½³æ£€æŸ¥ç‚¹
                    LOGGER.info(f"\nValidating {f}...")  # è®°å½•æ­£åœ¨éªŒè¯çš„æ£€æŸ¥ç‚¹
                    self.validator.args.plots = self.args.plots  # è®¾ç½®éªŒè¯å™¨çš„ç»˜å›¾å‚æ•°
                    self.metrics = self.validator(model=f)  # è¿è¡ŒéªŒè¯å™¨å¹¶è·å–æŒ‡æ ‡
                    self.metrics.pop("fitness", None)  # ç§»é™¤é€‚åº”åº¦æŒ‡æ ‡
                    self.run_callbacks("on_fit_epoch_end")  # è¿è¡Œæ¯ä¸ªfit epochç»“æŸçš„å›è°ƒå‡½æ•°
    
    def check_resume(self, overrides):
        """Check if resume checkpoint exists and update arguments accordingly."""
        # æ£€æŸ¥æ¢å¤æ£€æŸ¥ç‚¹æ˜¯å¦å­˜åœ¨ï¼Œå¹¶ç›¸åº”æ›´æ–°å‚æ•°ã€‚
        resume = self.args.resume  # è·å–æ¢å¤å‚æ•°
        if resume:
            try:
                exists = isinstance(resume, (str, Path)) and Path(resume).exists()  # æ£€æŸ¥æ¢å¤è·¯å¾„æ˜¯å¦å­˜åœ¨
                last = Path(check_file(resume) if exists else get_latest_run())  # è·å–æœ€åçš„æ£€æŸ¥ç‚¹è·¯å¾„
    
                # Check that resume data YAML exists, otherwise strip to force re-download of dataset
                # æ£€æŸ¥æ¢å¤æ•°æ®çš„YAMLæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦åˆ™å¼ºåˆ¶é‡æ–°ä¸‹è½½æ•°æ®é›†
                ckpt_args = attempt_load_weights(last).args  # å°è¯•åŠ è½½æœ€åæ£€æŸ¥ç‚¹çš„å‚æ•°
                if not Path(ckpt_args["data"]).exists():  # æ£€æŸ¥æ•°æ®è·¯å¾„æ˜¯å¦å­˜åœ¨
                    ckpt_args["data"] = self.args.data  # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨å½“å‰æ•°æ®è·¯å¾„
    
                resume = True  # è®¾ç½®æ¢å¤æ ‡å¿—ä¸ºTrue
                self.args = get_cfg(ckpt_args)  # æ›´æ–°å‚æ•°
                self.args.model = self.args.resume = str(last)  # æ¢å¤æ¨¡å‹è·¯å¾„
                for k in (
                    "imgsz",
                    "batch",
                    "device",
                    "close_mosaic",
                ):  # å…è®¸å‚æ•°æ›´æ–°ä»¥å‡å°‘å†…å­˜æˆ–åœ¨æ¢å¤æ—¶æ›´æ–°è®¾å¤‡
                    if k in overrides:
                        setattr(self.args, k, overrides[k])  # æ›´æ–°å‚æ•°
    
            except Exception as e:
                raise FileNotFoundError(
                    "Resume checkpoint not found. Please pass a valid checkpoint to resume from, "
                    "i.e. 'yolo train resume model=path/to/last.pt'"
                ) from e  # å¦‚æœå‘ç”Ÿå¼‚å¸¸ï¼ŒæŠ›å‡ºæ–‡ä»¶æœªæ‰¾åˆ°é”™è¯¯
        self.resume = resume  # æ›´æ–°æ¢å¤æ ‡å¿—
    
    def resume_training(self, ckpt):
        """Resume YOLO training from given epoch and best fitness."""
        # ä»ç»™å®šçš„epochå’Œæœ€ä½³é€‚åº”åº¦æ¢å¤YOLOè®­ç»ƒã€‚
        if ckpt is None or not self.resume:  # å¦‚æœæ£€æŸ¥ç‚¹ä¸ºç©ºæˆ–ä¸éœ€è¦æ¢å¤
            return
        best_fitness = 0.0  # åˆå§‹åŒ–æœ€ä½³é€‚åº”åº¦
        start_epoch = ckpt.get("epoch", -1) + 1  # è·å–èµ·å§‹epoch
        if ckpt.get("optimizer", None) is not None:
            self.optimizer.load_state_dict(ckpt["optimizer"])  # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            best_fitness = ckpt["best_fitness"]  # è·å–æœ€ä½³é€‚åº”åº¦
        if self.ema and ckpt.get("ema"):
            self.ema.ema.load_state_dict(ckpt["ema"].float().state_dict())  # åŠ è½½EMAçŠ¶æ€
            self.ema.updates = ckpt["updates"]  # æ›´æ–°EMAæ¬¡æ•°
        assert start_epoch > 0, (
            f"{self.args.model} training to {self.epochs} epochs is finished, nothing to resume.\n"
            f"Start a new training without resuming, i.e. 'yolo train model={self.args.model}'"
        )  # ç¡®ä¿èµ·å§‹epochå¤§äº0
        LOGGER.info(f"Resuming training {self.args.model} from epoch {start_epoch + 1} to {self.epochs} total epochs")
        # è®°å½•æ¢å¤è®­ç»ƒçš„ä¿¡æ¯
        if self.epochs < start_epoch:
            LOGGER.info(
                f"{self.model} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {self.epochs} more epochs."
            )  # è®°å½•å·²è®­ç»ƒçš„epochæ•°
            self.epochs += ckpt["epoch"]  # å¢åŠ æ€»epochæ•°
        self.best_fitness = best_fitness  # æ›´æ–°æœ€ä½³é€‚åº”åº¦
        self.start_epoch = start_epoch  # è®¾ç½®èµ·å§‹epoch
        if start_epoch > (self.epochs - self.args.close_mosaic):
            self._close_dataloader_mosaic()  # å…³é—­æ•°æ®åŠ è½½å™¨çš„é©¬èµ›å…‹å¢å¼º
    
    def _close_dataloader_mosaic(self):
        """Update dataloaders to stop using mosaic augmentation."""
        # æ›´æ–°æ•°æ®åŠ è½½å™¨ä»¥åœæ­¢ä½¿ç”¨é©¬èµ›å…‹å¢å¼ºã€‚
        if hasattr(self.train_loader.dataset, "mosaic"):
            self.train_loader.dataset.mosaic = False  # å…³é—­é©¬èµ›å…‹å¢å¼º
        if hasattr(self.train_loader.dataset, "close_mosaic"):
            LOGGER.info("Closing dataloader mosaic")  # è®°å½•å…³é—­é©¬èµ›å…‹çš„æ“ä½œ
            self.train_loader.dataset.close_mosaic(hyp=copy(self.args))  # å…³é—­é©¬èµ›å…‹
    
    def build_optimizer(self, model, name="auto", lr=0.001, momentum=0.9, decay=1e-5, iterations=1e5):
        """
        Constructs an optimizer for the given model, based on the specified optimizer name, learning rate, momentum,
        weight decay, and number of iterations.
    
        Args:
            model (torch.nn.Module): The model for which to build an optimizer.
            name (str, optional): The name of the optimizer to use. If 'auto', the optimizer is selected
                based on the number of iterations. Default: 'auto'.
            lr (float, optional): The learning rate for the optimizer. Default: 0.001.
            momentum (float, optional): The momentum factor for the optimizer. Default: 0.9.
            decay (float, optional): The weight decay for the optimizer. Default: 1e-5.
            iterations (float, optional): The number of iterations, which determines the optimizer if
                name is 'auto'. Default: 1e5.
    
        Returns:
            (torch.optim.Optimizer): The constructed optimizer.
        """
        # æ ¹æ®æŒ‡å®šçš„ä¼˜åŒ–å™¨åç§°ã€å­¦ä¹ ç‡ã€åŠ¨é‡ã€æƒé‡è¡°å‡å’Œè¿­ä»£æ¬¡æ•°æ„é€ ç»™å®šæ¨¡å‹çš„ä¼˜åŒ–å™¨ã€‚
        g = [], [], []  # ä¼˜åŒ–å™¨å‚æ•°ç»„
        bn = tuple(v for k, v in nn.__dict__.items() if "Norm" in k)  # å½’ä¸€åŒ–å±‚ï¼Œä¾‹å¦‚BatchNorm2d()
        if name == "auto":
            LOGGER.info(
                f"{colorstr('optimizer:')} 'optimizer=auto' found, "
                f"ignoring 'lr0={self.args.lr0}' and 'momentum={self.args.momentum}' and "
                f"determining best 'optimizer', 'lr0' and 'momentum' automatically... "
            )  # è®°å½•è‡ªåŠ¨é€‰æ‹©ä¼˜åŒ–å™¨çš„ä¿¡æ¯
            nc = self.data.get("nc", 10)  # ç±»åˆ«æ•°é‡
            lr_fit = round(0.002 * 5 / (4 + nc), 6)  # lr0æ‹Ÿåˆæ–¹ç¨‹ï¼Œä¿ç•™6ä½å°æ•°
            name, lr, momentum = ("SGD", 0.01, 0.9) if iterations > 10000 else ("AdamW", lr_fit, 0.9)  # æ ¹æ®è¿­ä»£æ¬¡æ•°é€‰æ‹©ä¼˜åŒ–å™¨
            self.args.warmup_bias_lr = 0.0  # å¯¹äºAdamï¼Œåç½®ä¸é«˜äº0.01
    
        for module_name, module in model.named_modules():  # éå†æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—
            for param_name, param in module.named_parameters(recurse=False):  # éå†æ¨¡å—çš„å‚æ•°
                fullname = f"{module_name}.{param_name}" if module_name else param_name  # å®Œæ•´å‚æ•°å
                if "bias" in fullname:  # åç½®ï¼ˆä¸è¡°å‡ï¼‰
                    g[2].append(param)  # æ·»åŠ åˆ°åç½®ç»„
                elif isinstance(module, bn):  # æƒé‡ï¼ˆä¸è¡°å‡ï¼‰
                    g[1].append(param)  # æ·»åŠ åˆ°å½’ä¸€åŒ–å±‚æƒé‡ç»„
                else:  # æƒé‡ï¼ˆè¡°å‡ï¼‰
                    g[0].append(param)  # æ·»åŠ åˆ°æƒé‡ç»„
    
        optimizers = {"Adam", "Adamax", "AdamW", "NAdam", "RAdam", "RMSProp", "SGD", "auto"}  # å¯ç”¨çš„ä¼˜åŒ–å™¨åˆ—è¡¨
        name = {x.lower(): x for x in optimizers}.get(name.lower())  # å°†ä¼˜åŒ–å™¨åç§°è½¬æ¢ä¸ºå°å†™
        if name in {"Adam", "Adamax", "AdamW", "NAdam", "RAdam"}:
            optimizer = getattr(optim, name, optim.Adam)(g[2], lr=lr, betas=(momentum, 0.999), weight_decay=0.0)  # åˆ›å»ºAdamç³»åˆ—ä¼˜åŒ–å™¨
        elif name == "RMSProp":
            optimizer = optim.RMSprop(g[2], lr=lr, momentum=momentum)  # åˆ›å»ºRMSPropä¼˜åŒ–å™¨
        elif name == "SGD":
            optimizer = optim.SGD(g[2], lr=lr, momentum=momentum, nesterov=True)  # åˆ›å»ºSGDä¼˜åŒ–å™¨
        else:
            raise NotImplementedError(
                f"Optimizer '{name}' not found in list of available optimizers {optimizers}. "
                "Request support for addition optimizers at https://github.com/ultralytics/ultralytics."
            )  # å¦‚æœæœªæ‰¾åˆ°ä¼˜åŒ–å™¨ï¼Œåˆ™å¼•å‘NotImplementedError
    
        optimizer.add_param_group({"params": g[0], "weight_decay": decay})  # æ·»åŠ g0ç»„ï¼ˆå¸¦æƒé‡è¡°å‡ï¼‰
        optimizer.add_param_group({"params": g[1], "weight_decay": 0.0})  # æ·»åŠ g1ç»„ï¼ˆBatchNorm2dæƒé‡ï¼Œä¸è¡°å‡ï¼‰
        LOGGER.info(
            f"{colorstr('optimizer:')} {type(optimizer).__name__}(lr={lr}, momentum={momentum}) with parameter groups "
            f"{len(g[1])} weight(decay=0.0), {len(g[0])} weight(decay={decay}), {len(g[2])} bias(decay=0.0)"
        )  # è®°å½•ä¼˜åŒ–å™¨ä¿¡æ¯
        return optimizer  # è¿”å›æ„é€ çš„ä¼˜åŒ–å™¨