# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import math
import random
from copy import copy

import numpy as np
import torch.nn as nn

from ultralytics.data import build_dataloader, build_yolo_dataset
from ultralytics.engine.trainer import BaseTrainer
from ultralytics.models import yolo
from ultralytics.nn.tasks import DetectionModel
from ultralytics.utils import LOGGER, RANK
from ultralytics.utils.plotting import plot_images, plot_labels, plot_results
from ultralytics.utils.torch_utils import de_parallel, torch_distributed_zero_first

class DetectionTrainer(BaseTrainer):
    """
    A class extending the BaseTrainer class for training based on a detection model.

    Example:
        ```python
        from ultralytics.models.yolo.detect import DetectionTrainer

        args = dict(model="yolo11n.pt", data="coco8.yaml", epochs=3)
        trainer = DetectionTrainer(overrides=args)
        trainer.train()
        ```
    """
    # ç»§æ‰¿è‡ªBaseTrainerçš„ç›®æ ‡æ£€æµ‹æ¨¡å‹è®­ç»ƒå™¨ç±»
    # ä¸“é—¨ç”¨äºYOLOç›®æ ‡æ£€æµ‹æ¨¡å‹çš„è®­ç»ƒ

    def build_dataset(self, img_path, mode="train", batch=None):
        """
        Build YOLO Dataset.

        Args:
            img_path (str): Path to the folder containing images.
            mode (str): [train](cci:1://file:///Users/gatilin/MyWork/Source-Code-Reading1/source/YOLO/ultralytics-8.3.74/ultralytics/models/yolo/classify/train.py:316:4-326:9) mode or [val](cci:1://file:///Users/gatilin/MyWork/Source-Code-Reading1/source/YOLO/ultralytics-8.3.74/ultralytics/models/yolo/classify/train.py:287:4-314:58) mode, users are able to customize different augmentations for each mode.
            batch (int, optional): Size of batches, this is for `rect`. Defaults to None.
        """
        # æ„å»ºYOLOæ•°æ®é›†
        
        # è®¡ç®—æ¨¡å‹çš„æœ€å¤§æ­¥é•¿ï¼Œç¡®ä¿æœ€å°ä¸º32
        gs = max(
            int(de_parallel(self.model).stride.max() if self.model else 0), 
            32
        )
        
        # ä½¿ç”¨build_yolo_datasetæ„å»ºæ•°æ®é›†
        return build_yolo_dataset(
            self.args,        # å‚æ•°é…ç½®
            img_path,         # å›¾åƒè·¯å¾„
            batch,            # æ‰¹æ¬¡å¤§å°
            self.data,        # æ•°æ®é…ç½®
            mode=mode,        # æ¨¡å¼ï¼ˆè®­ç»ƒ/éªŒè¯ï¼‰
            rect=mode == "val",  # æ˜¯å¦ä½¿ç”¨çŸ©å½¢è®­ç»ƒï¼ˆéªŒè¯æ¨¡å¼ï¼‰
            stride=gs         # æ­¥é•¿
        )

    def get_dataloader(self, dataset_path, batch_size=16, rank=0, mode="train"):
        """Construct and return dataloader."""
        # æ„å»ºå¹¶è¿”å›æ•°æ®åŠ è½½å™¨
        
        # ç¡®ä¿æ¨¡å¼åªèƒ½æ˜¯è®­ç»ƒæˆ–éªŒè¯
        assert mode in {"train", "val"}, f"Mode must be 'train' or 'val', not {mode}."
        
        # ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒçš„é›¶å·è¿›ç¨‹é¦–æ¬¡åˆå§‹åŒ–æ•°æ®é›†ç¼“å­˜
        with torch_distributed_zero_first(rank):
            dataset = self.build_dataset(dataset_path, mode, batch_size)
        
        # ç¡®å®šæ˜¯å¦éœ€è¦æ‰“ä¹±æ•°æ®
        shuffle = mode == "train"
        
        # å¦‚æœä½¿ç”¨çŸ©å½¢è®­ç»ƒä¸”éœ€è¦æ‰“ä¹±ï¼Œå‘å‡ºè­¦å‘Šå¹¶ç¦ç”¨æ‰“ä¹±
        if getattr(dataset, "rect", False) and shuffle:
            LOGGER.warning("WARNING âš ï¸ 'rect=True' is incompatible with DataLoader shuffle, setting shuffle=False")
            shuffle = False
        
        # è®¾ç½®å·¥ä½œè¿›ç¨‹æ•°ï¼ˆéªŒè¯æ¨¡å¼ä¸‹å¢åŠ å·¥ä½œè¿›ç¨‹ï¼‰
        workers = self.args.workers if mode == "train" else self.args.workers * 2
        
        # æ„å»ºå¹¶è¿”å›æ•°æ®åŠ è½½å™¨
        return build_dataloader(
            dataset,     # æ•°æ®é›†
            batch_size,  # æ‰¹æ¬¡å¤§å°
            workers,     # å·¥ä½œè¿›ç¨‹æ•°
            shuffle,     # æ˜¯å¦æ‰“ä¹±
            rank         # åˆ†å¸ƒå¼è®­ç»ƒç­‰çº§
        )

    def preprocess_batch(self, batch):
        """Preprocesses a batch of images by scaling and converting to float."""
        # é¢„å¤„ç†å›¾åƒæ‰¹æ¬¡
        
        # å°†å›¾åƒç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡å¹¶å½’ä¸€åŒ–ï¼ˆé™¤ä»¥255ï¼‰
        batch["img"] = batch["img"].to(self.device, non_blocking=True).float() / 255
        
        # å¦‚æœå¯ç”¨å¤šå°ºåº¦è®­ç»ƒ
        if self.args.multi_scale:
            imgs = batch["img"]
            
            # éšæœºç”Ÿæˆæ–°çš„å›¾åƒå°ºå¯¸
            sz = (
                random.randrange(
                    int(self.args.imgsz * 0.5),      # æœ€å°å°ºå¯¸
                    int(self.args.imgsz * 1.5 + self.stride)  # æœ€å¤§å°ºå¯¸
                ) // self.stride * self.stride       # ç¡®ä¿å°ºå¯¸æ˜¯æ­¥é•¿çš„å€æ•°
            )
            
            # è®¡ç®—ç¼©æ”¾å› å­
            sf = sz / max(imgs.shape[2:])
            
            # å¦‚æœéœ€è¦ç¼©æ”¾
            if sf != 1:
                # è®¡ç®—æ–°çš„å›¾åƒå°ºå¯¸ï¼ˆç¡®ä¿æ˜¯æ­¥é•¿çš„å€æ•°ï¼‰
                ns = [
                    math.ceil(x * sf / self.stride) * self.stride 
                    for x in imgs.shape[2:]
                ]
                
                # ä½¿ç”¨åŒçº¿æ€§æ’å€¼è°ƒæ•´å›¾åƒå¤§å°
                imgs = nn.functional.interpolate(
                    imgs, 
                    size=ns, 
                    mode="bilinear", 
                    align_corners=False
                )
            
            # æ›´æ–°æ‰¹æ¬¡ä¸­çš„å›¾åƒ
            batch["img"] = imgs
        
        return batch

    def set_model_attributes(self):
        """Nl = de_parallel(self.model).model[-1].nl  # number of detection layers (to scale hyps)."""
        # è®¾ç½®æ¨¡å‹å±æ€§çš„æ–¹æ³•
        # æ³¨é‡Šéƒ¨åˆ†æ˜¯å…³äºç¼©æ”¾è¶…å‚æ•°çš„å¯èƒ½å®ç°ï¼ˆå½“å‰æœªå¯ç”¨ï¼‰
        # self.args.box *= 3 / nl  # scale to layers
        # self.args.cls *= self.data["nc"] / 80 * 3 / nl  # scale to classes and layers
        # self.args.cls *= (self.args.imgsz / 640) ** 2 * 3 / nl  # scale to image size and layers
        
        # è®¾ç½®æ¨¡å‹çš„ç±»åˆ«æ•°é‡
        self.model.nc = self.data["nc"]
        
        # è®¾ç½®æ¨¡å‹çš„ç±»åˆ«åç§°
        self.model.names = self.data["names"]
        
        # å°†è®­ç»ƒå‚æ•°é™„åŠ åˆ°æ¨¡å‹
        self.model.args = self.args
        
        # TODO: æœªæ¥å¯èƒ½å®ç°çš„ç±»åˆ«æƒé‡è®¡ç®—
        # TODO: self.model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc
    
    def get_model(self, cfg=None, weights=None, verbose=True):
        """Return a YOLO detection model."""
        # åˆ›å»ºå¹¶è¿”å›YOLOæ£€æµ‹æ¨¡å‹
        
        # ä½¿ç”¨DetectionModelåˆ›å»ºæ¨¡å‹
        # ä¼ å…¥é…ç½®ã€ç±»åˆ«æ•°é‡å’Œè¯¦ç»†ä¿¡æ¯æ ‡å¿—
        model = DetectionModel(
            cfg, 
            nc=self.data["nc"],  # ç±»åˆ«æ•°é‡
            verbose=verbose and RANK == -1  # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        )
        
        # å¦‚æœæä¾›æƒé‡ï¼ŒåŠ è½½é¢„è®­ç»ƒæƒé‡
        if weights:
            model.load(weights)
        
        return model
    
    def get_validator(self):
        """Returns a DetectionValidator for YOLO model validation."""
        # è¿”å›YOLOæ¨¡å‹éªŒè¯å™¨
        
        # è®¾ç½®æŸå¤±åç§°ï¼ˆè¾¹ç•Œæ¡†æŸå¤±ã€ç±»åˆ«æŸå¤±ã€åˆ†å¸ƒæŸå¤±ï¼‰
        self.loss_names = "box_loss", "cls_loss", "dfl_loss"
        
        # åˆ›å»ºå¹¶è¿”å›æ£€æµ‹éªŒè¯å™¨
        return yolo.detect.DetectionValidator(
            self.test_loader,        # æµ‹è¯•æ•°æ®åŠ è½½å™¨
            save_dir=self.save_dir,  # ä¿å­˜ç›®å½•
            args=copy(self.args),    # å‚æ•°å‰¯æœ¬
            _callbacks=self.callbacks  # å›è°ƒå‡½æ•°
        )
    
    def label_loss_items(self, loss_items=None, prefix="train"):
        """
        Returns a loss dict with labelled training loss items tensor.
    
        Not needed for classification but necessary for segmentation & detection
        """
        # ä¸ºæŸå¤±é¡¹æ·»åŠ æ ‡ç­¾
        # å¯¹åˆ†ç±»ä¸éœ€è¦ï¼Œä½†å¯¹åˆ†å‰²å’Œæ£€æµ‹ä»»åŠ¡å¾ˆé‡è¦
        
        # ä¸ºæŸå¤±åç§°æ·»åŠ å‰ç¼€
        keys = [f"{prefix}/{x}" for x in self.loss_names]
        
        # å¦‚æœæä¾›æŸå¤±é¡¹
        if loss_items is not None:
            # å°†æŸå¤±é¡¹è½¬æ¢ä¸º5ä½å°æ•°çš„æµ®ç‚¹æ•°
            loss_items = [round(float(x), 5) for x in loss_items]
            # åˆ›å»ºæŸå¤±å­—å…¸
            return dict(zip(keys, loss_items))
        else:
            # å¦‚æœæ²¡æœ‰æŸå¤±é¡¹ï¼Œè¿”å›é”®åˆ—è¡¨
            return keys
    
    def progress_string(self):
        """Returns a formatted string of training progress with epoch, GPU memory, loss, instances and size."""
        # è¿”å›è®­ç»ƒè¿›åº¦çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
        return ("\n" + "%11s" * (4 + len(self.loss_names))) % (
            "Epoch",        # è®­ç»ƒè½®æ¬¡
            "GPU_mem",      # GPUå†…å­˜
            *self.loss_names,  # æŸå¤±åç§°
            "Instances",    # è®­ç»ƒå®ä¾‹æ•°
            "Size",         # å›¾åƒå¤§å°
        )
    
    def plot_training_samples(self, batch, ni):
        """Plots training samples with their annotations."""
        # ç»˜åˆ¶å¸¦æ³¨é‡Šçš„è®­ç»ƒæ ·æœ¬
        plot_images(
            images=batch["img"],             # è¾“å…¥å›¾åƒ
            batch_idx=batch["batch_idx"],    # æ‰¹æ¬¡ç´¢å¼•
            cls=batch["cls"].squeeze(-1),    # ç±»åˆ«æ ‡ç­¾
            bboxes=batch["bboxes"],          # è¾¹ç•Œæ¡†
            paths=batch["im_file"],          # å›¾åƒæ–‡ä»¶è·¯å¾„
            fname=self.save_dir / f"train_batch{ni}.jpg",  # ä¿å­˜æ–‡ä»¶å
            on_plot=self.on_plot,            # ç»˜å›¾å›è°ƒå‡½æ•°
        )
    
    def plot_metrics(self):
        """Plots metrics from a CSV file."""
        # ä»CSVæ–‡ä»¶ç»˜åˆ¶æŒ‡æ ‡
        plot_results(file=self.csv, on_plot=self.on_plot)  # save results.png
    
    def plot_training_labels(self):
        """Create a labeled training plot of the YOLO model."""
        # åˆ›å»ºYOLOæ¨¡å‹çš„æ ‡ç­¾è®­ç»ƒå›¾
        
        # æ”¶é›†æ‰€æœ‰è¾¹ç•Œæ¡†
        boxes = np.concatenate([lb["bboxes"] for lb in self.train_loader.dataset.labels], 0)
        
        # æ”¶é›†æ‰€æœ‰ç±»åˆ«
        cls = np.concatenate([lb["cls"] for lb in self.train_loader.dataset.labels], 0)
        
        # ç»˜åˆ¶æ ‡ç­¾åˆ†å¸ƒå›¾
        plot_labels(
            boxes, 
            cls.squeeze(), 
            names=self.data["names"],    # ç±»åˆ«åç§°
            save_dir=self.save_dir,      # ä¿å­˜ç›®å½•
            on_plot=self.on_plot         # ç»˜å›¾å›è°ƒå‡½æ•°
        )
    
    def auto_batch(self):
        """Get batch size by calculating memory occupation of model."""
        # é€šè¿‡è®¡ç®—æ¨¡å‹å†…å­˜å ç”¨æ¥è·å–æ‰¹æ¬¡å¤§å°
        
        # æ„å»ºè®­ç»ƒæ•°æ®é›†
        train_dataset = self.build_dataset(self.trainset, mode="train", batch=16)
        
        # è®¡ç®—æœ€å¤§å¯¹è±¡æ•°ï¼ˆè€ƒè™‘é©¬èµ›å…‹å¢å¼ºï¼‰
        # 4 for mosaic augmentation
        max_num_obj = max(len(label["cls"]) for label in train_dataset.labels) * 4
        
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•è‡ªåŠ¨ç¡®å®šæ‰¹æ¬¡å¤§å°
        return super().auto_batch(max_num_obj)