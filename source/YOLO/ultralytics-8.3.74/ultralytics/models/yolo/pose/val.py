# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

from pathlib import Path

import numpy as np
import torch

from ultralytics.models.yolo.detect import DetectionValidator
from ultralytics.utils import LOGGER, ops
from ultralytics.utils.checks import check_requirements
from ultralytics.utils.metrics import OKS_SIGMA, PoseMetrics, box_iou, kpt_iou
from ultralytics.utils.plotting import output_to_target, plot_images


class PoseValidator(DetectionValidator):
    """
    A class extending the DetectionValidator class for validation based on a pose model.
    ä¸€ä¸ªç»§æ‰¿è‡ªDetectionValidatorç±»çš„éªŒè¯å™¨ï¼Œä¸“é—¨ç”¨äºåŸºäºå§¿æ€ä¼°è®¡æ¨¡å‹çš„éªŒè¯ã€‚

    Example:
        ```python
        from ultralytics.models.yolo.pose import PoseValidator

        args = dict(model="yolo11n-pose.pt", data="coco8-pose.yaml")
        validator = PoseValidator(args=args)
        validator()
        ```
    """

    def __init__(self, dataloader=None, save_dir=None, pbar=None, args=None, _callbacks=None):
        """Initialize a 'PoseValidator' object with custom parameters and assigned attributes."""
        # ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°å’Œåˆ†é…çš„å±æ€§åˆå§‹åŒ–PoseValidatorå¯¹è±¡
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•
        super().__init__(dataloader, save_dir, pbar, args, _callbacks)
        
        # åˆå§‹åŒ–å…³é”®ç‚¹ç›¸å…³å±æ€§
        self.sigma = None  # å…³é”®ç‚¹åŒ¹é…çš„æƒé‡ç³»æ•°
        self.kpt_shape = None  # å…³é”®ç‚¹å½¢çŠ¶
        
        # å¼ºåˆ¶è®¾ç½®ä»»åŠ¡ä¸º"pose"ï¼ˆå§¿æ€ä¼°è®¡ï¼‰
        self.args.task = "pose"
        
        # åˆ›å»ºå§¿æ€ä¼°è®¡ä¸“ç”¨çš„åº¦é‡æŒ‡æ ‡å¯¹è±¡
        self.metrics = PoseMetrics(save_dir=self.save_dir, on_plot=self.on_plot)
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Apple MPSè®¾å¤‡
        if isinstance(self.args.device, str) and self.args.device.lower() == "mps":
            # å¯¹äºå§¿æ€ä¼°è®¡æ¨¡å‹ï¼Œè­¦å‘Šä½¿ç”¨MPSå¯èƒ½å­˜åœ¨å·²çŸ¥é—®é¢˜
            LOGGER.warning(
                "WARNING âš ï¸ Apple MPS known Pose bug. Recommend 'device=cpu' for Pose models. "
                "See https://github.com/ultralytics/ultralytics/issues/4031."
            )

    def preprocess(self, batch):
        """Preprocesses the batch by converting the 'keypoints' data into a float and moving it to the device."""
        # é¢„å¤„ç†æ‰¹æ¬¡æ•°æ®ï¼Œå°†å…³é”®ç‚¹æ•°æ®è½¬æ¢ä¸ºæµ®ç‚¹å‹å¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        
        # è°ƒç”¨çˆ¶ç±»é¢„å¤„ç†æ–¹æ³•
        batch = super().preprocess(batch)
        
        # å°†å…³é”®ç‚¹æ•°æ®è½¬æ¢ä¸ºæµ®ç‚¹å‹å¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        batch["keypoints"] = batch["keypoints"].to(self.device).float()
        
        return batch

    def get_desc(self):
        """Returns description of evaluation metrics in string format."""
        # è¿”å›è¯„ä¼°æŒ‡æ ‡çš„æè¿°å­—ç¬¦ä¸²
        return ("%22s" + "%11s" * 10) % (
            "Class",        # ç±»åˆ«
            "Images",       # å›¾åƒæ•°
            "Instances",    # å®ä¾‹æ•°
            "Box(P",        # è¾¹ç•Œæ¡†ç²¾ç¡®ç‡
            "R",            # è¾¹ç•Œæ¡†å¬å›ç‡
            "mAP50",        # è¾¹ç•Œæ¡†50%IoUå¹³å‡ç²¾åº¦
            "mAP50-95)",    # è¾¹ç•Œæ¡†0-95%IoUå¹³å‡ç²¾åº¦
            "Pose(P",       # å§¿æ€ç²¾ç¡®ç‡
            "R",            # å§¿æ€å¬å›ç‡
            "mAP50",        # å§¿æ€50%IoUå¹³å‡ç²¾åº¦
            "mAP50-95)",    # å§¿æ€0-95%IoUå¹³å‡ç²¾åº¦
        )

    def init_metrics(self, model):
        """Initiate pose estimation metrics for YOLO model."""
        # ä¸ºYOLOæ¨¡å‹åˆå§‹åŒ–å§¿æ€ä¼°è®¡æŒ‡æ ‡
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æŒ‡æ ‡æ–¹æ³•
        super().init_metrics(model)
        
        # è·å–å…³é”®ç‚¹å½¢çŠ¶
        self.kpt_shape = self.data["kpt_shape"]
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡å‡†å§¿æ€ä¼°è®¡ï¼ˆCOCOæ•°æ®é›†çš„17ä¸ªå…³é”®ç‚¹ï¼‰
        is_pose = self.kpt_shape == [17, 3]
        
        # è·å–å…³é”®ç‚¹æ•°é‡
        nkpt = self.kpt_shape[0]
        
        # è®¾ç½®å…³é”®ç‚¹åŒ¹é…çš„æƒé‡ç³»æ•°
        # - å¯¹äºæ ‡å‡†å§¿æ€ä¼°è®¡ï¼Œä½¿ç”¨é¢„å®šä¹‰çš„OKS_SIGMA
        # - å¯¹äºå…¶ä»–æƒ…å†µï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒçš„æƒé‡
        self.sigma = OKS_SIGMA if is_pose else np.ones(nkpt) / nkpt
        
        # åˆå§‹åŒ–ç»Ÿè®¡æŒ‡æ ‡å­—å…¸
        self.stats = dict(
            tp_p=[], # å…³é”®ç‚¹åŒ¹é…çš„çœŸæ­£ä¾‹
            tp=[],   # è¾¹ç•Œæ¡†åŒ¹é…çš„çœŸæ­£ä¾‹
            conf=[], # ç½®ä¿¡åº¦
            pred_cls=[], # é¢„æµ‹ç±»åˆ«
            target_cls=[], # ç›®æ ‡ç±»åˆ«
            target_img=[] # ç›®æ ‡å›¾åƒ
        )

    def _prepare_batch(self, si, batch):
        """Prepares a batch for processing by converting keypoints to float and moving to device."""
        # å‡†å¤‡æ‰¹æ¬¡å¤„ç†ï¼Œè½¬æ¢å…³é”®ç‚¹ä¸ºæµ®ç‚¹å‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
        
        # è°ƒç”¨çˆ¶ç±»æ‰¹æ¬¡å‡†å¤‡æ–¹æ³•
        pbatch = super()._prepare_batch(si, batch)
        
        # æå–æŒ‡å®šæ‰¹æ¬¡ç´¢å¼•çš„å…³é”®ç‚¹
        kpts = batch["keypoints"][batch["batch_idx"] == si]
        
        # è·å–å›¾åƒå°ºå¯¸
        h, w = pbatch["imgsz"]
        
        # å…‹éš†å…³é”®ç‚¹æ•°æ®
        kpts = kpts.clone()
        
        # å°†å…³é”®ç‚¹åæ ‡ç¼©æ”¾åˆ°å›¾åƒå°ºå¯¸
        kpts[..., 0] *= w
        kpts[..., 1] *= h
        
        # å°†å…³é”®ç‚¹åæ ‡ç¼©æ”¾åˆ°åŸå§‹å›¾åƒå°ºå¯¸
        kpts = ops.scale_coords(
            pbatch["imgsz"], 
            kpts, 
            pbatch["ori_shape"], 
            ratio_pad=pbatch["ratio_pad"]
        )
        
        # å°†å¤„ç†åçš„å…³é”®ç‚¹æ·»åŠ åˆ°æ‰¹æ¬¡æ•°æ®ä¸­
        pbatch["kpts"] = kpts
        
        return pbatch

    def _prepare_pred(self, pred, pbatch):
        """Prepares and scales keypoints in a batch for pose processing."""
        # ä¸ºå§¿æ€å¤„ç†å‡†å¤‡å’Œç¼©æ”¾æ‰¹æ¬¡ä¸­çš„å…³é”®ç‚¹
        
        # è°ƒç”¨çˆ¶ç±»é¢„æµ‹å‡†å¤‡æ–¹æ³•
        predn = super()._prepare_pred(pred, pbatch)
        
        # è·å–å…³é”®ç‚¹æ•°é‡
        nk = pbatch["kpts"].shape[1]
        
        # é‡å¡‘å…³é”®ç‚¹é¢„æµ‹
        pred_kpts = predn[:, 6:].view(len(predn), nk, -1)
        
        # å°†å…³é”®ç‚¹åæ ‡ç¼©æ”¾åˆ°åŸå§‹å›¾åƒå°ºå¯¸
        ops.scale_coords(
            pbatch["imgsz"], 
            pred_kpts, 
            pbatch["ori_shape"], 
            ratio_pad=pbatch["ratio_pad"]
        )
        
        return predn, pred_kpts


    def update_metrics(self, preds, batch):
        """Metrics."""
        # æ›´æ–°è¯„ä¼°æŒ‡æ ‡
        
        # éå†æ¯ä¸ªé¢„æµ‹ç»“æœ
        for si, pred in enumerate(preds):
            # å¢åŠ å·²å¤„ç†å›¾åƒæ•°
            self.seen += 1
            
            # è·å–é¢„æµ‹æ•°é‡
            npr = len(pred)
            
            # åˆå§‹åŒ–ç»Ÿè®¡å­—å…¸
            stat = dict(
                conf=torch.zeros(0, device=self.device),  # ç½®ä¿¡åº¦
                pred_cls=torch.zeros(0, device=self.device),  # é¢„æµ‹ç±»åˆ«
                tp=torch.zeros(npr, self.niou, dtype=torch.bool, device=self.device),  # è¾¹ç•Œæ¡†çœŸæ­£ä¾‹
                tp_p=torch.zeros(npr, self.niou, dtype=torch.bool, device=self.device),  # å…³é”®ç‚¹çœŸæ­£ä¾‹
            )
            
            # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
            pbatch = self._prepare_batch(si, batch)
            
            # æå–ç±»åˆ«å’Œè¾¹ç•Œæ¡†
            cls, bbox = pbatch.pop("cls"), pbatch.pop("bbox")
            
            # è·å–ç›®æ ‡æ•°é‡
            nl = len(cls)
            
            # è®°å½•ç›®æ ‡ç±»åˆ«å’Œå›¾åƒ
            stat["target_cls"] = cls
            stat["target_img"] = cls.unique()
            
            # å¤„ç†æ— é¢„æµ‹ç»“æœçš„æƒ…å†µ
            if npr == 0:
                if nl:
                    # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                    for k in self.stats.keys():
                        self.stats[k].append(stat[k])
                    
                    # å¤„ç†æ··æ·†çŸ©é˜µ
                    if self.args.plots:
                        self.confusion_matrix.process_batch(detections=None, gt_bboxes=bbox, gt_cls=cls)
                continue

            # å¤„ç†å•ç±»åˆ«æƒ…å†µ
            if self.args.single_cls:
                pred[:, 5] = 0
            
            # å‡†å¤‡é¢„æµ‹ç»“æœå’Œå…³é”®ç‚¹
            predn, pred_kpts = self._prepare_pred(pred, pbatch)
            
            # è®°å½•ç½®ä¿¡åº¦å’Œé¢„æµ‹ç±»åˆ«
            stat["conf"] = predn[:, 4]
            stat["pred_cls"] = predn[:, 5]

            # è¯„ä¼°é¢„æµ‹ç»“æœ
            if nl:
                # è¯„ä¼°è¾¹ç•Œæ¡†åŒ¹é…
                stat["tp"] = self._process_batch(predn, bbox, cls)
                
                # è¯„ä¼°å…³é”®ç‚¹åŒ¹é…
                stat["tp_p"] = self._process_batch(predn, bbox, cls, pred_kpts, pbatch["kpts"])
            
            # å¤„ç†æ··æ·†çŸ©é˜µ
            if self.args.plots:
                self.confusion_matrix.process_batch(predn, bbox, cls)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            for k in self.stats.keys():
                self.stats[k].append(stat[k])

            # ä¿å­˜ç»“æœ
            if self.args.save_json:
                # å°†é¢„æµ‹è½¬æ¢ä¸ºJSONæ ¼å¼
                self.pred_to_json(predn, batch["im_file"][si])
            
            if self.args.save_txt:
                # ä¿å­˜é¢„æµ‹ä¸ºæ–‡æœ¬æ–‡ä»¶
                self.save_one_txt(
                    predn,
                    pred_kpts,
                    self.args.save_conf,
                    pbatch["ori_shape"],
                    self.save_dir / "labels" / f"{Path(batch['im_file'][si]).stem}.txt",
                )

    def _process_batch(self, detections, gt_bboxes, gt_cls, pred_kpts=None, gt_kpts=None):
        """
        Return correct prediction matrix by computing Intersection over Union (IoU) between detections and ground truth.
        é€šè¿‡è®¡ç®—æ£€æµ‹ç»“æœå’ŒçœŸå®æ ‡æ³¨ä¹‹é—´çš„äº¤å¹¶æ¯”ï¼ˆIoUï¼‰è¿”å›æ­£ç¡®çš„é¢„æµ‹çŸ©é˜µã€‚

        Args:
            detections (torch.Tensor): Tensor with shape (N, 6) representing detection boxes and scores, where each
                detection is of the format (x1, y1, x2, y2, conf, class).
                å½¢çŠ¶ä¸º(N, 6)çš„å¼ é‡ï¼Œè¡¨ç¤ºæ£€æµ‹æ¡†å’Œç½®ä¿¡åº¦ï¼Œæ¯ä¸ªæ£€æµ‹çš„æ ¼å¼ä¸º(x1, y1, x2, y2, ç½®ä¿¡åº¦, ç±»åˆ«)ã€‚
            gt_bboxes (torch.Tensor): Tensor with shape (M, 4) representing ground truth bounding boxes, where each
                box is of the format (x1, y1, x2, y2).
                å½¢çŠ¶ä¸º(M, 4)çš„å¼ é‡ï¼Œè¡¨ç¤ºçœŸå®è¾¹ç•Œæ¡†ï¼Œæ¯ä¸ªæ¡†çš„æ ¼å¼ä¸º(x1, y1, x2, y2)ã€‚
            gt_cls (torch.Tensor): Tensor with shape (M,) representing ground truth class indices.
                å½¢çŠ¶ä¸º(M,)çš„å¼ é‡ï¼Œè¡¨ç¤ºçœŸå®ç±»åˆ«ç´¢å¼•ã€‚
            pred_kpts (torch.Tensor | None): Optional tensor with shape (N, 51) representing predicted keypoints, where
                51 corresponds to 17 keypoints each having 3 values.
                å¯é€‰çš„å½¢çŠ¶ä¸º(N, 51)çš„å¼ é‡ï¼Œè¡¨ç¤ºé¢„æµ‹å…³é”®ç‚¹ï¼Œå…¶ä¸­51å¯¹åº”17ä¸ªå…³é”®ç‚¹ï¼Œæ¯ä¸ªå…³é”®ç‚¹æœ‰3ä¸ªå€¼ã€‚
            gt_kpts (torch.Tensor | None): Optional tensor with shape (N, 51) representing ground truth keypoints.
                å¯é€‰çš„å½¢çŠ¶ä¸º(N, 51)çš„å¼ é‡ï¼Œè¡¨ç¤ºçœŸå®å…³é”®ç‚¹ã€‚

        Returns:
            (torch.Tensor): A tensor with shape (N, 10) representing the correct prediction matrix for 10 IoU levels,
                where N is the number of detections.
            å½¢çŠ¶ä¸º(N, 10)çš„å¼ é‡ï¼Œè¡¨ç¤º10ä¸ªIoUçº§åˆ«çš„æ­£ç¡®é¢„æµ‹çŸ©é˜µï¼Œå…¶ä¸­Næ˜¯æ£€æµ‹æ•°é‡ã€‚

        Example:
            ```python
            detections = torch.rand(100, 6)  # 100 predictions: (x1, y1, x2, y2, conf, class)
            gt_bboxes = torch.rand(50, 4)  # 50 ground truth boxes: (x1, y1, x2, y2)
            gt_cls = torch.randint(0, 2, (50,))  # 50 ground truth class indices
            pred_kpts = torch.rand(100, 51)  # 100 predicted keypoints
            gt_kpts = torch.rand(50, 51)  # 50 ground truth keypoints
            correct_preds = _process_batch(detections, gt_bboxes, gt_cls, pred_kpts, gt_kpts)
            ```

        Note:
            `0.53` scale factor used in area computation is referenced from https://github.com/jin-s13/xtcocoapi/blob/master/xtcocotools/cocoeval.py#L384.
            é¢ç§¯è®¡ç®—ä¸­ä½¿ç”¨çš„`0.53`æ¯”ä¾‹å› å­æ¥è‡ªäºæŒ‡å®šçš„GitHubä»“åº“é“¾æ¥ã€‚
        """
        # å¤„ç†å…³é”®ç‚¹åŒ¹é…çš„æƒ…å†µ
        if pred_kpts is not None and gt_kpts is not None:
            # ä½¿ç”¨`0.53`æ¯”ä¾‹å› å­è®¡ç®—è¾¹ç•Œæ¡†é¢ç§¯
            # å‚è€ƒ: https://github.com/jin-s13/xtcocoapi/blob/master/xtcocotools/cocoeval.py#L384
            area = ops.xyxy2xywh(gt_bboxes)[:, 2:].prod(1) * 0.53
            
            # è®¡ç®—å…³é”®ç‚¹IoU
            # - gt_kpts: çœŸå®å…³é”®ç‚¹
            # - pred_kpts: é¢„æµ‹å…³é”®ç‚¹
            # - sigma: å…³é”®ç‚¹åŒ¹é…æƒé‡
            # - area: è¾¹ç•Œæ¡†é¢ç§¯
            iou = kpt_iou(gt_kpts, pred_kpts, sigma=self.sigma, area=area)
        else:  # å¤„ç†è¾¹ç•Œæ¡†åŒ¹é…çš„æƒ…å†µ
            # è®¡ç®—è¾¹ç•Œæ¡†IoU
            iou = box_iou(gt_bboxes, detections[:, :4])

        # åŒ¹é…é¢„æµ‹ç»“æœ
        # - detections[:, 5]: é¢„æµ‹ç±»åˆ«
        # - gt_cls: çœŸå®ç±»åˆ«
        # - iou: äº¤å¹¶æ¯”
        return self.match_predictions(detections[:, 5], gt_cls, iou)


    def plot_val_samples(self, batch, ni):
        """
        Plots and saves validation set samples with predicted bounding boxes and keypoints.
        ç»˜åˆ¶å¹¶ä¿å­˜å¸¦æœ‰é¢„æµ‹è¾¹ç•Œæ¡†å’Œå…³é”®ç‚¹çš„éªŒè¯é›†æ ·æœ¬ã€‚
        """
        plot_images(
            batch["img"],                 # è¾“å…¥å›¾åƒ
            batch["batch_idx"],           # æ‰¹æ¬¡ç´¢å¼•
            batch["cls"].squeeze(-1),     # ç±»åˆ«æ ‡ç­¾ï¼ˆå»é™¤é¢å¤–ç»´åº¦ï¼‰
            batch["bboxes"],              # è¾¹ç•Œæ¡†
            kpts=batch["keypoints"],      # å…³é”®ç‚¹
            paths=batch["im_file"],       # å›¾åƒæ–‡ä»¶è·¯å¾„
            fname=self.save_dir / f"val_batch{ni}_labels.jpg",  # ä¿å­˜æ–‡ä»¶å
            names=self.names,             # ç±»åˆ«åç§°
            on_plot=self.on_plot,         # ç»˜å›¾å›è°ƒå‡½æ•°
        )

    def plot_predictions(self, batch, preds, ni):
        """
        Plots predictions for YOLO model.
        ç»˜åˆ¶YOLOæ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚
        """
        # æå–å¹¶é‡å¡‘é¢„æµ‹çš„å…³é”®ç‚¹
        pred_kpts = torch.cat([p[:, 6:].view(-1, *self.kpt_shape) for p in preds], 0)
        
        plot_images(
            batch["img"],                 # è¾“å…¥å›¾åƒ
            *output_to_target(preds, max_det=self.args.max_det),  # é¢„æµ‹ç›®æ ‡ï¼ˆä½¿ç”¨æœ€å¤§æ£€æµ‹æ•°é™åˆ¶ï¼‰
            kpts=pred_kpts,               # é¢„æµ‹å…³é”®ç‚¹
            paths=batch["im_file"],       # å›¾åƒæ–‡ä»¶è·¯å¾„
            fname=self.save_dir / f"val_batch{ni}_pred.jpg",  # ä¿å­˜æ–‡ä»¶å
            names=self.names,             # ç±»åˆ«åç§°
            on_plot=self.on_plot,         # ç»˜å›¾å›è°ƒå‡½æ•°
        )

    def save_one_txt(self, predn, pred_kpts, save_conf, shape, file):
        """
        Save YOLO detections to a txt file in normalized coordinates in a specific format.
        ä»¥ç‰¹å®šæ ¼å¼å°†YOLOæ£€æµ‹ç»“æœä¿å­˜ä¸ºtxtæ–‡ä»¶ï¼Œä½¿ç”¨å½’ä¸€åŒ–åæ ‡ã€‚
        """
        # å¯¼å…¥Resultsç±»
        from ultralytics.engine.results import Results

        # åˆ›å»ºResultså¯¹è±¡å¹¶ä¿å­˜
        Results(
            np.zeros((shape[0], shape[1]), dtype=np.uint8),  # åˆ›å»ºç©ºç™½å›¾åƒ
            path=None,                    # è·¯å¾„ä¸ºç©º
            names=self.names,             # ç±»åˆ«åç§°
            boxes=predn[:, :6],           # è¾¹ç•Œæ¡†ä¿¡æ¯
            keypoints=pred_kpts,          # å…³é”®ç‚¹
        ).save_txt(file, save_conf=save_conf)  # ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶

    def pred_to_json(self, predn, filename):
        """
        Converts YOLO predictions to COCO JSON format.
        å°†YOLOé¢„æµ‹ç»“æœè½¬æ¢ä¸ºCOCO JSONæ ¼å¼ã€‚
        """
        # è·å–æ–‡ä»¶åçš„stemï¼ˆä¸åŒ…å«æ‰©å±•åçš„æ–‡ä»¶åï¼‰
        stem = Path(filename).stem
        
        # å°è¯•å°†stemè½¬æ¢ä¸ºå›¾åƒIDï¼Œå¦‚æœä¸æ˜¯æ•°å­—åˆ™ä¿æŒåŸå€¼
        image_id = int(stem) if stem.isnumeric() else stem
        
        # å°†è¾¹ç•Œæ¡†åæ ‡è½¬æ¢ä¸ºXYWHæ ¼å¼
        box = ops.xyxy2xywh(predn[:, :4])  # xywh
        
        # è°ƒæ•´è¾¹ç•Œæ¡†åæ ‡ï¼šä»ä¸­å¿ƒç‚¹è½¬æ¢ä¸ºå·¦ä¸Šè§’
        box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner
        
        # éå†é¢„æµ‹ç»“æœå’Œè¾¹ç•Œæ¡†
        for p, b in zip(predn.tolist(), box.tolist()):
            # æ·»åŠ JSONå­—å…¸åˆ°ç»“æœåˆ—è¡¨
            self.jdict.append(
                {
                    "image_id": image_id,                          # å›¾åƒID
                    "category_id": self.class_map[int(p[5])],      # ç±»åˆ«IDï¼ˆä½¿ç”¨æ˜ å°„ï¼‰
                    "bbox": [round(x, 3) for x in b],              # è¾¹ç•Œæ¡†ï¼ˆä¿ç•™3ä½å°æ•°ï¼‰
                    "keypoints": p[6:],                            # å…³é”®ç‚¹
                    "score": round(p[4], 5),                       # ç½®ä¿¡åº¦ï¼ˆä¿ç•™5ä½å°æ•°ï¼‰
                }
            )

    def eval_json(self, stats):
        """
        Evaluates object detection model using COCO JSON format.
        ä½¿ç”¨COCO JSONæ ¼å¼è¯„ä¼°ç›®æ ‡æ£€æµ‹æ¨¡å‹ã€‚
        """
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜JSONä¸”ä¸ºCOCOæ•°æ®é›†
        if self.args.save_json and self.is_coco and len(self.jdict):
            # è®¾ç½®æ ‡æ³¨å’Œé¢„æµ‹JSONæ–‡ä»¶è·¯å¾„
            anno_json = self.data["path"] / "annotations/person_keypoints_val2017.json"  # æ ‡æ³¨æ–‡ä»¶
            pred_json = self.save_dir / "predictions.json"  # é¢„æµ‹æ–‡ä»¶
            
            LOGGER.info(f"\nEvaluating pycocotools mAP using {pred_json} and {anno_json}...")
            
            try:
                # æ£€æŸ¥å¹¶å¯¼å…¥pycocotools
                check_requirements("pycocotools>=2.0.6")
                from pycocotools.coco import COCO  # noqa
                from pycocotools.cocoeval import COCOeval  # noqa

                # éªŒè¯æ–‡ä»¶å­˜åœ¨
                for x in anno_json, pred_json:
                    assert x.is_file(), f"{x} file not found"
                
                # åˆå§‹åŒ–COCOæ ‡æ³¨å’Œé¢„æµ‹API
                anno = COCO(str(anno_json))  # åˆå§‹åŒ–æ ‡æ³¨API
                pred = anno.loadRes(str(pred_json))  # åˆå§‹åŒ–é¢„æµ‹API
                
                # è¯„ä¼°è¾¹ç•Œæ¡†å’Œå…³é”®ç‚¹
                for i, eval in enumerate([COCOeval(anno, pred, "bbox"), COCOeval(anno, pred, "keypoints")]):
                    if self.is_coco:
                        # è®¾ç½®è¦è¯„ä¼°çš„å›¾åƒID
                        eval.params.imgIds = [int(Path(x).stem) for x in self.dataloader.dataset.im_files]
                    
                    # æ‰§è¡Œè¯„ä¼°æµç¨‹
                    eval.evaluate()
                    eval.accumulate()
                    eval.summarize()
                    
                    # æ›´æ–°ç»Ÿè®¡æŒ‡æ ‡
                    idx = i * 4 + 2
                    stats[self.metrics.keys[idx + 1]], stats[self.metrics.keys[idx]] = eval.stats[:2]
            
            except Exception as e:
                # å¤„ç†pycocotoolsè¿è¡Œå¼‚å¸¸
                LOGGER.warning(f"pycocotools unable to run: {e}")
        
        return stats