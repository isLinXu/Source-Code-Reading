# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import glob
import math
import os
import random
from copy import deepcopy
from multiprocessing.pool import ThreadPool
from pathlib import Path
from typing import Optional

import cv2
import numpy as np
import psutil
from torch.utils.data import Dataset

from ultralytics.data.utils import FORMATS_HELP_MSG, HELP_URL, IMG_FORMATS
from ultralytics.utils import DEFAULT_CFG, LOCAL_RANK, LOGGER, NUM_THREADS, TQDM


class BaseDataset(Dataset):
    """
    Base dataset class for loading and processing image data.
    åŸºç¡€æ•°æ®é›†ç±»ï¼Œç”¨äºåŠ è½½å’Œå¤„ç†å›¾åƒæ•°æ®ã€‚

    Args:
        img_path (str): Path to the folder containing images.
        img_path (str): åŒ…å«å›¾åƒçš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
        imgsz (int, optional): Image size. Defaults to 640.
        imgsz (int, optional): å›¾åƒå¤§å°ã€‚é»˜è®¤ä¸º640ã€‚
        cache (bool, optional): Cache images to RAM or disk during training. Defaults to False.
        cache (bool, optional): åœ¨è®­ç»ƒæœŸé—´å°†å›¾åƒç¼“å­˜åˆ°RAMæˆ–ç£ç›˜ã€‚é»˜è®¤ä¸ºFalseã€‚
        augment (bool, optional): If True, data augmentation is applied. Defaults to True.
        augment (bool, optional): å¦‚æœä¸ºTrueï¼Œåˆ™åº”ç”¨æ•°æ®å¢å¼ºã€‚é»˜è®¤ä¸ºTrueã€‚
        hyp (dict, optional): Hyperparameters to apply data augmentation. Defaults to None.
        hyp (dict, optional): åº”ç”¨æ•°æ®å¢å¼ºçš„è¶…å‚æ•°ã€‚é»˜è®¤ä¸ºNoneã€‚
        prefix (str, optional): Prefix to print in log messages. Defaults to ''.
        prefix (str, optional): åœ¨æ—¥å¿—æ¶ˆæ¯ä¸­æ‰“å°çš„å‰ç¼€ã€‚é»˜è®¤ä¸º''ã€‚
        rect (bool, optional): If True, rectangular training is used. Defaults to False.
        rect (bool, optional): å¦‚æœä¸ºTrueï¼Œåˆ™ä½¿ç”¨çŸ©å½¢è®­ç»ƒã€‚é»˜è®¤ä¸ºFalseã€‚
        batch_size (int, optional): Size of batches. Defaults to None.
        batch_size (int, optional): æ‰¹æ¬¡å¤§å°ã€‚é»˜è®¤ä¸ºNoneã€‚
        stride (int, optional): Stride. Defaults to 32.
        stride (int, optional): æ­¥å¹…ã€‚é»˜è®¤ä¸º32ã€‚
        pad (float, optional): Padding. Defaults to 0.0.
        pad (float, optional): å¡«å……ã€‚é»˜è®¤ä¸º0.0ã€‚
        single_cls (bool, optional): If True, single class training is used. Defaults to False.
        single_cls (bool, optional): å¦‚æœä¸ºTrueï¼Œåˆ™ä½¿ç”¨å•ç±»è®­ç»ƒã€‚é»˜è®¤ä¸ºFalseã€‚
        classes (list): List of included classes. Default is None.
        classes (list): åŒ…å«çš„ç±»åˆ«åˆ—è¡¨ã€‚é»˜è®¤ä¸ºNoneã€‚
        fraction (float): Fraction of dataset to utilize. Default is 1.0 (use all data).
        fraction (float): è¦ä½¿ç”¨çš„æ•°æ®é›†çš„æ¯”ä¾‹ã€‚é»˜è®¤ä¸º1.0ï¼ˆä½¿ç”¨æ‰€æœ‰æ•°æ®ï¼‰ã€‚

    Attributes:
        im_files (list): List of image file paths.
        im_files (list): å›¾åƒæ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚
        labels (list): List of label data dictionaries.
        labels (list): æ ‡ç­¾æ•°æ®å­—å…¸åˆ—è¡¨ã€‚
        ni (int): Number of images in the dataset.
        ni (int): æ•°æ®é›†ä¸­å›¾åƒçš„æ•°é‡ã€‚
        ims (list): List of loaded images.
        ims (list): åŠ è½½çš„å›¾åƒåˆ—è¡¨ã€‚
        npy_files (list): List of numpy file paths.
        npy_files (list): numpyæ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚
        transforms (callable): Image transformation function.
        transforms (callable): å›¾åƒå˜æ¢å‡½æ•°ã€‚
    """

    def __init__(
        self,
        img_path,
        imgsz=640,
        cache=False,
        augment=True,
        hyp=DEFAULT_CFG,
        prefix="",
        rect=False,
        batch_size=16,
        stride=32,
        pad=0.5,
        single_cls=False,
        classes=None,
        fraction=1.0,
    ):
        """Initialize BaseDataset with given configuration and options."""
        super().__init__()
        self.img_path = img_path  # è®¾ç½®å›¾åƒè·¯å¾„
        self.imgsz = imgsz  # è®¾ç½®å›¾åƒå¤§å°
        self.augment = augment  # è®¾ç½®æ˜¯å¦åº”ç”¨æ•°æ®å¢å¼º
        self.single_cls = single_cls  # è®¾ç½®æ˜¯å¦ä½¿ç”¨å•ç±»è®­ç»ƒ
        self.prefix = prefix  # è®¾ç½®æ—¥å¿—å‰ç¼€
        self.fraction = fraction  # è®¾ç½®æ•°æ®é›†ä½¿ç”¨æ¯”ä¾‹
        self.im_files = self.get_img_files(self.img_path)  # è·å–å›¾åƒæ–‡ä»¶è·¯å¾„
        self.labels = self.get_labels()  # è·å–æ ‡ç­¾æ•°æ®
        self.update_labels(include_class=classes)  # æ›´æ–°æ ‡ç­¾ä»¥åŒ…å«æŒ‡å®šç±»åˆ«ï¼ˆå•ç±»å’ŒåŒ…å«ç±»åˆ«ï¼‰
        self.ni = len(self.labels)  # å›¾åƒæ•°é‡
        self.rect = rect  # è®¾ç½®æ˜¯å¦ä½¿ç”¨çŸ©å½¢è®­ç»ƒ
        self.batch_size = batch_size  # è®¾ç½®æ‰¹æ¬¡å¤§å°
        self.stride = stride  # è®¾ç½®æ­¥å¹…
        self.pad = pad  # è®¾ç½®å¡«å……
        if self.rect:
            assert self.batch_size is not None  # ç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸ä¸ºNone
            self.set_rectangle()  # è®¾ç½®çŸ©å½¢è®­ç»ƒ

        # Buffer thread for mosaic images
        self.buffer = []  # ç¼“å†²åŒºå¤§å° = æ‰¹æ¬¡å¤§å°
        self.max_buffer_length = min((self.ni, self.batch_size * 8, 1000)) if self.augment else 0  # æœ€å¤§ç¼“å†²åŒºé•¿åº¦

        # Cache images (options are cache = True, False, None, "ram", "disk")
        self.ims, self.im_hw0, self.im_hw = [None] * self.ni, [None] * self.ni, [None] * self.ni  # åˆå§‹åŒ–å›¾åƒç¼“å­˜
        self.npy_files = [Path(f).with_suffix(".npy") for f in self.im_files]  # è·å–numpyæ–‡ä»¶è·¯å¾„
        self.cache = cache.lower() if isinstance(cache, str) else "ram" if cache is True else None  # è®¾ç½®ç¼“å­˜ç±»å‹
        if self.cache == "ram" and self.check_cache_ram():  # æ£€æŸ¥RAMç¼“å­˜
            if hyp.deterministic:
                LOGGER.warning(
                    "WARNING âš ï¸ cache='ram' may produce non-deterministic training results. "
                    "Consider cache='disk' as a deterministic alternative if your disk space allows."
                )  # è­¦å‘Šï¼šä½¿ç”¨RAMç¼“å­˜å¯èƒ½å¯¼è‡´éç¡®å®šæ€§è®­ç»ƒç»“æœ
            self.cache_images()  # ç¼“å­˜å›¾åƒåˆ°RAM
        elif self.cache == "disk" and self.check_cache_disk():  # æ£€æŸ¥ç£ç›˜ç¼“å­˜
            self.cache_images()  # ç¼“å­˜å›¾åƒåˆ°ç£ç›˜

        # Transforms
        self.transforms = self.build_transforms(hyp=hyp)  # æ„å»ºå›¾åƒå˜æ¢

    def get_img_files(self, img_path):
        """Read image files."""
        try:
            f = []  # image files
            for p in img_path if isinstance(img_path, list) else [img_path]:
                p = Path(p)  # os-agnostic
                if p.is_dir():  # dir
                    f += glob.glob(str(p / "**" / "*.*"), recursive=True)  # é€’å½’è·å–æ–‡ä»¶
                elif p.is_file():  # file
                    with open(p) as t:
                        t = t.read().strip().splitlines()  # è¯»å–æ–‡ä»¶å†…å®¹
                        parent = str(p.parent) + os.sep  # è·å–çˆ¶ç›®å½•
                        f += [x.replace("./", parent) if x.startswith("./") else x for x in t]  # æœ¬åœ°è·¯å¾„è½¬æ¢ä¸ºå…¨å±€è·¯å¾„
                else:
                    raise FileNotFoundError(f"{self.prefix}{p} does not exist")  # æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯
            im_files = sorted(x.replace("/", os.sep) for x in f if x.split(".")[-1].lower() in IMG_FORMATS)  # è·å–æœ‰æ•ˆå›¾åƒæ–‡ä»¶
            assert im_files, f"{self.prefix}No images found in {img_path}. {FORMATS_HELP_MSG}"  # ç¡®ä¿æ‰¾åˆ°å›¾åƒ
        except Exception as e:
            raise FileNotFoundError(f"{self.prefix}Error loading data from {img_path}\n{HELP_URL}") from e  # æ•°æ®åŠ è½½é”™è¯¯
        if self.fraction < 1:
            im_files = im_files[: round(len(im_files) * self.fraction)]  # ä¿ç•™æ•°æ®é›†çš„ä¸€éƒ¨åˆ†
        return im_files  # è¿”å›å›¾åƒæ–‡ä»¶åˆ—è¡¨

    def update_labels(self, include_class: Optional[list]):
        """Update labels to include only these classes (optional)."""
        include_class_array = np.array(include_class).reshape(1, -1)  # è½¬æ¢ä¸ºæ•°ç»„
        for i in range(len(self.labels)):
            if include_class is not None:
                cls = self.labels[i]["cls"]  # è·å–ç±»åˆ«
                bboxes = self.labels[i]["bboxes"]  # è·å–è¾¹ç•Œæ¡†
                segments = self.labels[i]["segments"]  # è·å–åˆ†æ®µ
                keypoints = self.labels[i]["keypoints"]  # è·å–å…³é”®ç‚¹
                j = (cls == include_class_array).any(1)  # æ£€æŸ¥ç±»åˆ«æ˜¯å¦åœ¨åŒ…å«ç±»ä¸­
                self.labels[i]["cls"] = cls[j]  # æ›´æ–°ç±»åˆ«
                self.labels[i]["bboxes"] = bboxes[j]  # æ›´æ–°è¾¹ç•Œæ¡†
                if segments:
                    self.labels[i]["segments"] = [segments[si] for si, idx in enumerate(j) if idx]  # æ›´æ–°åˆ†æ®µ
                if keypoints is not None:
                    self.labels[i]["keypoints"] = keypoints[j]  # æ›´æ–°å…³é”®ç‚¹
            if self.single_cls:
                self.labels[i]["cls"][:, 0] = 0  # å•ç±»è®­ç»ƒæ—¶å°†ç±»è®¾ç½®ä¸º0

    def load_image(self, i, rect_mode=True):
        """Loads 1 image from dataset index 'i', returns (im, resized hw)."""
        im, f, fn = self.ims[i], self.im_files[i], self.npy_files[i]  # è·å–å›¾åƒå’Œæ–‡ä»¶è·¯å¾„
        if im is None:  # not cached in RAM
            if fn.exists():  # load npy
                try:
                    im = np.load(fn)  # åŠ è½½numpyæ–‡ä»¶
                except Exception as e:
                    LOGGER.warning(f"{self.prefix}WARNING âš ï¸ Removing corrupt *.npy image file {fn} due to: {e}")  # è­¦å‘Šï¼šç§»é™¤æŸåçš„npyæ–‡ä»¶
                    Path(fn).unlink(missing_ok=True)  # åˆ é™¤æ–‡ä»¶
                    im = cv2.imread(f)  # BGR
            else:  # read image
                im = cv2.imread(f)  # BGR
            if im is None:
                raise FileNotFoundError(f"Image Not Found {f}")  # å›¾åƒæœªæ‰¾åˆ°é”™è¯¯

            h0, w0 = im.shape[:2]  # orig hw è·å–åŸå§‹é«˜åº¦å’Œå®½åº¦
            if rect_mode:  # resize long side to imgsz while maintaining aspect ratio
                r = self.imgsz / max(h0, w0)  # ratio è®¡ç®—æ¯”ä¾‹
                if r != 1:  # if sizes are not equal
                    w, h = (min(math.ceil(w0 * r), self.imgsz), min(math.ceil(h0 * r), self.imgsz))  # è®¡ç®—è°ƒæ•´åçš„å®½é«˜
                    im = cv2.resize(im, (w, h), interpolation=cv2.INTER_LINEAR)  # è°ƒæ•´å›¾åƒå¤§å°
            elif not (h0 == w0 == self.imgsz):  # resize by stretching image to square imgsz
                im = cv2.resize(im, (self.imgsz, self.imgsz), interpolation=cv2.INTER_LINEAR)  # è°ƒæ•´å›¾åƒå¤§å°ä¸ºæ­£æ–¹å½¢

            # Add to buffer if training with augmentations
            if self.augment:  # å¦‚æœä½¿ç”¨å¢å¼º
                self.ims[i], self.im_hw0[i], self.im_hw[i] = im, (h0, w0), im.shape[:2]  # im, hw_original, hw_resized
                self.buffer.append(i)  # å°†ç´¢å¼•æ·»åŠ åˆ°ç¼“å†²åŒº
                if 1 < len(self.buffer) >= self.max_buffer_length:  # prevent empty buffer é˜²æ­¢ç¼“å†²åŒºä¸ºç©º
                    j = self.buffer.pop(0)  # ç§»é™¤ç¬¬ä¸€ä¸ªå…ƒç´ 
                    if self.cache != "ram":
                        self.ims[j], self.im_hw0[j], self.im_hw[j] = None, None, None  # æ¸…ç©ºç¼“å­˜

            return im, (h0, w0), im.shape[:2]  # è¿”å›å›¾åƒå’Œå°ºå¯¸

        return self.ims[i], self.im_hw0[i], self.im_hw[i]  # è¿”å›ç¼“å­˜çš„å›¾åƒå’Œå°ºå¯¸

    def cache_images(self):
        """Cache images to memory or disk."""
        b, gb = 0, 1 << 30  # bytes of cached images, bytes per gigabytes
        fcn, storage = (self.cache_images_to_disk, "Disk") if self.cache == "disk" else (self.load_image, "RAM")  # è®¾ç½®ç¼“å­˜å‡½æ•°å’Œå­˜å‚¨ç±»å‹
        with ThreadPool(NUM_THREADS) as pool:  # ä½¿ç”¨çº¿ç¨‹æ± 
            results = pool.imap(fcn, range(self.ni))  # å¹¶è¡ŒåŠ è½½å›¾åƒ
            pbar = TQDM(enumerate(results), total=self.ni, disable=LOCAL_RANK > 0)  # è¿›åº¦æ¡
            for i, x in pbar:
                if self.cache == "disk":
                    b += self.npy_files[i].stat().st_size  # æ›´æ–°ç¼“å­˜å¤§å°
                else:  # 'ram'
                    self.ims[i], self.im_hw0[i], self.im_hw[i] = x  # im, hw_orig, hw_resized = load_image(self, i)
                    b += self.ims[i].nbytes  # æ›´æ–°ç¼“å­˜å¤§å°
                pbar.desc = f"{self.prefix}Caching images ({b / gb:.1f}GB {storage})"  # æ›´æ–°è¿›åº¦æ¡æè¿°
            pbar.close()  # å…³é—­è¿›åº¦æ¡

    def cache_images_to_disk(self, i):
        """Saves an image as an *.npy file for faster loading."""
        f = self.npy_files[i]  # è·å–numpyæ–‡ä»¶è·¯å¾„
        if not f.exists():  # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨
            np.save(f.as_posix(), cv2.imread(self.im_files[i]), allow_pickle=False)  # ä¿å­˜å›¾åƒä¸ºnpyæ–‡ä»¶

    def check_cache_disk(self, safety_margin=0.5):
        """Check image caching requirements vs available disk space."""
        import shutil  # å¯¼å…¥shutilæ¨¡å—

        b, gb = 0, 1 << 30  # bytes of cached images, bytes per gigabytes
        n = min(self.ni, 30)  # extrapolate from 30 random images
        for _ in range(n):
            im_file = random.choice(self.im_files)  # éšæœºé€‰æ‹©å›¾åƒæ–‡ä»¶
            im = cv2.imread(im_file)  # è¯»å–å›¾åƒ
            if im is None:
                continue  # å¦‚æœå›¾åƒä¸ºç©ºï¼Œè·³è¿‡
            b += im.nbytes  # æ›´æ–°ç¼“å­˜å¤§å°
            if not os.access(Path(im_file).parent, os.W_OK):  # æ£€æŸ¥ç›®å½•æ˜¯å¦å¯å†™
                self.cache = None
                LOGGER.info(f"{self.prefix}Skipping caching images to disk, directory not writeable âš ï¸")  # è­¦å‘Šï¼šç›®å½•ä¸å¯å†™
                return False
        disk_required = b * self.ni / n * (1 + safety_margin)  # bytes required to cache dataset to disk
        total, used, free = shutil.disk_usage(Path(self.im_files[0]).parent)  # è·å–ç£ç›˜ä½¿ç”¨æƒ…å†µ
        if disk_required > free:  # å¦‚æœæ‰€éœ€ç£ç›˜ç©ºé—´è¶…è¿‡å¯ç”¨ç©ºé—´
            self.cache = None
            LOGGER.info(
                f"{self.prefix}{disk_required / gb:.1f}GB disk space required, "
                f"with {int(safety_margin * 100)}% safety margin but only "
                f"{free / gb:.1f}/{total / gb:.1f}GB free, not caching images to disk âš ï¸"
            )  # è­¦å‘Šï¼šç£ç›˜ç©ºé—´ä¸è¶³
            return False
        return True  # ç£ç›˜æ£€æŸ¥é€šè¿‡

    def check_cache_ram(self, safety_margin=0.5):
        """Check image caching requirements vs available memory."""
        b, gb = 0, 1 << 30  # bytes of cached images, bytes per gigabytes
        n = min(self.ni, 30)  # extrapolate from 30 random images
        for _ in range(n):
            im = cv2.imread(random.choice(self.im_files))  # sample image
            if im is None:
                continue  # å¦‚æœå›¾åƒä¸ºç©ºï¼Œè·³è¿‡
            ratio = self.imgsz / max(im.shape[0], im.shape[1])  # max(h, w)  # ratio è®¡ç®—æ¯”ä¾‹
            b += im.nbytes * ratio**2  # æ›´æ–°ç¼“å­˜å¤§å°
        mem_required = b * self.ni / n * (1 + safety_margin)  # GB required to cache dataset into RAM
        mem = psutil.virtual_memory()  # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
        if mem_required > mem.available:  # å¦‚æœæ‰€éœ€å†…å­˜è¶…è¿‡å¯ç”¨å†…å­˜
            self.cache = None
            LOGGER.info(
                f"{self.prefix}{mem_required / gb:.1f}GB RAM required to cache images "
                f"with {int(safety_margin * 100)}% safety margin but only "
                f"{mem.available / gb:.1f}/{mem.total / gb:.1f}GB available, not caching images âš ï¸"
            )  # è­¦å‘Šï¼šå†…å­˜ä¸è¶³
            return False
        return True  # å†…å­˜æ£€æŸ¥é€šè¿‡

    def set_rectangle(self):
        """Sets the shape of bounding boxes for YOLO detections as rectangles."""
        bi = np.floor(np.arange(self.ni) / self.batch_size).astype(int)  # batch index
        nb = bi[-1] + 1  # number of batches

        s = np.array([x.pop("shape") for x in self.labels])  # hw
        ar = s[:, 0] / s[:, 1]  # aspect ratio è®¡ç®—çºµæ¨ªæ¯”
        irect = ar.argsort()  # è·å–æ’åºç´¢å¼•
        self.im_files = [self.im_files[i] for i in irect]  # æ ¹æ®æ’åºç´¢å¼•æ›´æ–°å›¾åƒæ–‡ä»¶
        self.labels = [self.labels[i] for i in irect]  # æ ¹æ®æ’åºç´¢å¼•æ›´æ–°æ ‡ç­¾
        ar = ar[irect]  # æ›´æ–°çºµæ¨ªæ¯”

        # Set training image shapes
        shapes = [[1, 1]] * nb  # åˆå§‹åŒ–è®­ç»ƒå›¾åƒå½¢çŠ¶
        for i in range(nb):
            ari = ar[bi == i]  # è·å–å½“å‰æ‰¹æ¬¡çš„çºµæ¨ªæ¯”
            mini, maxi = ari.min(), ari.max()  # è·å–æœ€å°å’Œæœ€å¤§çºµæ¨ªæ¯”
            if maxi < 1:
                shapes[i] = [maxi, 1]  # è®¾ç½®å½¢çŠ¶
            elif mini > 1:
                shapes[i] = [1, 1 / mini]  # è®¾ç½®å½¢çŠ¶

        self.batch_shapes = np.ceil(np.array(shapes) * self.imgsz / self.stride + self.pad).astype(int) * self.stride  # è®¡ç®—æ‰¹æ¬¡å½¢çŠ¶
        self.batch = bi  # è®¾ç½®å›¾åƒçš„æ‰¹æ¬¡ç´¢å¼•

    def __getitem__(self, index):
        """Returns transformed label information for given index."""
        return self.transforms(self.get_image_and_label(index))  # è¿”å›å˜æ¢åçš„æ ‡ç­¾ä¿¡æ¯

    def get_image_and_label(self, index):
        """Get and return label information from the dataset."""
        label = deepcopy(self.labels[index])  # requires deepcopy() https://github.com/ultralytics/ultralytics/pull/1948
        label.pop("shape", None)  # shape is for rect, remove it
        label["img"], label["ori_shape"], label["resized_shape"] = self.load_image(index)  # è·å–å›¾åƒå’Œå°ºå¯¸
        label["ratio_pad"] = (
            label["resized_shape"][0] / label["ori_shape"][0],
            label["resized_shape"][1] / label["ori_shape"][1],
        )  # for evaluation ç”¨äºè¯„ä¼°
        if self.rect:
            label["rect_shape"] = self.batch_shapes[self.batch[index]]  # è®¾ç½®çŸ©å½¢å½¢çŠ¶
        return self.update_labels_info(label)  # è¿”å›æ›´æ–°åçš„æ ‡ç­¾ä¿¡æ¯

    def __len__(self):
        """Returns the length of the labels list for the dataset."""
        return len(self.labels)  # è¿”å›æ ‡ç­¾åˆ—è¡¨çš„é•¿åº¦

    def update_labels_info(self, label):
        """Custom your label format here."""
        return label  # è¿”å›æ ‡ç­¾

    def build_transforms(self, hyp=None):
        """
        Users can customize augmentations here.
        ç”¨æˆ·å¯ä»¥åœ¨æ­¤å¤„è‡ªå®šä¹‰å¢å¼ºã€‚

        Example:
            ```python
            if self.augment:
                # Training transforms
                return Compose([])
            else:
                # Val transforms
                return Compose([])
            ```
        """
        raise NotImplementedError  # æŠ›å‡ºæœªå®ç°é”™è¯¯

    def get_labels(self):
        """
        Users can customize their own format here.
        ç”¨æˆ·å¯ä»¥åœ¨æ­¤å¤„è‡ªå®šä¹‰è‡ªå·±çš„æ ¼å¼ã€‚

        Note:
            Ensure output is a dictionary with the following keys:
            ```python
            dict(
                im_file=im_file,
                shape=shape,  # format: (height, width)
                cls=cls,
                bboxes=bboxes,  # xywh
                segments=segments,  # xy
                keypoints=keypoints,  # xy
                normalized=True,  # or False
                bbox_format="xyxy",  # or xywh, ltwh
            )
            ```
        """
        raise NotImplementedError  # æŠ›å‡ºæœªå®ç°é”™è¯¯
