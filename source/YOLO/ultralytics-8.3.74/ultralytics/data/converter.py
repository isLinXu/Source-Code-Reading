# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import json
import random
import shutil
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

import cv2
import numpy as np
from PIL import Image

from ultralytics.utils import DATASETS_DIR, LOGGER, NUM_THREADS, TQDM
from ultralytics.utils.downloads import download
from ultralytics.utils.files import increment_path


def coco91_to_coco80_class():
    """
    Converts 91-index COCO class IDs to 80-index COCO class IDs.
    å°†91ç´¢å¼•çš„COCOç±»IDè½¬æ¢ä¸º80ç´¢å¼•çš„COCOç±»IDã€‚

    Returns:
        (list): A list of 91 class IDs where the index represents the 80-index class ID and the value is the
            corresponding 91-index class ID.
        (list): ä¸€ä¸ªåŒ…å«91ä¸ªç±»IDçš„åˆ—è¡¨ï¼Œå…¶ä¸­ç´¢å¼•è¡¨ç¤º80ç´¢å¼•ç±»IDï¼Œå€¼ä¸ºå¯¹åº”çš„91ç´¢å¼•ç±»IDã€‚
    """
    return [
        0,  # ç±»åˆ«0
        1,  # ç±»åˆ«1
        2,  # ç±»åˆ«2
        3,  # ç±»åˆ«3
        4,  # ç±»åˆ«4
        5,  # ç±»åˆ«5
        6,  # ç±»åˆ«6
        7,  # ç±»åˆ«7
        8,  # ç±»åˆ«8
        9,  # ç±»åˆ«9
        10,  # ç±»åˆ«10
        None,  # ç±»åˆ«11ï¼ˆæ— ï¼‰
        11,  # ç±»åˆ«12
        12,  # ç±»åˆ«13
        13,  # ç±»åˆ«14
        14,  # ç±»åˆ«15
        15,  # ç±»åˆ«16
        16,  # ç±»åˆ«17
        17,  # ç±»åˆ«18
        18,  # ç±»åˆ«19
        19,  # ç±»åˆ«20
        20,  # ç±»åˆ«21
        21,  # ç±»åˆ«22
        22,  # ç±»åˆ«23
        23,  # ç±»åˆ«24
        None,  # ç±»åˆ«25ï¼ˆæ— ï¼‰
        24,  # ç±»åˆ«26
        25,  # ç±»åˆ«27
        None,  # ç±»åˆ«28ï¼ˆæ— ï¼‰
        None,  # ç±»åˆ«29ï¼ˆæ— ï¼‰
        26,  # ç±»åˆ«30
        27,  # ç±»åˆ«31
        28,  # ç±»åˆ«32
        29,  # ç±»åˆ«33
        30,  # ç±»åˆ«34
        31,  # ç±»åˆ«35
        32,  # ç±»åˆ«36
        33,  # ç±»åˆ«37
        34,  # ç±»åˆ«38
        35,  # ç±»åˆ«39
        36,  # ç±»åˆ«40
        37,  # ç±»åˆ«41
        38,  # ç±»åˆ«42
        39,  # ç±»åˆ«43
        None,  # ç±»åˆ«44ï¼ˆæ— ï¼‰
        40,  # ç±»åˆ«45
        41,  # ç±»åˆ«46
        42,  # ç±»åˆ«47
        43,  # ç±»åˆ«48
        44,  # ç±»åˆ«49
        45,  # ç±»åˆ«50
        46,  # ç±»åˆ«51
        47,  # ç±»åˆ«52
        48,  # ç±»åˆ«53
        49,  # ç±»åˆ«54
        50,  # ç±»åˆ«55
        51,  # ç±»åˆ«56
        52,  # ç±»åˆ«57
        53,  # ç±»åˆ«58
        54,  # ç±»åˆ«59
        55,  # ç±»åˆ«60
        56,  # ç±»åˆ«61
        57,  # ç±»åˆ«62
        58,  # ç±»åˆ«63
        59,  # ç±»åˆ«64
        None,  # ç±»åˆ«65ï¼ˆæ— ï¼‰
        60,  # ç±»åˆ«66
        None,  # ç±»åˆ«67ï¼ˆæ— ï¼‰
        None,  # ç±»åˆ«68ï¼ˆæ— ï¼‰
        61,  # ç±»åˆ«69
        None,  # ç±»åˆ«70ï¼ˆæ— ï¼‰
        62,  # ç±»åˆ«71
        63,  # ç±»åˆ«72
        64,  # ç±»åˆ«73
        65,  # ç±»åˆ«74
        66,  # ç±»åˆ«75
        67,  # ç±»åˆ«76
        68,  # ç±»åˆ«77
        69,  # ç±»åˆ«78
        70,  # ç±»åˆ«79
        71,  # ç±»åˆ«80
        72,  # ç±»åˆ«81
        None,  # ç±»åˆ«82ï¼ˆæ— ï¼‰
        73,  # ç±»åˆ«83
        74,  # ç±»åˆ«84
        75,  # ç±»åˆ«85
        76,  # ç±»åˆ«86
        77,  # ç±»åˆ«87
        78,  # ç±»åˆ«88
        79,  # ç±»åˆ«89
        None,  # ç±»åˆ«90ï¼ˆæ— ï¼‰
    ]


def coco80_to_coco91_class():
    r"""
    Converts 80-index (val2014) to 91-index (paper).
    å°†80ç´¢å¼•ï¼ˆval2014ï¼‰è½¬æ¢ä¸º91ç´¢å¼•ï¼ˆè®ºæ–‡ï¼‰ã€‚

    For details see https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/.
    æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/ã€‚

    Example:
        ```python
        import numpy as np

        a = np.loadtxt("data/coco.names", dtype="str", delimiter="\n")  # åŠ è½½COCOç±»åç§°
        b = np.loadtxt("data/coco_paper.names", dtype="str", delimiter="\n")  # åŠ è½½COCOè®ºæ–‡ç±»åç§°
        x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet to coco darknetåˆ°coco
        x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]  # coco to darknet cocoåˆ°darknet
        ```
    """
    return [
        1,  # ç±»åˆ«1
        2,  # ç±»åˆ«2
        3,  # ç±»åˆ«3
        4,  # ç±»åˆ«4
        5,  # ç±»åˆ«5
        6,  # ç±»åˆ«6
        7,  # ç±»åˆ«7
        8,  # ç±»åˆ«8
        9,  # ç±»åˆ«9
        10,  # ç±»åˆ«10
        11,  # ç±»åˆ«11
        13,  # ç±»åˆ«12
        14,  # ç±»åˆ«13
        15,  # ç±»åˆ«14
        16,  # ç±»åˆ«15
        17,  # ç±»åˆ«16
        18,  # ç±»åˆ«17
        19,  # ç±»åˆ«18
        20,  # ç±»åˆ«19
        21,  # ç±»åˆ«20
        22,  # ç±»åˆ«21
        23,  # ç±»åˆ«22
        24,  # ç±»åˆ«23
        25,  # ç±»åˆ«24
        27,  # ç±»åˆ«25
        28,  # ç±»åˆ«26
        31,  # ç±»åˆ«27
        32,  # ç±»åˆ«28
        33,  # ç±»åˆ«29
        34,  # ç±»åˆ«30
        35,  # ç±»åˆ«31
        36,  # ç±»åˆ«32
        37,  # ç±»åˆ«33
        38,  # ç±»åˆ«34
        39,  # ç±»åˆ«35
        40,  # ç±»åˆ«36
        41,  # ç±»åˆ«37
        42,  # ç±»åˆ«38
        43,  # ç±»åˆ«39
        44,  # ç±»åˆ«40
        46,  # ç±»åˆ«41
        47,  # ç±»åˆ«42
        48,  # ç±»åˆ«43
        49,  # ç±»åˆ«44
        50,  # ç±»åˆ«45
        51,  # ç±»åˆ«46
        52,  # ç±»åˆ«47
        53,  # ç±»åˆ«48
        54,  # ç±»åˆ«49
        55,  # ç±»åˆ«50
        56,  # ç±»åˆ«51
        57,  # ç±»åˆ«52
        58,  # ç±»åˆ«53
        59,  # ç±»åˆ«54
        60,  # ç±»åˆ«55
        61,  # ç±»åˆ«56
        62,  # ç±»åˆ«57
        63,  # ç±»åˆ«58
        64,  # ç±»åˆ«59
        65,  # ç±»åˆ«60
        67,  # ç±»åˆ«61
        70,  # ç±»åˆ«62
        72,  # ç±»åˆ«63
        73,  # ç±»åˆ«64
        74,  # ç±»åˆ«65
        75,  # ç±»åˆ«66
        76,  # ç±»åˆ«67
        77,  # ç±»åˆ«68
        78,  # ç±»åˆ«69
        79,  # ç±»åˆ«70
        80,  # ç±»åˆ«71
        81,  # ç±»åˆ«72
        82,  # ç±»åˆ«73
        84,  # ç±»åˆ«74
        85,  # ç±»åˆ«75
        86,  # ç±»åˆ«76
        87,  # ç±»åˆ«77
        88,  # ç±»åˆ«78
        89,  # ç±»åˆ«79
        90,  # ç±»åˆ«80
    ]


def convert_coco(
    labels_dir="../coco/annotations/",
    save_dir="coco_converted/",
    use_segments=False,
    use_keypoints=False,
    cls91to80=True,
    lvis=False,
):
    """
    Converts COCO dataset annotations to a YOLO annotation format suitable for training YOLO models.
    å°†COCOæ•°æ®é›†æ³¨é‡Šè½¬æ¢ä¸ºé€‚åˆè®­ç»ƒYOLOæ¨¡å‹çš„YOLOæ³¨é‡Šæ ¼å¼ã€‚

    Args:
        labels_dir (str, optional): Path to directory containing COCO dataset annotation files.
        labels_dir (str, optional): åŒ…å«COCOæ•°æ®é›†æ³¨é‡Šæ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚
        save_dir (str, optional): Path to directory to save results to.
        save_dir (str, optional): ä¿å­˜ç»“æœçš„ç›®å½•è·¯å¾„ã€‚
        use_segments (bool, optional): Whether to include segmentation masks in the output.
        use_segments (bool, optional): æ˜¯å¦åœ¨è¾“å‡ºä¸­åŒ…å«åˆ†å‰²æ©ç ã€‚
        use_keypoints (bool, optional): Whether to include keypoint annotations in the output.
        use_keypoints (bool, optional): æ˜¯å¦åœ¨è¾“å‡ºä¸­åŒ…å«å…³é”®ç‚¹æ³¨é‡Šã€‚
        cls91to80 (bool, optional): Whether to map 91 COCO class IDs to the corresponding 80 COCO class IDs.
        cls91to80 (bool, optional): æ˜¯å¦å°†91ä¸ªCOCOç±»IDæ˜ å°„åˆ°ç›¸åº”çš„80ä¸ªCOCOç±»IDã€‚
        lvis (bool, optional): Whether to convert data in lvis dataset way.
        lvis (bool, optional): æ˜¯å¦ä»¥lvisæ•°æ®é›†çš„æ–¹å¼è½¬æ¢æ•°æ®ã€‚

    Example:
        ```python
        from ultralytics.data.converter import convert_coco

        convert_coco("../datasets/coco/annotations/", use_segments=True, use_keypoints=False, cls91to80=False)
        convert_coco(
            "../datasets/lvis/annotations/", use_segments=True, use_keypoints=False, cls91to80=False, lvis=True
        )
        ```

    Output:
        Generates output files in the specified output directory.
        ç”ŸæˆæŒ‡å®šè¾“å‡ºç›®å½•ä¸­çš„è¾“å‡ºæ–‡ä»¶ã€‚
    """
    # Create dataset directory
    save_dir = increment_path(save_dir)  # increment if save directory already exists
    for p in save_dir / "labels", save_dir / "images":
        p.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºç›®å½•

    # Convert classes
    coco80 = coco91_to_coco80_class()  # è·å–80ç±»çš„æ˜ å°„

    # Import json
    for json_file in sorted(Path(labels_dir).resolve().glob("*.json")):  # éå†æ‰€æœ‰JSONæ–‡ä»¶
        lname = "" if lvis else json_file.stem.replace("instances_", "")  # è·å–æ–‡ä»¶å
        fn = Path(save_dir) / "labels" / lname  # æ–‡ä»¶å¤¹åç§°
        fn.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºæ–‡ä»¶å¤¹
        if lvis:
            # NOTE: create folders for both train and val in advance,
            # since LVIS val set contains images from COCO 2017 train in addition to the COCO 2017 val split.
            (fn / "train2017").mkdir(parents=True, exist_ok=True)  # åˆ›å»ºè®­ç»ƒé›†æ–‡ä»¶å¤¹
            (fn / "val2017").mkdir(parents=True, exist_ok=True)  # åˆ›å»ºéªŒè¯é›†æ–‡ä»¶å¤¹
        with open(json_file, encoding="utf-8") as f:  # æ‰“å¼€JSONæ–‡ä»¶
            data = json.load(f)  # è¯»å–JSONæ•°æ®

        # Create image dict
        images = {f"{x['id']:d}": x for x in data["images"]}  # åˆ›å»ºå›¾åƒå­—å…¸
        # Create image-annotations dict
        imgToAnns = defaultdict(list)  # åˆ›å»ºå›¾åƒä¸æ³¨é‡Šçš„å­—å…¸
        for ann in data["annotations"]:
            imgToAnns[ann["image_id"]].append(ann)  # å°†æ³¨é‡Šæ·»åŠ åˆ°å¯¹åº”çš„å›¾åƒä¸­

        image_txt = []
        # Write labels file
        for img_id, anns in TQDM(imgToAnns.items(), desc=f"Annotations {json_file}"):  # éå†å›¾åƒåŠå…¶æ³¨é‡Š
            img = images[f"{img_id:d}"]  # è·å–å›¾åƒä¿¡æ¯
            h, w = img["height"], img["width"]  # è·å–å›¾åƒé«˜åº¦å’Œå®½åº¦
            f = str(Path(img["coco_url"]).relative_to("http://images.cocodataset.org")) if lvis else img["file_name"]  # è·å–æ–‡ä»¶å
            if lvis:
                image_txt.append(str(Path("./images") / f))  # å¦‚æœæ˜¯LVISï¼Œæ·»åŠ åˆ°å›¾åƒæ–‡æœ¬åˆ—è¡¨

            bboxes = []  # è¾¹ç•Œæ¡†åˆ—è¡¨
            segments = []  # åˆ†å‰²åˆ—è¡¨
            keypoints = []  # å…³é”®ç‚¹åˆ—è¡¨
            for ann in anns:
                if ann.get("iscrowd", False):  # å¦‚æœæ˜¯æ‹¥æŒ¤çš„æ³¨é‡Šï¼Œè·³è¿‡
                    continue
                # The COCO box format is [top left x, top left y, width, height]
                box = np.array(ann["bbox"], dtype=np.float64)  # è·å–è¾¹ç•Œæ¡†
                box[:2] += box[2:] / 2  # xy top-left corner to center  å°†å·¦ä¸Šè§’åæ ‡è½¬æ¢ä¸ºä¸­å¿ƒåæ ‡
                box[[0, 2]] /= w  # normalize x  å½’ä¸€åŒ–xåæ ‡
                box[[1, 3]] /= h  # normalize y  å½’ä¸€åŒ–yåæ ‡
                if box[2] <= 0 or box[3] <= 0:  # å¦‚æœå®½åº¦æˆ–é«˜åº¦å°äºç­‰äº0
                    continue

                cls = coco80[ann["category_id"] - 1] if cls91to80 else ann["category_id"] - 1  # class è·å–ç±»åˆ«ID
                box = [cls] + box.tolist()  # åˆ›å»ºè¾¹ç•Œæ¡†åˆ—è¡¨
                if box not in bboxes:  # å¦‚æœè¾¹ç•Œæ¡†ä¸åœ¨åˆ—è¡¨ä¸­
                    bboxes.append(box)  # æ·»åŠ è¾¹ç•Œæ¡†
                    if use_segments and ann.get("segmentation") is not None:  # å¦‚æœä½¿ç”¨åˆ†å‰²å¹¶ä¸”å­˜åœ¨åˆ†å‰²æ•°æ®
                        if len(ann["segmentation"]) == 0:  # å¦‚æœåˆ†å‰²ä¸ºç©º
                            segments.append([])  # æ·»åŠ ç©ºåˆ†å‰²
                            continue
                        elif len(ann["segmentation"]) > 1:  # å¦‚æœæœ‰å¤šä¸ªåˆ†å‰²
                            s = merge_multi_segment(ann["segmentation"])  # åˆå¹¶å¤šä¸ªåˆ†å‰²
                            s = (np.concatenate(s, axis=0) / np.array([w, h])).reshape(-1).tolist()  # å½’ä¸€åŒ–åˆ†å‰²
                        else:
                            s = [j for i in ann["segmentation"] for j in i]  # all segments concatenated  æ‰€æœ‰åˆ†å‰²åˆå¹¶
                            s = (np.array(s).reshape(-1, 2) / np.array([w, h])).reshape(-1).tolist()  # å½’ä¸€åŒ–åˆ†å‰²
                        s = [cls] + s  # æ·»åŠ ç±»åˆ«ID
                        segments.append(s)  # æ·»åŠ åˆ†å‰²
                    if use_keypoints and ann.get("keypoints") is not None:  # å¦‚æœä½¿ç”¨å…³é”®ç‚¹å¹¶ä¸”å­˜åœ¨å…³é”®ç‚¹æ•°æ®
                        keypoints.append(
                            box + (np.array(ann["keypoints"]).reshape(-1, 3) / np.array([w, h, 1])).reshape(-1).tolist()  # å½’ä¸€åŒ–å…³é”®ç‚¹
                        )

            # Write
            with open((fn / f).with_suffix(".txt"), "a") as file:  # æ‰“å¼€æ ‡ç­¾æ–‡ä»¶
                for i in range(len(bboxes)):
                    if use_keypoints:  # å¦‚æœä½¿ç”¨å…³é”®ç‚¹
                        line = (*(keypoints[i]),)  # cls, box, keypoints  ç±»åˆ«ï¼Œè¾¹ç•Œæ¡†ï¼Œå…³é”®ç‚¹
                    else:
                        line = (
                            *(segments[i] if use_segments and len(segments[i]) > 0 else bboxes[i]),  # cls, box or segments  ç±»åˆ«ï¼Œè¾¹ç•Œæ¡†æˆ–åˆ†å‰²
                        )
                    file.write(("%g " * len(line)).rstrip() % line + "\n")  # å†™å…¥æ–‡ä»¶

        if lvis:
            with open((Path(save_dir) / json_file.name.replace("lvis_v1_", "").replace(".json", ".txt")), "a") as f:  # æ‰“å¼€LVISæ–‡ä»¶
                f.writelines(f"{line}\n" for line in image_txt)  # å†™å…¥å›¾åƒæ–‡æœ¬

    LOGGER.info(f"{'LVIS' if lvis else 'COCO'} data converted successfully.\nResults saved to {save_dir.resolve()}")  # æ—¥å¿—è¾“å‡ºè½¬æ¢æˆåŠŸä¿¡æ¯

def convert_segment_masks_to_yolo_seg(masks_dir, output_dir, classes):
    """
    Converts a dataset of segmentation mask images to the YOLO segmentation format.
    å°†åˆ†å‰²æ©ç å›¾åƒæ•°æ®é›†è½¬æ¢ä¸ºYOLOåˆ†å‰²æ ¼å¼ã€‚

    This function takes the directory containing the binary format mask images and converts them into YOLO segmentation format.
    æ­¤å‡½æ•°æ¥å—åŒ…å«äºŒè¿›åˆ¶æ ¼å¼æ©ç å›¾åƒçš„ç›®å½•ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºYOLOåˆ†å‰²æ ¼å¼ã€‚
    The converted masks are saved in the specified output directory.
    è½¬æ¢åçš„æ©ç ä¿å­˜åœ¨æŒ‡å®šçš„è¾“å‡ºç›®å½•ä¸­ã€‚

    Args:
        masks_dir (str): The path to the directory where all mask images (png, jpg) are stored.
        masks_dir (str): æ‰€æœ‰æ©ç å›¾åƒï¼ˆpngï¼Œjpgï¼‰å­˜å‚¨çš„ç›®å½•è·¯å¾„ã€‚
        output_dir (str): The path to the directory where the converted YOLO segmentation masks will be stored.
        output_dir (str): è½¬æ¢åçš„YOLOåˆ†å‰²æ©ç å°†å­˜å‚¨çš„ç›®å½•è·¯å¾„ã€‚
        classes (int): Total classes in the dataset i.e. for COCO classes=80
        classes (int): æ•°æ®é›†ä¸­æ€»çš„ç±»åˆ«æ•°ï¼Œå³COCOæ•°æ®é›†çš„ç±»åˆ«æ•°ä¸º80ã€‚

    Example:
        ```python
        from ultralytics.data.converter import convert_segment_masks_to_yolo_seg

        # The classes here is the total classes in the dataset, for COCO dataset we have 80 classes
        # è¿™é‡Œçš„classesæ˜¯æ•°æ®é›†ä¸­çš„æ€»ç±»åˆ«æ•°ï¼Œå¯¹äºCOCOæ•°æ®é›†ï¼Œæˆ‘ä»¬æœ‰80ä¸ªç±»åˆ«
        convert_segment_masks_to_yolo_seg("path/to/masks_directory", "path/to/output/directory", classes=80)
        ```

    Notes:
        The expected directory structure for the masks is:
        æ©ç çš„é¢„æœŸç›®å½•ç»“æ„æ˜¯ï¼š

            - masks
                â”œâ”€ mask_image_01.png or mask_image_01.jpg
                â”œâ”€ mask_image_02.png or mask_image_02.jpg
                â”œâ”€ mask_image_03.png or mask_image_03.jpg
                â””â”€ mask_image_04.png or mask_image_04.jpg

        After execution, the labels will be organized in the following structure:
        æ‰§è¡Œåï¼Œæ ‡ç­¾å°†æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç»„ç»‡ï¼š

            - output_dir
                â”œâ”€ mask_yolo_01.txt
                â”œâ”€ mask_yolo_02.txt
                â”œâ”€ mask_yolo_03.txt
                â””â”€ mask_yolo_04.txt
    """
    pixel_to_class_mapping = {i + 1: i for i in range(classes)}  # Create a mapping from pixel values to class indices
    # åˆ›å»ºä»åƒç´ å€¼åˆ°ç±»ç´¢å¼•çš„æ˜ å°„
    for mask_path in Path(masks_dir).iterdir():  # Iterate through each mask image in the directory
        # éå†ç›®å½•ä¸­çš„æ¯ä¸ªæ©ç å›¾åƒ
        if mask_path.suffix in {".png", ".jpg"}:  # Check if the file is a PNG or JPG image
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºPNGæˆ–JPGå›¾åƒ
            mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)  # Read the mask image in grayscale
            # ä»¥ç°åº¦æ¨¡å¼è¯»å–æ©ç å›¾åƒ
            img_height, img_width = mask.shape  # Get image dimensions
            # è·å–å›¾åƒå°ºå¯¸
            LOGGER.info(f"Processing {mask_path} imgsz = {img_height} x {img_width}")  # Log the processing info
            # è®°å½•å¤„ç†ä¿¡æ¯

            unique_values = np.unique(mask)  # Get unique pixel values representing different classes
            # è·å–è¡¨ç¤ºä¸åŒç±»çš„å”¯ä¸€åƒç´ å€¼
            yolo_format_data = []  # Prepare a list to hold YOLO format data

            for value in unique_values:  # Iterate through each unique pixel value
                # éå†æ¯ä¸ªå”¯ä¸€çš„åƒç´ å€¼
                if value == 0:
                    continue  # Skip background
                    # è·³è¿‡èƒŒæ™¯
                class_index = pixel_to_class_mapping.get(value, -1)  # Get the class index from the mapping
                # ä»æ˜ å°„ä¸­è·å–ç±»ç´¢å¼•
                if class_index == -1:
                    LOGGER.warning(f"Unknown class for pixel value {value} in file {mask_path}, skipping.")
                    # è®°å½•è­¦å‘Šä¿¡æ¯ï¼šæœªçŸ¥ç±»
                    continue

                # Create a binary mask for the current class and find contours
                # ä¸ºå½“å‰ç±»åˆ›å»ºäºŒè¿›åˆ¶æ©ç å¹¶æ‰¾åˆ°è½®å»“
                contours, _ = cv2.findContours(
                    (mask == value).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
                )  # Find contours
                # æ‰¾åˆ°è½®å»“

                for contour in contours:  # Iterate through each contour found
                    # éå†æ‰¾åˆ°çš„æ¯ä¸ªè½®å»“
                    if len(contour) >= 3:  # YOLO requires at least 3 points for a valid segmentation
                        # YOLOè¦æ±‚æœ‰æ•ˆåˆ†å‰²è‡³å°‘æœ‰3ä¸ªç‚¹
                        contour = contour.squeeze()  # Remove single-dimensional entries
                        # å»æ‰å•ç»´æ¡ç›®
                        yolo_format = [class_index]  # Start the YOLO format with the class index
                        # ç”¨ç±»ç´¢å¼•å¼€å§‹YOLOæ ¼å¼
                        for point in contour:  # Iterate through each point in the contour
                            # éå†è½®å»“ä¸­çš„æ¯ä¸ªç‚¹
                            # Normalize the coordinates
                            yolo_format.append(round(point[0] / img_width, 6))  # Rounding to 6 decimal places
                            # å½’ä¸€åŒ–åæ ‡ï¼Œå››èˆäº”å…¥åˆ°å°æ•°ç‚¹å6ä½
                            yolo_format.append(round(point[1] / img_height, 6))
                        yolo_format_data.append(yolo_format)  # Append the formatted data to the list
            # Save Ultralytics YOLO format data to file
            # å°†Ultralytics YOLOæ ¼å¼æ•°æ®ä¿å­˜åˆ°æ–‡ä»¶
            output_path = Path(output_dir) / f"{mask_path.stem}.txt"  # Define the output path for the YOLO file
            # å®šä¹‰YOLOæ–‡ä»¶çš„è¾“å‡ºè·¯å¾„
            with open(output_path, "w") as file:  # Open the output file for writing
                # æ‰“å¼€è¾“å‡ºæ–‡ä»¶ä»¥è¿›è¡Œå†™å…¥
                for item in yolo_format_data:  # Iterate through the YOLO format data
                    # éå†YOLOæ ¼å¼æ•°æ®
                    line = " ".join(map(str, item))  # Create a line from the YOLO format data
                    # ä»YOLOæ ¼å¼æ•°æ®åˆ›å»ºä¸€è¡Œ
                    file.write(line + "\n")  # Write the line to the file
                    # å°†è¡Œå†™å…¥æ–‡ä»¶
            LOGGER.info(f"Processed and stored at {output_path} imgsz = {img_height} x {img_width}")  # Log completion
            # è®°å½•å®Œæˆä¿¡æ¯

def convert_dota_to_yolo_obb(dota_root_path: str):
    """
    Converts DOTA dataset annotations to YOLO OBB (Oriented Bounding Box) format.
    å°†DOTAæ•°æ®é›†æ³¨é‡Šè½¬æ¢ä¸ºYOLO OBBï¼ˆå®šå‘è¾¹ç•Œæ¡†ï¼‰æ ¼å¼ã€‚

    The function processes images in the 'train' and 'val' folders of the DOTA dataset. For each image, it reads the
    associated label from the original labels directory and writes new labels in YOLO OBB format to a new directory.
    æ­¤å‡½æ•°å¤„ç†DOTAæ•°æ®é›†çš„â€œtrainâ€å’Œâ€œvalâ€æ–‡ä»¶å¤¹ä¸­çš„å›¾åƒã€‚å¯¹äºæ¯ä¸ªå›¾åƒï¼Œå®ƒä»åŸå§‹æ ‡ç­¾ç›®å½•è¯»å–ç›¸å…³æ ‡ç­¾ï¼Œå¹¶å°†æ–°çš„YOLO OBBæ ¼å¼æ ‡ç­¾å†™å…¥æ–°ç›®å½•ã€‚

    Args:
        dota_root_path (str): The root directory path of the DOTA dataset.
        dota_root_path (str): DOTAæ•°æ®é›†çš„æ ¹ç›®å½•è·¯å¾„ã€‚

    Example:
        ```python
        from ultralytics.data.converter import convert_dota_to_yolo_obb

        convert_dota_to_yolo_obb("path/to/DOTA")
        ```

    Notes:
        The directory structure assumed for the DOTA dataset:
        å‡è®¾DOTAæ•°æ®é›†çš„ç›®å½•ç»“æ„ï¼š

            - DOTA
                â”œâ”€ images
                â”‚   â”œâ”€ train
                â”‚   â””â”€ val
                â””â”€ labels
                    â”œâ”€ train_original
                    â””â”€ val_original

        After execution, the function will organize the labels into:
        æ‰§è¡Œåï¼Œè¯¥å‡½æ•°å°†æŠŠæ ‡ç­¾ç»„ç»‡æˆï¼š

            - DOTA
                â””â”€ labels
                    â”œâ”€ train
                    â””â”€ val
    """
    dota_root_path = Path(dota_root_path)  # Convert the root path to a Path object

    # Class names to indices mapping
    # ç±»ååˆ°ç´¢å¼•çš„æ˜ å°„
    class_mapping = {
        "plane": 0,
        "ship": 1,
        "storage-tank": 2,
        "baseball-diamond": 3,
        "tennis-court": 4,
        "basketball-court": 5,
        "ground-track-field": 6,
        "harbor": 7,
        "bridge": 8,
        "large-vehicle": 9,
        "small-vehicle": 10,
        "helicopter": 11,
        "roundabout": 12,
        "soccer-ball-field": 13,
        "swimming-pool": 14,
        "container-crane": 15,
        "airport": 16,
        "helipad": 17,
    }

    def convert_label(image_name, image_width, image_height, orig_label_dir, save_dir):
        """Converts a single image's DOTA annotation to YOLO OBB format and saves it to a specified directory.
        å°†å•ä¸ªå›¾åƒçš„DOTAæ³¨é‡Šè½¬æ¢ä¸ºYOLO OBBæ ¼å¼ï¼Œå¹¶å°†å…¶ä¿å­˜åˆ°æŒ‡å®šç›®å½•ã€‚
        """
        orig_label_path = orig_label_dir / f"{image_name}.txt"  # Define the original label path
        # å®šä¹‰åŸå§‹æ ‡ç­¾è·¯å¾„
        save_path = save_dir / f"{image_name}.txt"  # Define the save path for the new label
        # å®šä¹‰æ–°æ ‡ç­¾çš„ä¿å­˜è·¯å¾„

        with orig_label_path.open("r") as f, save_path.open("w") as g:  # Open original and save paths
            # æ‰“å¼€åŸå§‹è·¯å¾„å’Œä¿å­˜è·¯å¾„
            lines = f.readlines()  # Read all lines from the original label file
            # ä»åŸå§‹æ ‡ç­¾æ–‡ä»¶ä¸­è¯»å–æ‰€æœ‰è¡Œ
            for line in lines:  # Iterate through each line
                # éå†æ¯ä¸€è¡Œ
                parts = line.strip().split()  # Split the line into parts
                # å°†è¡Œåˆ†å‰²æˆéƒ¨åˆ†
                if len(parts) < 9:  # Check if the line has enough parts
                    # æ£€æŸ¥è¡Œæ˜¯å¦æœ‰è¶³å¤Ÿçš„éƒ¨åˆ†
                    continue
                class_name = parts[8]  # Get the class name from the parts
                # ä»éƒ¨åˆ†ä¸­è·å–ç±»å
                class_idx = class_mapping[class_name]  # Get the class index from the mapping
                # ä»æ˜ å°„ä¸­è·å–ç±»ç´¢å¼•
                coords = [float(p) for p in parts[:8]]  # Convert coordinates to float
                # å°†åæ ‡è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                normalized_coords = [
                    coords[i] / image_width if i % 2 == 0 else coords[i] / image_height for i in range(8)
                ]  # Normalize the coordinates
                # å½’ä¸€åŒ–åæ ‡
                formatted_coords = [f"{coord:.6g}" for coord in normalized_coords]  # Format the coordinates
                # æ ¼å¼åŒ–åæ ‡
                g.write(f"{class_idx} {' '.join(formatted_coords)}\n")  # Write the class index and coordinates to the new file
                # å°†ç±»ç´¢å¼•å’Œåæ ‡å†™å…¥æ–°æ–‡ä»¶

    for phase in ["train", "val"]:  # Iterate through train and val phases
        # éå†è®­ç»ƒå’ŒéªŒè¯é˜¶æ®µ
        image_dir = dota_root_path / "images" / phase  # Define the image directory for the phase
        # å®šä¹‰è¯¥é˜¶æ®µçš„å›¾åƒç›®å½•
        orig_label_dir = dota_root_path / "labels" / f"{phase}_original"  # Define the original label directory
        # å®šä¹‰åŸå§‹æ ‡ç­¾ç›®å½•
        save_dir = dota_root_path / "labels" / phase  # Define the save directory for the new labels
        # å®šä¹‰æ–°æ ‡ç­¾çš„ä¿å­˜ç›®å½•

        save_dir.mkdir(parents=True, exist_ok=True)  # Create the save directory if it doesn't exist
        # å¦‚æœä¿å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º

        image_paths = list(image_dir.iterdir())  # Get a list of image paths in the directory
        # è·å–ç›®å½•ä¸­å›¾åƒè·¯å¾„çš„åˆ—è¡¨
        for image_path in TQDM(image_paths, desc=f"Processing {phase} images"):  # Iterate through images with a progress bar
            # éå†å›¾åƒå¹¶æ˜¾ç¤ºè¿›åº¦æ¡
            if image_path.suffix != ".png":  # Check if the image is a PNG
                # æ£€æŸ¥å›¾åƒæ˜¯å¦ä¸ºPNG
                continue
            image_name_without_ext = image_path.stem  # Get the image name without extension
            # è·å–ä¸å¸¦æ‰©å±•åçš„å›¾åƒåç§°
            img = cv2.imread(str(image_path))  # Read the image
            # è¯»å–å›¾åƒ
            h, w = img.shape[:2]  # Get the height and width of the image
            # è·å–å›¾åƒçš„é«˜åº¦å’Œå®½åº¦
            convert_label(image_name_without_ext, w, h, orig_label_dir, save_dir)  # Convert the label for the image
            # è½¬æ¢å›¾åƒçš„æ ‡ç­¾

def min_index(arr1, arr2):
    """
    Find a pair of indexes with the shortest distance between two arrays of 2D points.
    æ‰¾åˆ°ä¸¤ä¸ªäºŒç»´ç‚¹æ•°ç»„ä¹‹é—´è·ç¦»æœ€çŸ­çš„ä¸€å¯¹ç´¢å¼•ã€‚

    Args:
        arr1 (np.ndarray): A NumPy array of shape (N, 2) representing N 2D points.
        arr1 (np.ndarray): ä¸€ä¸ªå½¢çŠ¶ä¸º(N, 2)çš„NumPyæ•°ç»„ï¼Œè¡¨ç¤ºNä¸ªäºŒç»´ç‚¹ã€‚
        arr2 (np.ndarray): A NumPy array of shape (M, 2) representing M 2D points.
        arr2 (np.ndarray): ä¸€ä¸ªå½¢çŠ¶ä¸º(M, 2)çš„NumPyæ•°ç»„ï¼Œè¡¨ç¤ºMä¸ªäºŒç»´ç‚¹ã€‚

    Returns:
        (tuple): A tuple containing the indexes of the points with the shortest distance in arr1 and arr2 respectively.
        (tuple): ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«åœ¨arr1å’Œarr2ä¸­è·ç¦»æœ€çŸ­çš„ç‚¹çš„ç´¢å¼•ã€‚
    """
    dis = ((arr1[:, None, :] - arr2[None, :, :]) ** 2).sum(-1)  # Calculate the squared distance between each pair of points
    # è®¡ç®—æ¯å¯¹ç‚¹ä¹‹é—´çš„å¹³æ–¹è·ç¦»
    return np.unravel_index(np.argmin(dis, axis=None), dis.shape)  # Return the indices of the minimum distance
    # è¿”å›æœ€å°è·ç¦»çš„ç´¢å¼•


def merge_multi_segment(segments):
    """
    Merge multiple segments into one list by connecting the coordinates with the minimum distance between each segment.
    Merge multiple segments into one list by connecting the coordinates with the minimum distance between each segment.
    é€šè¿‡è¿æ¥æ¯ä¸ªæ®µä¹‹é—´æœ€å°è·ç¦»çš„åæ ‡ï¼Œå°†å¤šä¸ªæ®µåˆå¹¶ä¸ºä¸€ä¸ªåˆ—è¡¨ã€‚

    Args:
        segments (List[List]): Original segmentations in COCO's JSON file.
                               Each element is a list of coordinates, like [segmentation1, segmentation2,...].
        segments (List[List]): COCOçš„JSONæ–‡ä»¶ä¸­çš„åŸå§‹åˆ†å‰²ã€‚æ¯ä¸ªå…ƒç´ éƒ½æ˜¯åæ ‡çš„åˆ—è¡¨ï¼Œå¦‚[segmentation1, segmentation2,...]ã€‚

    Returns:
        s (List[np.ndarray]): A list of connected segments represented as NumPy arrays.
        s (List[np.ndarray]): ä½œä¸ºNumPyæ•°ç»„è¡¨ç¤ºçš„è¿æ¥æ®µçš„åˆ—è¡¨ã€‚
    """
    s = []  # Initialize an empty list to hold the merged segments
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ä»¥ä¿å­˜åˆå¹¶çš„æ®µ
    segments = [np.array(i).reshape(-1, 2) for i in segments]  # Convert each segment to a NumPy array and reshape
    # å°†æ¯ä¸ªæ®µè½¬æ¢ä¸ºNumPyæ•°ç»„å¹¶é‡å¡‘
    idx_list = [[] for _ in range(len(segments))]  # Create a list to hold the indices of minimum distances
    # åˆ›å»ºä¸€ä¸ªåˆ—è¡¨ä»¥ä¿å­˜æœ€å°è·ç¦»çš„ç´¢å¼•

    # Record the indexes with min distance between each segment
    # è®°å½•æ¯ä¸ªæ®µä¹‹é—´æœ€å°è·ç¦»çš„ç´¢å¼•
    for i in range(1, len(segments)):  # Iterate through segments starting from the second one
        # ä»ç¬¬äºŒä¸ªæ®µå¼€å§‹éå†æ®µ
        idx1, idx2 = min_index(segments[i - 1], segments[i])  # Find the closest points between the current and previous segment
        # æ‰¾åˆ°å½“å‰æ®µå’Œå‰ä¸€ä¸ªæ®µä¹‹é—´æœ€è¿‘çš„ç‚¹
        idx_list[i - 1].append(idx1)  # Append the index of the previous segment
        idx_list[i].append(idx2)  # Append the index of the current segment

    # Use two rounds to connect all the segments
    # ä½¿ç”¨ä¸¤è½®è¿æ¥æ‰€æœ‰æ®µ
    for k in range(2):  # Iterate twice for forward and backward connections
        # ä¸ºæ­£å‘å’Œåå‘è¿æ¥è¿­ä»£ä¸¤æ¬¡
        # Forward connection
        if k == 0:
            for i, idx in enumerate(idx_list):  # Iterate through the index list
                # éå†ç´¢å¼•åˆ—è¡¨
                # Middle segments have two indexes, reverse the index of middle segments
                # ä¸­é—´æ®µæœ‰ä¸¤ä¸ªç´¢å¼•ï¼Œåè½¬ä¸­é—´æ®µçš„ç´¢å¼•
                if len(idx) == 2 and idx[0] > idx[1]:  # Check if the current segment has two indices and reverse if necessary
                    # æ£€æŸ¥å½“å‰æ®µæ˜¯å¦æœ‰ä¸¤ä¸ªç´¢å¼•ï¼Œå¦‚æœéœ€è¦åˆ™åè½¬
                    idx = idx[::-1]  # Reverse the index
                    segments[i] = segments[i][::-1, :]  # Reverse the segment order

                segments[i] = np.roll(segments[i], -idx[0], axis=0)  # Roll the segment to align with the minimum distance
                # æ»šåŠ¨æ®µä»¥ä¸æœ€å°è·ç¦»å¯¹é½
                segments[i] = np.concatenate([segments[i], segments[i][:1]])  # Concatenate the first point to close the segment
                # è¿æ¥ç¬¬ä¸€ä¸ªç‚¹ä»¥é—­åˆæ®µ
                # Deal with the first segment and the last one
                if i in {0, len(idx_list) - 1}:  # If it's the first or last segment, append it directly
                    # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªæˆ–æœ€åä¸€ä¸ªæ®µï¼Œç›´æ¥é™„åŠ 
                    s.append(segments[i])
                else:
                    idx = [0, idx[1] - idx[0]]  # Calculate the range for middle segments
                    s.append(segments[i][idx[0]: idx[1] + 1])  # Append the relevant part of the segment

        else:  # Backward connection
            for i in range(len(idx_list) - 1, -1, -1):  # Iterate backward through the index list
                # åå‘éå†ç´¢å¼•åˆ—è¡¨
                if i not in {0, len(idx_list) - 1}:  # Skip the first and last segments
                    # è·³è¿‡ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªæ®µ
                    idx = idx_list[i]  # Get the index for the current segment
                    nidx = abs(idx[1] - idx[0])  # Calculate the absolute difference of the indices
                    s.append(segments[i][nidx:])  # Append the segment from the calculated index
    return s  # Return the merged segments
    # è¿”å›åˆå¹¶çš„æ®µ


def yolo_bbox2segment(im_dir, save_dir=None, sam_model="sam_b.pt", device=None):
    """
    Converts existing object detection dataset (bounding boxes) to segmentation dataset or oriented bounding box (OBB)
    in YOLO format. Generates segmentation data using SAM auto-annotator as needed.
    å°†ç°æœ‰çš„ç›®æ ‡æ£€æµ‹æ•°æ®é›†ï¼ˆè¾¹ç•Œæ¡†ï¼‰è½¬æ¢ä¸ºYOLOæ ¼å¼çš„åˆ†å‰²æ•°æ®é›†æˆ–å®šå‘è¾¹ç•Œæ¡†ï¼ˆOBBï¼‰ã€‚æ ¹æ®éœ€è¦ä½¿ç”¨SAMè‡ªåŠ¨æ ‡æ³¨å™¨ç”Ÿæˆåˆ†å‰²æ•°æ®ã€‚

    Args:
        im_dir (str | Path): Path to image directory to convert.
        im_dir (str | Path): è¦è½¬æ¢çš„å›¾åƒç›®å½•çš„è·¯å¾„ã€‚
        save_dir (str | Path): Path to save the generated labels, labels will be saved
            into `labels-segment` in the same directory level of `im_dir` if save_dir is None. Default: None.
        save_dir (str | Path): ä¿å­˜ç”Ÿæˆæ ‡ç­¾çš„è·¯å¾„ï¼Œå¦‚æœsave_dirä¸ºNoneï¼Œåˆ™æ ‡ç­¾å°†ä¿å­˜åœ¨ä¸im_diråŒä¸€ç›®å½•çº§åˆ«çš„`labels-segment`ä¸­ã€‚é»˜è®¤å€¼ï¼šNoneã€‚
        sam_model (str): Segmentation model to use for intermediate segmentation data; optional.
        sam_model (str): ç”¨äºä¸­é—´åˆ†å‰²æ•°æ®çš„åˆ†å‰²æ¨¡å‹ï¼›å¯é€‰ã€‚
        device (int | str): The specific device to run SAM models. Default: None.
        device (int | str): è¿è¡ŒSAMæ¨¡å‹çš„ç‰¹å®šè®¾å¤‡ã€‚é»˜è®¤å€¼ï¼šNoneã€‚

    Notes:
        The input directory structure assumed for dataset:
        å‡è®¾æ•°æ®é›†çš„è¾“å…¥ç›®å½•ç»“æ„ï¼š

            - im_dir
                â”œâ”€ 001.jpg
                â”œâ”€ ...
                â””â”€ NNN.jpg
            - labels
                â”œâ”€ 001.txt
                â”œâ”€ ...
                â””â”€ NNN.txt
    """
    from ultralytics import SAM  # Import the SAM model for segmentation
    from ultralytics.data import YOLODataset  # Import the YOLO dataset class
    from ultralytics.utils import LOGGER  # Import the logger for logging information
    from ultralytics.utils.ops import xywh2xyxy  # Import the function to convert bounding box formats

    # NOTE: add placeholder to pass class index check
    # æ³¨æ„ï¼šæ·»åŠ å ä½ç¬¦ä»¥é€šè¿‡ç±»ç´¢å¼•æ£€æŸ¥
    dataset = YOLODataset(im_dir, data=dict(names=list(range(1000))))  # Create a YOLO dataset object
    # åˆ›å»ºä¸€ä¸ªYOLOæ•°æ®é›†å¯¹è±¡
    if len(dataset.labels[0]["segments"]) > 0:  # if it's segment data
        # å¦‚æœæ˜¯åˆ†å‰²æ•°æ®
        LOGGER.info("Segmentation labels detected, no need to generate new ones!")  # Log that segmentation labels are already present
        # è®°å½•åˆ†å‰²æ ‡ç­¾å·²å­˜åœ¨çš„ä¿¡æ¯
        return  # Exit the function

    LOGGER.info("Detection labels detected, generating segment labels by SAM model!")  # Log that detection labels are found
    # è®°å½•æ£€æµ‹æ ‡ç­¾å·²æ‰¾åˆ°çš„ä¿¡æ¯
    sam_model = SAM(sam_model)  # Load the SAM model for segmentation
    # åŠ è½½ç”¨äºåˆ†å‰²çš„SAMæ¨¡å‹
    for label in TQDM(dataset.labels, total=len(dataset.labels), desc="Generating segment labels"):  # Iterate through labels with a progress bar
        # éå†æ ‡ç­¾å¹¶æ˜¾ç¤ºè¿›åº¦æ¡
        h, w = label["shape"]  # Get the shape of the image
        # è·å–å›¾åƒçš„å½¢çŠ¶
        boxes = label["bboxes"]  # Get the bounding boxes from the label
        # ä»æ ‡ç­¾ä¸­è·å–è¾¹ç•Œæ¡†
        if len(boxes) == 0:  # skip empty labels
            # è·³è¿‡ç©ºæ ‡ç­¾
            continue
        boxes[:, [0, 2]] *= w  # Scale the x-coordinates of the bounding boxes
        # ç¼©æ”¾è¾¹ç•Œæ¡†çš„xåæ ‡
        boxes[:, [1, 3]] *= h  # Scale the y-coordinates of the bounding boxes
        # ç¼©æ”¾è¾¹ç•Œæ¡†çš„yåæ ‡
        im = cv2.imread(label["im_file"])  # Read the image file
        # è¯»å–å›¾åƒæ–‡ä»¶
        sam_results = sam_model(im, bboxes=xywh2xyxy(boxes), verbose=False, save=False, device=device)  # Generate segmentation using SAM
        # ä½¿ç”¨SAMç”Ÿæˆåˆ†å‰²
        label["segments"] = sam_results[0].masks.xyn  # Store the segmentation results in the label
        # å°†åˆ†å‰²ç»“æœå­˜å‚¨åœ¨æ ‡ç­¾ä¸­

    save_dir = Path(save_dir) if save_dir else Path(im_dir).parent / "labels-segment"  # Define the save directory
    # å®šä¹‰ä¿å­˜ç›®å½•
    save_dir.mkdir(parents=True, exist_ok=True)  # Create the save directory if it doesn't exist
    # å¦‚æœä¿å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º
    for label in dataset.labels:  # Iterate through each label in the dataset
        # éå†æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ ‡ç­¾
        texts = []  # Initialize a list to hold the text lines for the label
        # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨ä»¥ä¿å­˜æ ‡ç­¾çš„æ–‡æœ¬è¡Œ
        lb_name = Path(label["im_file"]).with_suffix(".txt").name  # Get the label filename corresponding to the image
        # è·å–ä¸å›¾åƒå¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶å
        txt_file = save_dir / lb_name  # Define the path for the label text file
        # å®šä¹‰æ ‡ç­¾æ–‡æœ¬æ–‡ä»¶çš„è·¯å¾„
        cls = label["cls"]  # Get the class index from the label
        # ä»æ ‡ç­¾ä¸­è·å–ç±»ç´¢å¼•
        for i, s in enumerate(label["segments"]):  # Iterate through each segment in the label
            # éå†æ ‡ç­¾ä¸­çš„æ¯ä¸ªæ®µ
            if len(s) == 0:  # Skip empty segments
                # è·³è¿‡ç©ºæ®µ
                continue
            line = (int(cls[i]), *s.reshape(-1))  # Create a line with the class index and segment coordinates
            # åˆ›å»ºä¸€è¡Œï¼ŒåŒ…å«ç±»ç´¢å¼•å’Œæ®µåæ ‡
            texts.append(("%g " * len(line)).rstrip() % line)  # Format the line and append to the texts list
            # æ ¼å¼åŒ–è¡Œå¹¶é™„åŠ åˆ°æ–‡æœ¬åˆ—è¡¨ä¸­
        with open(txt_file, "a") as f:  # Open the text file for appending
            # æ‰“å¼€æ–‡æœ¬æ–‡ä»¶ä»¥è¿›è¡Œè¿½åŠ 
            f.writelines(text + "\n" for text in texts)  # Write the formatted lines to the file
            # å°†æ ¼å¼åŒ–çš„è¡Œå†™å…¥æ–‡ä»¶
    LOGGER.info(f"Generated segment labels saved in {save_dir}")  # Log the completion of label generation
    # è®°å½•æ ‡ç­¾ç”Ÿæˆå®Œæˆçš„ä¿¡æ¯


def create_synthetic_coco_dataset():
    """
    Creates a synthetic COCO dataset with random images based on filenames from label lists.
    åˆ›å»ºä¸€ä¸ªåŸºäºæ ‡ç­¾åˆ—è¡¨ä¸­çš„æ–‡ä»¶åçš„éšæœºå›¾åƒåˆæˆCOCOæ•°æ®é›†ã€‚

    This function downloads COCO labels, reads image filenames from label list files,
    creates synthetic images for train2017 and val2017 subsets, and organizes
    them in the COCO dataset structure. It uses multithreading to generate images efficiently.
    æ­¤å‡½æ•°ä¸‹è½½COCOæ ‡ç­¾ï¼Œä»æ ‡ç­¾åˆ—è¡¨æ–‡ä»¶ä¸­è¯»å–å›¾åƒæ–‡ä»¶åï¼Œä¸ºtrain2017å’Œval2017å­é›†åˆ›å»ºåˆæˆå›¾åƒï¼Œå¹¶å°†å®ƒä»¬ç»„ç»‡åœ¨COCOæ•°æ®é›†ç»“æ„ä¸­ã€‚å®ƒä½¿ç”¨å¤šçº¿ç¨‹é«˜æ•ˆç”Ÿæˆå›¾åƒã€‚

    Examples:
        >>> from ultralytics.data.converter import create_synthetic_coco_dataset
        >>> create_synthetic_coco_dataset()
    ç¤ºä¾‹ï¼š
        >>> from ultralytics.data.converter import create_synthetic_coco_dataset
        >>> create_synthetic_coco_dataset()

    Notes:
        - Requires internet connection to download label files.
        - éœ€è¦äº’è”ç½‘è¿æ¥ä»¥ä¸‹è½½æ ‡ç­¾æ–‡ä»¶ã€‚
        - Generates random RGB images of varying sizes (480x480 to 640x640 pixels).
        - ç”Ÿæˆä¸åŒå¤§å°ï¼ˆ480x480åˆ°640x640åƒç´ ï¼‰çš„éšæœºRGBå›¾åƒã€‚
        - Existing test2017 directory is removed as it's not needed.
        - åˆ é™¤ç°æœ‰çš„test2017ç›®å½•ï¼Œå› ä¸ºä¸éœ€è¦ã€‚
        - Reads image filenames from train2017.txt and val2017.txt files.
        - ä»train2017.txtå’Œval2017.txtæ–‡ä»¶ä¸­è¯»å–å›¾åƒæ–‡ä»¶åã€‚
    """

    def create_synthetic_image(image_file):
        """Generates synthetic images with random sizes and colors for dataset augmentation or testing purposes.
        ç”Ÿæˆå…·æœ‰éšæœºå¤§å°å’Œé¢œè‰²çš„åˆæˆå›¾åƒï¼Œç”¨äºæ•°æ®é›†å¢å¼ºæˆ–æµ‹è¯•ç›®çš„ã€‚
        """
        if not image_file.exists():  # Check if the image file already exists
            # æ£€æŸ¥å›¾åƒæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            size = (random.randint(480, 640), random.randint(480, 640))  # Generate random size for the image
            # ç”Ÿæˆå›¾åƒçš„éšæœºå¤§å°
            Image.new(
                "RGB",
                size=size,
                color=(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)),
            ).save(image_file)  # Create and save a new image with random color
            # åˆ›å»ºå¹¶ä¿å­˜å…·æœ‰éšæœºé¢œè‰²çš„æ–°å›¾åƒ

    # Download labels
    dir = DATASETS_DIR / "coco"  # Define the directory for COCO dataset
    # å®šä¹‰COCOæ•°æ®é›†çš„ç›®å½•
    url = "https://github.com/ultralytics/assets/releases/download/v0.0.0/"  # Base URL for downloading labels
    # ä¸‹è½½æ ‡ç­¾çš„åŸºæœ¬URL
    label_zip = "coco2017labels-segments.zip"  # Define the label zip file name
    # å®šä¹‰æ ‡ç­¾zipæ–‡ä»¶å
    download([url + label_zip], dir=dir.parent)  # Download the label zip file
    # ä¸‹è½½æ ‡ç­¾zipæ–‡ä»¶

    # Create synthetic images
    shutil.rmtree(dir / "labels" / "test2017", ignore_errors=True)  # Remove test2017 directory as not needed
    # åˆ é™¤test2017ç›®å½•ï¼Œå› ä¸ºä¸éœ€è¦
    with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:  # Create a thread pool for concurrent image generation
        # åˆ›å»ºçº¿ç¨‹æ± ä»¥å¹¶å‘ç”Ÿæˆå›¾åƒ
        for subset in ["train2017", "val2017"]:  # Iterate through the train and validation subsets
            # éå†è®­ç»ƒå’ŒéªŒè¯å­é›†
            subset_dir = dir / "images" / subset  # Define the directory for the current subset
            # å®šä¹‰å½“å‰å­é›†çš„ç›®å½•
            subset_dir.mkdir(parents=True, exist_ok=True)  # Create the subset directory if it doesn't exist
            # å¦‚æœå­é›†ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º

            # Read image filenames from label list file
            label_list_file = dir / f"{subset}.txt"  # Define the path for the label list file
            # å®šä¹‰æ ‡ç­¾åˆ—è¡¨æ–‡ä»¶çš„è·¯å¾„
            if label_list_file.exists():  # Check if the label list file exists
                # æ£€æŸ¥æ ‡ç­¾åˆ—è¡¨æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                with open(label_list_file) as f:  # Open the label list file
                    # æ‰“å¼€æ ‡ç­¾åˆ—è¡¨æ–‡ä»¶
                    image_files = [dir / line.strip() for line in f]  # Read image filenames from the file
                    # ä»æ–‡ä»¶ä¸­è¯»å–å›¾åƒæ–‡ä»¶å

                # Submit all tasks
                futures = [executor.submit(create_synthetic_image, image_file) for image_file in image_files]  # Submit tasks to create images
                # æäº¤ä»»åŠ¡ä»¥åˆ›å»ºå›¾åƒ
                for _ in TQDM(as_completed(futures), total=len(futures), desc=f"Generating images for {subset}"):
                    pass  # The actual work is done in the background
                    # å®é™…å·¥ä½œåœ¨åå°å®Œæˆ
            else:
                print(f"Warning: Labels file {label_list_file} does not exist. Skipping image creation for {subset}.")
                # è­¦å‘Šï¼šæ ‡ç­¾æ–‡ä»¶{label_list_file}ä¸å­˜åœ¨ã€‚è·³è¿‡{subset}çš„å›¾åƒåˆ›å»ºã€‚

    print("Synthetic COCO dataset created successfully.")  # Print success message
    # æ‰“å°æˆåŠŸæ¶ˆæ¯