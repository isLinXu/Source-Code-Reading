# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import json
from collections import defaultdict
from itertools import repeat
from multiprocessing.pool import ThreadPool
from pathlib import Path

import cv2
import numpy as np
import torch
from PIL import Image
from torch.utils.data import ConcatDataset

from ultralytics.utils import LOCAL_RANK, NUM_THREADS, TQDM, colorstr
from ultralytics.utils.ops import resample_segments
from ultralytics.utils.torch_utils import TORCHVISION_0_18

from .augment import (
    Compose,
    Format,
    Instances,
    LetterBox,
    RandomLoadText,
    classify_augmentations,
    classify_transforms,
    v8_transforms,
)
from .base import BaseDataset
from .utils import (
    HELP_URL,
    LOGGER,
    get_hash,
    img2label_paths,
    load_dataset_cache_file,
    save_dataset_cache_file,
    verify_image,
    verify_image_label,
)

# Ultralytics dataset *.cache version, >= 1.0.0 for YOLOv8
# Ultralyticsæ•°æ®é›†*.cacheç‰ˆæœ¬ï¼Œ>= 1.0.0ç”¨äºYOLOv8
DATASET_CACHE_VERSION = "1.0.3"


class YOLODataset(BaseDataset):
    """
    Dataset class for loading object detection and/or segmentation labels in YOLO format.
    ç”¨äºåŠ è½½YOLOæ ¼å¼çš„ç›®æ ‡æ£€æµ‹å’Œ/æˆ–åˆ†å‰²æ ‡ç­¾çš„æ•°æ®é›†ç±»ã€‚

    Args:
        data (dict, optional): A dataset YAML dictionary. Defaults to None.
        data (dict, optional): æ•°æ®é›†YAMLå­—å…¸ã€‚é»˜è®¤ä¸ºNoneã€‚
        task (str): An explicit arg to point current task, Defaults to 'detect'.
        task (str): æŒ‡å®šå½“å‰ä»»åŠ¡çš„æ˜¾å¼å‚æ•°ï¼Œé»˜è®¤ä¸º'detect'ã€‚

    Returns:
        (torch.utils.data.Dataset): A PyTorch dataset object that can be used for training an object detection model.
        (torch.utils.data.Dataset): å¯ä»¥ç”¨äºè®­ç»ƒç›®æ ‡æ£€æµ‹æ¨¡å‹çš„PyTorchæ•°æ®é›†å¯¹è±¡ã€‚
    """

    def __init__(self, *args, data=None, task="detect", **kwargs):
        """Initializes the YOLODataset with optional configurations for segments and keypoints.
        ä½¿ç”¨å¯é€‰çš„æ®µå’Œå…³é”®ç‚¹é…ç½®åˆå§‹åŒ–YOLODatasetã€‚
        """
        self.use_segments = task == "segment"  # Check if the task is segmentation
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦ä¸ºåˆ†å‰²
        self.use_keypoints = task == "pose"  # Check if the task is pose estimation
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦ä¸ºå§¿æ€ä¼°è®¡
        self.use_obb = task == "obb"  # Check if the task is oriented bounding box
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦ä¸ºå®šå‘è¾¹ç•Œæ¡†
        self.data = data  # Store the dataset information
        # å­˜å‚¨æ•°æ®é›†ä¿¡æ¯
        assert not (self.use_segments and self.use_keypoints), "Can not use both segments and keypoints."
        # ç¡®ä¿ä¸èƒ½åŒæ—¶ä½¿ç”¨æ®µå’Œå…³é”®ç‚¹
        super().__init__(*args, **kwargs)  # Initialize the base class


    def cache_labels(self, path=Path("./labels.cache")):
        """
        Cache dataset labels, check images and read shapes.
        ç¼“å­˜æ•°æ®é›†æ ‡ç­¾ï¼Œæ£€æŸ¥å›¾åƒå¹¶è¯»å–å½¢çŠ¶ã€‚

        Args:
            path (Path): Path where to save the cache file. Default is Path("./labels.cache").
            path (Path): ç¼“å­˜æ–‡ä»¶ä¿å­˜è·¯å¾„ã€‚é»˜è®¤æ˜¯Path("./labels.cache")ã€‚

        Returns:
            (dict): labels.
        """
        x = {"labels": []}  # Initialize a dictionary to hold labels
        # åˆå§‹åŒ–ä¸€ä¸ªå­—å…¸ä»¥ä¿å­˜æ ‡ç­¾
        nm, nf, ne, nc, msgs = 0, 0, 0, 0, []  # number missing, found, empty, corrupt, messages
        # ç¼ºå¤±ã€æ‰¾åˆ°ã€ç©ºã€æŸåçš„æ•°é‡å’Œæ¶ˆæ¯
        desc = f"{self.prefix}Scanning {path.parent / path.stem}..."
        total = len(self.im_files)  # Total number of images
        # å›¾åƒæ€»æ•°
        nkpt, ndim = self.data.get("kpt_shape", (0, 0))  # Get keypoint shape
        # è·å–å…³é”®ç‚¹å½¢çŠ¶
        if self.use_keypoints and (nkpt <= 0 or ndim not in {2, 3}):
            raise ValueError(
                "'kpt_shape' in data.yaml missing or incorrect. Should be a list with [number of "
                "keypoints, number of dims (2 for x,y or 3 for x,y,visible)], i.e. 'kpt_shape: [17, 3]'"
            )
        # æ£€æŸ¥å…³é”®ç‚¹å½¢çŠ¶æ˜¯å¦æ­£ç¡®
        with ThreadPool(NUM_THREADS) as pool:  # Create a thread pool for concurrent processing
            # åˆ›å»ºçº¿ç¨‹æ± ä»¥è¿›è¡Œå¹¶å‘å¤„ç†
            results = pool.imap(
                func=verify_image_label,  # Function to verify image and label
                iterable=zip(
                    self.im_files,  # Image files
                    self.label_files,  # Label files
                    repeat(self.prefix),  # Repeat prefix for logging
                    repeat(self.use_keypoints),  # Repeat use_keypoints flag
                    repeat(len(self.data["names"])),  # Repeat number of classes
                    repeat(nkpt),  # Repeat number of keypoints
                    repeat(ndim),  # Repeat dimension of keypoints
                ),
            )
            pbar = TQDM(results, desc=desc, total=total)  # Progress bar for processing
            # å¤„ç†çš„è¿›åº¦æ¡
            for im_file, lb, shape, segments, keypoint, nm_f, nf_f, ne_f, nc_f, msg in pbar:
                nm += nm_f  # Update number of missing files
                nf += nf_f  # Update number of found files
                ne += ne_f  # Update number of empty files
                nc += nc_f  # Update number of corrupt files
                if im_file:  # If the image file is valid
                    x["labels"].append(
                        {
                            "im_file": im_file,  # Add image file path
                            "shape": shape,  # Add image shape
                            "cls": lb[:, 0:1],  # n, 1
                            "bboxes": lb[:, 1:],  # n, 4
                            "segments": segments,  # Add segments
                            "keypoints": keypoint,  # Add keypoints
                            "normalized": True,  # Indicate that the data is normalized
                            "bbox_format": "xywh",  # Bounding box format
                        }
                    )
                if msg:  # If there are any messages
                    msgs.append(msg)  # Append messages to the list
                pbar.desc = f"{desc} {nf} images, {nm + ne} backgrounds, {nc} corrupt"  # Update progress bar description
                # æ›´æ–°è¿›åº¦æ¡æè¿°
            pbar.close()  # Close the progress bar

        if msgs:  # If there are any messages
            LOGGER.info("\n".join(msgs))  # Log all messages
        if nf == 0:  # If no labels found
            LOGGER.warning(f"{self.prefix}WARNING âš ï¸ No labels found in {path}. {HELP_URL}")
            # è®°å½•æ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾çš„è­¦å‘Šä¿¡æ¯
        x["hash"] = get_hash(self.label_files + self.im_files)  # Get hash of the dataset
        # è·å–æ•°æ®é›†çš„å“ˆå¸Œå€¼
        x["results"] = nf, nm, ne, nc, len(self.im_files)  # Store results in the dictionary
        # å°†ç»“æœå­˜å‚¨åœ¨å­—å…¸ä¸­
        x["msgs"] = msgs  # warnings
        save_dataset_cache_file(self.prefix, path, x, DATASET_CACHE_VERSION)  # Save the cache file
        # ä¿å­˜ç¼“å­˜æ–‡ä»¶
        return x  # Return the labels dictionary


    def get_labels(self):
        """Returns dictionary of labels for YOLO training.
        è¿”å›YOLOè®­ç»ƒçš„æ ‡ç­¾å­—å…¸ã€‚
        """
        self.label_files = img2label_paths(self.im_files)  # Get label files corresponding to the image files
        # è·å–ä¸å›¾åƒæ–‡ä»¶å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶
        cache_path = Path(self.label_files[0]).parent.with_suffix(".cache")  # Define cache path
        # å®šä¹‰ç¼“å­˜è·¯å¾„
        try:
            cache, exists = load_dataset_cache_file(cache_path), True  # attempt to load a *.cache file
            # å°è¯•åŠ è½½*.cacheæ–‡ä»¶
            assert cache["version"] == DATASET_CACHE_VERSION  # matches current version
            # ç¡®ä¿ç‰ˆæœ¬åŒ¹é…
            assert cache["hash"] == get_hash(self.label_files + self.im_files)  # identical hash
            # ç¡®ä¿å“ˆå¸Œå€¼ç›¸åŒ
        except (FileNotFoundError, AssertionError, AttributeError):
            cache, exists = self.cache_labels(cache_path), False  # run cache ops
            # è¿è¡Œç¼“å­˜æ“ä½œ

        # Display cache
        nf, nm, ne, nc, n = cache.pop("results")  # found, missing, empty, corrupt, total
        # æ‰¾åˆ°ã€ç¼ºå¤±ã€ç©ºã€æŸåã€æ€»æ•°
        if exists and LOCAL_RANK in {-1, 0}:  # If cache exists and is the main process
            # å¦‚æœç¼“å­˜å­˜åœ¨ä¸”æ˜¯ä¸»è¿›ç¨‹
            d = f"Scanning {cache_path}... {nf} images, {nm + ne} backgrounds, {nc} corrupt"
            TQDM(None, desc=self.prefix + d, total=n, initial=n)  # display results
            # æ˜¾ç¤ºç»“æœ
            if cache["msgs"]:  # If there are messages in cache
                LOGGER.info("\n".join(cache["msgs"]))  # display warnings
                # æ˜¾ç¤ºè­¦å‘Š

        # Read cache
        [cache.pop(k) for k in ("hash", "version", "msgs")]  # remove items
        # ç§»é™¤é¡¹ç›®
        labels = cache["labels"]  # Get labels from cache
        # ä»ç¼“å­˜ä¸­è·å–æ ‡ç­¾
        if not labels:  # If no labels found
            LOGGER.warning(f"WARNING âš ï¸ No images found in {cache_path}, training may not work correctly. {HELP_URL}")
            # è®°å½•æ²¡æœ‰æ‰¾åˆ°å›¾åƒçš„è­¦å‘Šä¿¡æ¯
        self.im_files = [lb["im_file"] for lb in labels]  # update im_files
        # æ›´æ–°im_files

        # Check if the dataset is all boxes or all segments
        lengths = ((len(lb["cls"]), len(lb["bboxes"]), len(lb["segments"])) for lb in labels)  # Get lengths of cls, bboxes, segments
        # è·å–clsã€bboxesã€segmentsçš„é•¿åº¦
        len_cls, len_boxes, len_segments = (sum(x) for x in zip(*lengths))  # Sum lengths
        # è®¡ç®—é•¿åº¦çš„æ€»å’Œ
        if len_segments and len_boxes != len_segments:  # If segments exist but don't match box count
            # å¦‚æœå­˜åœ¨æ®µä½†ä¸æ¡†è®¡æ•°ä¸åŒ¹é…
            LOGGER.warning(
                f"WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = {len_segments}, "
                f"len(boxes) = {len_boxes}. To resolve this only boxes will be used and all segments will be removed. "
                "To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset."
            )
            # è®°å½•æ¡†å’Œæ®µè®¡æ•°ä¸åŒ¹é…çš„è­¦å‘Šä¿¡æ¯
            for lb in labels:  # Remove segments from labels
                lb["segments"] = []  # æ¸…ç©ºæ ‡ç­¾ä¸­çš„æ®µ
        if len_cls == 0:  # If no class labels found
            LOGGER.warning(f"WARNING âš ï¸ No labels found in {cache_path}, training may not work correctly. {HELP_URL}")
            # è®°å½•æ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾çš„è­¦å‘Šä¿¡æ¯
        return labels  # Return the labels


    def build_transforms(self, hyp=None):
        """Builds and appends transforms to the list.
        æ„å»ºå¹¶é™„åŠ å˜æ¢åˆ°åˆ—è¡¨ã€‚
        """
        if self.augment:  # If augmentations are enabled
            hyp.mosaic = hyp.mosaic if self.augment and not self.rect else 0.0  # Set mosaic ratio
            # è®¾ç½®é©¬èµ›å…‹æ¯”ç‡
            hyp.mixup = hyp.mixup if self.augment and not self.rect else 0.0  # Set mixup ratio
            # è®¾ç½®æ··åˆæ¯”ç‡
            transforms = v8_transforms(self, self.imgsz, hyp)  # Build transforms using v8_transforms
            # ä½¿ç”¨v8_transformsæ„å»ºå˜æ¢
        else:
            transforms = Compose([LetterBox(new_shape=(self.imgsz, self.imgsz), scaleup=False)])  # Default transformation
            # é»˜è®¤å˜æ¢
        transforms.append(
            Format(
                bbox_format="xywh",  # Set bounding box format
                normalize=True,  # Normalize the data
                return_mask=self.use_segments,  # Return mask if using segments
                return_keypoint=self.use_keypoints,  # Return keypoints if using keypoints
                return_obb=self.use_obb,  # Return oriented bounding box if using OBB
                batch_idx=True,  # Include batch index
                mask_ratio=hyp.mask_ratio,  # Set mask ratio
                mask_overlap=hyp.overlap_mask,  # Set mask overlap
                bgr=hyp.bgr if self.augment else 0.0,  # only affect training.
                # ä»…å½±å“è®­ç»ƒ
            )
        )
        return transforms  # Return the list of transformations
        # è¿”å›å˜æ¢åˆ—è¡¨


    def close_mosaic(self, hyp):
        """Sets mosaic, copy_paste and mixup options to 0.0 and builds transformations.
        å°†é©¬èµ›å…‹ã€copy_pasteå’Œæ··åˆé€‰é¡¹è®¾ç½®ä¸º0.0å¹¶æ„å»ºå˜æ¢ã€‚
        """
        hyp.mosaic = 0.0  # set mosaic ratio=0.0
        # è®¾ç½®é©¬èµ›å…‹æ¯”ç‡ä¸º0.0
        hyp.copy_paste = 0.0  # keep the same behavior as previous v8 close-mosaic
        # ä¿æŒä¸ä¹‹å‰v8 close-mosaicç›¸åŒçš„è¡Œä¸º
        hyp.mixup = 0.0  # keep the same behavior as previous v8 close-mosaic
        # ä¿æŒä¸ä¹‹å‰v8 close-mosaicç›¸åŒçš„è¡Œä¸º
        self.transforms = self.build_transforms(hyp)  # Build transformations with updated hyperparameters
        # ä½¿ç”¨æ›´æ–°çš„è¶…å‚æ•°æ„å»ºå˜æ¢


    def update_labels_info(self, label):
        """
        Custom your label format here.
        åœ¨è¿™é‡Œè‡ªå®šä¹‰æ‚¨çš„æ ‡ç­¾æ ¼å¼ã€‚

        Note:
            cls is not with bboxes now, classification and semantic segmentation need an independent cls label
            Can also support classification and semantic segmentation by adding or removing dict keys there.
        æ³¨æ„ï¼š
            clsç°åœ¨ä¸ä¸è¾¹ç•Œæ¡†ä¸€èµ·ï¼Œåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²éœ€è¦ç‹¬ç«‹çš„clsæ ‡ç­¾
            ä¹Ÿå¯ä»¥é€šè¿‡æ·»åŠ æˆ–åˆ é™¤å­—å…¸é”®æ¥æ”¯æŒåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ã€‚
        """
        bboxes = label.pop("bboxes")  # Remove and get bounding boxes from the label
        # ä»æ ‡ç­¾ä¸­ç§»é™¤å¹¶è·å–è¾¹ç•Œæ¡†
        segments = label.pop("segments", [])  # Remove and get segments from the label, default to empty list
        # ä»æ ‡ç­¾ä¸­ç§»é™¤å¹¶è·å–æ®µï¼Œé»˜è®¤ä¸ºç©ºåˆ—è¡¨
        keypoints = label.pop("keypoints", None)  # Remove and get keypoints from the label
        # ä»æ ‡ç­¾ä¸­ç§»é™¤å¹¶è·å–å…³é”®ç‚¹
        bbox_format = label.pop("bbox_format")  # Remove and get bounding box format
        # ä»æ ‡ç­¾ä¸­ç§»é™¤å¹¶è·å–è¾¹ç•Œæ¡†æ ¼å¼
        normalized = label.pop("normalized")  # Remove and get normalization status
        # ä»æ ‡ç­¾ä¸­ç§»é™¤å¹¶è·å–å½’ä¸€åŒ–çŠ¶æ€

        # NOTE: do NOT resample oriented boxes
        # æ³¨æ„ï¼šä¸è¦é‡æ–°é‡‡æ ·å®šå‘æ¡†
        segment_resamples = 100 if self.use_obb else 1000  # Set number of resamples for segments
        # è®¾ç½®æ®µçš„é‡æ–°é‡‡æ ·æ•°é‡
        if len(segments) > 0:  # If segments exist
            # å¦‚æœå­˜åœ¨æ®µ
            # make sure segments interpolate correctly if original length is greater than segment_resamples
            # ç¡®ä¿å¦‚æœåŸå§‹é•¿åº¦å¤§äºæ®µé‡æ–°é‡‡æ ·æ•°é‡ï¼Œåˆ™æ®µæ’å€¼æ­£ç¡®
            max_len = max(len(s) for s in segments)  # Get the maximum length of segments
            # è·å–æ®µçš„æœ€å¤§é•¿åº¦
            segment_resamples = (max_len + 1) if segment_resamples < max_len else segment_resamples
            # å¦‚æœé‡æ–°é‡‡æ ·æ•°é‡å°äºæœ€å¤§é•¿åº¦ï¼Œåˆ™æ›´æ–°é‡æ–°é‡‡æ ·æ•°é‡
            # list[np.array(segment_resamples, 2)] * num_samples
            segments = np.stack(resample_segments(segments, n=segment_resamples), axis=0)  # Resample segments
            # é‡æ–°é‡‡æ ·æ®µ
        else:
            segments = np.zeros((0, segment_resamples, 2), dtype=np.float32)  # Create an empty array for segments
            # ä¸ºæ®µåˆ›å»ºä¸€ä¸ªç©ºæ•°ç»„
        label["instances"] = Instances(bboxes, segments, keypoints, bbox_format=bbox_format, normalized=normalized)  # Create instances and add to label
        # åˆ›å»ºå®ä¾‹å¹¶æ·»åŠ åˆ°æ ‡ç­¾ä¸­
        return label  # Return the updated label
        # è¿”å›æ›´æ–°åçš„æ ‡ç­¾


    @staticmethod
    def collate_fn(batch):
        """Collates data samples into batches.
        å°†æ•°æ®æ ·æœ¬åˆå¹¶ä¸ºæ‰¹æ¬¡ã€‚
        """
        new_batch = {}  # Initialize a new batch dictionary
        # åˆå§‹åŒ–æ–°çš„æ‰¹æ¬¡å­—å…¸
        keys = batch[0].keys()  # Get keys from the first sample
        # ä»ç¬¬ä¸€ä¸ªæ ·æœ¬ä¸­è·å–é”®
        values = list(zip(*[list(b.values()) for b in batch]))  # Zip values from all samples
        # ä»æ‰€æœ‰æ ·æœ¬ä¸­å‹ç¼©å€¼
        for i, k in enumerate(keys):  # Iterate through keys
            # éå†é”®
            value = values[i]  # Get corresponding values
            # è·å–ç›¸åº”çš„å€¼
            if k == "img":  # If the key is 'img'
                value = torch.stack(value, 0)  # Stack images into a tensor
                # å°†å›¾åƒå †å åˆ°ä¸€ä¸ªå¼ é‡ä¸­
            if k in {"masks", "keypoints", "bboxes", "cls", "segments", "obb"}:  # If key is one of the specified types
                value = torch.cat(value, 0)  # Concatenate values into a single tensor
                # å°†å€¼è¿æ¥åˆ°ä¸€ä¸ªå¼ é‡ä¸­
            new_batch[k] = value  # Add the processed value to the new batch
            # å°†å¤„ç†åçš„å€¼æ·»åŠ åˆ°æ–°æ‰¹æ¬¡ä¸­
        new_batch["batch_idx"] = list(new_batch["batch_idx"])  # Convert batch index to a list
        # å°†æ‰¹æ¬¡ç´¢å¼•è½¬æ¢ä¸ºåˆ—è¡¨
        for i in range(len(new_batch["batch_idx"])):  # Iterate through batch indices
            new_batch["batch_idx"][i] += i  # add target image index for build_targets()
            # ä¸ºbuild_targets()æ·»åŠ ç›®æ ‡å›¾åƒç´¢å¼•
        new_batch["batch_idx"] = torch.cat(new_batch["batch_idx"], 0)  # Concatenate batch indices into a tensor
        # å°†æ‰¹æ¬¡ç´¢å¼•è¿æ¥åˆ°ä¸€ä¸ªå¼ é‡ä¸­
        return new_batch  # Return the collated batch
        # è¿”å›åˆå¹¶çš„æ‰¹æ¬¡


class YOLOMultiModalDataset(YOLODataset):
    """
    Dataset class for loading object detection and/or segmentation labels in YOLO format.

    Args:
        data (dict, optional): A dataset YAML dictionary. Defaults to None.
        task (str): An explicit arg to point current task, Defaults to 'detect'.

    Returns:
        (torch.utils.data.Dataset): A PyTorch dataset object that can be used for training an object detection model.
    """

    def __init__(self, *args, data=None, task="detect", **kwargs):
        """Initializes a dataset object for object detection tasks with optional specifications."""
        super().__init__(*args, data=data, task=task, **kwargs)

    def update_labels_info(self, label):
        """Add texts information for multi-modal model training."""
        labels = super().update_labels_info(label)
        # NOTE: some categories are concatenated with its synonyms by `/`.
        labels["texts"] = [v.split("/") for _, v in self.data["names"].items()]
        return labels

    def build_transforms(self, hyp=None):
        """Enhances data transformations with optional text augmentation for multi-modal training."""
        transforms = super().build_transforms(hyp)
        if self.augment:
            # NOTE: hard-coded the args for now.
            transforms.insert(-1, RandomLoadText(max_samples=min(self.data["nc"], 80), padding=True))
        return transforms


class GroundingDataset(YOLODataset):
    """Handles object detection tasks by loading annotations from a specified JSON file, supporting YOLO format.
    é€šè¿‡åŠ è½½æŒ‡å®šJSONæ–‡ä»¶ä¸­çš„æ³¨é‡Šæ¥å¤„ç†ç›®æ ‡æ£€æµ‹ä»»åŠ¡ï¼Œæ”¯æŒYOLOæ ¼å¼ã€‚
    """

    def __init__(self, *args, task="detect", json_file, **kwargs):
        """Initializes a GroundingDataset for object detection, loading annotations from a specified JSON file.
        åˆå§‹åŒ–ä¸€ä¸ªGroundingDatasetç”¨äºç›®æ ‡æ£€æµ‹ï¼Œä»æŒ‡å®šçš„JSONæ–‡ä»¶åŠ è½½æ³¨é‡Šã€‚
        """
        assert task == "detect", "[GroundingDataset](cci:2://file:///Users/gatilin/MyWork/Source-Code-Reading1/source/YOLO/ultralytics-8.3.74/ultralytics/data/dataset.py:382:0-454:25) only support `detect` task for now!"
        # ç¡®ä¿ä»»åŠ¡æ˜¯æ£€æµ‹ä»»åŠ¡ï¼ŒGroundingDatasetç›®å‰ä»…æ”¯æŒæ£€æµ‹ä»»åŠ¡
        self.json_file = json_file  # Store the path to the JSON file
        # å­˜å‚¨JSONæ–‡ä»¶çš„è·¯å¾„
        super().__init__(*args, task=task, data={}, **kwargs)  # Initialize the base class


    def get_img_files(self, img_path):
        """The image files would be read in [get_labels](cci:1://file:///Users/gatilin/MyWork/Source-Code-Reading1/source/YOLO/ultralytics-8.3.74/ultralytics/data/dataset.py:395:4-446:21) function, return empty list here.
        å›¾åƒæ–‡ä»¶å°†åœ¨[get_labels](cci:1://file:///Users/gatilin/MyWork/Source-Code-Reading1/source/YOLO/ultralytics-8.3.74/ultralytics/data/dataset.py:395:4-446:21)å‡½æ•°ä¸­è¯»å–ï¼Œæ­¤å¤„è¿”å›ç©ºåˆ—è¡¨ã€‚
        """
        return []  # Return an empty list


    def get_labels(self):
        """Loads annotations from a JSON file, filters, and normalizes bounding boxes for each image.
        ä»JSONæ–‡ä»¶åŠ è½½æ³¨é‡Šï¼Œè¿‡æ»¤å¹¶å½’ä¸€åŒ–æ¯ä¸ªå›¾åƒçš„è¾¹ç•Œæ¡†ã€‚
        """
        labels = []  # Initialize a list to hold labels
        # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨ä»¥ä¿å­˜æ ‡ç­¾
        LOGGER.info("Loading annotation file...")  # Log the loading process
        # è®°å½•åŠ è½½è¿‡ç¨‹
        with open(self.json_file) as f:  # Open the JSON file
            annotations = json.load(f)  # Load annotations from the file
            # ä»æ–‡ä»¶ä¸­åŠ è½½æ³¨é‡Š
        images = {f"{x['id']:d}": x for x in annotations["images"]}  # Create a dictionary of images
        # åˆ›å»ºå›¾åƒå­—å…¸
        img_to_anns = defaultdict(list)  # Create a default dictionary to hold annotations for each image
        # åˆ›å»ºä¸€ä¸ªé»˜è®¤å­—å…¸ä»¥ä¿å­˜æ¯ä¸ªå›¾åƒçš„æ³¨é‡Š
        for ann in annotations["annotations"]:  # Iterate through annotations
            # éå†æ³¨é‡Š
            img_to_anns[ann["image_id"]].append(ann)  # Append annotation to the corresponding image
            # å°†æ³¨é‡Šé™„åŠ åˆ°ç›¸åº”çš„å›¾åƒ

        for img_id, anns in TQDM(img_to_anns.items(), desc=f"Reading annotations {self.json_file}"):
            # éå†å›¾åƒIDå’Œæ³¨é‡Šï¼Œæ˜¾ç¤ºè¿›åº¦æ¡
            img = images[f"{img_id:d}"]  # Get the image information
            # è·å–å›¾åƒä¿¡æ¯
            h, w, f = img["height"], img["width"], img["file_name"]  # Get height, width, and filename
            # è·å–é«˜åº¦ã€å®½åº¦å’Œæ–‡ä»¶å
            im_file = Path(self.img_path) / f  # Define the full path to the image file
            # å®šä¹‰å›¾åƒæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
            if not im_file.exists():  # Check if the image file exists
                # æ£€æŸ¥å›¾åƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                continue  # Skip if the image file does not exist
                # å¦‚æœå›¾åƒæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™è·³è¿‡
            self.im_files.append(str(im_file))  # Add the image file path to the list
            # å°†å›¾åƒæ–‡ä»¶è·¯å¾„æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            bboxes = []  # Initialize a list to hold bounding boxes
            # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨ä»¥ä¿å­˜è¾¹ç•Œæ¡†
            cat2id = {}  # Initialize a dictionary to map category names to IDs
            # åˆå§‹åŒ–ä¸€ä¸ªå­—å…¸ä»¥å°†ç±»åˆ«åç§°æ˜ å°„åˆ°ID
            texts = []  # Initialize a list to hold category names
            # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨ä»¥ä¿å­˜ç±»åˆ«åç§°
            for ann in anns:  # Iterate through annotations for the current image
                # éå†å½“å‰å›¾åƒçš„æ³¨é‡Š
                if ann["iscrowd"]:  # Skip crowd annotations
                    # è·³è¿‡äººç¾¤æ³¨é‡Š
                    continue
                box = np.array(ann["bbox"], dtype=np.float32)  # Get the bounding box
                # è·å–è¾¹ç•Œæ¡†
                box[:2] += box[2:] / 2  # Convert to center format
                # è½¬æ¢ä¸ºä¸­å¿ƒæ ¼å¼
                box[[0, 2]] /= float(w)  # Normalize x-coordinates
                # å½’ä¸€åŒ–xåæ ‡
                box[[1, 3]] /= float(h)  # Normalize y-coordinates
                # å½’ä¸€åŒ–yåæ ‡
                if box[2] <= 0 or box[3] <= 0:  # Skip invalid boxes
                    # è·³è¿‡æ— æ•ˆçš„è¾¹ç•Œæ¡†
                    continue

                caption = img["caption"]  # Get the caption for the image
                # è·å–å›¾åƒçš„æ ‡é¢˜
                cat_name = " ".join([caption[t[0]:t[1]] for t in ann["tokens_positive"]])  # Get category name from tokens
                # ä»tokensä¸­è·å–ç±»åˆ«åç§°
                if cat_name not in cat2id:  # If category name is not in the dictionary
                    # å¦‚æœç±»åˆ«åç§°ä¸åœ¨å­—å…¸ä¸­
                    cat2id[cat_name] = len(cat2id)  # Assign a new ID to the category
                    # å°†æ–°IDåˆ†é…ç»™ç±»åˆ«
                    texts.append([cat_name])  # Add the category name to the texts list
                cls = cat2id[cat_name]  # Get the class ID
                # è·å–ç±»ID
                box = [cls] + box.tolist()  # Combine class ID with bounding box
                # å°†ç±»IDä¸è¾¹ç•Œæ¡†ç»„åˆ
                if box not in bboxes:  # Avoid duplicates
                    # é¿å…é‡å¤
                    bboxes.append(box)  # Add bounding box to the list
            lb = np.array(bboxes, dtype=np.float32) if len(bboxes) else np.zeros((0, 5), dtype=np.float32)  # Convert to array
            # è½¬æ¢ä¸ºæ•°ç»„
            labels.append(
                {
                    "im_file": im_file,  # Add image file path
                    "shape": (h, w),  # Add image shape
                    "cls": lb[:, 0:1],  # n, 1
                    "bboxes": lb[:, 1:],  # n, 4
                    "normalized": True,  # Indicate that the data is normalized
                    "bbox_format": "xywh",  # Bounding box format
                    "texts": texts,  # Add category names
                }
            )
        return labels  # Return the list of labels


    def build_transforms(self, hyp=None):
        """Configures augmentations for training with optional text loading; `hyp` adjusts augmentation intensity.
        é…ç½®ç”¨äºè®­ç»ƒçš„å¢å¼ºï¼Œæ”¯æŒå¯é€‰çš„æ–‡æœ¬åŠ è½½ï¼›`hyp`è°ƒæ•´å¢å¼ºå¼ºåº¦ã€‚
        """
        transforms = super().build_transforms(hyp)  # Build base transforms
        # æ„å»ºåŸºç¡€å˜æ¢
        if self.augment:  # If augmentations are enabled
            # å¦‚æœå¯ç”¨äº†å¢å¼º
            # NOTE: hard-coded the args for now.
            transforms.insert(-1, RandomLoadText(max_samples=80, padding=True))  # Insert text loading transform
            # æ’å…¥æ–‡æœ¬åŠ è½½å˜æ¢
        return transforms  # Return the list of transformations
        # è¿”å›å˜æ¢åˆ—è¡¨


class YOLOConcatDataset(ConcatDataset):
    """
    Dataset as a concatenation of multiple datasets.
    ä½œä¸ºå¤šä¸ªæ•°æ®é›†çš„è¿æ¥çš„æ•°æ®é›†ã€‚

    This class is useful to assemble different existing datasets.
    æ­¤ç±»å¯¹äºç»„åˆä¸åŒçš„ç°æœ‰æ•°æ®é›†éå¸¸æœ‰ç”¨ã€‚
    """

    @staticmethod
    def collate_fn(batch):
        """Collates data samples into batches.
        å°†æ•°æ®æ ·æœ¬åˆå¹¶ä¸ºæ‰¹æ¬¡ã€‚
        """
        return YOLODataset.collate_fn(batch)  # Use the collate function from YOLODataset
        # ä½¿ç”¨YOLODatasetä¸­çš„åˆå¹¶å‡½æ•°


# TODO: support semantic segmentation
class SemanticDataset(BaseDataset):
    """
    Semantic Segmentation Dataset.
    è¯­ä¹‰åˆ†å‰²æ•°æ®é›†ã€‚

    This class is responsible for handling datasets used for semantic segmentation tasks. It inherits functionalities
    from the BaseDataset class.
    è¯¥ç±»è´Ÿè´£å¤„ç†ç”¨äºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡çš„æ•°æ®é›†ã€‚å®ƒç»§æ‰¿è‡ªBaseDatasetç±»çš„åŠŸèƒ½ã€‚

    Note:
        This class is currently a placeholder and needs to be populated with methods and attributes for supporting
        semantic segmentation tasks.
    æ³¨æ„ï¼š
        è¯¥ç±»ç›®å‰æ˜¯ä¸€ä¸ªå ä½ç¬¦ï¼Œéœ€è¦å¡«å……æ–¹æ³•å’Œå±æ€§ä»¥æ”¯æŒè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚
    """

    def __init__(self):
        """Initialize a SemanticDataset object.
        åˆå§‹åŒ–ä¸€ä¸ªSemanticDatasetå¯¹è±¡ã€‚
        """
        super().__init__()  # Initialize the base class


class ClassificationDataset:
    """
    Extends torchvision ImageFolder to support YOLO classification tasks, offering functionalities like image
    augmentation, caching, and verification. It's designed to efficiently handle large datasets for training deep
    learning models, with optional image transformations and caching mechanisms to speed up training.
    æ‰©å±•torchvisionçš„ImageFolderä»¥æ”¯æŒYOLOåˆ†ç±»ä»»åŠ¡ï¼Œæä¾›å›¾åƒå¢å¼ºã€ç¼“å­˜å’ŒéªŒè¯ç­‰åŠŸèƒ½ã€‚å®ƒæ—¨åœ¨é«˜æ•ˆå¤„ç†å¤§å‹æ•°æ®é›†ä»¥è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ”¯æŒå¯é€‰çš„å›¾åƒå˜æ¢å’Œç¼“å­˜æœºåˆ¶ä»¥åŠ é€Ÿè®­ç»ƒã€‚

    This class allows for augmentations using both torchvision and Albumentations libraries, and supports caching images
    in RAM or on disk to reduce IO overhead during training. Additionally, it implements a robust verification process
    to ensure data integrity and consistency.
    è¯¥ç±»å…è®¸ä½¿ç”¨torchvisionå’ŒAlbumentationsåº“è¿›è¡Œå¢å¼ºï¼Œå¹¶æ”¯æŒå°†å›¾åƒç¼“å­˜åˆ°RAMæˆ–ç£ç›˜ä»¥å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„IOå¼€é”€ã€‚æ­¤å¤–ï¼Œå®ƒå®ç°äº†å¼ºå¤§çš„éªŒè¯è¿‡ç¨‹ï¼Œä»¥ç¡®ä¿æ•°æ®çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§ã€‚

    Attributes:
        cache_ram (bool): Indicates if caching in RAM is enabled.
        cache_ram (bool): æŒ‡ç¤ºæ˜¯å¦å¯ç”¨RAMä¸­çš„ç¼“å­˜ã€‚
        cache_disk (bool): Indicates if caching on disk is enabled.
        cache_disk (bool): æŒ‡ç¤ºæ˜¯å¦å¯ç”¨ç£ç›˜ä¸Šçš„ç¼“å­˜ã€‚
        samples (list): A list of tuples, each containing the path to an image, its class index, path to its .npy cache
                        file (if caching on disk), and optionally the loaded image array (if caching in RAM).
        samples (list): ä¸€ä¸ªå…ƒç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«å›¾åƒè·¯å¾„ã€ç±»ç´¢å¼•ã€.npyç¼“å­˜æ–‡ä»¶çš„è·¯å¾„ï¼ˆå¦‚æœåœ¨ç£ç›˜ä¸Šç¼“å­˜ï¼‰ä»¥åŠå¯é€‰çš„åŠ è½½å›¾åƒæ•°ç»„ï¼ˆå¦‚æœåœ¨RAMä¸­ç¼“å­˜ï¼‰ã€‚
        torch_transforms (callable): PyTorch transforms to be applied to the images.
        torch_transforms (callable): åº”ç”¨äºå›¾åƒçš„PyTorchå˜æ¢ã€‚
    """

    def __init__(self, root, args, augment=False, prefix=""):
        """
        Initialize YOLO object with root, image size, augmentations, and cache settings.
        ä½¿ç”¨æ ¹ç›®å½•ã€å›¾åƒå¤§å°ã€å¢å¼ºå’Œç¼“å­˜è®¾ç½®åˆå§‹åŒ–YOLOå¯¹è±¡ã€‚

        Args:
            root (str): Path to the dataset directory where images are stored in a class-specific folder structure.
            root (str): æ•°æ®é›†ç›®å½•çš„è·¯å¾„ï¼Œå›¾åƒå­˜å‚¨åœ¨ç‰¹å®šç±»åˆ«çš„æ–‡ä»¶å¤¹ç»“æ„ä¸­ã€‚
            args (Namespace): Configuration containing dataset-related settings such as image size, augmentation
                parameters, and cache settings. It includes attributes like `imgsz` (image size), `fraction` (fraction
                of data to use), `scale`, `fliplr`, `flipud`, [cache](cci:1://file:///Users/gatilin/MyWork/Source-Code-Reading1/source/YOLO/ultralytics-8.3.74/ultralytics/data/dataset.py:78:4-157:48) (disk or RAM caching for faster training),
                `auto_augment`, `hsv_h`, `hsv_s`, `hsv_v`, and `crop_fraction`.
            args (Namespace): é…ç½®åŒ…å«ä¸æ•°æ®é›†ç›¸å…³çš„è®¾ç½®ï¼Œå¦‚å›¾åƒå¤§å°ã€å¢å¼ºå‚æ•°å’Œç¼“å­˜è®¾ç½®ã€‚å®ƒåŒ…æ‹¬å±æ€§ï¼Œå¦‚`imgsz`ï¼ˆå›¾åƒå¤§å°ï¼‰ã€`fraction`ï¼ˆä½¿ç”¨çš„æ•°æ®æ¯”ä¾‹ï¼‰ã€`scale`ã€`fliplr`ã€`flipud`ã€[cache](cci:1://file:///Users/gatilin/MyWork/Source-Code-Reading1/source/YOLO/ultralytics-8.3.74/ultralytics/data/dataset.py:78:4-157:48)ï¼ˆç”¨äºæ›´å¿«è®­ç»ƒçš„ç£ç›˜æˆ–RAMç¼“å­˜ï¼‰ã€`auto_augment`ã€`hsv_h`ã€`hsv_s`ã€`hsv_v`å’Œ`crop_fraction`ã€‚
            augment (bool, optional): Whether to apply augmentations to the dataset. Default is False.
            augment (bool, optional): æ˜¯å¦å¯¹æ•°æ®é›†åº”ç”¨å¢å¼ºã€‚é»˜è®¤å€¼ä¸ºFalseã€‚
            prefix (str, optional): Prefix for logging and cache filenames, aiding in dataset identification and
                debugging. Default is an empty string.
            prefix (str, optional): æ—¥å¿—å’Œç¼“å­˜æ–‡ä»¶åçš„å‰ç¼€ï¼Œæœ‰åŠ©äºæ•°æ®é›†çš„è¯†åˆ«å’Œè°ƒè¯•ã€‚é»˜è®¤å€¼ä¸ºç©ºå­—ç¬¦ä¸²ã€‚
        """
        import torchvision  # scope for faster 'import ultralytics'
        # ä¸ºäº†æ›´å¿«åœ°å¯¼å…¥'ultralytics'è€Œå¼•å…¥torchvision

        # Base class assigned as attribute rather than used as base class to allow for scoping slow torchvision import
        # åŸºç±»ä½œä¸ºå±æ€§åˆ†é…ï¼Œè€Œä¸æ˜¯ç”¨ä½œåŸºç±»ï¼Œä»¥å…è®¸ä½œç”¨åŸŸæ…¢é€Ÿtorchvisionå¯¼å…¥
        if TORCHVISION_0_18:  # 'allow_empty' argument first introduced in torchvision 0.18
            self.base = torchvision.datasets.ImageFolder(root=root, allow_empty=True)  # Allow empty folders
            # å…è®¸ç©ºæ–‡ä»¶å¤¹
        else:
            self.base = torchvision.datasets.ImageFolder(root=root)  # Create ImageFolder dataset
            # åˆ›å»ºImageFolderæ•°æ®é›†
        self.samples = self.base.samples  # Get samples from the base dataset
        # ä»åŸºç±»æ•°æ®é›†ä¸­è·å–æ ·æœ¬
        self.root = self.base.root  # Store root directory of the dataset
        # å­˜å‚¨æ•°æ®é›†çš„æ ¹ç›®å½•

        # Initialize attributes
        if augment and args.fraction < 1.0:  # reduce training fraction
            # å¦‚æœå¢å¼ºå’Œargs.fractionå°äº1.0
            self.samples = self.samples[: round(len(self.samples) * args.fraction)]  # Reduce samples based on fraction
            # æ ¹æ®æ¯”ä¾‹å‡å°‘æ ·æœ¬
        self.prefix = colorstr(f"{prefix}: ") if prefix else ""  # Set prefix for logging
        # è®¾ç½®æ—¥å¿—çš„å‰ç¼€
        self.cache_ram = args.cache is True or str(args.cache).lower() == "ram"  # cache images into RAM
        # å°†å›¾åƒç¼“å­˜åˆ°RAM
        if self.cache_ram:  # If caching in RAM
            # å¦‚æœåœ¨RAMä¸­ç¼“å­˜
            LOGGER.warning(
                "WARNING âš ï¸ Classification `cache_ram` training has known memory leak in "
                "https://github.com/ultralytics/ultralytics/issues/9824, setting `cache_ram=False`."
            )
            # è®°å½•æœ‰å…³RAMç¼“å­˜è®­ç»ƒå·²çŸ¥å†…å­˜æ³„æ¼çš„è­¦å‘Šä¿¡æ¯
            self.cache_ram = False  # Disable RAM caching due to memory leak
        self.cache_disk = str(args.cache).lower() == "disk"  # cache images on hard drive as uncompressed *.npy files
        # å°†å›¾åƒç¼“å­˜åˆ°ç¡¬ç›˜ä½œä¸ºæœªå‹ç¼©çš„*.npyæ–‡ä»¶
        self.samples = self.verify_images()  # filter out bad images
        # è¿‡æ»¤æ‰åå›¾åƒ
        self.samples = [list(x) + [Path(x[0]).with_suffix(".npy"), None] for x in self.samples]  # file, index, npy, im
        # æ–‡ä»¶ã€ç´¢å¼•ã€npyã€å›¾åƒ
        scale = (1.0 - args.scale, 1.0)  # (0.08, 1.0)  # Set scale range for augmentations
        # è®¾ç½®å¢å¼ºçš„ç¼©æ”¾èŒƒå›´
        self.torch_transforms = (
            classify_augmentations(
                size=args.imgsz,  # Set image size for augmentations
                scale=scale,  # Set scale for augmentations
                hflip=args.fliplr,  # Set horizontal flip augmentation
                vflip=args.flipud,  # Set vertical flip augmentation
                erasing=args.erasing,  # Set erasing augmentation
                auto_augment=args.auto_augment,  # Set auto augment flag
                hsv_h=args.hsv_h,  # Set hue augmentation
                hsv_s=args.hsv_s,  # Set saturation augmentation
                hsv_v=args.hsv_v,  # Set value augmentation
            )
            if augment
            else classify_transforms(size=args.imgsz, crop_fraction=args.crop_fraction)  # Default transformations
            # é»˜è®¤å˜æ¢
        )

    def __getitem__(self, i):
        """Returns subset of data and targets corresponding to given indices.
        è¿”å›ä¸ç»™å®šç´¢å¼•å¯¹åº”çš„æ•°æ®å’Œç›®æ ‡çš„å­é›†ã€‚
        """
        f, j, fn, im = self.samples[i]  # filename, index, filename.with_suffix('.npy'), image
        # æ–‡ä»¶åã€ç´¢å¼•ã€æ–‡ä»¶åå¸¦åç¼€ä¸º'.npy'ã€å›¾åƒ
        if self.cache_ram:  # If caching in RAM
            # å¦‚æœåœ¨RAMä¸­ç¼“å­˜
            if im is None:  # Warning: two separate if statements required here, do not combine this with previous line
                # è­¦å‘Šï¼šè¿™é‡Œéœ€è¦ä¸¤ä¸ªå•ç‹¬çš„ifè¯­å¥ï¼Œä¸è¦å°†å…¶ä¸å‰ä¸€è¡Œåˆå¹¶
                im = self.samples[i][3] = cv2.imread(f)  # Read image if not already loaded
                # å¦‚æœæœªåŠ è½½ï¼Œåˆ™è¯»å–å›¾åƒ
        elif self.cache_disk:  # If caching on disk
            # å¦‚æœåœ¨ç£ç›˜ä¸Šç¼“å­˜
            if not fn.exists():  # load npy
                # åŠ è½½npy
                np.save(fn.as_posix(), cv2.imread(f), allow_pickle=False)  # Save image as .npy file
                # å°†å›¾åƒä¿å­˜ä¸º.npyæ–‡ä»¶
            im = np.load(fn)  # Load image from .npy file
            # ä».npyæ–‡ä»¶åŠ è½½å›¾åƒ
        else:  # read image
            im = cv2.imread(f)  # BGR  # Read image from file
            # ä»æ–‡ä»¶ä¸­è¯»å–å›¾åƒ
        # Convert NumPy array to PIL image
        im = Image.fromarray(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))  # Convert image to RGB format
        # å°†å›¾åƒè½¬æ¢ä¸ºRGBæ ¼å¼
        sample = self.torch_transforms(im)  # Apply transformations to the image
        # å¯¹å›¾åƒåº”ç”¨å˜æ¢
        return {"img": sample, "cls": j}  # Return dictionary with image and class index
        # è¿”å›åŒ…å«å›¾åƒå’Œç±»ç´¢å¼•çš„å­—å…¸

    def __len__(self) -> int:
        """Return the total number of samples in the dataset.
        è¿”å›æ•°æ®é›†ä¸­çš„æ ·æœ¬æ€»æ•°ã€‚
        """
        return len(self.samples)  # Return the length of samples
        # è¿”å›æ ·æœ¬çš„é•¿åº¦

    def verify_images(self):
        """Verify all images in dataset.
        éªŒè¯æ•°æ®é›†ä¸­çš„æ‰€æœ‰å›¾åƒã€‚
        """
        desc = f"{self.prefix}Scanning {self.root}..."  # Description for progress bar
        # è¿›åº¦æ¡æè¿°
        path = Path(self.root).with_suffix(".cache")  # *.cache file path
        # *.cacheæ–‡ä»¶è·¯å¾„

        try:
            cache = load_dataset_cache_file(path)  # attempt to load a *.cache file
            # å°è¯•åŠ è½½*.cacheæ–‡ä»¶
            assert cache["version"] == DATASET_CACHE_VERSION  # matches current version
            # ç¡®ä¿ç‰ˆæœ¬åŒ¹é…
            assert cache["hash"] == get_hash([x[0] for x in self.samples])  # identical hash
            # ç¡®ä¿å“ˆå¸Œå€¼ç›¸åŒ
            nf, nc, n, samples = cache.pop("results")  # found, missing, empty, corrupt, total
            # æ‰¾åˆ°ã€ç¼ºå¤±ã€ç©ºã€æŸåã€æ€»æ•°
            if LOCAL_RANK in {-1, 0}:  # If cache exists and is the main process
                # å¦‚æœç¼“å­˜å­˜åœ¨ä¸”æ˜¯ä¸»è¿›ç¨‹
                d = f"{desc} {nf} images, {nc} corrupt"  # Update description with found and corrupt counts
                # ä½¿ç”¨æ‰¾åˆ°å’ŒæŸåçš„è®¡æ•°æ›´æ–°æè¿°
                TQDM(None, desc=d, total=n, initial=n)  # display results
                # æ˜¾ç¤ºç»“æœ
                if cache["msgs"]:  # If there are messages in cache
                    LOGGER.info("\n".join(cache["msgs"]))  # display warnings
                    # æ˜¾ç¤ºè­¦å‘Š
            return samples  # Return the verified samples

        except (FileNotFoundError, AssertionError, AttributeError):
            # Run scan if *.cache retrieval failed
            # å¦‚æœ*.cacheæ£€ç´¢å¤±è´¥ï¼Œåˆ™è¿è¡Œæ‰«æ
            nf, nc, msgs, samples, x = 0, 0, [], [], {}
            with ThreadPool(NUM_THREADS) as pool:  # Create a thread pool for concurrent processing
                # åˆ›å»ºçº¿ç¨‹æ± ä»¥è¿›è¡Œå¹¶å‘å¤„ç†
                results = pool.imap(func=verify_image, iterable=zip(self.samples, repeat(self.prefix)))  # Verify images
                # éªŒè¯å›¾åƒ
                pbar = TQDM(results, desc=desc, total=len(self.samples))  # Progress bar for verification
                # éªŒè¯çš„è¿›åº¦æ¡
                for sample, nf_f, nc_f, msg in pbar:  # Iterate through verification results
                    # éå†éªŒè¯ç»“æœ
                    if nf_f:  # If the image is found
                        samples.append(sample)  # Add sample to the list
                        # å°†æ ·æœ¬æ·»åŠ åˆ°åˆ—è¡¨ä¸­
                    if msg:  # If there are any messages
                        msgs.append(msg)  # Append messages to the list
                    nf += nf_f  # Update number of found images
                    nc += nc_f  # Update number of corrupt images
                    pbar.desc = f"{desc} {nf} images, {nc} corrupt"  # Update progress bar description
                    # æ›´æ–°è¿›åº¦æ¡æè¿°
                pbar.close()  # Close the progress bar
            if msgs:  # If there are any messages
                LOGGER.info("\n".join(msgs))  # Log all messages
                # è®°å½•æ‰€æœ‰æ¶ˆæ¯
            x["hash"] = get_hash([x[0] for x in self.samples])  # Get hash of the dataset
            # è·å–æ•°æ®é›†çš„å“ˆå¸Œå€¼
            x["results"] = nf, nc, len(samples), samples  # Store results in the dictionary
            # å°†ç»“æœå­˜å‚¨åœ¨å­—å…¸ä¸­
            x["msgs"] = msgs  # warnings
            save_dataset_cache_file(self.prefix, path, x, DATASET_CACHE_VERSION)  # Save the cache file
            # ä¿å­˜ç¼“å­˜æ–‡ä»¶
            return samples  # Return the verified samples
            # è¿”å›éªŒè¯åçš„æ ·æœ¬
