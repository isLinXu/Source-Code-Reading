# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import hashlib
import json
import os
import random
import subprocess
import time
import zipfile
from multiprocessing.pool import ThreadPool
from pathlib import Path
from tarfile import is_tarfile

import cv2
import numpy as np
from PIL import Image, ImageOps

from ultralytics.nn.autobackend import check_class_names
from ultralytics.utils import (
    DATASETS_DIR,
    LOGGER,
    NUM_THREADS,
    ROOT,
    SETTINGS_FILE,
    TQDM,
    clean_url,
    colorstr,
    emojis,
    is_dir_writeable,
    yaml_load,
    yaml_save,
)
from ultralytics.utils.checks import check_file, check_font, is_ascii
from ultralytics.utils.downloads import download, safe_download, unzip_file
from ultralytics.utils.ops import segments2boxes

HELP_URL = "See https://docs.ultralytics.com/datasets for dataset formatting guidance."  # æŸ¥çœ‹æ•°æ®é›†æ ¼å¼æŒ‡å¯¼çš„é“¾æ¥
IMG_FORMATS = {"bmp", "dng", "jpeg", "jpg", "mpo", "png", "tif", "tiff", "webp", "pfm", "heic"}  # image suffixes  # å›¾åƒåç¼€
VID_FORMATS = {"asf", "avi", "gif", "m4v", "mkv", "mov", "mp4", "mpeg", "mpg", "ts", "wmv", "webm"}  # video suffixes  # è§†é¢‘åç¼€
PIN_MEMORY = str(os.getenv("PIN_MEMORY", True)).lower() == "true"  # global pin_memory for dataloaders  # å…¨å±€ pin_memory ç”¨äºæ•°æ®åŠ è½½å™¨
FORMATS_HELP_MSG = f"Supported formats are:\nimages: {IMG_FORMATS}\nvideos: {VID_FORMATS}"  # æ”¯æŒçš„æ ¼å¼æ¶ˆæ¯

def img2label_paths(img_paths):
    """Define label paths as a function of image paths."""  # æ ¹æ®å›¾åƒè·¯å¾„å®šä¹‰æ ‡ç­¾è·¯å¾„
    sa, sb = f"{os.sep}images{os.sep}", f"{os.sep}labels{os.sep}"  # /images/, /labels/ substrings  # /images/ å’Œ /labels/ å­å­—ç¬¦ä¸²
    return [sb.join(x.rsplit(sa, 1)).rsplit(".", 1)[0] + ".txt" for x in img_paths]  # è¿”å›æ ‡ç­¾è·¯å¾„åˆ—è¡¨

def get_hash(paths):
    """Returns a single hash value of a list of paths (files or dirs)."""  # è¿”å›è·¯å¾„åˆ—è¡¨ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰çš„å•ä¸ªå“ˆå¸Œå€¼
    size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # sizes  # è®¡ç®—è·¯å¾„çš„æ€»å¤§å°
    h = hashlib.sha256(str(size).encode())  # hash sizes  # å¯¹å¤§å°è¿›è¡Œå“ˆå¸Œ
    h.update("".join(paths).encode())  # hash paths  # å¯¹è·¯å¾„è¿›è¡Œå“ˆå¸Œ
    return h.hexdigest()  # return hash  # è¿”å›å“ˆå¸Œå€¼

def exif_size(img: Image.Image):
    """Returns exif-corrected PIL size."""  # è¿”å›ç»è¿‡ EXIF æ ¡æ­£çš„ PIL å¤§å°
    s = img.size  # (width, height)  # è·å–å›¾åƒçš„å°ºå¯¸ï¼ˆå®½åº¦ï¼Œé«˜åº¦ï¼‰
    if img.format == "JPEG":  # only support JPEG images  # ä»…æ”¯æŒ JPEG å›¾åƒ
        try:
            if exif := img.getexif():  # è·å–å›¾åƒçš„ EXIF ä¿¡æ¯
                rotation = exif.get(274, None)  # the EXIF key for the orientation tag is 274  # EXIF æ–¹å‘æ ‡ç­¾çš„é”®ä¸º 274
                if rotation in {6, 8}:  # rotation 270 or 90  # æ—‹è½¬ 270 æˆ– 90 åº¦
                    s = s[1], s[0]  # äº¤æ¢å®½é«˜
        except Exception:
            pass
    return s  # è¿”å›æ ¡æ­£åçš„å°ºå¯¸

def verify_image(args):
    """Verify one image."""  # éªŒè¯ä¸€å¼ å›¾åƒ
    (im_file, cls), prefix = args  # è§£åŒ…å‚æ•°
    # Number (found, corrupt), message  # æ•°é‡ï¼ˆæ‰¾åˆ°ï¼ŒæŸåï¼‰ï¼Œæ¶ˆæ¯
    nf, nc, msg = 0, 0, ""  # åˆå§‹åŒ–æ‰¾åˆ°å’ŒæŸåçš„è®¡æ•°ï¼Œä»¥åŠæ¶ˆæ¯
    try:
        im = Image.open(im_file)  # æ‰“å¼€å›¾åƒæ–‡ä»¶
        im.verify()  # PIL verify  # ä½¿ç”¨ PIL éªŒè¯å›¾åƒ
        shape = exif_size(im)  # image size  # è·å–å›¾åƒå°ºå¯¸
        shape = (shape[1], shape[0])  # hw  # äº¤æ¢å®½é«˜
        assert (shape[0] > 9) & (shape[1] > 9), f"image size {shape} <10 pixels"  # ç¡®ä¿å°ºå¯¸å¤§äº 10 åƒç´ 
        assert im.format.lower() in IMG_FORMATS, f"Invalid image format {im.format}. {FORMATS_HELP_MSG}"  # ç¡®ä¿æ ¼å¼æœ‰æ•ˆ
        if im.format.lower() in {"jpg", "jpeg"}:  # å¦‚æœæ˜¯ JPEG æ ¼å¼
            with open(im_file, "rb") as f:  # ä»¥äºŒè¿›åˆ¶æ¨¡å¼æ‰“å¼€å›¾åƒæ–‡ä»¶
                f.seek(-2, 2)  # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾å‰ä¸¤ä¸ªå­—èŠ‚
                if f.read() != b"\xff\xd9":  # corrupt JPEG  # æ£€æŸ¥æ˜¯å¦ä¸ºæŸåçš„ JPEG
                    ImageOps.exif_transpose(Image.open(im_file)).save(im_file, "JPEG", subsampling=0, quality=100)  # æ¢å¤å¹¶ä¿å­˜æŸåçš„ JPEG
                    msg = f"{prefix}WARNING âš ï¸ {im_file}: corrupt JPEG restored and saved"  # è®°å½•æ¢å¤æ¶ˆæ¯
        nf = 1  # æ‰¾åˆ°å›¾åƒ
    except Exception as e:
        nc = 1  # è®°å½•æŸåçš„å›¾åƒ
        msg = f"{prefix}WARNING âš ï¸ {im_file}: ignoring corrupt image/label: {e}"  # è®°å½•å¿½ç•¥çš„æ¶ˆæ¯
    return (im_file, cls), nf, nc, msg  # è¿”å›ç»“æœ

def verify_image_label(args):
    """Verify one image-label pair."""  # éªŒè¯ä¸€å¯¹å›¾åƒ-æ ‡ç­¾
    im_file, lb_file, prefix, keypoint, num_cls, nkpt, ndim = args  # è§£åŒ…å‚æ•°
    # Number (missing, found, empty, corrupt), message, segments, keypoints  # æ•°é‡ï¼ˆç¼ºå¤±ï¼Œæ‰¾åˆ°ï¼Œç©ºï¼ŒæŸåï¼‰ï¼Œæ¶ˆæ¯ï¼Œæ®µï¼Œå…³é”®ç‚¹
    nm, nf, ne, nc, msg, segments, keypoints = 0, 0, 0, 0, "", [], None  # åˆå§‹åŒ–è®¡æ•°å’Œæ¶ˆæ¯
    try:
        # Verify images  # éªŒè¯å›¾åƒ
        im = Image.open(im_file)  # æ‰“å¼€å›¾åƒæ–‡ä»¶
        im.verify()  # PIL verify  # ä½¿ç”¨ PIL éªŒè¯å›¾åƒ
        shape = exif_size(im)  # image size  # è·å–å›¾åƒå°ºå¯¸
        shape = (shape[1], shape[0])  # hw  # äº¤æ¢å®½é«˜
        assert (shape[0] > 9) & (shape[1] > 9), f"image size {shape} <10 pixels"  # ç¡®ä¿å°ºå¯¸å¤§äº 10 åƒç´ 
        assert im.format.lower() in IMG_FORMATS, f"invalid image format {im.format}. {FORMATS_HELP_MSG}"  # ç¡®ä¿æ ¼å¼æœ‰æ•ˆ
        if im.format.lower() in {"jpg", "jpeg"}:  # å¦‚æœæ˜¯ JPEG æ ¼å¼
            with open(im_file, "rb") as f:  # ä»¥äºŒè¿›åˆ¶æ¨¡å¼æ‰“å¼€å›¾åƒæ–‡ä»¶
                f.seek(-2, 2)  # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾å‰ä¸¤ä¸ªå­—èŠ‚
                if f.read() != b"\xff\xd9":  # corrupt JPEG  # æ£€æŸ¥æ˜¯å¦ä¸ºæŸåçš„ JPEG
                    ImageOps.exif_transpose(Image.open(im_file)).save(im_file, "JPEG", subsampling=0, quality=100)  # æ¢å¤å¹¶ä¿å­˜æŸåçš„ JPEG
                    msg = f"{prefix}WARNING âš ï¸ {im_file}: corrupt JPEG restored and saved"  # è®°å½•æ¢å¤æ¶ˆæ¯

        # Verify labels  # éªŒè¯æ ‡ç­¾
        if os.path.isfile(lb_file):  # å¦‚æœæ ‡ç­¾æ–‡ä»¶å­˜åœ¨
            nf = 1  # label found  # æ‰¾åˆ°æ ‡ç­¾
            with open(lb_file) as f:  # æ‰“å¼€æ ‡ç­¾æ–‡ä»¶
                lb = [x.split() for x in f.read().strip().splitlines() if len(x)]  # è¯»å–æ ‡ç­¾å¹¶åˆ†å‰²
                if any(len(x) > 6 for x in lb) and (not keypoint):  # is segment  # å¦‚æœæ ‡ç­¾è¶…è¿‡ 6 åˆ—ä¸”ä¸æ˜¯å…³é”®ç‚¹
                    classes = np.array([x[0] for x in lb], dtype=np.float32)  # è·å–ç±»åˆ«
                    segments = [np.array(x[1:], dtype=np.float32).reshape(-1, 2) for x in lb]  # (cls, xy1...)  # è·å–æ®µ
                    lb = np.concatenate((classes.reshape(-1, 1), segments2boxes(segments)), 1)  # (cls, xywh)  # åˆå¹¶ç±»åˆ«å’Œæ®µ
                lb = np.array(lb, dtype=np.float32)  # è½¬æ¢ä¸º numpy æ•°ç»„
            if nl := len(lb):  # å¦‚æœæ ‡ç­¾æ•°é‡å¤§äº 0
                if keypoint:  # å¦‚æœæ˜¯å…³é”®ç‚¹
                    assert lb.shape[1] == (5 + nkpt * ndim), f"labels require {(5 + nkpt * ndim)} columns each"  # æ£€æŸ¥åˆ—æ•°
                    points = lb[:, 5:].reshape(-1, ndim)[:, :2]  # è·å–å…³é”®ç‚¹
                else:
                    assert lb.shape[1] == 5, f"labels require 5 columns, {lb.shape[1]} columns detected"  # æ£€æŸ¥åˆ—æ•°
                    points = lb[:, 1:]  # è·å–æ ‡ç­¾ç‚¹
                assert points.max() <= 1, f"non-normalized or out of bounds coordinates {points[points > 1]}"  # æ£€æŸ¥åæ ‡æ˜¯å¦è¶…å‡ºèŒƒå›´
                assert lb.min() >= 0, f"negative label values {lb[lb < 0]}"  # æ£€æŸ¥æ ‡ç­¾å€¼æ˜¯å¦ä¸ºè´Ÿ

                # All labels  # æ‰€æœ‰æ ‡ç­¾
                max_cls = lb[:, 0].max()  # max label count  # è·å–æœ€å¤§æ ‡ç­¾æ•°é‡
                assert max_cls < num_cls, (  # ç¡®ä¿æœ€å¤§æ ‡ç­¾å°äºæ•°æ®é›†ç±»åˆ«æ•°é‡
                    f"Label class {int(max_cls)} exceeds dataset class count {num_cls}. "
                    f"Possible class labels are 0-{num_cls - 1}"
                )
                _, i = np.unique(lb, axis=0, return_index=True)  # è·å–å”¯ä¸€æ ‡ç­¾åŠå…¶ç´¢å¼•
                if len(i) < nl:  # duplicate row check  # æ£€æŸ¥é‡å¤è¡Œ
                    lb = lb[i]  # remove duplicates  # ç§»é™¤é‡å¤æ ‡ç­¾
                    if segments:  # å¦‚æœæœ‰æ®µ
                        segments = [segments[x] for x in i]  # æ ¹æ®ç´¢å¼•ä¿ç•™æ®µ
                    msg = f"{prefix}WARNING âš ï¸ {im_file}: {nl - len(i)} duplicate labels removed"  # è®°å½•é‡å¤æ ‡ç­¾ç§»é™¤æ¶ˆæ¯
            else:
                ne = 1  # label empty  # æ ‡ç­¾ä¸ºç©º
                lb = np.zeros((0, (5 + nkpt * ndim) if keypoint else 5), dtype=np.float32)  # åˆ›å»ºç©ºæ ‡ç­¾
        else:
            nm = 1  # label missing  # æ ‡ç­¾ç¼ºå¤±
            lb = np.zeros((0, (5 + nkpt * ndim) if keypoints else 5), dtype=np.float32)  # åˆ›å»ºç©ºæ ‡ç­¾
        if keypoint:  # å¦‚æœæ˜¯å…³é”®ç‚¹
            keypoints = lb[:, 5:].reshape(-1, nkpt, ndim)  # è·å–å…³é”®ç‚¹
            if ndim == 2:  # å¦‚æœç»´åº¦ä¸º 2
                kpt_mask = np.where((keypoints[..., 0] < 0) | (keypoints[..., 1] < 0), 0.0, 1.0).astype(np.float32)  # åˆ›å»ºå…³é”®ç‚¹æ©ç 
                keypoints = np.concatenate([keypoints, kpt_mask[..., None]], axis=-1)  # (nl, nkpt, 3)  # åˆå¹¶å…³é”®ç‚¹å’Œæ©ç 
        lb = lb[:, :5]  # ä¿ç•™æ ‡ç­¾çš„å‰ 5 åˆ—
        return im_file, lb, shape, segments, keypoints, nm, nf, ne, nc, msg  # è¿”å›ç»“æœ
    except Exception as e:
        nc = 1  # è®°å½•æŸåçš„æ ‡ç­¾
        msg = f"{prefix}WARNING âš ï¸ {im_file}: ignoring corrupt image/label: {e}"  # è®°å½•å¿½ç•¥çš„æ¶ˆæ¯
        return [None, None, None, None, None, nm, nf, ne, nc, msg]  # è¿”å›ç»“æœ


def visualize_image_annotations(image_path, txt_path, label_map):
    """
    Visualizes YOLO annotations (bounding boxes and class labels) on an image.  # åœ¨å›¾åƒä¸Šå¯è§†åŒ– YOLO æ³¨é‡Šï¼ˆè¾¹ç•Œæ¡†å’Œç±»åˆ«æ ‡ç­¾ï¼‰

    This function reads an image and its corresponding annotation file in YOLO format, then  # æ­¤å‡½æ•°è¯»å–å›¾åƒåŠå…¶å¯¹åº”çš„ YOLO æ ¼å¼æ³¨é‡Šæ–‡ä»¶ï¼Œç„¶å
    draws bounding boxes around detected objects and labels them with their respective class names.  # åœ¨æ£€æµ‹åˆ°çš„ç‰©ä½“å‘¨å›´ç»˜åˆ¶è¾¹ç•Œæ¡†ï¼Œå¹¶ç”¨ç›¸åº”çš„ç±»åˆ«åç§°æ ‡è®°å®ƒä»¬ã€‚
    The bounding box colors are assigned based on the class ID, and the text color is dynamically  # è¾¹ç•Œæ¡†çš„é¢œè‰²æ ¹æ®ç±»åˆ« ID åˆ†é…ï¼Œæ–‡æœ¬é¢œè‰²æ ¹æ®èƒŒæ™¯é¢œè‰²çš„äº®åº¦åŠ¨æ€è°ƒæ•´
    adjusted for readability, depending on the background color's luminance.  # ä»¥æé«˜å¯è¯»æ€§ï¼Œå–å†³äºèƒŒæ™¯é¢œè‰²çš„äº®åº¦ã€‚

    Args:
        image_path (str): The path to the image file to annotate, and it can be in formats supported by PIL (e.g., .jpg, .png).  # å›¾åƒæ–‡ä»¶çš„è·¯å¾„ï¼Œå¯ä»¥æ˜¯ PIL æ”¯æŒçš„æ ¼å¼ï¼ˆä¾‹å¦‚ .jpgï¼Œ.pngï¼‰ã€‚
        txt_path (str): The path to the annotation file in YOLO format, that should contain one line per object with:  # YOLO æ ¼å¼æ³¨é‡Šæ–‡ä»¶çš„è·¯å¾„ï¼Œæ¯ä¸ªå¯¹è±¡åº”åŒ…å«ä¸€è¡Œï¼š
                        - class_id (int): The class index.  # ç±»åˆ« IDï¼ˆæ•´æ•°ï¼‰ï¼šç±»åˆ«ç´¢å¼•ã€‚
                        - x_center (float): The X center of the bounding box (relative to image width).  # x_centerï¼ˆæµ®ç‚¹æ•°ï¼‰ï¼šè¾¹ç•Œæ¡†çš„ X ä¸­å¿ƒï¼ˆç›¸å¯¹äºå›¾åƒå®½åº¦ï¼‰ã€‚
                        - y_center (float): The Y center of the bounding box (relative to image height).  # y_centerï¼ˆæµ®ç‚¹æ•°ï¼‰ï¼šè¾¹ç•Œæ¡†çš„ Y ä¸­å¿ƒï¼ˆç›¸å¯¹äºå›¾åƒé«˜åº¦ï¼‰ã€‚
                        - width (float): The width of the bounding box (relative to image width).  # widthï¼ˆæµ®ç‚¹æ•°ï¼‰ï¼šè¾¹ç•Œæ¡†çš„å®½åº¦ï¼ˆç›¸å¯¹äºå›¾åƒå®½åº¦ï¼‰ã€‚
                        - height (float): The height of the bounding box (relative to image height).  # heightï¼ˆæµ®ç‚¹æ•°ï¼‰ï¼šè¾¹ç•Œæ¡†çš„é«˜åº¦ï¼ˆç›¸å¯¹äºå›¾åƒé«˜åº¦ï¼‰ã€‚
        label_map (dict): A dictionary that maps class IDs (integers) to class labels (strings).  # å°†ç±»åˆ« IDï¼ˆæ•´æ•°ï¼‰æ˜ å°„åˆ°ç±»åˆ«æ ‡ç­¾ï¼ˆå­—ç¬¦ä¸²ï¼‰çš„å­—å…¸ã€‚

    Example:
        >>> label_map = {0: "cat", 1: "dog", 2: "bird"}  # It should include all annotated classes details  # åº”åŒ…æ‹¬æ‰€æœ‰æ³¨é‡Šç±»çš„è¯¦ç»†ä¿¡æ¯
        >>> visualize_image_annotations("path/to/image.jpg", "path/to/annotations.txt", label_map)  # è°ƒç”¨ç¤ºä¾‹
    """
    import matplotlib.pyplot as plt  # å¯¼å…¥ matplotlib.pyplot åº“ç”¨äºç»˜å›¾

    from ultralytics.utils.plotting import colors  # ä» ultralytics.utils.plotting å¯¼å…¥é¢œè‰²å‡½æ•°

    img = np.array(Image.open(image_path))  # è¯»å–å›¾åƒå¹¶è½¬æ¢ä¸º numpy æ•°ç»„
    img_height, img_width = img.shape[:2]  # è·å–å›¾åƒçš„é«˜åº¦å’Œå®½åº¦
    annotations = []  # åˆå§‹åŒ–æ³¨é‡Šåˆ—è¡¨
    with open(txt_path) as file:  # æ‰“å¼€æ³¨é‡Šæ–‡ä»¶
        for line in file:  # éå†æ¯ä¸€è¡Œ
            class_id, x_center, y_center, width, height = map(float, line.split())  # å°†è¡Œå†…å®¹æ‹†åˆ†å¹¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°
            x = (x_center - width / 2) * img_width  # è®¡ç®—è¾¹ç•Œæ¡†å·¦ä¸Šè§’çš„ x åæ ‡
            y = (y_center - height / 2) * img_height  # è®¡ç®—è¾¹ç•Œæ¡†å·¦ä¸Šè§’çš„ y åæ ‡
            w = width * img_width  # è®¡ç®—è¾¹ç•Œæ¡†çš„å®é™…å®½åº¦
            h = height * img_height  # è®¡ç®—è¾¹ç•Œæ¡†çš„å®é™…é«˜åº¦
            annotations.append((x, y, w, h, int(class_id)))  # å°†è¾¹ç•Œæ¡†ä¿¡æ¯æ·»åŠ åˆ°æ³¨é‡Šåˆ—è¡¨
    fig, ax = plt.subplots(1)  # åˆ›å»ºä¸€ä¸ªå›¾å½¢å’Œåæ ‡è½´ï¼Œç”¨äºç»˜åˆ¶å›¾åƒå’Œæ³¨é‡Š
    for x, y, w, h, label in annotations:  # éå†æ‰€æœ‰æ³¨é‡Š
        color = tuple(c / 255 for c in colors(label, True))  # è·å–å¹¶å½’ä¸€åŒ– RGB é¢œè‰²
        rect = plt.Rectangle((x, y), w, h, linewidth=2, edgecolor=color, facecolor="none")  # åˆ›å»ºçŸ©å½¢è¾¹ç•Œæ¡†
        ax.add_patch(rect)  # å°†çŸ©å½¢æ·»åŠ åˆ°åæ ‡è½´
        luminance = 0.2126 * color[0] + 0.7152 * color[1] + 0.0722 * color[2]  # è®¡ç®—é¢œè‰²çš„äº®åº¦
        ax.text(x, y - 5, label_map[label], color="white" if luminance < 0.5 else "black", backgroundcolor=color)  # åœ¨è¾¹ç•Œæ¡†ä¸Šæ–¹æ·»åŠ æ–‡æœ¬æ ‡ç­¾
    ax.imshow(img)  # æ˜¾ç¤ºå›¾åƒ
    plt.show()  # å±•ç¤ºå›¾å½¢


def polygon2mask(imgsz, polygons, color=1, downsample_ratio=1):
    """
    Convert a list of polygons to a binary mask of the specified image size.  # å°†å¤šè¾¹å½¢åˆ—è¡¨è½¬æ¢ä¸ºæŒ‡å®šå›¾åƒå¤§å°çš„äºŒè¿›åˆ¶æ©ç 

    Args:
        imgsz (tuple): The size of the image as (height, width).  # å›¾åƒçš„å¤§å°ï¼Œæ ¼å¼ä¸ºï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰ã€‚
        polygons (list[np.ndarray]): A list of polygons. Each polygon is an array with shape [N, M], where  # å¤šè¾¹å½¢åˆ—è¡¨ã€‚æ¯ä¸ªå¤šè¾¹å½¢æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [N, M] çš„æ•°ç»„ï¼Œ
                                     N is the number of polygons, and M is the number of points such that M % 2 = 0.  # N æ˜¯å¤šè¾¹å½¢çš„æ•°é‡ï¼ŒM æ˜¯ç‚¹çš„æ•°é‡ï¼Œä¸” M % 2 = 0ã€‚
        color (int, optional): The color value to fill in the polygons on the mask. Defaults to 1.  # åœ¨æ©ç ä¸Šå¡«å……å¤šè¾¹å½¢çš„é¢œè‰²å€¼ã€‚é»˜è®¤ä¸º 1ã€‚
        downsample_ratio (int, optional): Factor by which to downsample the mask. Defaults to 1.  # é™é‡‡æ ·æ©ç çš„å› å­ã€‚é»˜è®¤ä¸º 1ã€‚

    Returns:
        (np.ndarray): A binary mask of the specified image size with the polygons filled in.  # è¿”å›å¡«å……å¤šè¾¹å½¢çš„æŒ‡å®šå›¾åƒå¤§å°çš„äºŒè¿›åˆ¶æ©ç ã€‚
    """
    mask = np.zeros(imgsz, dtype=np.uint8)  # åˆ›å»ºä¸€ä¸ªå…¨é›¶çš„æ©ç 
    polygons = np.asarray(polygons, dtype=np.int32)  # å°†å¤šè¾¹å½¢è½¬æ¢ä¸º numpy æ•°ç»„
    polygons = polygons.reshape((polygons.shape[0], -1, 2))  # é‡å¡‘å¤šè¾¹å½¢æ•°ç»„å½¢çŠ¶
    cv2.fillPoly(mask, polygons, color=color)  # åœ¨æ©ç ä¸Šå¡«å……å¤šè¾¹å½¢
    nh, nw = (imgsz[0] // downsample_ratio, imgsz[1] // downsample_ratio)  # è®¡ç®—é™é‡‡æ ·åçš„é«˜åº¦å’Œå®½åº¦
    # Note: fillPoly first then resize is trying to keep the same loss calculation method when mask-ratio=1  # æ³¨æ„ï¼šå…ˆå¡«å……å¤šè¾¹å½¢ç„¶åå†è°ƒæ•´å¤§å°æ˜¯ä¸ºäº†ä¿æŒåœ¨ mask-ratio=1 æ—¶ç›¸åŒçš„æŸå¤±è®¡ç®—æ–¹æ³•
    return cv2.resize(mask, (nw, nh))  # è¿”å›è°ƒæ•´å¤§å°åçš„æ©ç 


def polygons2masks(imgsz, polygons, color, downsample_ratio=1):
    """
    Convert a list of polygons to a set of binary masks of the specified image size.  # å°†å¤šè¾¹å½¢åˆ—è¡¨è½¬æ¢ä¸ºæŒ‡å®šå›¾åƒå¤§å°çš„ä¸€ç»„äºŒè¿›åˆ¶æ©ç 

    Args:
        imgsz (tuple): The size of the image as (height, width).  # å›¾åƒçš„å¤§å°ï¼Œæ ¼å¼ä¸ºï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰ã€‚
        polygons (list[np.ndarray]): A list of polygons. Each polygon is an array with shape [N, M], where  # å¤šè¾¹å½¢åˆ—è¡¨ã€‚æ¯ä¸ªå¤šè¾¹å½¢æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [N, M] çš„æ•°ç»„ï¼Œ
                                     N is the number of polygons, and M is the number of points such that M % 2 = 0.  # N æ˜¯å¤šè¾¹å½¢çš„æ•°é‡ï¼ŒM æ˜¯ç‚¹çš„æ•°é‡ï¼Œä¸” M % 2 = 0ã€‚
        color (int): The color value to fill in the polygons on the masks.  # åœ¨æ©ç ä¸Šå¡«å……å¤šè¾¹å½¢çš„é¢œè‰²å€¼ã€‚
        downsample_ratio (int, optional): Factor by which to downsample each mask. Defaults to 1.  # é™é‡‡æ ·æ¯ä¸ªæ©ç çš„å› å­ã€‚é»˜è®¤ä¸º 1ã€‚

    Returns:
        (np.ndarray): A set of binary masks of the specified image size with the polygons filled in.  # è¿”å›å¡«å……å¤šè¾¹å½¢çš„æŒ‡å®šå›¾åƒå¤§å°çš„ä¸€ç»„äºŒè¿›åˆ¶æ©ç ã€‚
    """
    return np.array([polygon2mask(imgsz, [x.reshape(-1)], color, downsample_ratio) for x in polygons])  # å¯¹æ¯ä¸ªå¤šè¾¹å½¢è°ƒç”¨ polygon2mask å‡½æ•°å¹¶è¿”å›ç»“æœæ•°ç»„


def polygons2masks_overlap(imgsz, segments, downsample_ratio=1):
    """Return a (640, 640) overlap mask."""  # è¿”å› (640, 640) çš„é‡å æ©ç 
    masks = np.zeros(  # åˆ›å»ºä¸€ä¸ªå…¨é›¶çš„æ©ç 
        (imgsz[0] // downsample_ratio, imgsz[1] // downsample_ratio),  # æ ¹æ®é™é‡‡æ ·å› å­è°ƒæ•´å¤§å°
        dtype=np.int32 if len(segments) > 255 else np.uint8,  # æ ¹æ®æ®µçš„æ•°é‡é€‰æ‹©æ•°æ®ç±»å‹
    )
    areas = []  # åˆå§‹åŒ–åŒºåŸŸåˆ—è¡¨
    ms = []  # åˆå§‹åŒ–æ©ç åˆ—è¡¨
    for si in range(len(segments)):  # éå†æ¯ä¸ªæ®µ
        mask = polygon2mask(imgsz, [segments[si].reshape(-1)], downsample_ratio=downsample_ratio, color=1)  # åˆ›å»ºæ©ç 
        ms.append(mask.astype(masks.dtype))  # å°†æ©ç æ·»åŠ åˆ°åˆ—è¡¨ï¼Œå¹¶è½¬æ¢ä¸ºç›¸åº”çš„æ•°æ®ç±»å‹
        areas.append(mask.sum())  # è®¡ç®—å¹¶æ·»åŠ åŒºåŸŸæ€»å’Œ
    areas = np.asarray(areas)  # è½¬æ¢åŒºåŸŸåˆ—è¡¨ä¸º numpy æ•°ç»„
    index = np.argsort(-areas)  # æŒ‰åŒºåŸŸå¤§å°æ’åºç´¢å¼•
    ms = np.array(ms)[index]  # æ ¹æ®æ’åºç´¢å¼•é‡æ–°æ’åˆ—æ©ç 
    for i in range(len(segments)):  # éå†æ¯ä¸ªæ®µ
        mask = ms[i] * (i + 1)  # å°†æ©ç ä¹˜ä»¥ç´¢å¼•å€¼
        masks = masks + mask  # æ›´æ–°é‡å æ©ç 
        masks = np.clip(masks, a_min=0, a_max=i + 1)  # é™åˆ¶æ©ç å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
    return masks, index  # è¿”å›é‡å æ©ç å’Œç´¢å¼•


def find_dataset_yaml(path: Path) -> Path:
    """
    Find and return the YAML file associated with a Detect, Segment or Pose dataset.  # æŸ¥æ‰¾å¹¶è¿”å›ä¸æ£€æµ‹ã€åˆ†å‰²æˆ–å§¿æ€æ•°æ®é›†ç›¸å…³çš„ YAML æ–‡ä»¶

    This function searches for a YAML file at the root level of the provided directory first, and if not found, it  # æ­¤å‡½æ•°é¦–å…ˆåœ¨æä¾›çš„ç›®å½•çš„æ ¹çº§åˆ«æœç´¢ YAML æ–‡ä»¶ï¼Œå¦‚æœæœªæ‰¾åˆ°ï¼Œ
    performs a recursive search. It prefers YAML files that have the same stem as the provided path. An AssertionError  # è¿›è¡Œé€’å½’æœç´¢ã€‚å®ƒä¼˜å…ˆé€‰æ‹©ä¸æä¾›è·¯å¾„å…·æœ‰ç›¸åŒä¸»å¹²çš„ YAML æ–‡ä»¶ã€‚å¦‚æœæœªæ‰¾åˆ° YAML æ–‡ä»¶æˆ–æ‰¾åˆ°å¤šä¸ª YAML æ–‡ä»¶ï¼Œåˆ™å¼•å‘ AssertionErrorã€‚
    is raised if no YAML file is found or if multiple YAML files are found.

    Args:
        path (Path): The directory path to search for the YAML file.  # è¦æœç´¢ YAML æ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚

    Returns:
        (Path): The path of the found YAML file.  # è¿”å›æ‰¾åˆ°çš„ YAML æ–‡ä»¶çš„è·¯å¾„ã€‚
    """
    files = list(path.glob("*.yaml")) or list(path.rglob("*.yaml"))  # try root level first and then recursive  # é¦–å…ˆå°è¯•æ ¹çº§åˆ«ï¼Œç„¶åè¿›è¡Œé€’å½’æœç´¢
    assert files, f"No YAML file found in '{path.resolve()}'"  # å¦‚æœæœªæ‰¾åˆ°æ–‡ä»¶ï¼Œåˆ™å¼•å‘ AssertionError
    if len(files) > 1:  # å¦‚æœæ‰¾åˆ°å¤šä¸ªæ–‡ä»¶
        files = [f for f in files if f.stem == path.stem]  # prefer *.yaml files that match  # ä¼˜å…ˆé€‰æ‹©ä¸æä¾›è·¯å¾„ä¸»å¹²åŒ¹é…çš„ *.yaml æ–‡ä»¶
    assert len(files) == 1, f"Expected 1 YAML file in '{path.resolve()}', but found {len(files)}.\n{files}"  # ç¡®ä¿åªæ‰¾åˆ°ä¸€ä¸ª YAML æ–‡ä»¶
    return files[0]  # è¿”å›æ‰¾åˆ°çš„ YAML æ–‡ä»¶


def check_det_dataset(dataset, autodownload=True):
    """
    Download, verify, and/or unzip a dataset if not found locally.  # å¦‚æœåœ¨æœ¬åœ°æœªæ‰¾åˆ°æ•°æ®é›†ï¼Œåˆ™ä¸‹è½½ã€éªŒè¯å’Œ/æˆ–è§£å‹æ•°æ®é›†

    This function checks the availability of a specified dataset, and if not found, it has the option to download and  # æ­¤å‡½æ•°æ£€æŸ¥æŒ‡å®šæ•°æ®é›†çš„å¯ç”¨æ€§ï¼Œå¦‚æœæœªæ‰¾åˆ°ï¼Œåˆ™å¯ä»¥é€‰æ‹©ä¸‹è½½å’Œ
    unzip the dataset. It then reads and parses the accompanying YAML data, ensuring key requirements are met and also  # è§£å‹æ•°æ®é›†ã€‚ç„¶åè¯»å–å’Œè§£æéšé™„çš„ YAML æ•°æ®ï¼Œç¡®ä¿æ»¡è¶³å…³é”®è¦æ±‚ï¼Œå¹¶è§£æä¸æ•°æ®é›†ç›¸å…³çš„è·¯å¾„ã€‚
    resolves paths related to the dataset.

    Args:
        dataset (str): Path to the dataset or dataset descriptor (like a YAML file).  # æ•°æ®é›†çš„è·¯å¾„æˆ–æ•°æ®é›†æè¿°ç¬¦ï¼ˆå¦‚ YAML æ–‡ä»¶ï¼‰ã€‚
        autodownload (bool, optional): Whether to automatically download the dataset if not found. Defaults to True.  # å¦‚æœæœªæ‰¾åˆ°ï¼Œæ˜¯å¦è‡ªåŠ¨ä¸‹è½½æ•°æ®é›†ã€‚é»˜è®¤ä¸º Trueã€‚

    Returns:
        (dict): Parsed dataset information and paths.  # è¿”å›è§£æåçš„æ•°æ®é›†ä¿¡æ¯å’Œè·¯å¾„ã€‚
    """
    file = check_file(dataset)  # æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶

    # Download (optional)  # ä¸‹è½½ï¼ˆå¯é€‰ï¼‰
    extract_dir = ""  # åˆå§‹åŒ–æå–ç›®å½•
    if zipfile.is_zipfile(file) or is_tarfile(file):  # å¦‚æœæ–‡ä»¶æ˜¯ zip æˆ– tar æ ¼å¼
        new_dir = safe_download(file, dir=DATASETS_DIR, unzip=True, delete=False)  # å®‰å…¨ä¸‹è½½å¹¶è§£å‹æ–‡ä»¶
        file = find_dataset_yaml(DATASETS_DIR / new_dir)  # æŸ¥æ‰¾è§£å‹åçš„ YAML æ–‡ä»¶
        extract_dir, autodownload = file.parent, False  # æ›´æ–°æå–ç›®å½•å¹¶è®¾ç½®è‡ªåŠ¨ä¸‹è½½ä¸º False

    # Read YAML  # è¯»å– YAML
    data = yaml_load(file, append_filename=True)  # dictionary  # åŠ è½½ YAML æ•°æ®ä¸ºå­—å…¸

    # Checks  # æ£€æŸ¥
    for k in "train", "val":  # éå†è®­ç»ƒå’ŒéªŒè¯é”®
        if k not in data:  # å¦‚æœé”®ä¸åœ¨æ•°æ®ä¸­
            if k != "val" or "validation" not in data:  # å¦‚æœä¸æ˜¯éªŒè¯ä¸”éªŒè¯é”®ä¸åœ¨æ•°æ®ä¸­
                raise SyntaxError(  # å¼•å‘è¯­æ³•é”™è¯¯
                    emojis(f"{dataset} '{k}:' key missing âŒ.\n'train' and 'val' are required in all data YAMLs.")  # é”™è¯¯æ¶ˆæ¯
                )
            LOGGER.info("WARNING âš ï¸ renaming data YAML 'validation' key to 'val' to match YOLO format.")  # è®°å½•è­¦å‘Šä¿¡æ¯
            data["val"] = data.pop("validation")  # replace 'validation' key with 'val' key  # å°† 'validation' é”®æ›¿æ¢ä¸º 'val' é”®
    if "names" not in data and "nc" not in data:  # å¦‚æœæ•°æ®ä¸­æ²¡æœ‰ 'names' å’Œ 'nc'
        raise SyntaxError(emojis(f"{dataset} key missing âŒ.\n either 'names' or 'nc' are required in all data YAMLs."))  # å¼•å‘è¯­æ³•é”™è¯¯
    if "names" in data and "nc" in data and len(data["names"]) != data["nc"]:  # å¦‚æœåŒæ—¶å­˜åœ¨ 'names' å’Œ 'nc'ï¼Œä¸”é•¿åº¦ä¸åŒ¹é…
        raise SyntaxError(emojis(f"{dataset} 'names' length {len(data['names'])} and 'nc: {data['nc']}' must match."))  # å¼•å‘è¯­æ³•é”™è¯¯
    if "names" not in data:  # å¦‚æœæ•°æ®ä¸­æ²¡æœ‰ 'names'
        data["names"] = [f"class_{i}" for i in range(data["nc"])]  # æ ¹æ® 'nc' åˆ›å»ºé»˜è®¤ç±»å
    else:
        data["nc"] = len(data["names"])  # æ›´æ–° 'nc' ä¸º 'names' çš„é•¿åº¦

    data["names"] = check_class_names(data["names"])  # æ£€æŸ¥ç±»åçš„æœ‰æ•ˆæ€§

    # Resolve paths  # è§£æè·¯å¾„
    path = Path(extract_dir or data.get("path") or Path(data.get("yaml_file", "")).parent)  # dataset root  # æ•°æ®é›†æ ¹ç›®å½•
    if not path.is_absolute():  # å¦‚æœè·¯å¾„ä¸æ˜¯ç»å¯¹è·¯å¾„
        path = (DATASETS_DIR / path).resolve()  # è§£æä¸ºç»å¯¹è·¯å¾„

    # Set paths  # è®¾ç½®è·¯å¾„
    data["path"] = path  # download scripts  # ä¸‹è½½è„šæœ¬
    for k in "train", "val", "test", "minival":  # éå†è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•å’Œå°éªŒè¯é”®
        if data.get(k):  # å¦‚æœé”®å­˜åœ¨
            if isinstance(data[k], str):  # å¦‚æœæ˜¯å­—ç¬¦ä¸²
                x = (path / data[k]).resolve()  # è§£æä¸ºç»å¯¹è·¯å¾„
                if not x.exists() and data[k].startswith("../"):  # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ä¸”ä»¥ "../" å¼€å¤´
                    x = (path / data[k][3:]).resolve()  # è§£æä¸ºç»å¯¹è·¯å¾„
                data[k] = str(x)  # æ›´æ–°ä¸ºå­—ç¬¦ä¸²è·¯å¾„
            else:
                data[k] = [str((path / x).resolve()) for x in data[k]]  # æ›´æ–°ä¸ºè§£æåçš„è·¯å¾„åˆ—è¡¨

    # Parse YAML  # è§£æ YAML
    val, s = (data.get(x) for x in ("val", "download"))  # è·å–éªŒè¯å’Œä¸‹è½½è·¯å¾„
    if val:  # å¦‚æœéªŒè¯è·¯å¾„å­˜åœ¨
        val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])]  # val path  # è§£æéªŒè¯è·¯å¾„
        if not all(x.exists() for x in val):  # å¦‚æœéªŒè¯è·¯å¾„ä¸­æœ‰ä¸å­˜åœ¨çš„è·¯å¾„
            name = clean_url(dataset)  # dataset name with URL auth stripped  # æ¸…ç†æ•°æ®é›†åç§°
            m = f"\nDataset '{name}' images not found âš ï¸, missing path '{[x for x in val if not x.exists()][0]}'"  # é”™è¯¯æ¶ˆæ¯
            if s and autodownload:  # å¦‚æœä¸‹è½½é“¾æ¥å­˜åœ¨ä¸”å…è®¸è‡ªåŠ¨ä¸‹è½½
                LOGGER.warning(m)  # è®°å½•è­¦å‘Šä¿¡æ¯
            else:  # å¦‚æœä¸å…è®¸è‡ªåŠ¨ä¸‹è½½
                m += f"\nNote dataset download directory is '{DATASETS_DIR}'. You can update this in '{SETTINGS_FILE}'"  # æç¤ºä¸‹è½½ç›®å½•
                raise FileNotFoundError(m)  # å¼•å‘æ–‡ä»¶æœªæ‰¾åˆ°é”™è¯¯
            t = time.time()  # è®°å½•å½“å‰æ—¶é—´
            r = None  # success  # åˆå§‹åŒ–æˆåŠŸæ ‡å¿—
            if s.startswith("http") and s.endswith(".zip"):  # å¦‚æœæ˜¯ URL
                safe_download(url=s, dir=DATASETS_DIR, delete=True)  # å®‰å…¨ä¸‹è½½æ•°æ®é›†
            elif s.startswith("bash "):  # å¦‚æœæ˜¯ bash è„šæœ¬
                LOGGER.info(f"Running {s} ...")  # è®°å½•æ­£åœ¨è¿è¡Œçš„è„šæœ¬ä¿¡æ¯
                r = os.system(s)  # æ‰§è¡Œ bash è„šæœ¬
            else:  # python script  # å¦‚æœæ˜¯ Python è„šæœ¬
                exec(s, {"yaml": data})  # æ‰§è¡Œ Python è„šæœ¬
            dt = f"({round(time.time() - t, 1)}s)"  # è®¡ç®—ä¸‹è½½æ—¶é—´
            s = f"success âœ… {dt}, saved to {colorstr('bold', DATASETS_DIR)}" if r in {0, None} else f"failure {dt} âŒ"  # ä¸‹è½½ç»“æœæ¶ˆæ¯
            LOGGER.info(f"Dataset download {s}\n")  # è®°å½•ä¸‹è½½ç»“æœ
    check_font("Arial.ttf" if is_ascii(data["names"]) else "Arial.Unicode.ttf")  # download fonts  # æ£€æŸ¥å­—ä½“

    return data  # dictionary  # è¿”å›æ•°æ®é›†ä¿¡æ¯å­—å…¸

def check_cls_dataset(dataset, split=""):
    """
    Checks a classification dataset such as Imagenet.  # æ£€æŸ¥åˆ†ç±»æ•°æ®é›†ï¼Œä¾‹å¦‚ Imagenet

    This function accepts a [dataset](cci:1://file:///Users/gatilin/MyWork/Source-Code-Reading1/source/YOLO/ultralytics-8.3.74/ultralytics/data/utils.py:387:0-462:91) name and attempts to retrieve the corresponding dataset information.  # æ­¤å‡½æ•°æ¥å—ä¸€ä¸ª [dataset](cci:1://file:///Users/gatilin/MyWork/Source-Code-Reading1/source/YOLO/ultralytics-8.3.74/ultralytics/data/utils.py:387:0-462:91) åç§°ï¼Œå¹¶å°è¯•æ£€ç´¢ç›¸åº”çš„æ•°æ®é›†ä¿¡æ¯ã€‚
    If the dataset is not found locally, it attempts to download the dataset from the internet and save it locally.  # å¦‚æœåœ¨æœ¬åœ°æœªæ‰¾åˆ°æ•°æ®é›†ï¼Œå®ƒå°†å°è¯•ä»äº’è”ç½‘ä¸‹è½½æ•°æ®é›†å¹¶å°†å…¶ä¿å­˜åœ¨æœ¬åœ°ã€‚

    Args:
        dataset (str | Path): The name of the dataset.  # æ•°æ®é›†çš„åç§°ã€‚
        split (str, optional): The split of the dataset. Either 'val', 'test', or ''. Defaults to ''.  # æ•°æ®é›†çš„åˆ’åˆ†ã€‚å¯ä»¥æ˜¯ 'val'ã€'test' æˆ– ''ã€‚é»˜è®¤ä¸º ''ã€‚

    Returns:
        (dict): A dictionary containing the following keys:  # è¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹é”®ï¼š
            - 'train' (Path): The directory path containing the training set of the dataset.  # 'train'ï¼ˆPathï¼‰ï¼šåŒ…å«æ•°æ®é›†è®­ç»ƒé›†çš„ç›®å½•è·¯å¾„ã€‚
            - 'val' (Path): The directory path containing the validation set of the dataset.  # 'val'ï¼ˆPathï¼‰ï¼šåŒ…å«æ•°æ®é›†éªŒè¯é›†çš„ç›®å½•è·¯å¾„ã€‚
            - 'test' (Path): The directory path containing the test set of the dataset.  # 'test'ï¼ˆPathï¼‰ï¼šåŒ…å«æ•°æ®é›†æµ‹è¯•é›†çš„ç›®å½•è·¯å¾„ã€‚
            - 'nc' (int): The number of classes in the dataset.  # 'nc'ï¼ˆintï¼‰ï¼šæ•°æ®é›†ä¸­çš„ç±»åˆ«æ•°é‡ã€‚
            - 'names' (dict): A dictionary of class names in the dataset.  # 'names'ï¼ˆdictï¼‰ï¼šæ•°æ®é›†ä¸­çš„ç±»åˆ«åç§°å­—å…¸ã€‚
    """
    # Download (optional if dataset=https://file.zip is passed directly)  # ä¸‹è½½ï¼ˆå¦‚æœç›´æ¥ä¼ é€’ dataset=https://file.zipï¼Œåˆ™ä¸ºå¯é€‰ï¼‰
    if str(dataset).startswith(("http:/", "https:/")):  # å¦‚æœæ•°æ®é›†æ˜¯ URL
        dataset = safe_download(dataset, dir=DATASETS_DIR, unzip=True, delete=False)  # å®‰å…¨ä¸‹è½½æ•°æ®é›†
    elif Path(dataset).suffix in {".zip", ".tar", ".gz"}:  # å¦‚æœæ•°æ®é›†æ–‡ä»¶æ˜¯å‹ç¼©æ ¼å¼
        file = check_file(dataset)  # æ£€æŸ¥æ–‡ä»¶æœ‰æ•ˆæ€§
        dataset = safe_download(file, dir=DATASETS_DIR, unzip=True, delete=False)  # å®‰å…¨ä¸‹è½½å¹¶è§£å‹æ–‡ä»¶

    dataset = Path(dataset)  # å°†æ•°æ®é›†è½¬æ¢ä¸º Path å¯¹è±¡
    data_dir = (dataset if dataset.is_dir() else (DATASETS_DIR / dataset)).resolve()  # è§£ææ•°æ®é›†ç›®å½•
    if not data_dir.is_dir():  # å¦‚æœæ•°æ®ç›®å½•ä¸å­˜åœ¨
        LOGGER.warning(f"\nDataset not found âš ï¸, missing path {data_dir}, attempting download...")  # è®°å½•è­¦å‘Šä¿¡æ¯ï¼Œå°è¯•ä¸‹è½½
        t = time.time()  # è®°å½•å½“å‰æ—¶é—´
        if str(dataset) == "imagenet":  # å¦‚æœæ•°æ®é›†æ˜¯ imagenet
            subprocess.run(f"bash {ROOT / 'data/scripts/get_imagenet.sh'}", shell=True, check=True)  # è¿è¡Œä¸‹è½½è„šæœ¬
        else:  # å…¶ä»–æ•°æ®é›†
            url = f"https://github.com/ultralytics/assets/releases/download/v0.0.0/{dataset}.zip"  # æ„å»ºä¸‹è½½ URL
            download(url, dir=data_dir.parent)  # ä¸‹è½½æ•°æ®é›†
        s = f"Dataset download success âœ… ({time.time() - t:.1f}s), saved to {colorstr('bold', data_dir)}\n"  # ä¸‹è½½æˆåŠŸæ¶ˆæ¯
        LOGGER.info(s)  # è®°å½•ä¸‹è½½æˆåŠŸä¿¡æ¯
    train_set = data_dir / "train"  # è·å–è®­ç»ƒé›†è·¯å¾„
    val_set = (  # è·å–éªŒè¯é›†è·¯å¾„
        data_dir / "val"  # ä¼˜å…ˆæ£€æŸ¥ 'val'
        if (data_dir / "val").exists()  # å¦‚æœ 'val' å­˜åœ¨
        else data_dir / "validation"  # å¦åˆ™æ£€æŸ¥ 'validation'
        if (data_dir / "validation").exists()  # å¦‚æœ 'validation' å­˜åœ¨
        else None  # å¦åˆ™è¿”å› None
    )  # data/test or data/val
    test_set = data_dir / "test" if (data_dir / "test").exists() else None  # è·å–æµ‹è¯•é›†è·¯å¾„
    if split == "val" and not val_set:  # å¦‚æœè¯·æ±‚éªŒè¯é›†ä½†æœªæ‰¾åˆ°
        LOGGER.warning("WARNING âš ï¸ Dataset 'split=val' not found, using 'split=test' instead.")  # è®°å½•è­¦å‘Šä¿¡æ¯ï¼Œä½¿ç”¨æµ‹è¯•é›†
    elif split == "test" and not test_set:  # å¦‚æœè¯·æ±‚æµ‹è¯•é›†ä½†æœªæ‰¾åˆ°
        LOGGER.warning("WARNING âš ï¸ Dataset 'split=test' not found, using 'split=val' instead.")  # è®°å½•è­¦å‘Šä¿¡æ¯ï¼Œä½¿ç”¨éªŒè¯é›†

    nc = len([x for x in (data_dir / "train").glob("*") if x.is_dir()])  # è®¡ç®—ç±»åˆ«æ•°é‡
    names = [x.name for x in (data_dir / "train").iterdir() if x.is_dir()]  # è·å–ç±»åˆ«åç§°åˆ—è¡¨
    names = dict(enumerate(sorted(names)))  # å°†ç±»åˆ«åç§°è½¬æ¢ä¸ºå­—å…¸

    # Print to console  # æ‰“å°åˆ°æ§åˆ¶å°
    for k, v in {"train": train_set, "val": val_set, "test": test_set}.items():  # éå†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†
        prefix = f"{colorstr(f'{k}:')} {v}..."  # æ„å»ºå‰ç¼€
        if v is None:  # å¦‚æœè·¯å¾„ä¸º None
            LOGGER.info(prefix)  # è®°å½•ä¿¡æ¯
        else:
            files = [path for path in v.rglob("*.*") if path.suffix[1:].lower() in IMG_FORMATS]  # è·å–å›¾åƒæ–‡ä»¶
            nf = len(files)  # è®¡ç®—æ–‡ä»¶æ•°é‡
            nd = len({file.parent for file in files})  # è®¡ç®—ç›®å½•æ•°é‡
            if nf == 0:  # å¦‚æœæ²¡æœ‰æ–‡ä»¶
                if k == "train":  # å¦‚æœæ˜¯è®­ç»ƒé›†
                    raise FileNotFoundError(emojis(f"{dataset} '{k}:' no training images found âŒ "))  # å¼•å‘æ–‡ä»¶æœªæ‰¾åˆ°é”™è¯¯
                else:
                    LOGGER.warning(f"{prefix} found {nf} images in {nd} classes: WARNING âš ï¸ no images found")  # è®°å½•è­¦å‘Šä¿¡æ¯
            elif nd != nc:  # å¦‚æœç›®å½•æ•°é‡ä¸ç±»åˆ«æ•°é‡ä¸åŒ¹é…
                LOGGER.warning(f"{prefix} found {nf} images in {nd} classes: ERROR âŒï¸ requires {nc} classes, not {nd}")  # è®°å½•é”™è¯¯ä¿¡æ¯
            else:
                LOGGER.info(f"{prefix} found {nf} images in {nd} classes âœ… ")  # è®°å½•æˆåŠŸä¿¡æ¯

    return {"train": train_set, "val": val_set, "test": test_set, "nc": nc, "names": names}  # è¿”å›æ•°æ®é›†ä¿¡æ¯å­—å…¸


class HUBDatasetStats:
    """
    A class for generating HUB dataset JSON and `-hub` dataset directory.  # ç”¨äºç”Ÿæˆ HUB æ•°æ®é›† JSON å’Œ `-hub` æ•°æ®é›†ç›®å½•çš„ç±»

    Args:
        path (str): Path to data.yaml or data.zip (with data.yaml inside data.zip). Default is 'coco8.yaml'.  # data.yaml æˆ– data.zip çš„è·¯å¾„ï¼ˆå…¶ä¸­ data.yaml åœ¨ data.zip å†…ï¼‰ã€‚é»˜è®¤ä¸º 'coco8.yaml'ã€‚
        task (str): Dataset task. Options are 'detect', 'segment', 'pose', 'classify'. Default is 'detect'.  # æ•°æ®é›†ä»»åŠ¡ã€‚é€‰é¡¹ä¸º 'detect'ã€'segment'ã€'pose'ã€'classify'ã€‚é»˜è®¤ä¸º 'detect'ã€‚
        autodownload (bool): Attempt to download dataset if not found locally. Default is False.  # å¦‚æœæœªæ‰¾åˆ°æ•°æ®é›†ï¼Œæ˜¯å¦å°è¯•ä¸‹è½½ã€‚é»˜è®¤ä¸º Falseã€‚

    Example:
        Download *.zip files from https://github.com/ultralytics/hub/tree/main/example_datasets  # ä» https://github.com/ultralytics/hub/tree/main/example_datasets ä¸‹è½½ *.zip æ–‡ä»¶
            i.e. https://github.com/ultralytics/hub/raw/main/example_datasets/coco8.zip for coco8.zip.  # ä¾‹å¦‚ï¼Œcoco8.zip çš„é“¾æ¥ã€‚
        ```python
        from ultralytics.data.utils import HUBDatasetStats

        stats = HUBDatasetStats("path/to/coco8.zip", task="detect")  # detect dataset  # æ£€æµ‹æ•°æ®é›†
        stats = HUBDatasetStats("path/to/coco8-seg.zip", task="segment")  # segment dataset  # åˆ†å‰²æ•°æ®é›†
        stats = HUBDatasetStats("path/to/coco8-pose.zip", task="pose")  # pose dataset  # å§¿æ€æ•°æ®é›†
        stats = HUBDatasetStats("path/to/dota8.zip", task="obb")  # OBB dataset  # OBB æ•°æ®é›†
        stats = HUBDatasetStats("path/to/imagenet10.zip", task="classify")  # classification dataset  # åˆ†ç±»æ•°æ®é›†

        stats.get_json(save=True)  # è·å– JSON
        stats.process_images()  # å¤„ç†å›¾åƒ
        ```
    """

    def __init__(self, path="coco8.yaml", task="detect", autodownload=False):
        """Initialize class."""  # åˆå§‹åŒ–ç±»
        path = Path(path).resolve()  # è§£æè·¯å¾„
        LOGGER.info(f"Starting HUB dataset checks for {path}....")  # è®°å½•å¼€å§‹æ£€æŸ¥æ•°æ®é›†çš„ä¿¡æ¯

        self.task = task  # detect, segment, pose, classify, obb  # ä»»åŠ¡ç±»å‹
        if self.task == "classify":  # å¦‚æœä»»åŠ¡æ˜¯åˆ†ç±»
            unzip_dir = unzip_file(path)  # è§£å‹æ–‡ä»¶
            data = check_cls_dataset(unzip_dir)  # æ£€æŸ¥åˆ†ç±»æ•°æ®é›†
            data["path"] = unzip_dir  # è®¾ç½®æ•°æ®è·¯å¾„
        else:  # detect, segment, pose, obb  # å¯¹äºå…¶ä»–ä»»åŠ¡
            _, data_dir, yaml_path = self._unzip(Path(path))  # è§£å‹å¹¶è·å–æ•°æ®ç›®å½•å’Œ YAML è·¯å¾„
            try:
                # Load YAML with checks  # åŠ è½½ YAML å¹¶è¿›è¡Œæ£€æŸ¥
                data = yaml_load(yaml_path)  # åŠ è½½ YAML æ•°æ®
                data["path"] = ""  # strip path since YAML should be in dataset root for all HUB datasets  # æ¸…é™¤è·¯å¾„ï¼Œå› ä¸º YAML åº”åœ¨æ‰€æœ‰ HUB æ•°æ®é›†çš„æ ¹ç›®å½•ä¸­
                yaml_save(yaml_path, data)  # ä¿å­˜ YAML æ•°æ®
                data = check_det_dataset(yaml_path, autodownload)  # dict  # æ£€æŸ¥æ£€æµ‹æ•°æ®é›†
                data["path"] = data_dir  # YAML è·¯å¾„åº”è®¾ç½®ä¸º ''ï¼ˆç›¸å¯¹ï¼‰æˆ–çˆ¶ç›®å½•ï¼ˆç»å¯¹ï¼‰
            except Exception as e:
                raise Exception("error/HUB/dataset_stats/init") from e  # å¼•å‘å¼‚å¸¸

        self.hub_dir = Path(f"{data['path']}-hub")  # è®¾ç½® HUB ç›®å½•
        self.im_dir = self.hub_dir / "images"  # è®¾ç½®å›¾åƒç›®å½•
        self.stats = {"nc": len(data["names"]), "names": list(data["names"].values())}  # statistics dictionary  # ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        self.data = data  # ä¿å­˜æ•°æ®

    @staticmethod
    def _unzip(path):
        """Unzip data.zip."""  # è§£å‹ data.zip
        if not str(path).endswith(".zip"):  # path is data.yaml  # å¦‚æœè·¯å¾„ä¸æ˜¯ ZIP æ–‡ä»¶
            return False, None, path  # è¿”å› False å’Œè·¯å¾„
        unzip_dir = unzip_file(path, path=path.parent)  # è§£å‹æ–‡ä»¶
        assert unzip_dir.is_dir(), (  # ç¡®ä¿è§£å‹åçš„ç›®å½•å­˜åœ¨
            f"Error unzipping {path}, {unzip_dir} not found. path/to/abc.zip MUST unzip to path/to/abc/"  # é”™è¯¯æ¶ˆæ¯
        )
        return True, str(unzip_dir), find_dataset_yaml(unzip_dir)  # zipped, data_dir, yaml_path  # è¿”å› Trueã€è§£å‹ç›®å½•å’Œ YAML è·¯å¾„

    def _hub_ops(self, f):
        """Saves a compressed image for HUB previews."""  # ä¿å­˜å‹ç¼©å›¾åƒä»¥ä¾› HUB é¢„è§ˆ
        compress_one_image(f, self.im_dir / Path(f).name)  # save to dataset-hub  # ä¿å­˜åˆ° dataset-hub

    def get_json(self, save=False, verbose=False):
        """Return dataset JSON for Ultralytics HUB."""  # è¿”å› Ultralytics HUB çš„æ•°æ®é›† JSON

        def _round(labels):
            """Update labels to integer class and 4 decimal place floats."""  # æ›´æ–°æ ‡ç­¾ä¸ºæ•´æ•°ç±»å’Œ 4 ä½å°æ•°æµ®ç‚¹æ•°
            if self.task == "detect":  # å¦‚æœä»»åŠ¡æ˜¯æ£€æµ‹
                coordinates = labels["bboxes"]  # è·å–è¾¹ç•Œæ¡†åæ ‡
            elif self.task in {"segment", "obb"}:  # Segment and OBB use segments. OBB segments are normalized xyxyxyxy  # åˆ†å‰²å’Œ OBB ä½¿ç”¨æ®µã€‚OBB æ®µæ˜¯æ ‡å‡†åŒ–çš„ xyxyxyxy
                coordinates = [x.flatten() for x in labels["segments"]]  # è·å–æ®µåæ ‡
            elif self.task == "pose":  # å¦‚æœä»»åŠ¡æ˜¯å§¿æ€
                n, nk, nd = labels["keypoints"].shape  # è·å–å…³é”®ç‚¹çš„å½¢çŠ¶
                coordinates = np.concatenate((labels["bboxes"], labels["keypoints"].reshape(n, nk * nd)), 1)  # åˆå¹¶è¾¹ç•Œæ¡†å’Œå…³é”®ç‚¹
            else:
                raise ValueError(f"Undefined dataset task={self.task}.")  # å¼•å‘æœªå®šä¹‰ä»»åŠ¡çš„é”™è¯¯
            zipped = zip(labels["cls"], coordinates)  # å°†ç±»åˆ«å’Œåæ ‡å‹ç¼©åœ¨ä¸€èµ·
            return [[int(c[0]), *(round(float(x), 4) for x in points)] for c, points in zipped]  # è¿”å›æ›´æ–°åçš„æ ‡ç­¾

        for split in "train", "val", "test":  # éå†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†
            self.stats[split] = None  # predefine  # é¢„å®šä¹‰
            path = self.data.get(split)  # è·å–å½“å‰åˆ’åˆ†çš„è·¯å¾„

            # Check split  # æ£€æŸ¥åˆ’åˆ†
            if path is None:  # no split  # å¦‚æœæ²¡æœ‰åˆ’åˆ†
                continue  # è·³è¿‡
            files = [f for f in Path(path).rglob("*.*") if f.suffix[1:].lower() in IMG_FORMATS]  # è·å–åˆ’åˆ†ä¸­çš„å›¾åƒæ–‡ä»¶
            if not files:  # no images  # å¦‚æœæ²¡æœ‰å›¾åƒ
                continue  # è·³è¿‡

            # Get dataset statistics  # è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
            if self.task == "classify":  # å¦‚æœä»»åŠ¡æ˜¯åˆ†ç±»
                from torchvision.datasets import ImageFolder  # scope for faster 'import ultralytics'  # ä¸ºäº†æ›´å¿«çš„ 'import ultralytics'

                dataset = ImageFolder(self.data[split])  # åˆ›å»º ImageFolder æ•°æ®é›†

                x = np.zeros(len(dataset.classes)).astype(int)  # åˆå§‹åŒ–ç±»åˆ«è®¡æ•°
                for im in dataset.imgs:  # éå†å›¾åƒ
                    x[im[1]] += 1  # æ›´æ–°ç±»åˆ«è®¡æ•°

                self.stats[split] = {  # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                    "instance_stats": {"total": len(dataset), "per_class": x.tolist()},  # å®ä¾‹ç»Ÿè®¡
                    "image_stats": {"total": len(dataset), "unlabelled": 0, "per_class": x.tolist()},  # å›¾åƒç»Ÿè®¡
                    "labels": [{Path(k).name: v} for k, v in dataset.imgs],  # æ ‡ç­¾ä¿¡æ¯
                }
            else:  # detect, segment, pose, obb  # å¯¹äºå…¶ä»–ä»»åŠ¡
                from ultralytics.data import YOLODataset  # å¯¼å…¥ YOLODataset

                dataset = YOLODataset(img_path=self.data[split], data=self.data, task=self.task)  # åˆ›å»º YOLO æ•°æ®é›†
                x = np.array(  # è·å–ç±»åˆ«è®¡æ•°
                    [
                        np.bincount(label["cls"].astype(int).flatten(), minlength=self.data["nc"])  # è®¡ç®—ç±»åˆ«è®¡æ•°
                        for label in TQDM(dataset.labels, total=len(dataset), desc="Statistics")  # æ˜¾ç¤ºè¿›åº¦æ¡
                    ]
                )  # shape(128x80)
                self.stats[split] = {  # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                    "instance_stats": {"total": int(x.sum()), "per_class": x.sum(0).tolist()},  # å®ä¾‹ç»Ÿè®¡
                    "image_stats": {  # å›¾åƒç»Ÿè®¡
                        "total": len(dataset),
                        "unlabelled": int(np.all(x == 0, 1).sum()),  # ç»Ÿè®¡æœªæ ‡è®°çš„å›¾åƒ
                        "per_class": (x > 0).sum(0).tolist(),  # æ¯ä¸ªç±»çš„å›¾åƒæ•°é‡
                    },
                    "labels": [{Path(k).name: _round(v)} for k, v in zip(dataset.im_files, dataset.labels)],  # æ ‡ç­¾ä¿¡æ¯
                }

        # Save, print and return  # ä¿å­˜ã€æ‰“å°å¹¶è¿”å›
        if save:  # å¦‚æœéœ€è¦ä¿å­˜
            self.hub_dir.mkdir(parents=True, exist_ok=True)  # åˆ›å»º dataset-hub/
            stats_path = self.hub_dir / "stats.json"  # è®¾ç½®ç»Ÿè®¡ä¿¡æ¯è·¯å¾„
            LOGGER.info(f"Saving {stats_path.resolve()}...")  # è®°å½•ä¿å­˜ä¿¡æ¯
            with open(stats_path, "w") as f:  # æ‰“å¼€æ–‡ä»¶ä»¥å†™å…¥
                json.dump(self.stats, f)  # ä¿å­˜ stats.json
        if verbose:  # å¦‚æœéœ€è¦è¯¦ç»†ä¿¡æ¯
            LOGGER.info(json.dumps(self.stats, indent=2, sort_keys=False))  # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        return self.stats  # è¿”å›ç»Ÿè®¡ä¿¡æ¯

    def process_images(self):
        """Compress images for Ultralytics HUB."""  # å‹ç¼©å›¾åƒä»¥ä¾› Ultralytics HUB ä½¿ç”¨
        from ultralytics.data import YOLODataset  # ClassificationDataset  # åˆ†ç±»æ•°æ®é›†

        self.im_dir.mkdir(parents=True, exist_ok=True)  # åˆ›å»º dataset-hub/images/
        for split in "train", "val", "test":  # éå†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†
            if self.data.get(split) is None:  # å¦‚æœæ²¡æœ‰æ•°æ®
                continue  # è·³è¿‡
            dataset = YOLODataset(img_path=self.data[split], data=self.data)  # åˆ›å»º YOLO æ•°æ®é›†
            with ThreadPool(NUM_THREADS) as pool:  # ä½¿ç”¨çº¿ç¨‹æ± 
                for _ in TQDM(pool.imap(self._hub_ops, dataset.im_files), total=len(dataset), desc=f"{split} images"):  # å¤„ç†å›¾åƒ
                    pass
        LOGGER.info(f"Done. All images saved to {self.im_dir}")  # è®°å½•å®Œæˆä¿¡æ¯
        return self.im_dir  # è¿”å›å›¾åƒç›®å½•


def compress_one_image(f, f_new=None, max_dim=1920, quality=50):
    """
    Compresses a single image file to reduced size while preserving its aspect ratio and quality using either the Python  # å‹ç¼©å•ä¸ªå›¾åƒæ–‡ä»¶ä»¥å‡å°‘å¤§å°ï¼ŒåŒæ—¶ä½¿ç”¨ Python Imaging Library (PIL) æˆ– OpenCV åº“ä¿æŒå…¶çºµæ¨ªæ¯”å’Œè´¨é‡ã€‚
    Imaging Library (PIL) or OpenCV library. If the input image is smaller than the maximum dimension, it will not be  # å¦‚æœè¾“å…¥å›¾åƒå°äºæœ€å¤§å°ºå¯¸ï¼Œåˆ™ä¸ä¼šè°ƒæ•´å¤§å°ã€‚
    resized.

    Args:
        f (str): The path to the input image file.  # è¾“å…¥å›¾åƒæ–‡ä»¶çš„è·¯å¾„ã€‚
        f_new (str, optional): The path to the output image file. If not specified, the input file will be overwritten.  # è¾“å‡ºå›¾åƒæ–‡ä»¶çš„è·¯å¾„ã€‚å¦‚æœæœªæŒ‡å®šï¼Œåˆ™è¾“å…¥æ–‡ä»¶å°†è¢«è¦†ç›–ã€‚
        max_dim (int, optional): The maximum dimension (width or height) of the output image. Default is 1920 pixels.  # è¾“å‡ºå›¾åƒçš„æœ€å¤§å°ºå¯¸ï¼ˆå®½åº¦æˆ–é«˜åº¦ï¼‰ã€‚é»˜è®¤ä¸º 1920 åƒç´ ã€‚
        quality (int, optional): The image compression quality as a percentage. Default is 50%.  # å›¾åƒå‹ç¼©è´¨é‡çš„ç™¾åˆ†æ¯”ã€‚é»˜è®¤ä¸º 50%ã€‚

    Example:
        ```python
        from pathlib import Path
        from ultralytics.data.utils import compress_one_image

        for f in Path("path/to/dataset").rglob("*.jpg"):  # éå†æ•°æ®é›†ä¸­æ‰€æœ‰çš„ JPG æ–‡ä»¶
            compress_one_image(f)  # å‹ç¼©å›¾åƒ
        ```
    """
    try:  # use PIL  # ä½¿ç”¨ PIL
        im = Image.open(f)  # æ‰“å¼€å›¾åƒæ–‡ä»¶
        r = max_dim / max(im.height, im.width)  # ratio  # è®¡ç®—æ¯”ä¾‹
        if r < 1.0:  # image too large  # å¦‚æœå›¾åƒå¤ªå¤§
            im = im.resize((int(im.width * r), int(im.height * r)))  # è°ƒæ•´å›¾åƒå¤§å°
        im.save(f_new or f, "JPEG", quality=quality, optimize=True)  # ä¿å­˜å›¾åƒ
    except Exception as e:  # use OpenCV  # ä½¿ç”¨ OpenCV
        LOGGER.info(f"WARNING âš ï¸ HUB ops PIL failure {f}: {e}")  # è®°å½•è­¦å‘Šä¿¡æ¯
        im = cv2.imread(f)  # è¯»å–å›¾åƒ
        im_height, im_width = im.shape[:2]  # è·å–å›¾åƒé«˜åº¦å’Œå®½åº¦
        r = max_dim / max(im_height, im_width)  # ratio  # è®¡ç®—æ¯”ä¾‹
        if r < 1.0:  # image too large  # å¦‚æœå›¾åƒå¤ªå¤§
            im = cv2.resize(im, (int(im_width * r), int(im_height * r)), interpolation=cv2.INTER_AREA)  # è°ƒæ•´å›¾åƒå¤§å°
        cv2.imwrite(str(f_new or f), im)  # ä¿å­˜å›¾åƒ


def autosplit(path=DATASETS_DIR / "coco8/images", weights=(0.9, 0.1, 0.0), annotated_only=False):
    """
    Automatically split a dataset into train/val/test splits and save the resulting splits into autosplit_*.txt files.  # è‡ªåŠ¨å°†æ•°æ®é›†æ‹†åˆ†ä¸º train/val/test åˆ’åˆ†ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ° autosplit_*.txt æ–‡ä»¶ä¸­ã€‚

    Args:
        path (Path, optional): Path to images directory. Defaults to DATASETS_DIR / 'coco8/images'.  # å›¾åƒç›®å½•çš„è·¯å¾„ã€‚é»˜è®¤ä¸º DATASETS_DIR / 'coco8/images'ã€‚
        weights (list | tuple, optional): Train, validation, and test split fractions. Defaults to (0.9, 0.1, 0.0).  # è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•åˆ’åˆ†çš„æ¯”ä¾‹ã€‚é»˜è®¤ä¸º (0.9, 0.1, 0.0)ã€‚
        annotated_only (bool, optional): If True, only images with an associated txt file are used. Defaults to False.  # å¦‚æœä¸º Trueï¼Œåˆ™ä»…ä½¿ç”¨ä¸ txt æ–‡ä»¶å…³è”çš„å›¾åƒã€‚é»˜è®¤ä¸º Falseã€‚

    Example:
        ```python
        from ultralytics.data.utils import autosplit  # å¯¼å…¥ autosplit å‡½æ•°

        autosplit()  # è°ƒç”¨ autosplit å‡½æ•°
        ```
    """
    path = Path(path)  # images dir  # å›¾åƒç›®å½•
    files = sorted(x for x in path.rglob("*.*") if x.suffix[1:].lower() in IMG_FORMATS)  # image files only  # ä»…è·å–å›¾åƒæ–‡ä»¶
    n = len(files)  # number of files  # æ–‡ä»¶æ•°é‡
    random.seed(0)  # for reproducibility  # ä¸ºå¯é‡å¤æ€§è®¾ç½®éšæœºç§å­
    indices = random.choices([0, 1, 2], weights=weights, k=n)  # assign each image to a split  # ä¸ºæ¯ä¸ªå›¾åƒåˆ†é…åˆ’åˆ†

    txt = ["autosplit_train.txt", "autosplit_val.txt", "autosplit_test.txt"]  # 3 txt files  # 3 ä¸ª txt æ–‡ä»¶
    for x in txt:  # éå† txt æ–‡ä»¶
        if (path.parent / x).exists():  # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨
            (path.parent / x).unlink()  # remove existing  # åˆ é™¤ç°æœ‰æ–‡ä»¶

    LOGGER.info(f"Autosplitting images from {path}" + ", using *.txt labeled images only" * annotated_only)  # è®°å½•è‡ªåŠ¨æ‹†åˆ†ä¿¡æ¯
    for i, img in TQDM(zip(indices, files), total=n):  # éå†å›¾åƒå’Œç´¢å¼•
        if not annotated_only or Path(img2label_paths([str(img)])[0]).exists():  # check label  # æ£€æŸ¥æ ‡ç­¾
            with open(path.parent / txt[i], "a") as f:  # æ‰“å¼€ç›¸åº”çš„ txt æ–‡ä»¶ä»¥è¿½åŠ 
                f.write(f"./{img.relative_to(path.parent).as_posix()}" + "\n")  # å°†å›¾åƒè·¯å¾„æ·»åŠ åˆ° txt æ–‡ä»¶


def load_dataset_cache_file(path):
    """Load an Ultralytics *.cache dictionary from path."""  # ä»è·¯å¾„åŠ è½½ Ultralytics *.cache å­—å…¸
    import gc  # å¯¼å…¥åƒåœ¾å›æ”¶æ¨¡å—

    gc.disable()  # reduce pickle load time https://github.com/ultralytics/ultralytics/pull/1585  # ç¦ç”¨åƒåœ¾å›æ”¶ä»¥å‡å°‘åŠ è½½æ—¶é—´
    cache = np.load(str(path), allow_pickle=True).item()  # load dict  # åŠ è½½å­—å…¸
    gc.enable()  # å¯ç”¨åƒåœ¾å›æ”¶
    return cache  # è¿”å›ç¼“å­˜


def save_dataset_cache_file(prefix, path, x, version):
    """Save an Ultralytics dataset *.cache dictionary x to path."""  # å°† Ultralytics æ•°æ®é›† *.cache å­—å…¸ x ä¿å­˜åˆ°è·¯å¾„
    x["version"] = version  # add cache version  # æ·»åŠ ç¼“å­˜ç‰ˆæœ¬
    if is_dir_writeable(path.parent):  # æ£€æŸ¥ç›®å½•æ˜¯å¦å¯å†™
        if path.exists():  # å¦‚æœè·¯å¾„å·²å­˜åœ¨
            path.unlink()  # remove *.cache file if exists  # åˆ é™¤ç°æœ‰çš„ *.cache æ–‡ä»¶
        np.save(str(path), x)  # save cache for next time  # ä¿å­˜ç¼“å­˜ä»¥ä¾›ä¸‹æ¬¡ä½¿ç”¨
        path.with_suffix(".cache.npy").rename(path)  # remove .npy suffix  # åˆ é™¤ .npy åç¼€
        LOGGER.info(f"{prefix}New cache created: {path}")  # è®°å½•æ–°ç¼“å­˜åˆ›å»ºä¿¡æ¯
    else:
        LOGGER.warning(f"{prefix}WARNING âš ï¸ Cache directory {path.parent} is not writeable, cache not saved.")  # è®°å½•è­¦å‘Šä¿¡æ¯ï¼Œç¼“å­˜æœªä¿å­˜