# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import os  # å¯¼å…¥osæ¨¡å—ï¼Œç”¨äºä¸æ“ä½œç³»ç»Ÿè¿›è¡Œäº¤äº’
import shutil  # å¯¼å…¥shutilæ¨¡å—ï¼Œç”¨äºæ–‡ä»¶å’Œç›®å½•çš„æ“ä½œ
import socket  # å¯¼å…¥socketæ¨¡å—ï¼Œç”¨äºç½‘ç»œé€šä¿¡
import sys  # å¯¼å…¥sysæ¨¡å—ï¼Œç”¨äºä¸Pythonè§£é‡Šå™¨è¿›è¡Œäº¤äº’
import tempfile  # å¯¼å…¥tempfileæ¨¡å—ï¼Œç”¨äºåˆ›å»ºä¸´æ—¶æ–‡ä»¶

from . import USER_CONFIG_DIR  # ä»å½“å‰åŒ…å¯¼å…¥USER_CONFIG_DIR
from .torch_utils import TORCH_1_9  # ä»torch_utilsæ¨¡å—å¯¼å…¥TORCH_1_9

def find_free_network_port() -> int:
    """
    Finds a free port on localhost.
    åœ¨æœ¬åœ°ä¸»æœºä¸ŠæŸ¥æ‰¾ä¸€ä¸ªç©ºé—²ç«¯å£ã€‚
    
    It is useful in single-node training when we don't want to connect to a real main node but have to set the
    `MASTER_PORT` environment variable.
    å½“æˆ‘ä»¬ä¸æƒ³è¿æ¥åˆ°çœŸå®çš„ä¸»èŠ‚ç‚¹ï¼Œä½†å¿…é¡»è®¾ç½®`MASTER_PORT`ç¯å¢ƒå˜é‡æ—¶ï¼Œè¿™åœ¨å•èŠ‚ç‚¹è®­ç»ƒä¸­éå¸¸æœ‰ç”¨ã€‚
    """
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:  # åˆ›å»ºä¸€ä¸ªTCP/IPå¥—æ¥å­—
        s.bind(("127.0.0.1", 0))  # ç»‘å®šåˆ°æœ¬åœ°ä¸»æœºçš„ä¸€ä¸ªç©ºé—²ç«¯å£
        return s.getsockname()[1]  # è¿”å›ç«¯å£å·


def generate_ddp_file(trainer):
    """Generates a DDP file and returns its file name.
    ç”Ÿæˆä¸€ä¸ªDDPæ–‡ä»¶å¹¶è¿”å›å…¶æ–‡ä»¶åã€‚"""
    module, name = f"{trainer.__class__.__module__}.{trainer.__class__.__name__}".rsplit(".", 1)  # è·å–è®­ç»ƒå™¨çš„æ¨¡å—åå’Œç±»å

    content = f"""
# Ultralytics Multi-GPU training temp file (should be automatically deleted after use)
# Ultralyticså¤šGPUè®­ç»ƒä¸´æ—¶æ–‡ä»¶ï¼ˆä½¿ç”¨ååº”è‡ªåŠ¨åˆ é™¤ï¼‰
overrides = {vars(trainer.args)}  # è·å–è®­ç»ƒå™¨å‚æ•°çš„å˜é‡å­—å…¸

if __name__ == "__main__":  # å¦‚æœè¯¥æ–‡ä»¶æ˜¯ä¸»ç¨‹åºè¿è¡Œ
    from {module} import {name}  # ä»æ¨¡å—ä¸­å¯¼å…¥è®­ç»ƒå™¨ç±»
    from ultralytics.utils import DEFAULT_CFG_DICT  # ä»utilsæ¨¡å—å¯¼å…¥é»˜è®¤é…ç½®å­—å…¸

    cfg = DEFAULT_CFG_DICT.copy()  # å¤åˆ¶é»˜è®¤é…ç½®å­—å…¸
    cfg.update(save_dir='')  # å¤„ç†é¢å¤–çš„é”®'save_dir'
    trainer = {name}(cfg=cfg, overrides=overrides)  # åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹
    trainer.args.model = "{getattr(trainer.hub_session, "model_url", trainer.args.model)}"  # è®¾ç½®æ¨¡å‹å‚æ•°
    results = trainer.train()  # å¼€å§‹è®­ç»ƒå¹¶è·å–ç»“æœ
"""
    (USER_CONFIG_DIR / "DDP").mkdir(exist_ok=True)  # åˆ›å»ºDDPç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    with tempfile.NamedTemporaryFile(
        prefix="_temp_",  # ä¸´æ—¶æ–‡ä»¶å‰ç¼€
        suffix=f"{id(trainer)}.py",  # ä¸´æ—¶æ–‡ä»¶åç¼€ï¼ŒåŒ…å«è®­ç»ƒå™¨çš„ID
        mode="w+",  # è¯»å†™æ¨¡å¼
        encoding="utf-8",  # æ–‡ä»¶ç¼–ç 
        dir=USER_CONFIG_DIR / "DDP",  # ä¸´æ—¶æ–‡ä»¶ç›®å½•
        delete=False,  # ä¸åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    ) as file:
        file.write(content)  # å†™å…¥å†…å®¹åˆ°ä¸´æ—¶æ–‡ä»¶
    return file.name  # è¿”å›ä¸´æ—¶æ–‡ä»¶å


def generate_ddp_command(world_size, trainer):
    """Generates and returns command for distributed training.
    ç”Ÿæˆå¹¶è¿”å›åˆ†å¸ƒå¼è®­ç»ƒçš„å‘½ä»¤ã€‚"""
    import __main__  # noqa local import to avoid https://github.com/Lightning-AI/lightning/issues/15218
    # æœ¬åœ°å¯¼å…¥ä»¥é¿å…ç‰¹å®šé—®é¢˜

    if not trainer.resume:  # å¦‚æœè®­ç»ƒå™¨æ²¡æœ‰æ¢å¤
        shutil.rmtree(trainer.save_dir)  # åˆ é™¤ä¿å­˜ç›®å½•
    file = generate_ddp_file(trainer)  # ç”ŸæˆDDPæ–‡ä»¶
    dist_cmd = "torch.distributed.run" if TORCH_1_9 else "torch.distributed.launch"  # é€‰æ‹©åˆ†å¸ƒå¼å‘½ä»¤
    port = find_free_network_port()  # æŸ¥æ‰¾ç©ºé—²ç½‘ç»œç«¯å£
    cmd = [sys.executable, "-m", dist_cmd, "--nproc_per_node", f"{world_size}", "--master_port", f"{port}", file]  # ç”Ÿæˆå‘½ä»¤
    return cmd, file  # è¿”å›å‘½ä»¤å’Œæ–‡ä»¶


def ddp_cleanup(trainer, file):
    """Delete temp file if created.
    å¦‚æœåˆ›å»ºäº†ä¸´æ—¶æ–‡ä»¶ï¼Œåˆ™åˆ é™¤å®ƒã€‚"""
    if f"{id(trainer)}.py" in file:  # å¦‚æœä¸´æ—¶æ–‡ä»¶åç¼€åœ¨æ–‡ä»¶åä¸­
        os.remove(file)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶