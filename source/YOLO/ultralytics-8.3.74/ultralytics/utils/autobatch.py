# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""Functions for estimating the best YOLO batch size to use a fraction of the available CUDA memory in PyTorch.  # ç”¨äºä¼°ç®—åœ¨ PyTorch ä¸­ä½¿ç”¨å¯ç”¨ CUDA å†…å­˜çš„ä¸€éƒ¨åˆ†çš„æœ€ä½³ YOLO æ‰¹é‡å¤§å°çš„å‡½æ•°ã€‚"""

import os  # å¯¼å…¥ os æ¨¡å—
from copy import deepcopy  # ä» copy å¯¼å…¥ deepcopy

import numpy as np  # å¯¼å…¥ numpy ä½œä¸º np
import torch  # å¯¼å…¥ torch æ¨¡å—

from ultralytics.utils import DEFAULT_CFG, LOGGER, colorstr  # ä» ultralytics.utils å¯¼å…¥ DEFAULT_CFGã€LOGGER å’Œ colorstr
from ultralytics.utils.torch_utils import autocast, profile  # ä» ultralytics.utils.torch_utils å¯¼å…¥ autocast å’Œ profile


def check_train_batch_size(model, imgsz=640, amp=True, batch=-1, max_num_obj=1):
    """
    Compute optimal YOLO training batch size using the autobatch() function.  # ä½¿ç”¨ autobatch() å‡½æ•°è®¡ç®—æœ€ä½³ YOLO è®­ç»ƒæ‰¹é‡å¤§å°ã€‚

    Args:  # å‚æ•°ï¼š
        model (torch.nn.Module): YOLO model to check batch size for.  # model (torch.nn.Module): ç”¨äºæ£€æŸ¥æ‰¹é‡å¤§å°çš„ YOLO æ¨¡å‹ã€‚
        imgsz (int, optional): Image size used for training.  # imgsz (int, optional): ç”¨äºè®­ç»ƒçš„å›¾åƒå¤§å°ã€‚
        amp (bool, optional): Use automatic mixed precision if True.  # amp (bool, optional): å¦‚æœä¸º Trueï¼Œåˆ™ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ã€‚
        batch (float, optional): Fraction of GPU memory to use. If -1, use default.  # batch (float, optional): è¦ä½¿ç”¨çš„ GPU å†…å­˜çš„æ¯”ä¾‹ã€‚å¦‚æœä¸º -1ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼ã€‚
        max_num_obj (int, optional): The maximum number of objects from dataset.  # max_num_obj (int, optional): æ•°æ®é›†ä¸­æœ€å¤§å¯¹è±¡æ•°é‡ã€‚

    Returns:  # è¿”å›ï¼š
        (int): Optimal batch size computed using the autobatch() function.  # (int): ä½¿ç”¨ autobatch() å‡½æ•°è®¡ç®—çš„æœ€ä½³æ‰¹é‡å¤§å°ã€‚

    Note:  # æ³¨æ„ï¼š
        If 0.0 < batch < 1.0, it's used as the fraction of GPU memory to use.  # å¦‚æœ 0.0 < batch < 1.0ï¼Œåˆ™å°†å…¶ç”¨ä½œè¦ä½¿ç”¨çš„ GPU å†…å­˜çš„æ¯”ä¾‹ã€‚
        Otherwise, a default fraction of 0.6 is used.  # å¦åˆ™ï¼Œä½¿ç”¨é»˜è®¤æ¯”ä¾‹ 0.6ã€‚
    """
    with autocast(enabled=amp):  # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
        return autobatch(  # è¿”å›è°ƒç”¨ autobatch å‡½æ•°
            deepcopy(model).train(), imgsz, fraction=batch if 0.0 < batch < 1.0 else 0.6, max_num_obj=max_num_obj
        )


def autobatch(model, imgsz=640, fraction=0.60, batch_size=DEFAULT_CFG.batch, max_num_obj=1):
    """
    Automatically estimate the best YOLO batch size to use a fraction of the available CUDA memory.  # è‡ªåŠ¨ä¼°ç®—æœ€ä½³ YOLO æ‰¹é‡å¤§å°ï¼Œä»¥ä½¿ç”¨å¯ç”¨ CUDA å†…å­˜çš„ä¸€éƒ¨åˆ†ã€‚

    Args:  # å‚æ•°ï¼š
        model (torch.nn.module): YOLO model to compute batch size for.  # model (torch.nn.module): ç”¨äºè®¡ç®—æ‰¹é‡å¤§å°çš„ YOLO æ¨¡å‹ã€‚
        imgsz (int, optional): The image size used as input for the YOLO model. Defaults to 640.  # imgsz (int, optional): ç”¨ä½œ YOLO æ¨¡å‹è¾“å…¥çš„å›¾åƒå¤§å°ã€‚é»˜è®¤ä¸º 640ã€‚
        fraction (float, optional): The fraction of available CUDA memory to use. Defaults to 0.60.  # fraction (float, optional): è¦ä½¿ç”¨çš„å¯ç”¨ CUDA å†…å­˜çš„æ¯”ä¾‹ã€‚é»˜è®¤ä¸º 0.60ã€‚
        batch_size (int, optional): The default batch size to use if an error is detected. Defaults to 16.  # batch_size (int, optional): å¦‚æœæ£€æµ‹åˆ°é”™è¯¯ï¼Œåˆ™ä½¿ç”¨çš„é»˜è®¤æ‰¹é‡å¤§å°ã€‚é»˜è®¤ä¸º 16ã€‚
        max_num_obj (int, optional): The maximum number of objects from dataset.  # max_num_obj (int, optional): æ•°æ®é›†ä¸­æœ€å¤§å¯¹è±¡æ•°é‡ã€‚

    Returns:  # è¿”å›ï¼š
        (int): The optimal batch size.  # (int): æœ€ä½³æ‰¹é‡å¤§å°ã€‚
    """
    # Check device  # æ£€æŸ¥è®¾å¤‡
    prefix = colorstr("AutoBatch: ")  # è®¾ç½®å‰ç¼€ä¸º "AutoBatch: "
    LOGGER.info(f"{prefix}Computing optimal batch size for imgsz={imgsz} at {fraction * 100}% CUDA memory utilization.")  # è®°å½•è®¡ç®—æœ€ä½³æ‰¹é‡å¤§å°çš„ä¿¡æ¯
    device = next(model.parameters()).device  # get model device  # è·å–æ¨¡å‹è®¾å¤‡
    if device.type in {"cpu", "mps"}:  # å¦‚æœè®¾å¤‡ç±»å‹ä¸º CPU æˆ– MPS
        LOGGER.info(f"{prefix} âš ï¸ intended for CUDA devices, using default batch-size {batch_size}")  # è®°å½•è­¦å‘Šä¿¡æ¯
        return batch_size  # è¿”å›é»˜è®¤æ‰¹é‡å¤§å°
    if torch.backends.cudnn.benchmark:  # å¦‚æœå¯ç”¨äº† cudnn.benchmark
        LOGGER.info(f"{prefix} âš ï¸ Requires torch.backends.cudnn.benchmark=False, using default batch-size {batch_size}")  # è®°å½•è­¦å‘Šä¿¡æ¯
        return batch_size  # è¿”å›é»˜è®¤æ‰¹é‡å¤§å°

    # Inspect CUDA memory  # æ£€æŸ¥ CUDA å†…å­˜
    gb = 1 << 30  # bytes to GiB (1024 ** 3)  # å­—èŠ‚è½¬æ¢ä¸º GiBï¼ˆ1024 ** 3ï¼‰
    d = f"CUDA:{os.getenv('CUDA_VISIBLE_DEVICES', '0').strip()[0]}"  # 'CUDA:0'  # è·å– CUDA è®¾å¤‡
    properties = torch.cuda.get_device_properties(device)  # device properties  # è·å–è®¾å¤‡å±æ€§
    t = properties.total_memory / gb  # GiB total  # GiB æ€»å†…å­˜
    r = torch.cuda.memory_reserved(device) / gb  # GiB reserved  # GiB ä¿ç•™å†…å­˜
    a = torch.cuda.memory_allocated(device) / gb  # GiB allocated  # GiB åˆ†é…å†…å­˜
    f = t - (r + a)  # GiB free  # GiB å¯ç”¨å†…å­˜
    LOGGER.info(f"{prefix}{d} ({properties.name}) {t:.2f}G total, {r:.2f}G reserved, {a:.2f}G allocated, {f:.2f}G free")  # è®°å½•å†…å­˜ä¿¡æ¯

    # Profile batch sizes  # è®°å½•æ‰¹é‡å¤§å°
    batch_sizes = [1, 2, 4, 8, 16] if t < 16 else [1, 2, 4, 8, 16, 32, 64]  # æ ¹æ®æ€»å†…å­˜é€‰æ‹©æ‰¹é‡å¤§å°
    try:
        img = [torch.empty(b, 3, imgsz, imgsz) for b in batch_sizes]  # åˆ›å»ºè¾“å…¥å›¾åƒ
        results = profile(img, model, n=1, device=device, max_num_obj=max_num_obj)  # è®°å½•æ€§èƒ½

        # Fit a solution  # æ‹Ÿåˆè§£å†³æ–¹æ¡ˆ
        xy = [  # åˆ›å»ºæœ‰æ•ˆç»“æœåˆ—è¡¨
            [x, y[2]]
            for i, (x, y) in enumerate(zip(batch_sizes, results))
            if y  # valid result  # æœ‰æ•ˆç»“æœ
            and isinstance(y[2], (int, float))  # is numeric  # æ˜¯æ•°å­—
            and 0 < y[2] < t  # between 0 and GPU limit  # åœ¨ 0 å’Œ GPU é™åˆ¶ä¹‹é—´
            and (i == 0 or not results[i - 1] or y[2] > results[i - 1][2])  # first item or increasing memory  # ç¬¬ä¸€ä¸ªé¡¹æˆ–å¢åŠ å†…å­˜
        ]
        fit_x, fit_y = zip(*xy) if xy else ([], [])  # æ‹†åˆ†æœ‰æ•ˆç»“æœ
        p = np.polyfit(np.log(fit_x), np.log(fit_y), deg=1)  # first-degree polynomial fit in log space  # åœ¨å¯¹æ•°ç©ºé—´ä¸­è¿›è¡Œä¸€æ¬¡å¤šé¡¹å¼æ‹Ÿåˆ
        b = int(round(np.exp((np.log(f * fraction) - p[1]) / p[0])))  # y intercept (optimal batch size)  # y æˆªè·ï¼ˆæœ€ä½³æ‰¹é‡å¤§å°ï¼‰
        if None in results:  # some sizes failed  # ä¸€äº›å¤§å°å¤±è´¥
            i = results.index(None)  # first fail index  # ç¬¬ä¸€ä¸ªå¤±è´¥ç´¢å¼•
            if b >= batch_sizes[i]:  # y intercept above failure point  # y æˆªè·åœ¨å¤±è´¥ç‚¹ä¹‹ä¸Š
                b = batch_sizes[max(i - 1, 0)]  # select prior safe point  # é€‰æ‹©ä¹‹å‰çš„å®‰å…¨ç‚¹
        if b < 1 or b > 1024:  # b outside of safe range  # b è¶…å‡ºå®‰å…¨èŒƒå›´
            LOGGER.info(f"{prefix}WARNING âš ï¸ batch={b} outside safe range, using default batch-size {batch_size}.")  # è®°å½•è­¦å‘Šä¿¡æ¯
            b = batch_size  # ä½¿ç”¨é»˜è®¤æ‰¹é‡å¤§å°

        fraction = (np.exp(np.polyval(p, np.log(b))) + r + a) / t  # predicted fraction  # é¢„æµ‹çš„æ¯”ä¾‹
        LOGGER.info(f"{prefix}Using batch-size {b} for {d} {t * fraction:.2f}G/{t:.2f}G ({fraction * 100:.0f}%) âœ…")  # è®°å½•ä½¿ç”¨çš„æ‰¹é‡å¤§å°
        return b  # è¿”å›æœ€ä½³æ‰¹é‡å¤§å°
    except Exception as e:  # æ•è·å¼‚å¸¸
        LOGGER.warning(f"{prefix}WARNING âš ï¸ error detected: {e},  using default batch-size {batch_size}.")  # è®°å½•è­¦å‘Šä¿¡æ¯
        return batch_size  # è¿”å›é»˜è®¤æ‰¹é‡å¤§å°
    finally:
        torch.cuda.empty_cache()  # æ¸…ç©º CUDA ç¼“å­˜