# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""
Benchmark a YOLO model formats for speed and accuracy.

Usage:
    from ultralytics.utils.benchmarks import ProfileModels, benchmark
    ProfileModels(['yolo11n.yaml', 'yolov8s.yaml']).profile()
    benchmark(model='yolo11n.pt', imgsz=160)

Format                  | `format=argument`         | Model
---                     | ---                       | ---
PyTorch                 | -                         | yolo11n.pt
TorchScript             | `torchscript`             | yolo11n.torchscript
ONNX                    | `onnx`                    | yolo11n.onnx
OpenVINO                | `openvino`                | yolo11n_openvino_model/
TensorRT                | `engine`                  | yolo11n.engine
CoreML                  | `coreml`                  | yolo11n.mlpackage
TensorFlow SavedModel   | `saved_model`             | yolo11n_saved_model/
TensorFlow GraphDef     | `pb`                      | yolo11n.pb
TensorFlow Lite         | `tflite`                  | yolo11n.tflite
TensorFlow Edge TPU     | `edgetpu`                 | yolo11n_edgetpu.tflite
TensorFlow.js           | `tfjs`                    | yolo11n_web_model/
PaddlePaddle            | `paddle`                  | yolo11n_paddle_model/
MNN                     | `mnn`                     | yolo11n.mnn
NCNN                    | `ncnn`                    | yolo11n_ncnn_model/
RKNN                    | `rknn`                    | yolo11n_rknn_model/
"""

import glob  # å¯¼å…¥ glob æ¨¡å—
import os  # å¯¼å…¥ os æ¨¡å—
import platform  # å¯¼å…¥ platform æ¨¡å—
import re  # å¯¼å…¥ re æ¨¡å—
import shutil  # å¯¼å…¥ shutil æ¨¡å—
import time  # å¯¼å…¥ time æ¨¡å—
from pathlib import Path  # ä» pathlib å¯¼å…¥ Path

import numpy as np  # å¯¼å…¥ numpy ä½œä¸º np
import torch.cuda  # å¯¼å…¥ torch.cuda æ¨¡å—
import yaml  # å¯¼å…¥ yaml æ¨¡å—

from ultralytics import YOLO, YOLOWorld  # ä» ultralytics å¯¼å…¥ YOLO å’Œ YOLOWorld
from ultralytics.cfg import TASK2DATA, TASK2METRIC  # ä» ultralytics.cfg å¯¼å…¥ TASK2DATA å’Œ TASK2METRIC
from ultralytics.engine.exporter import export_formats  # ä» ultralytics.engine.exporter å¯¼å…¥ export_formats
from ultralytics.utils import ARM64, ASSETS, LINUX, LOGGER, MACOS, TQDM, WEIGHTS_DIR  # ä» ultralytics.utils å¯¼å…¥ç›¸å…³å¸¸é‡
from ultralytics.utils.checks import IS_PYTHON_3_12, check_imgsz, check_requirements, check_yolo, is_rockchip  # ä» ultralytics.utils.checks å¯¼å…¥æ£€æŸ¥å‡½æ•°
from ultralytics.utils.downloads import safe_download  # ä» ultralytics.utils.downloads å¯¼å…¥ safe_download
from ultralytics.utils.files import file_size  # ä» ultralytics.utils.files å¯¼å…¥ file_size
from ultralytics.utils.torch_utils import get_cpu_info, select_device  # ä» ultralytics.utils.torch_utils å¯¼å…¥ get_cpu_info å’Œ select_device


def benchmark(
    model=WEIGHTS_DIR / "yolo11n.pt",  # æ¨¡å‹çš„è·¯å¾„ï¼Œé»˜è®¤ä¸º yolo11n.pt
    data=None,  # æ•°æ®é›†ï¼Œé»˜è®¤ä¸º None
    imgsz=160,  # å›¾åƒå¤§å°ï¼Œé»˜è®¤ä¸º 160
    half=False,  # æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦ï¼Œé»˜è®¤ä¸º False
    int8=False,  # æ˜¯å¦ä½¿ç”¨ int8 ç²¾åº¦ï¼Œé»˜è®¤ä¸º False
    device="cpu",  # è¿è¡ŒåŸºå‡†æµ‹è¯•çš„è®¾å¤‡ï¼Œé»˜è®¤ä¸º 'cpu'
    verbose=False,  # æ˜¯å¦è¯¦ç»†è¾“å‡ºï¼Œé»˜è®¤ä¸º False
    eps=1e-3,  # é˜²æ­¢é™¤ä»¥é›¶çš„ epsilon å€¼
    format="",  # å¯¼å‡ºæ ¼å¼ï¼Œé»˜è®¤ä¸ºç©º
):
    """
    Benchmark a YOLO model across different formats for speed and accuracy.  # åœ¨ä¸åŒæ ¼å¼ä¸‹å¯¹ YOLO æ¨¡å‹è¿›è¡Œé€Ÿåº¦å’Œå‡†ç¡®æ€§åŸºå‡†æµ‹è¯•ã€‚

    Args:  # å‚æ•°ï¼š
        model (str | Path): Path to the model file or directory.  # model (str | Path): æ¨¡å‹æ–‡ä»¶æˆ–ç›®å½•çš„è·¯å¾„ã€‚
        data (str | None): Dataset to evaluate on, inherited from TASK2DATA if not passed.  # data (str | None): è¦è¯„ä¼°çš„æ•°æ®é›†ï¼Œå¦‚æœæœªä¼ é€’ï¼Œåˆ™ä» TASK2DATA ä¸­ç»§æ‰¿ã€‚
        imgsz (int): Image size for the benchmark.  # imgsz (int): åŸºå‡†æµ‹è¯•çš„å›¾åƒå¤§å°ã€‚
        half (bool): Use half-precision for the model if True.  # half (bool): å¦‚æœä¸º Trueï¼Œåˆ™å¯¹æ¨¡å‹ä½¿ç”¨åŠç²¾åº¦ã€‚
        int8 (bool): Use int8-precision for the model if True.  # int8 (bool): å¦‚æœä¸º Trueï¼Œåˆ™å¯¹æ¨¡å‹ä½¿ç”¨ int8 ç²¾åº¦ã€‚
        device (str): Device to run the benchmark on, either 'cpu' or 'cuda'.  # device (str): è¿è¡ŒåŸºå‡†æµ‹è¯•çš„è®¾å¤‡ï¼Œå¯ä»¥æ˜¯ 'cpu' æˆ– 'cuda'ã€‚
        verbose (bool | float): If True or a float, assert benchmarks pass with given metric.  # verbose (bool | float): å¦‚æœä¸º True æˆ–æµ®ç‚¹æ•°ï¼Œåˆ™æ–­è¨€åŸºå‡†æµ‹è¯•é€šè¿‡ç»™å®šçš„æŒ‡æ ‡ã€‚
        eps (float): Epsilon value for divide by zero prevention.  # eps (float): é˜²æ­¢é™¤ä»¥é›¶çš„ epsilon å€¼ã€‚
        format (str): Export format for benchmarking. If not supplied all formats are benchmarked.  # format (str): åŸºå‡†æµ‹è¯•çš„å¯¼å‡ºæ ¼å¼ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å¯¹æ‰€æœ‰æ ¼å¼è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚

    Returns:  # è¿”å›ï¼š
        (pandas.DataFrame): A pandas DataFrame with benchmark results for each format, including file size, metric,  # (pandas.DataFrame): åŒ…å«æ¯ç§æ ¼å¼åŸºå‡†æµ‹è¯•ç»“æœçš„ pandas DataFrameï¼ŒåŒ…æ‹¬æ–‡ä»¶å¤§å°ã€æŒ‡æ ‡ï¼Œ
            and inference time.  # å’Œæ¨ç†æ—¶é—´ã€‚

    Examples:  # ç¤ºä¾‹ï¼š
        Benchmark a YOLO model with default settings:  # ä½¿ç”¨é»˜è®¤è®¾ç½®å¯¹ YOLO æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼š
        >>> from ultralytics.utils.benchmarks import benchmark  # ä» ultralytics.utils.benchmarks å¯¼å…¥ benchmark
        >>> benchmark(model="yolo11n.pt", imgsz=640)  # å¯¹ yolo11n.pt æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå›¾åƒå¤§å°ä¸º 640
    """
    imgsz = check_imgsz(imgsz)  # æ£€æŸ¥å›¾åƒå¤§å°
    assert imgsz[0] == imgsz[1] if isinstance(imgsz, list) else True, "benchmark() only supports square imgsz."  # ç¡®ä¿å›¾åƒå¤§å°æ˜¯æ­£æ–¹å½¢

    import pandas as pd  # scope for faster 'import ultralytics'  # ä¸ºäº†æ›´å¿«çš„ 'import ultralytics' è€Œå¯¼å…¥ pandas

    pd.options.display.max_columns = 10  # è®¾ç½® pandas æ˜¾ç¤ºçš„æœ€å¤§åˆ—æ•°
    pd.options.display.width = 120  # è®¾ç½® pandas æ˜¾ç¤ºçš„å®½åº¦
    device = select_device(device, verbose=False)  # é€‰æ‹©è®¾å¤‡
    if isinstance(model, (str, Path)):  # å¦‚æœæ¨¡å‹æ˜¯å­—ç¬¦ä¸²æˆ–è·¯å¾„
        model = YOLO(model)  # åˆ›å»º YOLO æ¨¡å‹å®ä¾‹
    is_end2end = getattr(model.model.model[-1], "end2end", False)  # æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä¸ºç«¯åˆ°ç«¯æ¨¡å‹
    data = data or TASK2DATA[model.task]  # task to dataset, i.e. coco8.yaml for task=detect  # ä»»åŠ¡å¯¹åº”çš„æ•°æ®é›†ï¼Œä¾‹å¦‚ä»»åŠ¡ä¸ºæ£€æµ‹æ—¶å¯¹åº” coco8.yaml
    key = TASK2METRIC[model.task]  # task to metric, i.e. metrics/mAP50-95(B) for task=detect  # ä»»åŠ¡å¯¹åº”çš„æŒ‡æ ‡ï¼Œä¾‹å¦‚ä»»åŠ¡ä¸ºæ£€æµ‹æ—¶å¯¹åº” metrics/mAP50-95(B)

    y = []  # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
    t0 = time.time()  # è®°å½•å¼€å§‹æ—¶é—´

    format_arg = format.lower()  # å°†æ ¼å¼è½¬æ¢ä¸ºå°å†™
    if format_arg:  # å¦‚æœæä¾›äº†æ ¼å¼
        formats = frozenset(export_formats()["Argument"])  # è·å–å¯ç”¨çš„å¯¼å‡ºæ ¼å¼
        assert format in formats, f"Expected format to be one of {formats}, but got '{format_arg}'."  # ç¡®ä¿æ ¼å¼æœ‰æ•ˆ
    for i, (name, format, suffix, cpu, gpu, _) in enumerate(zip(*export_formats().values())):  # éå†å¯ç”¨çš„å¯¼å‡ºæ ¼å¼
        emoji, filename = "âŒ", None  # å¯¼å‡ºé»˜è®¤å€¼
        try:
            if format_arg and format_arg != format:  # å¦‚æœæŒ‡å®šäº†æ ¼å¼ä¸”ä¸åŒ¹é…
                continue  # è·³è¿‡

            # Checks  # æ£€æŸ¥
            if i == 7:  # TF GraphDef
                assert model.task != "obb", "TensorFlow GraphDef not supported for OBB task"  # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦æ”¯æŒ
            elif i == 9:  # Edge TPU
                assert LINUX and not ARM64, "Edge TPU export only supported on non-aarch64 Linux"  # æ£€æŸ¥æ˜¯å¦æ”¯æŒ Edge TPU å¯¼å‡º
            elif i in {5, 10}:  # CoreML and TF.js
                assert MACOS or (LINUX and not ARM64), (  # æ£€æŸ¥æ˜¯å¦æ”¯æŒ CoreML å’Œ TF.js å¯¼å‡º
                    "CoreML and TF.js export only supported on macOS and non-aarch64 Linux"
                )
            if i in {5}:  # CoreML
                assert not IS_PYTHON_3_12, "CoreML not supported on Python 3.12"  # æ£€æŸ¥ Python ç‰ˆæœ¬
            if i in {6, 7, 8}:  # TF SavedModel, TF GraphDef, and TFLite
                assert not isinstance(model, YOLOWorld), "YOLOWorldv2 TensorFlow exports not supported by onnx2tf yet"  # æ£€æŸ¥æ¨¡å‹ç±»å‹
            if i in {9, 10}:  # TF EdgeTPU and TF.js
                assert not isinstance(model, YOLOWorld), "YOLOWorldv2 TensorFlow exports not supported by onnx2tf yet"  # æ£€æŸ¥æ¨¡å‹ç±»å‹
            if i == 11:  # Paddle
                assert not isinstance(model, YOLOWorld), "YOLOWorldv2 Paddle exports not supported yet"  # æ£€æŸ¥æ¨¡å‹ç±»å‹
                assert not is_end2end, "End-to-end models not supported by PaddlePaddle yet"  # æ£€æŸ¥æ¨¡å‹ç±»å‹
                assert LINUX or MACOS, "Windows Paddle exports not supported yet"  # æ£€æŸ¥æ“ä½œç³»ç»Ÿ
            if i == 12:  # MNN
                assert not isinstance(model, YOLOWorld), "YOLOWorldv2 MNN exports not supported yet"  # æ£€æŸ¥æ¨¡å‹ç±»å‹
            if i == 13:  # NCNN
                assert not isinstance(model, YOLOWorld), "YOLOWorldv2 NCNN exports not supported yet"  # æ£€æŸ¥æ¨¡å‹ç±»å‹
            if i == 14:  # IMX
                assert not is_end2end  # æ£€æŸ¥æ¨¡å‹ç±»å‹
                assert not isinstance(model, YOLOWorld), "YOLOWorldv2 IMX exports not supported"  # æ£€æŸ¥æ¨¡å‹ç±»å‹
                assert model.task == "detect", "IMX only supported for detection task"  # æ£€æŸ¥ä»»åŠ¡ç±»å‹
                assert "C2f" in model.__str__(), "IMX only supported for YOLOv8"  # æ£€æŸ¥æ¨¡å‹ç±»å‹
            if i == 15:  # RKNN
                assert not isinstance(model, YOLOWorld), "YOLOWorldv2 RKNN exports not supported yet"  # æ£€æŸ¥æ¨¡å‹ç±»å‹
                assert not is_end2end, "End-to-end models not supported by RKNN yet"  # æ£€æŸ¥æ¨¡å‹ç±»å‹
                assert LINUX, "RKNN only supported on Linux"  # æ£€æŸ¥æ“ä½œç³»ç»Ÿ
                assert not is_rockchip(), "RKNN Inference only supported on Rockchip devices"  # æ£€æŸ¥è®¾å¤‡ç±»å‹
            if "cpu" in device.type:  # å¦‚æœè®¾å¤‡ç±»å‹ä¸º CPU
                assert cpu, "inference not supported on CPU"  # æ£€æŸ¥æ¨ç†æ˜¯å¦æ”¯æŒ
            if "cuda" in device.type:  # å¦‚æœè®¾å¤‡ç±»å‹ä¸º CUDA
                assert gpu, "inference not supported on GPU"  # æ£€æŸ¥æ¨ç†æ˜¯å¦æ”¯æŒ

            # Export  # å¯¼å‡º
            if format == "-":  # å¦‚æœæ ¼å¼ä¸º "-"
                filename = model.pt_path or model.ckpt_path or model.model_name  # è·å–æ¨¡å‹æ–‡ä»¶å
                exported_model = model  # PyTorch format  # PyTorch æ ¼å¼
            else:
                filename = model.export(  # å¯¼å‡ºæ¨¡å‹
                    imgsz=imgsz, format=format, half=half, int8=int8, data=data, device=device, verbose=False
                )
                exported_model = YOLO(filename, task=model.task)  # åˆ›å»º YOLO æ¨¡å‹å®ä¾‹
                assert suffix in str(filename), "export failed"  # æ£€æŸ¥å¯¼å‡ºæ˜¯å¦æˆåŠŸ
            emoji = "â"  # indicates export succeeded  # æŒ‡ç¤ºå¯¼å‡ºæˆåŠŸ

            # Predict  # é¢„æµ‹
            assert model.task != "pose" or i != 7, "GraphDef Pose inference is not supported"  # æ£€æŸ¥ä»»åŠ¡ç±»å‹
            assert i not in {9, 10}, "inference not supported"  # Edge TPU and TF.js are unsupported  # Edge TPU å’Œ TF.js ä¸æ”¯æŒ
            assert i != 5 or platform.system() == "Darwin", "inference only supported on macOS>=10.13"  # CoreML ä»…æ”¯æŒ macOS
            if i in {13}:  # NCNN
                assert not is_end2end, "End-to-end torch.topk operation is not supported for NCNN prediction yet"  # æ£€æŸ¥æ¨¡å‹ç±»å‹
            exported_model.predict(ASSETS / "bus.jpg", imgsz=imgsz, device=device, half=half, verbose=False)  # è¿›è¡Œé¢„æµ‹

            # Validate  # éªŒè¯
            results = exported_model.val(  # éªŒè¯æ¨¡å‹
                data=data, batch=1, imgsz=imgsz, plots=False, device=device, half=half, int8=int8, verbose=False
            )
            metric, speed = results.results_dict[key], results.speed["inference"]  # è·å–æŒ‡æ ‡å’Œé€Ÿåº¦
            fps = round(1000 / (speed + eps), 2)  # frames per second  # å¸§ç‡
            y.append([name, "âœ…", round(file_size(filename), 1), round(metric, 4), round(speed, 2), fps])  # æ·»åŠ ç»“æœ
        except Exception as e:  # æ•è·å¼‚å¸¸
            if verbose:  # å¦‚æœè¯¦ç»†è¾“å‡º
                assert type(e) is AssertionError, f"Benchmark failure for {name}: {e}"  # æ£€æŸ¥å¼‚å¸¸ç±»å‹
            LOGGER.warning(f"ERROR âŒï¸ Benchmark failure for {name}: {e}")  # è®°å½•é”™è¯¯ä¿¡æ¯
            y.append([name, emoji, round(file_size(filename), 1), None, None, None])  # mAP, t_inference

    # Print results  # æ‰“å°ç»“æœ
    check_yolo(device=device)  # print system info  # æ‰“å°ç³»ç»Ÿä¿¡æ¯
    df = pd.DataFrame(y, columns=["Format", "Statusâ”", "Size (MB)", key, "Inference time (ms/im)", "FPS"])  # åˆ›å»ºç»“æœ DataFrame

    name = model.model_name  # è·å–æ¨¡å‹åç§°
    dt = time.time() - t0  # è®¡ç®—è€—æ—¶
    legend = "Benchmarks legend:  - âœ… Success  - â Export passed but validation failed  - âŒï¸ Export failed"  # åŸºå‡†æµ‹è¯•å›¾ä¾‹
    s = f"\nBenchmarks complete for {name} on {data} at imgsz={imgsz} ({dt:.2f}s)\n{legend}\n{df.fillna('-')}\n"  # æ ¼å¼åŒ–è¾“å‡ºå­—ç¬¦ä¸²
    LOGGER.info(s)  # è®°å½•åŸºå‡†æµ‹è¯•ç»“æœ
    with open("benchmarks.log", "a", errors="ignore", encoding="utf-8") as f:  # æ‰“å¼€æ—¥å¿—æ–‡ä»¶
        f.write(s)  # å†™å…¥ç»“æœ

    if verbose and isinstance(verbose, float):  # å¦‚æœè¯¦ç»†è¾“å‡ºä¸”ä¸ºæµ®ç‚¹æ•°
        metrics = df[key].array  # values to compare to floor  # è·å–æ¯”è¾ƒçš„å€¼
        floor = verbose  # minimum metric floor to pass, i.e. = 0.29 mAP for YOLOv5n  # æœ€å°æŒ‡æ ‡é˜ˆå€¼
        assert all(x > floor for x in metrics if pd.notna(x)), f"Benchmark failure: metric(s) < floor {floor}"  # æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦æ»¡è¶³é˜ˆå€¼

    return df  # è¿”å›ç»“æœ DataFrame


class RF100Benchmark:
    """Benchmark YOLO model performance across various formats for speed and accuracy.  # å¯¹ YOLO æ¨¡å‹åœ¨ä¸åŒæ ¼å¼ä¸‹çš„é€Ÿåº¦å’Œå‡†ç¡®æ€§è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚"""

    def __init__(self):
        """Initialize the RF100Benchmark class for benchmarking YOLO model performance across various formats.  # åˆå§‹åŒ– RF100Benchmark ç±»ä»¥å¯¹ YOLO æ¨¡å‹åœ¨ä¸åŒæ ¼å¼ä¸‹çš„æ€§èƒ½è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚"""
        self.ds_names = []  # æ•°æ®é›†åç§°åˆ—è¡¨
        self.ds_cfg_list = []  # æ•°æ®é›†é…ç½®åˆ—è¡¨
        self.rf = None  # Roboflow å®ä¾‹
        self.val_metrics = ["class", "images", "targets", "precision", "recall", "map50", "map95"]  # éªŒè¯æŒ‡æ ‡åˆ—è¡¨

    def set_key(self, api_key):
        """
        Set Roboflow API key for processing.  # è®¾ç½® Roboflow API å¯†é’¥ä»¥è¿›è¡Œå¤„ç†ã€‚

        Args:  # å‚æ•°ï¼š
            api_key (str): The API key.  # api_key (str): API å¯†é’¥ã€‚

        Examples:  # ç¤ºä¾‹ï¼š
            Set the Roboflow API key for accessing datasets:  # è®¾ç½® Roboflow API å¯†é’¥ä»¥è®¿é—®æ•°æ®é›†ï¼š
            >>> benchmark = RF100Benchmark()  # åˆ›å»º RF100Benchmark å®ä¾‹
            >>> benchmark.set_key("your_roboflow_api_key")  # è®¾ç½® API å¯†é’¥
        """
        check_requirements("roboflow")  # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† roboflow
        from roboflow import Roboflow  # ä» roboflow å¯¼å…¥ Roboflow

        self.rf = Roboflow(api_key=api_key)  # åˆ›å»º Roboflow å®ä¾‹

    def parse_dataset(self, ds_link_txt="datasets_links.txt"):
        """
        Parse dataset links and download datasets.  # è§£ææ•°æ®é›†é“¾æ¥å¹¶ä¸‹è½½æ•°æ®é›†ã€‚

        Args:  # å‚æ•°ï¼š
            ds_link_txt (str): Path to the file containing dataset links.  # ds_link_txt (str): åŒ…å«æ•°æ®é›†é“¾æ¥çš„æ–‡ä»¶è·¯å¾„ã€‚

        Examples:  # ç¤ºä¾‹ï¼š
            >>> benchmark = RF100Benchmark()  # åˆ›å»º RF100Benchmark å®ä¾‹
            >>> benchmark.set_key("api_key")  # è®¾ç½® API å¯†é’¥
            >>> benchmark.parse_dataset("datasets_links.txt")  # è§£ææ•°æ®é›†
        """
        (shutil.rmtree("rf-100"), os.mkdir("rf-100")) if os.path.exists("rf-100") else os.mkdir("rf-100")  # å¦‚æœ rf-100 ç›®å½•å­˜åœ¨åˆ™åˆ é™¤å¹¶é‡æ–°åˆ›å»º
        os.chdir("rf-100")  # åˆ‡æ¢åˆ° rf-100 ç›®å½•
        os.mkdir("ultralytics-benchmarks")  # åˆ›å»º ultralytics-benchmarks ç›®å½•
        safe_download("https://github.com/ultralytics/assets/releases/download/v0.0.0/datasets_links.txt")  # ä¸‹è½½æ•°æ®é›†é“¾æ¥æ–‡ä»¶

        with open(ds_link_txt) as file:  # æ‰“å¼€æ•°æ®é›†é“¾æ¥æ–‡ä»¶
            for line in file:  # éå†æ¯ä¸€è¡Œ
                try:
                    _, url, workspace, project, version = re.split("/+", line.strip())  # è§£æé“¾æ¥
                    self.ds_names.append(project)  # æ·»åŠ é¡¹ç›®åç§°åˆ°åˆ—è¡¨
                    proj_version = f"{project}-{version}"  # åˆ›å»ºé¡¹ç›®ç‰ˆæœ¬å­—ç¬¦ä¸²
                    if not Path(proj_version).exists():  # å¦‚æœé¡¹ç›®ç‰ˆæœ¬ç›®å½•ä¸å­˜åœ¨
                        self.rf.workspace(workspace).project(project).version(version).download("yolov8")  # ä¸‹è½½æ•°æ®é›†
                    else:
                        print("Dataset already downloaded.")  # æ•°æ®é›†å·²ä¸‹è½½
                    self.ds_cfg_list.append(Path.cwd() / proj_version / "data.yaml")  # æ·»åŠ æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„åˆ°åˆ—è¡¨
                except Exception:  # æ•è·å¼‚å¸¸
                    continue  # ç»§ç»­ä¸‹ä¸€ä¸ªå¾ªç¯

        return self.ds_names, self.ds_cfg_list  # è¿”å›æ•°æ®é›†åç§°å’Œé…ç½®åˆ—è¡¨

    @staticmethod
    def fix_yaml(path):
        """
        Fixes the train and validation paths in a given YAML file.  # ä¿®å¤ç»™å®š YAML æ–‡ä»¶ä¸­çš„è®­ç»ƒå’ŒéªŒè¯è·¯å¾„ã€‚

        Args:  # å‚æ•°ï¼š
            path (str): Path to the YAML file to be fixed.  # path (str): è¦ä¿®å¤çš„ YAML æ–‡ä»¶è·¯å¾„ã€‚

        Examples:  # ç¤ºä¾‹ï¼š
            >>> RF100Benchmark.fix_yaml("path/to/data.yaml")  # ä¿®å¤æ•°æ® YAML æ–‡ä»¶
        """
        with open(path) as file:  # æ‰“å¼€ YAML æ–‡ä»¶
            yaml_data = yaml.safe_load(file)  # åŠ è½½ YAML æ•°æ®
        yaml_data["train"] = "train/images"  # è®¾ç½®è®­ç»ƒè·¯å¾„
        yaml_data["val"] = "valid/images"  # è®¾ç½®éªŒè¯è·¯å¾„
        with open(path, "w") as file:  # ä»¥å†™å…¥æ¨¡å¼æ‰“å¼€ YAML æ–‡ä»¶
            yaml.safe_dump(yaml_data, file)  # ä¿å­˜ä¿®å¤åçš„ YAML æ•°æ®

    def evaluate(self, yaml_path, val_log_file, eval_log_file, list_ind):
        """
        Evaluate model performance on validation results.  # åœ¨éªŒè¯ç»“æœä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

        Args:  # å‚æ•°ï¼š
            yaml_path (str): Path to the YAML configuration file.  # yaml_path (str): YAML é…ç½®æ–‡ä»¶çš„è·¯å¾„ã€‚
            val_log_file (str): Path to the validation log file.  # val_log_file (str): éªŒè¯æ—¥å¿—æ–‡ä»¶çš„è·¯å¾„ã€‚
            eval_log_file (str): Path to the evaluation log file.  # eval_log_file (str): è¯„ä¼°æ—¥å¿—æ–‡ä»¶çš„è·¯å¾„ã€‚
            list_ind (int): Index of the current dataset in the list.  # list_ind (int): å½“å‰æ•°æ®é›†åœ¨åˆ—è¡¨ä¸­çš„ç´¢å¼•ã€‚

        Returns:  # è¿”å›ï¼š
            (float): The mean average precision (mAP) value for the evaluated model.  # (float): è¯„ä¼°æ¨¡å‹çš„å¹³å‡ç²¾åº¦ (mAP) å€¼ã€‚

        Examples:  # ç¤ºä¾‹ï¼š
            Evaluate a model on a specific dataset  # åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹
            >>> benchmark = RF100Benchmark()  # åˆ›å»º RF100Benchmark å®ä¾‹
            >>> benchmark.evaluate("path/to/data.yaml", "path/to/val_log.txt", "path/to/eval_log.txt", 0)  # è¯„ä¼°æ¨¡å‹
        """
        skip_symbols = ["ğŸš€", "âš ï¸", "ğŸ’¡", "âŒ"]  # è·³è¿‡çš„ç¬¦å·åˆ—è¡¨
        with open(yaml_path) as stream:  # æ‰“å¼€ YAML é…ç½®æ–‡ä»¶
            class_names = yaml.safe_load(stream)["names"]  # åŠ è½½ç±»åˆ«åç§°
        with open(val_log_file, encoding="utf-8") as f:  # æ‰“å¼€éªŒè¯æ—¥å¿—æ–‡ä»¶
            lines = f.readlines()  # è¯»å–æ‰€æœ‰è¡Œ
            eval_lines = []  # åˆå§‹åŒ–è¯„ä¼°è¡Œåˆ—è¡¨
            for line in lines:  # éå†æ¯ä¸€è¡Œ
                if any(symbol in line for symbol in skip_symbols):  # å¦‚æœè¡Œä¸­åŒ…å«è·³è¿‡çš„ç¬¦å·
                    continue  # è·³è¿‡è¯¥è¡Œ
                entries = line.split(" ")  # æŒ‰ç©ºæ ¼åˆ†å‰²è¡Œ
                entries = list(filter(lambda val: val != "", entries))  # è¿‡æ»¤ç©ºå€¼
                entries = [e.strip("\n") for e in entries]  # å»é™¤æ¢è¡Œç¬¦
                eval_lines.extend(  # æ·»åŠ è¯„ä¼°è¡Œ
                    {
                        "class": entries[0],  # ç±»åˆ«
                        "images": entries[1],  # å›¾åƒæ•°é‡
                        "targets": entries[2],  # ç›®æ ‡æ•°é‡
                        "precision": entries[3],  # ç²¾åº¦
                        "recall": entries[4],  # å¬å›ç‡
                        "map50": entries[5],  # mAP50
                        "map95": entries[6],  # mAP95
                    }
                    for e in entries  # éå†æ¡ç›®
                    if e in class_names or (e == "all" and "(AP)" not in entries and "(AR)" not in entries)  # æ£€æŸ¥ç±»åˆ«åç§°
                )
        map_val = 0.0  # åˆå§‹åŒ– mAP å€¼
        if len(eval_lines) > 1:  # å¦‚æœæœ‰å¤šæ¡è¯„ä¼°è¡Œ
            print("There's more dicts")  # æœ‰å¤šä¸ªå­—å…¸
            for lst in eval_lines:  # éå†è¯„ä¼°è¡Œ
                if lst["class"] == "all":  # å¦‚æœç±»åˆ«ä¸º "all"
                    map_val = lst["map50"]  # è·å– mAP50 å€¼
        else:  # åªæœ‰ä¸€æ¡è¯„ä¼°è¡Œ
            print("There's only one dict res")  # åªæœ‰ä¸€ä¸ªå­—å…¸
            map_val = [res["map50"] for res in eval_lines][0]  # è·å– mAP50 å€¼

        with open(eval_log_file, "a") as f:  # ä»¥è¿½åŠ æ¨¡å¼æ‰“å¼€è¯„ä¼°æ—¥å¿—æ–‡ä»¶
            f.write(f"{self.ds_names[list_ind]}: {map_val}\n")  # å†™å…¥æ•°æ®é›†åç§°å’Œ mAP å€¼

class ProfileModels:
    """
    ProfileModels class for profiling different models on ONNX and TensorRT.  # ProfileModels ç±»ç”¨äºå¯¹ä¸åŒæ¨¡å‹åœ¨ ONNX å’Œ TensorRT ä¸Šè¿›è¡Œæ€§èƒ½åˆ†æã€‚

    This class profiles the performance of different models, returning results such as model speed and FLOPs.  # æ­¤ç±»åˆ†æä¸åŒæ¨¡å‹çš„æ€§èƒ½ï¼Œè¿”å›æ¨¡å‹é€Ÿåº¦å’Œ FLOPs ç­‰ç»“æœã€‚

    Attributes:  # å±æ€§ï¼š
        paths (List[str]): Paths of the models to profile.  # paths (List[str]): è¦åˆ†æçš„æ¨¡å‹è·¯å¾„åˆ—è¡¨ã€‚
        num_timed_runs (int): Number of timed runs for the profiling.  # num_timed_runs (int): æ€§èƒ½åˆ†æçš„è®¡æ—¶è¿è¡Œæ¬¡æ•°ã€‚
        num_warmup_runs (int): Number of warmup runs before profiling.  # num_warmup_runs (int): åœ¨æ€§èƒ½åˆ†æå‰çš„é¢„çƒ­è¿è¡Œæ¬¡æ•°ã€‚
        min_time (float): Minimum number of seconds to profile for.  # min_time (float): æ€§èƒ½åˆ†æçš„æœ€å°æ—¶é—´ï¼ˆç§’ï¼‰ã€‚
        imgsz (int): Image size used in the models.  # imgsz (int): æ¨¡å‹ä½¿ç”¨çš„å›¾åƒå¤§å°ã€‚
        half (bool): Flag to indicate whether to use FP16 half-precision for TensorRT profiling.  # half (bool): æ ‡å¿—ï¼ŒæŒ‡ç¤ºæ˜¯å¦åœ¨ TensorRT æ€§èƒ½åˆ†æä¸­ä½¿ç”¨ FP16 åŠç²¾åº¦ã€‚
        trt (bool): Flag to indicate whether to profile using TensorRT.  # trt (bool): æ ‡å¿—ï¼ŒæŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨ TensorRT è¿›è¡Œæ€§èƒ½åˆ†æã€‚
        device (torch.device): Device used for profiling.  # device (torch.device): ç”¨äºæ€§èƒ½åˆ†æçš„è®¾å¤‡ã€‚

    Methods:  # æ–¹æ³•ï¼š
        profile: Profiles the models and prints the result.  # profile: åˆ†ææ¨¡å‹å¹¶æ‰“å°ç»“æœã€‚

    Examples:  # ç¤ºä¾‹ï¼š
        Profile models and print results  # åˆ†ææ¨¡å‹å¹¶æ‰“å°ç»“æœ
        >>> from ultralytics.utils.benchmarks import ProfileModels  # ä» ultralytics.utils.benchmarks å¯¼å…¥ ProfileModels
        >>> profiler = ProfileModels(["yolo11n.yaml", "yolov8s.yaml"], imgsz=640)  # åˆ›å»º ProfileModels å®ä¾‹
        >>> profiler.profile()  # æ‰§è¡Œæ€§èƒ½åˆ†æ
    """

    def __init__(
        self,
        paths: list,  # paths (list): è¦åˆ†æçš„æ¨¡å‹è·¯å¾„åˆ—è¡¨
        num_timed_runs=100,  # num_timed_runs (int): æ€§èƒ½åˆ†æçš„è®¡æ—¶è¿è¡Œæ¬¡æ•°ï¼Œé»˜è®¤ä¸º 100
        num_warmup_runs=10,  # num_warmup_runs (int): åœ¨æ€§èƒ½åˆ†æå‰çš„é¢„çƒ­è¿è¡Œæ¬¡æ•°ï¼Œé»˜è®¤ä¸º 10
        min_time=60,  # min_time (float): æ€§èƒ½åˆ†æçš„æœ€å°æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä¸º 60
        imgsz=640,  # imgsz (int): æ¨¡å‹ä½¿ç”¨çš„å›¾åƒå¤§å°ï¼Œé»˜è®¤ä¸º 640
        half=True,  # half (bool): æ˜¯å¦ä½¿ç”¨ FP16 åŠç²¾åº¦ï¼Œé»˜è®¤ä¸º True
        trt=True,  # trt (bool): æ˜¯å¦ä½¿ç”¨ TensorRT è¿›è¡Œæ€§èƒ½åˆ†æï¼Œé»˜è®¤ä¸º True
        device=None,  # device (torch.device | None): ç”¨äºæ€§èƒ½åˆ†æçš„è®¾å¤‡ï¼Œé»˜è®¤ä¸º None
    ):
        """
        Initialize the ProfileModels class for profiling models.  # åˆå§‹åŒ– ProfileModels ç±»ä»¥å¯¹æ¨¡å‹è¿›è¡Œæ€§èƒ½åˆ†æã€‚

        Args:  # å‚æ•°ï¼š
            paths (List[str]): List of paths of the models to be profiled.  # paths (List[str]): è¦åˆ†æçš„æ¨¡å‹è·¯å¾„åˆ—è¡¨ã€‚
            num_timed_runs (int): Number of timed runs for the profiling.  # num_timed_runs (int): æ€§èƒ½åˆ†æçš„è®¡æ—¶è¿è¡Œæ¬¡æ•°ã€‚
            num_warmup_runs (int): Number of warmup runs before the actual profiling starts.  # num_warmup_runs (int): åœ¨å®é™…æ€§èƒ½åˆ†æå¼€å§‹å‰çš„é¢„çƒ­è¿è¡Œæ¬¡æ•°ã€‚
            min_time (float): Minimum time in seconds for profiling a model.  # min_time (float): æ€§èƒ½åˆ†æçš„æœ€å°æ—¶é—´ï¼ˆç§’ï¼‰ã€‚
            imgsz (int): Size of the image used during profiling.  # imgsz (int): æ€§èƒ½åˆ†æä¸­ä½¿ç”¨çš„å›¾åƒå¤§å°ã€‚
            half (bool): Flag to indicate whether to use FP16 half-precision for TensorRT profiling.  # half (bool): æ ‡å¿—ï¼ŒæŒ‡ç¤ºæ˜¯å¦åœ¨ TensorRT æ€§èƒ½åˆ†æä¸­ä½¿ç”¨ FP16 åŠç²¾åº¦ã€‚
            trt (bool): Flag to indicate whether to profile using TensorRT.  # trt (bool): æ ‡å¿—ï¼ŒæŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨ TensorRT è¿›è¡Œæ€§èƒ½åˆ†æã€‚
            device (torch.device | None): Device used for profiling. If None, it is determined automatically.  # device (torch.device | None): ç”¨äºæ€§èƒ½åˆ†æçš„è®¾å¤‡ã€‚å¦‚æœä¸º Noneï¼Œåˆ™è‡ªåŠ¨ç¡®å®šã€‚

        Notes:  # æ³¨æ„ï¼š
            FP16 'half' argument option removed for ONNX as slower on CPU than FP32.  # å¯¹äº ONNXï¼Œå·²ç§»é™¤ FP16 'half' å‚æ•°é€‰é¡¹ï¼Œå› ä¸ºåœ¨ CPU ä¸Šæ¯” FP32 æ›´æ…¢ã€‚

        Examples:  # ç¤ºä¾‹ï¼š
            Initialize and profile models  # åˆå§‹åŒ–å¹¶åˆ†ææ¨¡å‹
            >>> from ultralytics.utils.benchmarks import ProfileModels  # ä» ultralytics.utils.benchmarks å¯¼å…¥ ProfileModels
            >>> profiler = ProfileModels(["yolo11n.yaml", "yolov8s.yaml"], imgsz=640)  # åˆ›å»º ProfileModels å®ä¾‹
            >>> profiler.profile()  # æ‰§è¡Œæ€§èƒ½åˆ†æ
        """
        self.paths = paths  # è®¾ç½®æ¨¡å‹è·¯å¾„
        self.num_timed_runs = num_timed_runs  # è®¾ç½®è®¡æ—¶è¿è¡Œæ¬¡æ•°
        self.num_warmup_runs = num_warmup_runs  # è®¾ç½®é¢„çƒ­è¿è¡Œæ¬¡æ•°
        self.min_time = min_time  # è®¾ç½®æœ€å°åˆ†ææ—¶é—´
        self.imgsz = imgsz  # è®¾ç½®å›¾åƒå¤§å°
        self.half = half  # è®¾ç½®åŠç²¾åº¦æ ‡å¿—
        self.trt = trt  # run TensorRT profiling  # è¿è¡Œ TensorRT æ€§èƒ½åˆ†æ
        self.device = device or torch.device(0 if torch.cuda.is_available() else "cpu")  # è®¾ç½®è®¾å¤‡ï¼Œä¼˜å…ˆä½¿ç”¨ CUDA

    def profile(self):
        """Profiles YOLO models for speed and accuracy across various formats including ONNX and TensorRT.  # å¯¹ YOLO æ¨¡å‹åœ¨ä¸åŒæ ¼å¼ä¸‹ï¼ˆåŒ…æ‹¬ ONNX å’Œ TensorRTï¼‰è¿›è¡Œé€Ÿåº¦å’Œå‡†ç¡®æ€§åˆ†æã€‚"""
        files = self.get_files()  # è·å–æ¨¡å‹æ–‡ä»¶

        if not files:  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶
            print("No matching *.pt or *.onnx files found.")  # æ‰“å°æç¤ºä¿¡æ¯
            return  # è¿”å›

        table_rows = []  # åˆå§‹åŒ–è¡¨æ ¼è¡Œåˆ—è¡¨
        output = []  # åˆå§‹åŒ–è¾“å‡ºåˆ—è¡¨
        for file in files:  # éå†æ¯ä¸ªæ–‡ä»¶
            engine_file = file.with_suffix(".engine")  # åˆ›å»ºå¼•æ“æ–‡ä»¶å
            if file.suffix in {".pt", ".yaml", ".yml"}:  # å¦‚æœæ–‡ä»¶åç¼€ä¸º .ptã€.yaml æˆ– .yml
                model = YOLO(str(file))  # åˆ›å»º YOLO æ¨¡å‹å®ä¾‹
                model.fuse()  # to report correct params and GFLOPs in model.info()  # èåˆæ¨¡å‹ä»¥æŠ¥å‘Šæ­£ç¡®çš„å‚æ•°å’Œ GFLOPs
                model_info = model.info()  # è·å–æ¨¡å‹ä¿¡æ¯
                if self.trt and self.device.type != "cpu" and not engine_file.is_file():  # å¦‚æœä½¿ç”¨ TensorRT ä¸”ä¸æ˜¯ CPU è®¾å¤‡ä¸”å¼•æ“æ–‡ä»¶ä¸å­˜åœ¨
                    engine_file = model.export(  # å¯¼å‡ºæ¨¡å‹ä¸ºå¼•æ“æ ¼å¼
                        format="engine",
                        half=self.half,
                        imgsz=self.imgsz,
                        device=self.device,
                        verbose=False,
                    )
                onnx_file = model.export(  # å¯¼å‡ºæ¨¡å‹ä¸º ONNX æ ¼å¼
                    format="onnx",
                    imgsz=self.imgsz,
                    device=self.device,
                    verbose=False,
                )
            elif file.suffix == ".onnx":  # å¦‚æœæ–‡ä»¶åç¼€ä¸º .onnx
                model_info = self.get_onnx_model_info(file)  # è·å– ONNX æ¨¡å‹ä¿¡æ¯
                onnx_file = file  # è®¾ç½® ONNX æ–‡ä»¶
            else:  # å…¶ä»–æƒ…å†µ
                continue  # è·³è¿‡

            t_engine = self.profile_tensorrt_model(str(engine_file))  # å¯¹ TensorRT æ¨¡å‹è¿›è¡Œæ€§èƒ½åˆ†æ
            t_onnx = self.profile_onnx_model(str(onnx_file))  # å¯¹ ONNX æ¨¡å‹è¿›è¡Œæ€§èƒ½åˆ†æ
            table_rows.append(self.generate_table_row(file.stem, t_onnx, t_engine, model_info))  # ç”Ÿæˆè¡¨æ ¼è¡Œ
            output.append(self.generate_results_dict(file.stem, t_onnx, t_engine, model_info))  # ç”Ÿæˆç»“æœå­—å…¸

        self.print_table(table_rows)  # æ‰“å°è¡¨æ ¼
        return output  # è¿”å›è¾“å‡ºç»“æœ

    def get_files(self):
        """Returns a list of paths for all relevant model files given by the user.  # è¿”å›ç”¨æˆ·æä¾›çš„æ‰€æœ‰ç›¸å…³æ¨¡å‹æ–‡ä»¶çš„è·¯å¾„åˆ—è¡¨ã€‚"""
        files = []  # åˆå§‹åŒ–æ–‡ä»¶åˆ—è¡¨
        for path in self.paths:  # éå†æ¯ä¸ªè·¯å¾„
            path = Path(path)  # å°†è·¯å¾„è½¬æ¢ä¸º Path å¯¹è±¡
            if path.is_dir():  # å¦‚æœè·¯å¾„æ˜¯ç›®å½•
                extensions = ["*.pt", "*.onnx", "*.yaml"]  # å®šä¹‰æ–‡ä»¶æ‰©å±•å
                files.extend([file for ext in extensions for file in glob.glob(str(path / ext))])  # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶å¹¶æ·»åŠ åˆ°åˆ—è¡¨
            elif path.suffix in {".pt", ".yaml", ".yml"}:  # å¦‚æœè·¯å¾„æ˜¯æ–‡ä»¶ä¸”åç¼€ä¸º .ptã€.yaml æˆ– .yml
                files.append(str(path))  # æ·»åŠ æ–‡ä»¶è·¯å¾„åˆ°åˆ—è¡¨
            else:  # å…¶ä»–æƒ…å†µ
                files.extend(glob.glob(str(path)))  # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶å¹¶æ·»åŠ åˆ°åˆ—è¡¨

        print(f"Profiling: {sorted(files)}")  # æ‰“å°æ­£åœ¨åˆ†æçš„æ–‡ä»¶
        return [Path(file) for file in sorted(files)]  # è¿”å›æ’åºåçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨

    @staticmethod
    def get_onnx_model_info(onnx_file: str):
        """Extracts metadata from an ONNX model file including parameters, GFLOPs, and input shape.  # ä» ONNX æ¨¡å‹æ–‡ä»¶ä¸­æå–å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬å‚æ•°ã€GFLOPs å’Œè¾“å…¥å½¢çŠ¶ã€‚"""
        return 0.0, 0.0, 0.0, 0.0  # return (num_layers, num_params, num_gradients, num_flops)  # è¿”å› (num_layers, num_params, num_gradients, num_flops)

    @staticmethod
    def iterative_sigma_clipping(data, sigma=2, max_iters=3):
        """Applies iterative sigma clipping to data to remove outliers based on specified sigma and iteration count.  # å¯¹æ•°æ®åº”ç”¨è¿­ä»£ sigma å‰ªåˆ‡ï¼Œä»¥æ ¹æ®æŒ‡å®šçš„ sigma å’Œè¿­ä»£æ¬¡æ•°å»é™¤å¼‚å¸¸å€¼ã€‚"""
        data = np.array(data)  # å°†æ•°æ®è½¬æ¢ä¸º numpy æ•°ç»„
        for _ in range(max_iters):  # è¿›è¡Œæœ€å¤§è¿­ä»£æ¬¡æ•°
            mean, std = np.mean(data), np.std(data)  # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
            clipped_data = data[(data > mean - sigma * std) & (data < mean + sigma * std)]  # å‰ªåˆ‡æ•°æ®
            if len(clipped_data) == len(data):  # å¦‚æœå‰ªåˆ‡åçš„æ•°æ®é•¿åº¦ä¸åŸæ•°æ®ç›¸åŒ
                break  # é€€å‡ºå¾ªç¯
            data = clipped_data  # æ›´æ–°æ•°æ®
        return data  # è¿”å›å‰ªåˆ‡åçš„æ•°æ®

    def profile_tensorrt_model(self, engine_file: str, eps: float = 1e-3):
        """Profiles YOLO model performance with TensorRT, measuring average run time and standard deviation.  # ä½¿ç”¨ TensorRT å¯¹ YOLO æ¨¡å‹æ€§èƒ½è¿›è¡Œåˆ†æï¼Œæµ‹é‡å¹³å‡è¿è¡Œæ—¶é—´å’Œæ ‡å‡†å·®ã€‚"""
        if not self.trt or not Path(engine_file).is_file():  # å¦‚æœä¸ä½¿ç”¨ TensorRT æˆ–å¼•æ“æ–‡ä»¶ä¸å­˜åœ¨
            return 0.0, 0.0  # è¿”å› 0.0ï¼Œ0.0

        # Model and input  # æ¨¡å‹å’Œè¾“å…¥
        model = YOLO(engine_file)  # åˆ›å»º YOLO æ¨¡å‹å®ä¾‹
        input_data = np.zeros((self.imgsz, self.imgsz, 3), dtype=np.uint8)  # use uint8 for Classify  # ä½¿ç”¨ uint8 ç±»å‹çš„è¾“å…¥æ•°æ®

        # Warmup runs  # é¢„çƒ­è¿è¡Œ
        elapsed = 0.0  # åˆå§‹åŒ–å·²ç”¨æ—¶é—´
        for _ in range(3):  # è¿›è¡Œ 3 æ¬¡é¢„çƒ­
            start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
            for _ in range(self.num_warmup_runs):  # è¿›è¡Œé¢„çƒ­è¿è¡Œ
                model(input_data, imgsz=self.imgsz, verbose=False)  # æ‰§è¡Œæ¨¡å‹æ¨ç†
            elapsed = time.time() - start_time  # è®¡ç®—å·²ç”¨æ—¶é—´

        # Compute number of runs as higher of min_time or num_timed_runs  # è®¡ç®—è¿è¡Œæ¬¡æ•°ï¼Œå– min_time å’Œ num_timed_runs ä¸­çš„è¾ƒå¤§å€¼
        num_runs = max(round(self.min_time / (elapsed + eps) * self.num_warmup_runs), self.num_timed_runs * 50)  # è®¡ç®—è¿è¡Œæ¬¡æ•°

        # Timed runs  # è®¡æ—¶è¿è¡Œ
        run_times = []  # åˆå§‹åŒ–è¿è¡Œæ—¶é—´åˆ—è¡¨
        for _ in TQDM(range(num_runs), desc=engine_file):  # è¿›è¡Œè®¡æ—¶è¿è¡Œ
            results = model(input_data, imgsz=self.imgsz, verbose=False)  # æ‰§è¡Œæ¨¡å‹æ¨ç†
            run_times.append(results[0].speed["inference"])  # Convert to milliseconds  # è½¬æ¢ä¸ºæ¯«ç§’

        run_times = self.iterative_sigma_clipping(np.array(run_times), sigma=2, max_iters=3)  # sigma clipping  # è¿›è¡Œ sigma å‰ªåˆ‡
        return np.mean(run_times), np.std(run_times)  # è¿”å›å¹³å‡è¿è¡Œæ—¶é—´å’Œæ ‡å‡†å·®

    def profile_onnx_model(self, onnx_file: str, eps: float = 1e-3):
        """Profiles an ONNX model, measuring average inference time and standard deviation across multiple runs.  # å¯¹ ONNX æ¨¡å‹è¿›è¡Œæ€§èƒ½åˆ†æï¼Œæµ‹é‡å¤šæ¬¡è¿è¡Œçš„å¹³å‡æ¨ç†æ—¶é—´å’Œæ ‡å‡†å·®ã€‚"""
        check_requirements("onnxruntime")  # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† onnxruntime
        import onnxruntime as ort  # å¯¼å…¥ onnxruntime

        # Session with either 'TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'  # ä½¿ç”¨ 'TensorrtExecutionProvider'ã€'CUDAExecutionProvider' æˆ– 'CPUExecutionProvider' åˆ›å»ºä¼šè¯
        sess_options = ort.SessionOptions()  # åˆ›å»ºä¼šè¯é€‰é¡¹
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL  # è®¾ç½®å›¾ä¼˜åŒ–çº§åˆ«
        sess_options.intra_op_num_threads = 8  # Limit the number of threads  # é™åˆ¶çº¿ç¨‹æ•°é‡
        sess = ort.InferenceSession(onnx_file, sess_options, providers=["CPUExecutionProvider"])  # åˆ›å»ºæ¨ç†ä¼šè¯

        input_tensor = sess.get_inputs()[0]  # è·å–è¾“å…¥å¼ é‡
        input_type = input_tensor.type  # è·å–è¾“å…¥ç±»å‹
        dynamic = not all(isinstance(dim, int) and dim >= 0 for dim in input_tensor.shape)  # dynamic input shape  # æ£€æŸ¥è¾“å…¥å½¢çŠ¶æ˜¯å¦ä¸ºåŠ¨æ€
        input_shape = (1, 3, self.imgsz, self.imgsz) if dynamic else input_tensor.shape  # è®¾ç½®è¾“å…¥å½¢çŠ¶

        # Mapping ONNX datatype to numpy datatype  # å°† ONNX æ•°æ®ç±»å‹æ˜ å°„åˆ° numpy æ•°æ®ç±»å‹
        if "float16" in input_type:  # å¦‚æœè¾“å…¥ç±»å‹ä¸º float16
            input_dtype = np.float16  # è®¾ç½®è¾“å…¥æ•°æ®ç±»å‹ä¸º float16
        elif "float" in input_type:  # å¦‚æœè¾“å…¥ç±»å‹ä¸º float
            input_dtype = np.float32  # è®¾ç½®è¾“å…¥æ•°æ®ç±»å‹ä¸º float32
        elif "double" in input_type:  # å¦‚æœè¾“å…¥ç±»å‹ä¸º double
            input_dtype = np.float64  # è®¾ç½®è¾“å…¥æ•°æ®ç±»å‹ä¸º float64
        elif "int64" in input_type:  # å¦‚æœè¾“å…¥ç±»å‹ä¸º int64
            input_dtype = np.int64  # è®¾ç½®è¾“å…¥æ•°æ®ç±»å‹ä¸º int64
        elif "int32" in input_type:  # å¦‚æœè¾“å…¥ç±»å‹ä¸º int32
            input_dtype = np.int32  # è®¾ç½®è¾“å…¥æ•°æ®ç±»å‹ä¸º int32
        else:  # å…¶ä»–æƒ…å†µ
            raise ValueError(f"Unsupported ONNX datatype {input_type}")  # æŠ›å‡ºä¸æ”¯æŒçš„æ•°æ®ç±»å‹å¼‚å¸¸

        input_data = np.random.rand(*input_shape).astype(input_dtype)  # åˆ›å»ºéšæœºè¾“å…¥æ•°æ®
        input_name = input_tensor.name  # è·å–è¾“å…¥å¼ é‡åç§°
        output_name = sess.get_outputs()[0].name  # è·å–è¾“å‡ºå¼ é‡åç§°

        # Warmup runs  # é¢„çƒ­è¿è¡Œ
        elapsed = 0.0  # åˆå§‹åŒ–å·²ç”¨æ—¶é—´
        for _ in range(3):  # è¿›è¡Œ 3 æ¬¡é¢„çƒ­
            start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
            for _ in range(self.num_warmup_runs):  # è¿›è¡Œé¢„çƒ­è¿è¡Œ
                sess.run([output_name], {input_name: input_data})  # æ‰§è¡Œæ¨ç†
            elapsed = time.time() - start_time  # è®¡ç®—å·²ç”¨æ—¶é—´

        # Compute number of runs as higher of min_time or num_timed_runs  # è®¡ç®—è¿è¡Œæ¬¡æ•°ï¼Œå– min_time å’Œ num_timed_runs ä¸­çš„è¾ƒå¤§å€¼
        num_runs = max(round(self.min_time / (elapsed + eps) * self.num_warmup_runs), self.num_timed_runs)  # è®¡ç®—è¿è¡Œæ¬¡æ•°

        # Timed runs  # è®¡æ—¶è¿è¡Œ
        run_times = []  # åˆå§‹åŒ–è¿è¡Œæ—¶é—´åˆ—è¡¨
        for _ in TQDM(range(num_runs), desc=onnx_file):  # è¿›è¡Œè®¡æ—¶è¿è¡Œ
            start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
            sess.run([output_name], {input_name: input_data})  # æ‰§è¡Œæ¨ç†
            run_times.append((time.time() - start_time) * 1000)  # Convert to milliseconds  # è½¬æ¢ä¸ºæ¯«ç§’

        run_times = self.iterative_sigma_clipping(np.array(run_times), sigma=2, max_iters=5)  # sigma clipping  # è¿›è¡Œ sigma å‰ªåˆ‡
        return np.mean(run_times), np.std(run_times)  # è¿”å›å¹³å‡è¿è¡Œæ—¶é—´å’Œæ ‡å‡†å·®

    def generate_table_row(self, model_name, t_onnx, t_engine, model_info):
        """Generates a table row string with model performance metrics including inference times and model details.  # ç”ŸæˆåŒ…å«æ¨¡å‹æ€§èƒ½æŒ‡æ ‡ï¼ˆåŒ…æ‹¬æ¨ç†æ—¶é—´å’Œæ¨¡å‹è¯¦ç»†ä¿¡æ¯ï¼‰çš„è¡¨æ ¼è¡Œå­—ç¬¦ä¸²ã€‚"""
        layers, params, gradients, flops = model_info  # è§£åŒ…æ¨¡å‹ä¿¡æ¯
        return (  # è¿”å›æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
            f"| {model_name:18s} | {self.imgsz} | - | {t_onnx[0]:.1f}Â±{t_onnx[1]:.1f} ms | {t_engine[0]:.1f}Â±"
            f"{t_engine[1]:.1f} ms | {params / 1e6:.1f} | {flops:.1f} |"
        )

    @staticmethod
    def generate_results_dict(model_name, t_onnx, t_engine, model_info):
        """Generates a dictionary of profiling results including model name, parameters, GFLOPs, and speed metrics.  # ç”ŸæˆåŒ…å«æ¨¡å‹åç§°ã€å‚æ•°ã€GFLOPs å’Œé€Ÿåº¦æŒ‡æ ‡çš„æ€§èƒ½åˆ†æç»“æœå­—å…¸ã€‚"""
        layers, params, gradients, flops = model_info  # è§£åŒ…æ¨¡å‹ä¿¡æ¯
        return {  # è¿”å›ç»“æœå­—å…¸
            "model/name": model_name,  # æ¨¡å‹åç§°
            "model/parameters": params,  # æ¨¡å‹å‚æ•°
            "model/GFLOPs": round(flops, 3),  # æ¨¡å‹ GFLOPs
            "model/speed_ONNX(ms)": round(t_onnx[0], 3),  # ONNX æ¨¡å‹é€Ÿåº¦
            "model/speed_TensorRT(ms)": round(t_engine[0], 3),  # TensorRT æ¨¡å‹é€Ÿåº¦
        }

    @staticmethod
    def print_table(table_rows):
        """Prints a formatted table of model profiling results, including speed and accuracy metrics.  # æ‰“å°æ ¼å¼åŒ–çš„æ¨¡å‹æ€§èƒ½åˆ†æç»“æœè¡¨ï¼ŒåŒ…æ‹¬é€Ÿåº¦å’Œå‡†ç¡®æ€§æŒ‡æ ‡ã€‚"""
        gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else "GPU"  # è·å– GPU åç§°
        headers = [  # è¡¨å¤´
            "Model",  # æ¨¡å‹
            "size<br><sup>(pixels)",  # å¤§å°ï¼ˆåƒç´ ï¼‰
            "mAP<sup>val<br>50-95",  # mAP å€¼ï¼ˆ50-95ï¼‰
            f"Speed<br><sup>CPU ({get_cpu_info()}) ONNX<br>(ms)",  # CPU ä¸Šçš„ ONNX æ¨¡å‹é€Ÿåº¦
            f"Speed<br><sup>{gpu} TensorRT<br>(ms)",  # GPU ä¸Šçš„ TensorRT æ¨¡å‹é€Ÿåº¦
            "params<br><sup>(M)",  # å‚æ•°æ•°é‡ï¼ˆç™¾ä¸‡ï¼‰
            "FLOPs<br><sup>(B)",  # FLOPsï¼ˆåäº¿ï¼‰
        ]
        header = "|" + "|".join(f" {h} " for h in headers) + "|"  # æ ¼å¼åŒ–è¡¨å¤´
        separator = "|" + "|".join("-" * (len(h) + 2) for h in headers) + "|"  # æ ¼å¼åŒ–åˆ†éš”ç¬¦

        print(f"\n\n{header}")  # æ‰“å°è¡¨å¤´
        print(separator)  # æ‰“å°åˆ†éš”ç¬¦
        for row in table_rows:  # éå†æ¯ä¸€è¡Œ
            print(row)  # æ‰“å°è¡Œ