# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

from ultralytics.utils import LOGGER, SETTINGS, TESTS_RUNNING, colorstr  # ä» ultralytics.utils å¯¼å…¥ LOGGERã€SETTINGSã€TESTS_RUNNING å’Œ colorstr

try:
    # WARNING: do not move SummaryWriter import due to protobuf bug https://github.com/ultralytics/ultralytics/pull/4674
    from torch.utils.tensorboard import SummaryWriter  # ä» torch.utils.tensorboard å¯¼å…¥ SummaryWriter

    assert not TESTS_RUNNING  # do not log pytest  # ä¸è®°å½• pytest
    assert SETTINGS["tensorboard"] is True  # verify integration is enabled  # éªŒè¯é›†æˆæ˜¯å¦å¯ç”¨
    WRITER = None  # TensorBoard SummaryWriter instance  # TensorBoard SummaryWriter å®ä¾‹
    PREFIX = colorstr("TensorBoard: ")  # è®¾ç½®å‰ç¼€ä¸º "TensorBoard: "

    # Imports below only required if TensorBoard enabled
    import warnings  # å¯¼å…¥ warnings æ¨¡å—
    from copy import deepcopy  # ä» copy å¯¼å…¥ deepcopy

    from ultralytics.utils.torch_utils import de_parallel, torch  # ä» ultralytics.utils.torch_utils å¯¼å…¥ de_parallel å’Œ torch

except (ImportError, AssertionError, TypeError, AttributeError):  # æ•è·å¯¼å…¥é”™è¯¯ã€æ–­è¨€é”™è¯¯ã€ç±»å‹é”™è¯¯å’Œå±æ€§é”™è¯¯
    # TypeError for handling 'Descriptors cannot not be created directly.' protobuf errors in Windows  # å¤„ç† Windows ä¸­ 'Descriptors cannot not be created directly.' çš„ protobuf é”™è¯¯
    # AttributeError: module 'tensorflow' has no attribute 'io' if 'tensorflow' not installed  # å¦‚æœæœªå®‰è£… 'tensorflow'ï¼Œåˆ™ä¼šå¼•å‘ AttributeError: module 'tensorflow' has no attribute 'io'
    SummaryWriter = None  # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ™å°† SummaryWriter è®¾ç½®ä¸º None


def _log_scalars(scalars, step=0):
    """Logs scalar values to TensorBoard.  # å°†æ ‡é‡å€¼è®°å½•åˆ° TensorBoardã€‚"""
    if WRITER:  # å¦‚æœ WRITER å®ä¾‹å­˜åœ¨
        for k, v in scalars.items():  # éå†æ ‡é‡å­—å…¸
            WRITER.add_scalar(k, v, step)  # è®°å½•æ ‡é‡å€¼å’Œæ­¥éª¤


def _log_tensorboard_graph(trainer):
    """Log model graph to TensorBoard.  # å°†æ¨¡å‹å›¾è®°å½•åˆ° TensorBoardã€‚"""
    # Input image  # è¾“å…¥å›¾åƒ
    imgsz = trainer.args.imgsz  # è·å–å›¾åƒå¤§å°
    imgsz = (imgsz, imgsz) if isinstance(imgsz, int) else imgsz  # å¦‚æœæ˜¯æ•´æ•°ï¼Œåˆ™è½¬æ¢ä¸ºå…ƒç»„
    p = next(trainer.model.parameters())  # for device, type  # è·å–æ¨¡å‹å‚æ•°ä»¥ç¡®å®šè®¾å¤‡å’Œç±»å‹
    im = torch.zeros((1, 3, *imgsz), device=p.device, dtype=p.dtype)  # input image (must be zeros, not empty)  # è¾“å…¥å›¾åƒï¼ˆå¿…é¡»ä¸ºé›¶ï¼Œä¸å¯ä¸ºç©ºï¼‰

    with warnings.catch_warnings():  # æ•è·è­¦å‘Š
        warnings.simplefilter("ignore", category=UserWarning)  # suppress jit trace warning  # æŠ‘åˆ¶ jit è·Ÿè¸ªè­¦å‘Š
        warnings.simplefilter("ignore", category=torch.jit.TracerWarning)  # suppress jit trace warning  # æŠ‘åˆ¶ jit è·Ÿè¸ªè­¦å‘Š

        # Try simple method first (YOLO)  # é¦–å…ˆå°è¯•ç®€å•æ–¹æ³•ï¼ˆYOLOï¼‰
        try:
            trainer.model.eval()  # place in .eval() mode to avoid BatchNorm statistics changes  # ç½®äº .eval() æ¨¡å¼ä»¥é¿å… BatchNorm ç»Ÿè®¡å˜åŒ–
            WRITER.add_graph(torch.jit.trace(de_parallel(trainer.model), im, strict=False), [])  # è®°å½•æ¨¡å‹å›¾
            LOGGER.info(f"{PREFIX}model graph visualization added âœ…")  # è®°å½•æ¨¡å‹å›¾å¯è§†åŒ–å·²æ·»åŠ 
            return  # è¿”å›

        except Exception:  # æ•è·å¼‚å¸¸
            # Fallback to TorchScript export steps (RTDETR)  # å›é€€åˆ° TorchScript å¯¼å‡ºæ­¥éª¤ï¼ˆRTDETRï¼‰
            try:
                model = deepcopy(de_parallel(trainer.model))  # æ·±æ‹·è´æ¨¡å‹
                model.eval()  # ç½®äºè¯„ä¼°æ¨¡å¼
                model = model.fuse(verbose=False)  # èåˆæ¨¡å‹
                for m in model.modules():  # éå†æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—
                    if hasattr(m, "export"):  # Detect, RTDETRDecoder (Segment and Pose use Detect base class)  # æ£€æµ‹ RTDETRDecoderï¼ˆSegment å’Œ Pose ä½¿ç”¨ Detect åŸºç±»ï¼‰
                        m.export = True  # è®¾ç½®ä¸ºå¯å¯¼å‡º
                        m.format = "torchscript"  # è®¾ç½®æ ¼å¼ä¸º torchscript
                model(im)  # dry run  # å¹²è¿è¡Œ
                WRITER.add_graph(torch.jit.trace(model, im, strict=False), [])  # è®°å½•æ¨¡å‹å›¾
                LOGGER.info(f"{PREFIX}model graph visualization added âœ…")  # è®°å½•æ¨¡å‹å›¾å¯è§†åŒ–å·²æ·»åŠ 
            except Exception as e:  # æ•è·å¼‚å¸¸
                LOGGER.warning(f"{PREFIX}WARNING âš ï¸ TensorBoard graph visualization failure {e}")  # è®°å½•è­¦å‘Šä¿¡æ¯


def on_pretrain_routine_start(trainer):
    """Initialize TensorBoard logging with SummaryWriter.  # ä½¿ç”¨ SummaryWriter åˆå§‹åŒ– TensorBoard æ—¥å¿—è®°å½•ã€‚"""
    if SummaryWriter:  # å¦‚æœ SummaryWriter å¯ç”¨
        try:
            global WRITER  # å£°æ˜å…¨å±€å˜é‡ WRITER
            WRITER = SummaryWriter(str(trainer.save_dir))  # åˆå§‹åŒ– SummaryWriter
            LOGGER.info(f"{PREFIX}Start with 'tensorboard --logdir {trainer.save_dir}', view at http://localhost:6006/")  # è®°å½•å¯åŠ¨ä¿¡æ¯
        except Exception as e:  # æ•è·å¼‚å¸¸
            LOGGER.warning(f"{PREFIX}WARNING âš ï¸ TensorBoard not initialized correctly, not logging this run. {e}")  # è®°å½•è­¦å‘Šä¿¡æ¯


def on_train_start(trainer):
    """Log TensorBoard graph.  # è®°å½• TensorBoard å›¾ã€‚"""
    if WRITER:  # å¦‚æœ WRITER å®ä¾‹å­˜åœ¨
        _log_tensorboard_graph(trainer)  # è®°å½•æ¨¡å‹å›¾


def on_train_epoch_end(trainer):
    """Logs scalar statistics at the end of a training epoch.  # åœ¨è®­ç»ƒå‘¨æœŸç»“æŸæ—¶è®°å½•æ ‡é‡ç»Ÿè®¡ã€‚"""
    _log_scalars(trainer.label_loss_items(trainer.tloss, prefix="train"), trainer.epoch + 1)  # è®°å½•è®­ç»ƒæŸå¤±é¡¹
    _log_scalars(trainer.lr, trainer.epoch + 1)  # è®°å½•å­¦ä¹ ç‡


def on_fit_epoch_end(trainer):
    """Logs epoch metrics at end of training epoch.  # åœ¨è®­ç»ƒå‘¨æœŸç»“æŸæ—¶è®°å½•å‘¨æœŸæŒ‡æ ‡ã€‚"""
    _log_scalars(trainer.metrics, trainer.epoch + 1)  # è®°å½•å½“å‰æŒ‡æ ‡


callbacks = (  # å®šä¹‰å›è°ƒå‡½æ•°
    {
        "on_pretrain_routine_start": on_pretrain_routine_start,  # é¢„è®­ç»ƒä¾‹ç¨‹å¼€å§‹æ—¶çš„å›è°ƒ
        "on_train_start": on_train_start,  # è®­ç»ƒå¼€å§‹æ—¶çš„å›è°ƒ
        "on_fit_epoch_end": on_fit_epoch_end,  # æ‹Ÿåˆå‘¨æœŸç»“æŸæ—¶çš„å›è°ƒ
        "on_train_epoch_end": on_train_epoch_end,  # è®­ç»ƒå‘¨æœŸç»“æŸæ—¶çš„å›è°ƒ
    }
    if SummaryWriter  # å¦‚æœ SummaryWriter å¯ç”¨
    else {}
)