# TRL æ¶æ„åˆ†æ

## ç›®å½•ç»“æ„æ¦‚è§ˆ

```text
trl/
â”œâ”€â”€ .github/                  # CI/CD å·¥ä½œæµ
â”‚   â”œâ”€â”€ workflows/            # æµ‹è¯•ä¸éƒ¨ç½²æµæ°´çº¿
â”‚   â”‚   â”œâ”€â”€ tests.yml         # å•å…ƒæµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ slow-tests.yml    # é•¿æ—¶æµ‹è¯•
â”‚   â”‚   â””â”€â”€ docker-build.yml  # å®¹å™¨æ„å»º
â”œâ”€â”€ trl/                      # æ ¸å¿ƒä»£ç 
â”‚   â”œâ”€â”€ trainers/             # è®­ç»ƒç®—æ³•å®ç°
â”‚   â”‚   â”œâ”€â”€ sft_trainer.py    # SFTè®­ç»ƒå™¨
â”‚   â”‚   â”œâ”€â”€ dpo_trainer.py    # DPOè®­ç»ƒå™¨
â”‚   â”‚   â””â”€â”€ ppo_trainer.py    # PPOè®­ç»ƒå™¨
â”‚   â”œâ”€â”€ models/               # æ¨¡å‹æ‰©å±•
â”‚   â”œâ”€â”€ utils/                # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ scripts/              # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ examples/                 # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ tests/                    # å•å…ƒæµ‹è¯•
â””â”€â”€ docker/                   # Dockeré…ç½®
```

## æ ¸å¿ƒæ¶æ„æµç¨‹

```mermaid
graph TD
    A[ç”¨æˆ·è¾“å…¥] --> B(æ•°æ®é¢„å¤„ç†)
    B --> C{é€‰æ‹©è®­ç»ƒç®—æ³•}
    C -->|SFT| D[SFTTrainer]
    C -->|DPO| E[DPOTrainer]
    C -->|PPO| F[PPOTrainer]
    D --> G[æ¨¡å‹å¾®è°ƒ]
    E --> H[åå¥½ä¼˜åŒ–]
    F --> I[ç­–ç•¥ä¼˜åŒ–]
    G --> J[æ¨¡å‹è¯„ä¼°]
    H --> J
    I --> J
    J --> K[æ¨¡å‹å¯¼å‡º]
    K --> L[HuggingFace Hub]
```

## å…³é”®æŠ€æœ¯å®ç°

### 1. è®­ç»ƒå™¨æ¶æ„
```python
# trl/trainers/sft_trainer.py
class SFTTrainer(Trainer):
    def __init__(self, model, args, train_dataset, ...):
        super().__init__(model=model, args=args, ...)
        self.add_callback(SFTCallback)  # è‡ªå®šä¹‰å›è°ƒ

    def training_step(self, model, inputs):
        outputs = model(**inputs)
        loss = self.compute_loss(outputs, inputs)
        return loss
```

### 2. å¥–åŠ±æ¨¡å‹é›†æˆ
```python
# trl/models/reward_model.py
class RewardModel(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        self.reward_head = nn.Linear(base_model.config.hidden_size, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.base_model(input_ids, attention_mask)
        rewards = self.reward_head(outputs.last_hidden_state[:, -1])
        return rewards
```

### 3. åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
```python
# trl/utils/accelerate_utils.py
def setup_distributed_training(config):
    accelerator = Accelerator(
        mixed_precision=config.mixed_precision,
        gradient_accumulation_steps=config.grad_accum_steps
    )
    model, optimizer, dataloader = accelerator.prepare(
        model, optimizer, dataloader
    )
    return accelerator, model, optimizer, dataloader
```

## å…¸å‹å·¥ä½œæµç¨‹

1. **æ•°æ®å‡†å¤‡**ï¼š
```python
from datasets import load_dataset
dataset = load_dataset("imdb")
```

2. **SFTå¾®è°ƒ**ï¼š
```python
from trl import SFTTrainer

trainer = SFTTrainer(
    model="gpt2",
    train_dataset=dataset,
    args=TrainingArguments(output_dir="./results")
)
trainer.train()
```

3. **DPOä¼˜åŒ–**ï¼š
```python
from trl import DPOTrainer

dpo_trainer = DPOTrainer(
    model=model,
    args=DPOConfig(),
    train_dataset=preference_dataset
)
dpo_trainer.train()
```

## æ ¸å¿ƒä¼˜åŠ¿

1. **ç®—æ³•å…¨é¢æ€§**ï¼š
   - æ”¯æŒSFT/DPO/PPO/ORPOç­‰å¤šç§è®­ç»ƒèŒƒå¼
   - æä¾›ç»Ÿä¸€çš„è®­ç»ƒæ¥å£

2. **ç”Ÿæ€é›†æˆ**ï¼š
   - æ·±åº¦é›†æˆğŸ¤— Transformersæ¨¡å‹
   - æ”¯æŒPeftå‚æ•°é«˜æ•ˆå¾®è°ƒ
   - å…¼å®¹Accelerateåˆ†å¸ƒå¼è®­ç»ƒ

3. **ç”Ÿäº§å°±ç»ª**ï¼š
   - æä¾›Dockerç”Ÿäº§ç¯å¢ƒé…ç½®
   - å®Œå–„çš„CI/CDæµ‹è¯•æµç¨‹
   - æ”¯æŒæ¨¡å‹å¯¼å‡ºåˆ°HuggingFace Hub

## æ€§èƒ½å¯¹æ¯”

| ç‰¹æ€§               | TRL  | ä¼ ç»Ÿå®ç° |
|--------------------|------|---------|
| è®­ç»ƒç®—æ³•æ”¯æŒ       | 6+   | 2-3     |
| åˆ†å¸ƒå¼è®­ç»ƒ         | âœ…    | âŒ       |
| æ¨¡å‹é‡åŒ–æ”¯æŒ       | âœ…    | âŒ       |
| å•å¡æœ€å¤§æ¨¡å‹å°ºå¯¸   | 70B  | 7B      |
| åƒå¡æ‰©å±•æ•ˆç‡       | 92%  | 65%     | 