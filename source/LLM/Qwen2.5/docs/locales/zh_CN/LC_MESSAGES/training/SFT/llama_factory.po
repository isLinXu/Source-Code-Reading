# Copyright (C) 2024, Qwen Team, Alibaba Group.
# This file is distributed under the same license as the Qwen package.
#
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-09-18 21:18+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../source/training/SFT/llama_factory.rst:2
#: 68febcbbc0494a58a6f7847621f43c8a
msgid "LLaMA-Factory"
msgstr ""

#: ../../source/training/SFT/llama_factory.rst:4
#: eb81bdea49a044768076a3b6d5b96c6b
msgid "Here we provide a script for supervised finetuning Qwen2.5 with `LLaMA-Factory <https://github.com/hiyouga/LLaMA-Factory>`__. This script for supervised finetuning (SFT) has the following features:"
msgstr "我们将介绍如何使用 `LLaMA-Factory <https://github.com/hiyouga/LLaMA-Factory>`__ 微调 Qwen2.5 模型。本脚本包含如下特点："

#: ../../source/training/SFT/llama_factory.rst:8
#: 35fa1b5c50b2481993528c89ba866607
msgid "Support single-GPU and multi-GPU training;"
msgstr "支持单卡和多卡分布式训练"

#: ../../source/training/SFT/llama_factory.rst:10
#: 3b87bf3c51b14c0a816de5b651073412
msgid "Support full-parameter tuning, LoRA, Q-LoRA, Dora."
msgstr "支持全参数微调、LoRA、Q-LoRA 和 DoRA 。"

#: ../../source/training/SFT/llama_factory.rst:12
#: d0f877c79ad940379e6fb76efd3252f5
msgid "In the following, we introduce more details about the usage of the script."
msgstr "下文将介绍更多关于脚本的用法。"

#: ../../source/training/SFT/llama_factory.rst:16
#: e7ccffc6e55441dc8f3167113a57b448
msgid "Installation"
msgstr "安装"

#: ../../source/training/SFT/llama_factory.rst:18
#: c830b89e259e49efa3278df0203b00ad
msgid "Before you start, make sure you have installed the following packages:"
msgstr "开始之前，确保你已经安装了以下代码库："

#: ../../source/training/SFT/llama_factory.rst:20
#: 180689ab6ba04fe094e9270f1c038b5a
msgid "Follow the instructions of `LLaMA-Factory <https://github.com/hiyouga/LLaMA-Factory>`__, and build the environment."
msgstr "根据 `LLaMA-Factory <https://github.com/hiyouga/LLaMA-Factory>`__ 官方指引构建好你的环境"

#: ../../source/training/SFT/llama_factory.rst:23
#: 212a562d08aa438f975ab72d50b2e52b
msgid "Install these packages (Optional):"
msgstr "安装下列代码库（可选）："

#: ../../source/training/SFT/llama_factory.rst:30
#: 5f717bfa34774573a5ce49feedf3c6b6
msgid "If you want to use `FlashAttention-2 <https://github.com/Dao-AILab/flash-attention>`__, make sure your CUDA is 11.6 and above."
msgstr "如你使用 `FlashAttention-2 <https://github.com/Dao-AILab/flash-attention>`__  ，请确保你的CUDA版本在11.6以上。"

#: ../../source/training/SFT/llama_factory.rst:35
#: 7f60c94192c04cb6a1d22be116518f08
msgid "Data Preparation"
msgstr "准备数据"

#: ../../source/training/SFT/llama_factory.rst:37
#: 21161fc7605847d4b94aa6bbf40f288e
msgid "LLaMA-Factory provides several training datasets in ``data`` folder, you can use it directly. If you are using a custom dataset, please prepare your dataset as follows."
msgstr "LLaMA-Factory 在 ``data`` 文件夹中提供了多个训练数据集，您可以直接使用它们。如果您打算使用自定义数据集，请按照以下方式准备您的数据集。"

#: ../../source/training/SFT/llama_factory.rst:41
#: 1146f8d06b8f447fb8b42bbaab3d01f1
msgid "Organize your data in a **json** file and put your data in ``data`` folder. LLaMA-Factory supports dataset in ``alpaca`` or ``sharegpt`` format."
msgstr "请将您的数据以 ``json`` 格式进行组织，并将数据放入 data 文件夹中。LLaMA-Factory 支持以 ``alpaca`` 或 ``sharegpt`` 格式的数据集。"

#: ../../source/training/SFT/llama_factory.rst:45
#: 3706d5d7db8540aa9ead87365ee8c483
msgid "The dataset in ``alpaca`` format should follow the below format:"
msgstr "``alpaca`` 格式的数据集应遵循以下格式："

#: ../../source/training/SFT/llama_factory.rst:62
#: 92f408c7fa894709ad1d5b9792320362
msgid "The dataset in ``sharegpt`` format should follow the below format:"
msgstr "``sharegpt`` 格式的数据集应遵循以下格式："

#: ../../source/training/SFT/llama_factory.rst:83
#: e1e27d0afbce48119c7a2153cbdaabf7
msgid "Provide your dataset definition in ``data/dataset_info.json`` in the following format ."
msgstr "在 ``data/dataset_info.json`` 文件中提供您的数据集定义，并采用以下格式："

#: ../../source/training/SFT/llama_factory.rst:86
#: 711bfaba832c48f28464d57a1f7d1169
msgid "For ``alpaca`` format dataset, the columns in ``dataset_info.json`` should be:"
msgstr "对于 ``alpaca`` 格式的数据集，其 ``dataset_info.json`` 文件中的列应为："

#: ../../source/training/SFT/llama_factory.rst:102
#: 544cc33ea8f84e45b887887466a6cd1b
msgid "For ``sharegpt`` format dataset, the columns in ``dataset_info.json`` should be:"
msgstr "对于 ``sharegpt`` 格式的数据集，``dataset_info.json`` 文件中的列应该包括："

#: ../../source/training/SFT/llama_factory.rst:124
#: 3d7da197c0394832a6278df32f39a1fb
msgid "Training"
msgstr "训练"

#: ../../source/training/SFT/llama_factory.rst:126
#: 8f894e07af7e4c60bff6b6011e0b03e0
msgid "Execute the following training command:"
msgstr "执行下列命令："

#: ../../source/training/SFT/llama_factory.rst:166
#: 355e1c85d29249c3af30cdc9b5ed5770
msgid "and enjoy the training process. To make changes to your training, you can modify the arguments in the training command to adjust the hyperparameters. One argument to note is ``cutoff_len``, which is the maximum length of the training data. Control this parameter to avoid OOM error."
msgstr "并享受训练过程。若要调整您的训练，您可以通过修改训练命令中的参数来调整超参数。其中一个需要注意的参数是 ``cutoff_len`` ，它代表训练数据的最大长度。通过控制这个参数，可以避免出现OOM（内存溢出）错误。"

#: ../../source/training/SFT/llama_factory.rst:173
#: b6910242cca94fe1b5cda3da730dc984
msgid "Merge LoRA"
msgstr "合并LoRA"

#: ../../source/training/SFT/llama_factory.rst:175
#: f1ef4d16d0b1495c99467a7783430bad
msgid "If you train your model with LoRA, you probably need to merge adapter parameters to the main branch. Run the following command to perform the merging of LoRA adapters."
msgstr "如果你使用 LoRA 训练模型，可能需要将adapter参数合并到主分支中。请运行以下命令以执行 LoRA adapter 的合并操作。"

#: ../../source/training/SFT/llama_factory.rst:191
#: fea078e8ebd64be7acd9d4d8afd43fec
msgid "Conclusion"
msgstr "结语"

#: ../../source/training/SFT/llama_factory.rst:193
#: 47b180aa689a493488dfb60b490c1de7
msgid "The above content is the simplest way to use LLaMA-Factory to train Qwen. Feel free to dive into the details by checking the official repo!"
msgstr "上述内容是使用LLaMA-Factory训练Qwen的最简单方法。 欢迎通过查看官方仓库深入了解详细信息！"

