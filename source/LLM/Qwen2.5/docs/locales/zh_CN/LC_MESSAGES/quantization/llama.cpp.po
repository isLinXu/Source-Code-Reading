# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2024, Qwen Team
# This file is distributed under the same license as the Qwen package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-09-18 21:18+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../source/quantization/llama.cpp.md:1 b82ced39c7fd4a1b8e83f9a978d446c7
msgid "llama.cpp"
msgstr ""

#: ../../source/quantization/llama.cpp.md:3 c40a0db7dee44d6585abe57d44cca7f3
msgid "Quantization is a major topic for local inference of LLMs, as it reduces the memory footprint. Undoubtably, llama.cpp natively supports LLM quantization and of course, with flexibility as always."
msgstr "量化(Quantization)是本地运行大规模语言模型的主要议题，因为它能减少内存占用。毫无疑问，llama.cpp原生支持大规模语言模型的量化，并且一如既往地保持了灵活性。"

#: ../../source/quantization/llama.cpp.md:6 2094f56dd911476a87c94f2f474d47c4
msgid "At high-level, all quantization supported by llama.cpp is weight quantization:  Model parameters are quantized into lower bits, and in inference, they are dequantized and used in computation."
msgstr "在高层次上，llama.cpp所支持的所有量化都是权重量化(weight quantization)：模型参数被量化为低位(bit)数，在推理过程中，它们会被反量化(dequantize)并用于计算。"

#: ../../source/quantization/llama.cpp.md:9 413c9de0ec884114877b7fef278ebb2d
msgid "In addition, you can mix different quantization data types in a single quantized model, e.g., you can quantize the embedding weights using a quantization data type and other weights using a different one. With an adequate mixture of quantization types, much lower quantization error can be attained with just a slight increase of bit-per-weight. The example program `llama-quantize` supports many quantization presets, such as Q4_K_M and Q8_0."
msgstr "此外，你可以在单一的量化模型中混合使用不同的量化数据类型，例如，你可以使用一种量化数据类型量化嵌入权重(embedding)，而使用另一种量化其他权重。通过适当的量化类型组合，只需略微增加bpw （bit-per-weight, 位权比），就能达到更低的量化误差。示例程序`llama-quantize`支持许多量化预设，如Q4_K_M和Q8_0。"

#: ../../source/quantization/llama.cpp.md:13 3569f89d79ab42cfae858593c4153ee4
msgid "If you find the quantization errors still more than expected, you can bring your own scales, e.g., as computed by AWQ, or use calibration data to compute an importance matrix using `llama-imatrix`, which can then be used during quantization to enhance the quality of the quantized models."
msgstr "如果你发现量化误差仍然超出预期，你可以引入自己的量化尺度，例如由AWQ计算的，或者使用校准数据用`llama-imatrix`来计算一个“重要性矩阵”(importance matrix)，然后在量化过程中使用以提高量化模型的质量。"

#: ../../source/quantization/llama.cpp.md:15 3fa48eb9181240a187d33fa7af1f1733
msgid "In this document, we demonstrate the common way to quantize your model and evaluate the performance of the quantized model. We will assume you have the example programs from llama.cpp at your hand. If you don't, check our guide [here](../run_locally/llama.cpp.html#getting-the-program){.external}."
msgstr "在本文档中，我们将展示量化和评估量化模型性能的常见方法。我们会假设你手头有llama.cpp的示例程序。如果没有，请查看我们的[指南](../run_locally/llama.cpp.html#getting-the-program){.external}。"

#: ../../source/quantization/llama.cpp.md:19 395df1504ab3410493394aade455a426
msgid "Getting the GGUF"
msgstr "获取GGUF"

#: ../../source/quantization/llama.cpp.md:21 1cb4b2d8fbe043b18d8b6eabc27eb1bf
msgid "Now, suppose you would like to quantize `Qwen2.5-7B-Instruct`.  You need to first make a GGUF file as shown below:"
msgstr "现在，假设你想量化`Qwen2.5-7B-Instruct`。你需要首先创建一个GGUF文件，如下所示："

#: ../../source/quantization/llama.cpp.md:27 53c668326e5a45b1ad1c705841ee0654
msgid "Sometimes, it may be better to use fp32 as the start point for quantization. In that case, use"
msgstr "有时，可能最好将fp32作为量化的起点。在这种情况下，使用"

#: ../../source/quantization/llama.cpp.md:33 d673ccd39c414bbb93b5e04b865741ba
msgid "Quantizing the GGUF without Calibration"
msgstr "无校准量化GGUF"

#: ../../source/quantization/llama.cpp.md:35 7a574480a54845b69c6ef63a7f6b335b
msgid "For the simplest way, you can directly quantize the model to lower-bits based on your requirements.  An example of quantizing the model to 8 bits is shown below:"
msgstr "最简单的方法是，你可以根据需求直接将模型量化到低位数。下面是一个将模型量化到8 bit的例子："

#: ../../source/quantization/llama.cpp.md:41 19170e6a81e345859d3e8898bebef862
msgid "`Q8_0` is a code for a quantization preset. You can find all the presets in [the source code of `llama-quantize`](https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/quantize.cpp). Look for the variable `QUANT_OPTIONS`. Common ones used for 7B models include `Q8_0`, `Q5_0`, and `Q4_K_M`.  The letter case doesn't matter, so `q8_0` or `q4_K_m` are perfectly fine."
msgstr "`Q8_0`是一个量化预设的代号。你可以在[`llama-quantize`的源代码](https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/quantize.cpp)中找到所有预设。寻找变量`QUANT_OPTIONS`。对于7B模型常用的包括`Q8_0`、`Q5_0`和`Q4_K_M`。字母大小写不重要，所以`q8_0`或`q4_K_m`都是可以接受的。"

#: ../../source/quantization/llama.cpp.md:47 a50f96e24c28425392ee4946ca1ac4a1
msgid "Now you can use the GGUF file of the quantized model with applications based on llama.cpp. Very simple indeed."
msgstr "现在，你可以使用基于llama.cpp的应用程序中的量化模型的GGUF文件。确实很简单。"

#: ../../source/quantization/llama.cpp.md:50 a3cc74c1860e450cba3269167ac08230
msgid "However, the accuracy of the quantized model could be lower than expected occasionally, especially for lower-bit quantization. The program may even prevent you from doing that."
msgstr "然而，量化模型的准确性偶尔可能低于预期，特别是对于低位数量化。程序甚至可能阻止你这样做。"

#: ../../source/quantization/llama.cpp.md:53 659757454010443e83428478736c3e5f
msgid "There are several ways to improve quality of quantized models. A common way is to use a calibration dataset in the target domain to identify the weights that really matter and quantize the model in a way that those weights have lower quantization errors, as introduced in the next two methods."
msgstr "有几种方法可以提高量化模型的质量。一种常见的方法是在目标域中使用校准数据集来识别真正重要的权重，并以这些权重具有较低量化误差的方式量化模型，如下两种方法中将介绍。"

#: ../../source/quantization/llama.cpp.md:57 c3a2be1439af41258d3f23139703810d
msgid "Quantizing the GGUF with AWQ Scale"
msgstr "使用AWQ尺度量化GGUF"

#: ../../source/quantization/llama.cpp.md:59 5a91fafdc6e842538e1aad3c3bf27ea8
msgid "To improve the quality of your quantized models, one possible solution is to apply the AWQ scale, following [this script](https://github.com/casper-hansen/AutoAWQ/blob/main/docs/examples.md#gguf-export). First, when you run `model.quantize()` with `autoawq`, remember to add `export_compatible=True` as shown below:"
msgstr "为了提高量化模型的质量，一种可能的解决方案是应用AWQ尺度，遵循[这个脚本](https://github.com/casper-hansen/AutoAWQ/blob/main/docs/examples.md#gguf-export)。首先，当你使用`autoawq`运行`model.quantize()`时，记得添加`export_compatible=True`，如下所示："

#: ../../source/quantization/llama.cpp.md:72 a3c98e44309d4b25937ca7e56a7ce114
msgid "The above code will not actually quantize the weights. Instead, it adjusts weights based on a dataset so that they are \"easier\" to quantize.[^AWQ]"
msgstr "上述代码实际上不会量化权重。相反，它会根据数据集调整权重，使它们“更容易”量化。[^AWQ]"

#: ../../source/quantization/llama.cpp.md:75 68ac3cc7037f49b68e5b79282178de94
msgid "Then, when you run `convert-hf-to-gguf.py`, remember to replace the model path with the path to the new model:"
msgstr "然后，当你运行`convert-hf-to-gguf.py`时，记得将模型路径替换为新模型的路径："

#: ../../source/quantization/llama.cpp.md:80 0f7449065d9c413cb06a9e37a81acc3d
msgid "Finally, you can quantize the model as in the last example:"
msgstr "最后，你可以像最后一个例子那样量化模型："

#: ../../source/quantization/llama.cpp.md:85 ffba6579b2134c4695ebf2b93ac11db1
msgid "In this way, it should be possible to achieve similar quality with lower bit-per-weight."
msgstr "这样，应该有可能以更低的bpw实现相似的质量。"

#: ../../source/quantization/llama.cpp.md:91 d4a33884637e44adba0ca348e9416bf4
msgid "Quantizing the GGUF with Importance Matrix"
msgstr "使用重要性矩阵量化GGUF"

#: ../../source/quantization/llama.cpp.md:93 ec2f9c87533041179c8cc999963e8fb9
msgid "Another possible solution is to use the \"important matrix\"[^imatrix], following [this](https://github.com/ggerganov/llama.cpp/tree/master/examples/imatrix)."
msgstr "另一个可能的解决方案是使用\"重要矩阵\"[^imatrix]，参照[这里](https://github.com/ggerganov/llama.cpp/tree/master/examples/imatrix)。"

#: ../../source/quantization/llama.cpp.md:95 f8089a3caef2459cb86bce3f51c82c9b
msgid "First, you need to compute the importance matrix data of the weights of a model (`-m`) using a calibration dataset (`-f`):"
msgstr "首先，你需要使用校准数据集（`-f`）计算模型权重的重要性矩阵数据（`-m`）："

#: ../../source/quantization/llama.cpp.md:100 ac6d40bc73554e27921966ca13e5875b
msgid "The text is cut in chunks of length `--chunk` for computation. Preferably, the text should be representative of the target domain. The final results will be saved in a file named `qwen2.5-7b-instruct-imatrix.dat` (`-o`), which can then be used:"
msgstr "文本被切割成长度为`--chunk`的块进行计算。最好，文本应代表目标领域。最终结果将保存在名为`qwen2.5-7b-instruct-imatrix.dat`（`-o`）的文件中，然后可以使用："

#: ../../source/quantization/llama.cpp.md:108 e8267132477c431abccc512c8ff696f3
msgid "For lower-bit quantization mixtures for 1-bit or 2-bit, if you do not provide `--imatrix`, a helpful warning will be printed by `llama-quantize`."
msgstr "对于1 bit或2 bit的低位数量化混合，如果你不提供`--imatrix`，`llama-quantize`将打印出有用的警告。"

#: ../../source/quantization/llama.cpp.md:112 c84470b684894c1db588fe3edd9d0a7e
msgid "Perplexity Evaluation"
msgstr "困惑度(Perplexity)评估"

#: ../../source/quantization/llama.cpp.md:114 f9a1ebf738204dec88593718f1b52bf7
msgid "`llama.cpp` provides an example program for us to calculate the perplexity, which evaluate how unlikely the given text is to the model. It should be mostly used for comparisons: the lower the perplexity, the better the model remembers the given text."
msgstr "`llama.cpp`为我们提供了一个示例程序来计算困惑度，这评估了给定文本对模型而言的“不可能”程度。它主要用于比较：困惑度越低，模型对给定文本的记忆越好。"

#: ../../source/quantization/llama.cpp.md:117 bcaf364cda6d4861ae2841ff54711fd0
msgid "To do this, you need to prepare a dataset, say \"wiki test\"[^wiki].  You can download the dataset with:"
msgstr "要做到这一点，你需要准备一个数据集，比如\"wiki测试集\"[^wiki]。你可以使用以下命令下载数据集："

#: ../../source/quantization/llama.cpp.md:124 4948c6377c6e4f3bbc4a34e08d00d2fe
msgid "Then you can run the test with the following command:"
msgstr "然后你可以使用以下命令运行测试："

#: ../../source/quantization/llama.cpp.md:128 7be799caec7947a1bf6e455a087151ff
msgid "Wait for some time and you will get the perplexity of the model. There are some numbers of different kinds of quantization mixture [here](https://github.com/ggerganov/llama.cpp/blob/master/examples/perplexity/README.md). It might be helpful to look at the difference and grab a sense of how that kind of quantization might perform."
msgstr "稍等一段时间，你将得到模型的困惑度。[这里](https://github.com/ggerganov/llama.cpp/blob/master/examples/perplexity/README.md)提供了不同类型的量化模型的数值。观察差异可能有助于理解不同量化方式的潜在表现。"

#: ../../source/quantization/llama.cpp.md:135 0cc3e3f987e64ef8b7d160160277a13b
msgid "Finally"
msgstr "结束语"

#: ../../source/quantization/llama.cpp.md:137 4631588f0f6c45f5b5dfa13f81802749
msgid "In this guide, we demonstrate how to conduct quantization and evaluate the perplexity with llama.cpp. For more information, please visit the [llama.cpp GitHub repo](https://github.com/ggerganov/llama.cpp)."
msgstr "在本指南中，我们展示了如何使用llama.cpp进行量化和评估困惑度。更多信息，请访问[llama.cpp GitHub仓库](https://github.com/ggerganov/llama.cpp)。"

#: ../../source/quantization/llama.cpp.md:140 a52f4b1681f74d468e45d743ecd2730c
msgid "We usually quantize the fp16 model to 2, 3, 4, 5, 6, and 8-bit models with different quantization mixtures, but sometimes a particular mixture just does not work, so we don't provide those in our HuggingFace Hub. However, others in the community may have success, so if you haven't found what you need in our repos, look around."
msgstr "我们通常将fp16模型量化为2、3、4、5、6和8位模型，采用不同的量化混合，但有时特定的混合就是不起作用，所以我们不在HuggingFace Hub中提供这些。但是，社区中的其他人可能会成功，因此，如果你在我们的仓库中没有找到所需的内容，请四处看看。"

#: ../../source/quantization/llama.cpp.md:143 604a8bae42a64bf98a51576ed839b01e
msgid "Enjoy your freshly quantized models!"
msgstr "享受你新鲜量化的模型吧！"

#: ../../source/quantization/llama.cpp.md:87 89684a1a2d514ed4b29bcdb4e698c76c
msgid "If you are interested in what this means, refer to [the AWQ paper](https://arxiv.org/abs/2306.00978).     Basically, important weights (called salient weights in the paper) are identified based on activations across data examples.     The weights are scaled accordingly such that the salient weights are protected even after quantization."
msgstr "如果你对这意味着什么感兴趣，请参阅[AWQ论文](https://arxiv.org/abs/2306.00978)。基本上，根据数据实例上的激活，识别出重要的权重（在论文中称为显著权重）。相应地缩放权重，以便即使在量化后也能保护显著权重。"

#: ../../source/quantization/llama.cpp.md:110 6522f9b442bd4719b8c04b1d045aeae0
msgid "Here, the importance matrix keeps record of how weights affect the output: the weight should be important is a slight change in its value causes huge difference in the results, akin to the [GPTQ](https://arxiv.org/abs/2210.17323) algorithm."
msgstr "在这里，重要性矩阵记录了权重如何影响输出：如果权重的微小变化导致结果的巨大差异，则该权重应该是重要的，类似于[GPTQ](https://arxiv.org/abs/2210.17323)算法。"

#: ../../source/quantization/llama.cpp.md:132 2ac6459c5c9842cfb4d8fc87618d1b6f
msgid "It is not a good evaluation dataset for instruct models though, but it is very common and easily accessible.      You probably want to use a dataset similar to your target domain."
msgstr "虽然它不是指导模型的良好评估数据集，但它非常常见且易于访问。你可能希望使用与目标领域相似的数据集。"

