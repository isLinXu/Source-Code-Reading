# Copyright (C) 2024, Qwen Team, Alibaba Group.
# This file is distributed under the same license as the Qwen package.
#
msgid ""
msgstr ""
"Project-Id-Version: Qwen \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-09-18 21:18+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../source/deployment/vllm.md:1 d489b70947374f31a45500037edcf4c8
msgid "vLLM"
msgstr "vLLM"

#: ../../source/deployment/vllm.md:3 71f139fb40514158a3ba2e6d49ba98e6
msgid "We recommend you trying [vLLM](https://github.com/vllm-project/vllm) for your deployment of Qwen.  It is simple to use, and it is fast with state-of-the-art serving throughput, efficient management of attention key value memory with PagedAttention, continuous batching of input requests, optimized CUDA kernels, etc.  To learn more about vLLM, please refer to the [paper](https://arxiv.org/abs/2309.06180) and [documentation](https://vllm.readthedocs.io/)."
msgstr "我们建议您在部署Qwen时尝试使用 [vLLM](https://github.com/vllm-project/vllm)。它易于使用，且具有最先进的服务吞吐量、高效的注意力键值内存管理（通过PagedAttention实现）、连续批处理输入请求、优化的CUDA内核等功能。要了解更多关于vLLM的信息，请参阅 [论文](https://arxiv.org/abs/2309.06180) 和 [文档](https://vllm.readthedocs.io/)。"

#: ../../source/deployment/vllm.md:7 ff7726a4886f4338812be84c0efee59a
msgid "Installation"
msgstr "安装"

#: ../../source/deployment/vllm.md:9 8abf21ad31cb4444bc0bbea5d9510b45
msgid "By default, you can install `vllm` by pip in a clean environment:"
msgstr "默认情况下，你可以通过 `pip` 在新环境中安装vLLM： "

#: ../../source/deployment/vllm.md:15 a338c1d41d4b44a2b2093847b1c70285
msgid "Please note that the prebuilt `vllm` has strict dependencies on `torch` and its CUDA versions. Check the note in the official document for installation ([link](https://docs.vllm.ai/en/latest/getting_started/installation.html)) for some help. We also advise you to install ray by `pip install ray` for distributed serving."
msgstr "请留意预构建的`vllm`对`torch`和其CUDA版本有强依赖。请查看[vLLM官方文档](https://docs.vllm.ai/en/latest/getting_started/installation.html)中的注意事项以获取有关安装的帮助。我们也建议你通过 `pip install ray` 安装ray， 以便支持分布式服务。"

#: ../../source/deployment/vllm.md:19 ec6066fb88854a8db179f5e61f903e69
msgid "Offline Batched Inference"
msgstr "离线推理"

#: ../../source/deployment/vllm.md:21 2ccc5f13b9404afeb0c884770c89d1bf
msgid "Models supported by Qwen2.5 codes are supported by vLLM. The simplest usage of vLLM is offline batched inference as demonstrated below."
msgstr "Qwen2.5代码支持的模型都被vLLM所支持。 vLLM最简单的使用方式是通过以下演示进行离线批量推理。"

#: ../../source/deployment/vllm.md:60 ddc63e937f194ce6940739985767525d
msgid "OpenAI-Compatible API Service"
msgstr "OpenAI兼容的API服务"

#: ../../source/deployment/vllm.md:62 ad6c5353316245e4aa9b999cf3a477cf
msgid "It is easy to build an OpenAI-compatible API service with vLLM, which can be deployed as a server that implements OpenAI API protocol. By default, it starts the server at `http://localhost:8000`.  You can specify the address with `--host` and `--port` arguments.  Run the command as shown below:"
msgstr "借助vLLM，构建一个与OpenAI API兼容的API服务十分简便，该服务可以作为实现OpenAI API协议的服务器进行部署。默认情况下，它将在 `http://localhost:8000` 启动服务器。您可以通过 `--host` 和 `--port` 参数来自定义地址。请按照以下所示运行命令："

#: ../../source/deployment/vllm.md:71 cbc0abc56e7e49bea661c2d6adb68f8d
msgid "You don't need to worry about chat template as it by default uses the chat template provided by the tokenizer."
msgstr "你无需担心chat模板，因为它默认会使用由tokenizer提供的chat模板。"

#: ../../source/deployment/vllm.md:73 391470ef89914f4f97431902f1993855
msgid "Then, you can use the [create chat interface](https://platform.openai.com/docs/api-reference/chat/completions/create) to communicate with Qwen:"
msgstr "然后，您可以利用 [create chat interface](https://platform.openai.com/docs/api-reference/chat/completions/create) 来与Qwen进行对话："

#: ../../source/deployment/vllm.md 745a3e89d5964af8a0bbbee3aef2a263
msgid "curl"
msgstr ""

#: ../../source/deployment/vllm.md bb58d6efd3eb4fec9f6f620e40d9aef6
msgid "Python"
msgstr ""

#: ../../source/deployment/vllm.md:94 479508ff82514d0abead680139ebcf62
msgid "You can use the API client with the `openai` Python package as shown below:"
msgstr "或者您可以如下面所示使用 `openai` Python 包中的 API 客户端："

#: ../../source/deployment/vllm.md:126 bb487791f79d4acba8cc955d0034f05b
msgid "The OpenAI-compatible server in `vllm` comes with [a default set of sampling parameters](https://github.com/vllm-project/vllm/blob/v0.5.2/vllm/entrypoints/openai/protocol.py#L130), which are not suitable for Qwen2.5 models and prone to repetition. We advise you to always pass sampling parameters to the API."
msgstr "`vllm` 中的 OpenAI 兼容服务器使用 [一组默认的采样参数](https://github.com/vllm-project/vllm/blob/v0.5.2/vllm/entrypoints/openai/protocol.py#L130)。这组默认参数并不适用于 Qwen2.5 模型，并可能加重重复问题。我们建议您总是为该API传入合适的采样参数。"

#: ../../source/deployment/vllm.md:132 5ed6ba9bac19408f8a2e3b7519ed0515
msgid "Tool Use"
msgstr "工具使用"

#: ../../source/deployment/vllm.md:134 a58148a5643b47a09b8efee34906294e
msgid "The OpenAI-compatible API could be configured to support tool call of Qwen2.5. For information, please refer to [our guide on Function Calling](../framework/function_call.md#vllm)."
msgstr "vLLM中的OpenAI兼容API 可以配置为支持 Qwen2.5 的工具调用。详细信息，请参阅[我们关于函数调用的指南](../framework/function_call.md#vllm)。"

#: ../../source/deployment/vllm.md:137 4227255cc9b4484389ef020dec08e83e
msgid "Structured/JSON Output"
msgstr "结构化/JSON输出"

#: ../../source/deployment/vllm.md:139 af4375e1e44e49fc816684f3516c4e2d
msgid "Qwen 2.5, when used with vLLM, supports structured/JSON output.  Please refer to [vllm's documentation](https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html#extra-parameters-for-chat-api) for the `guided_json` parameters. Besides, it is also recommended to instruct the model to generate the specific format in the system message or in your prompt."
msgstr "当 Qwen 2.5 与 vLLM 结合使用时，支持结构化/JSON 输出。请参照[vllm 的文档](https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html#extra-parameters-for-chat-api)了解 `guided_json` 参数。此外，也建议在系统消息或用户提示中指示模型生成特定格式，避免仅依赖于推理参数配置。"

#: ../../source/deployment/vllm.md:143 6520f937eec94220b6aacb1a262f4a18
msgid "Multi-GPU Distributed Serving"
msgstr "多卡分布式部署"

#: ../../source/deployment/vllm.md:145 bfd3575a0fac4564a8732ea0e6817f6b
msgid "To scale up your serving throughput, distributed serving helps you by leveraging more GPU devices.  Besides, for large models like `Qwen2.5-72B-Instruct`, it is impossible to serve it on a single GPU. Here, we demonstrate how to run `Qwen2.5-72B-Instruct` with tensor parallelism just by passing in the argument `tensor_parallel_size`:"
msgstr "要提高模型的处理吞吐量，分布式服务可以通过利用更多的GPU设备来帮助您。特别是对于像 `Qwen2.5-72B-Instruct` 这样的大模型，单个GPU无法支撑其在线服务。在这里，我们通过演示如何仅通过传入参数 `tensor_parallel_size` ，来使用张量并行来运行 `Qwen2.5-72B-Instruct` 模型："

#: ../../source/deployment/vllm.md 082281e6812c4379a7433990adb2b63f
#: 3469af071bd640f99f782214650d9f42 5f5a0a4bdcd74ffda16605b892127619
#: 7d5dfeed4579413f9af610193eccfe1e
msgid "Offline"
msgstr "离线推理"

#: ../../source/deployment/vllm.md 0b6438dec7de4a22a3c4cf6736137c16
#: 14b9bc39fc9b4f2e846f01fe336fe0d1 a6ce1107913c43fca7ba98917724ab97
#: b8cbdd48cf9b41f8a3954ddeea9faa75
msgid "API"
msgstr ""

#: ../../source/deployment/vllm.md:165 373a6bdbcaa241e88a8a6b5d6b52836c
msgid "Extended Context Support"
msgstr "上下文支持扩展"

#: ../../source/deployment/vllm.md:167 4a0b4bc1050e4bf18425a92a4b30fae7
msgid "By default, the context length for Qwen2.5 models are set to 32,768 tokens. To handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts."
msgstr "Qwen2.5 模型的上下文长度默认设置为 3 2768 个token。为了处理超出 3 2768 个token的大量输入，我们使用了 [YaRN](https://arxiv.org/abs/2309.00071)，这是一种增强模型长度外推的技术，确保在处理长文本时的最优性能。"

#: ../../source/deployment/vllm.md:170 eecc382d155247fea94eed5e49e7b817
msgid "vLLM supports YARN and it can be enabled by add a `rope_scaling` field to the `config.json` file of the model. For example,"
msgstr "vLLM 支持 YaRN，并且可以通过在模型的 `config.json` 文件中添加一个 `rope_scaling` 字段来启用它。例如，"

#: ../../source/deployment/vllm.md:183 4b1193f9c0be455ea191bb353e18bc58
msgid "However, vLLM only supports _static_ YARN at present, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.  We advise adding the `rope_scaling` configuration only when processing long contexts is required."
msgstr "然而，目前 vLLM 只支持 _静态_ YaRN，这意味着无论输入长度如何，缩放因子都是固定的，这可能会影响处理较短文本时的性能。我们建议仅在需要处理长上下文时才添加 `rope_scaling` 配置。"

#: ../../source/deployment/vllm.md:187 439ada31f3ec43b28d7da6838a16a3f0
msgid "Serving Quantized Models"
msgstr "部署量化模型"

#: ../../source/deployment/vllm.md:189 fc24e5ab88274986a1670406b08b99cb
msgid "vLLM supports different types of quantized models, including AWQ, GPTQ, SqueezeLLM, etc.  Here we show how to deploy AWQ and GPTQ models.  The usage is almost the same as above except for an additional argument for quantization.  For example, to run an AWQ model. e.g., `Qwen2.5-7B-Instruct-AWQ`:"
msgstr "vLLM 支持多种类型的量化模型，例如 AWQ、GPTQ、SqueezeLLM 等。这里我们将展示如何部署 AWQ 和 GPTQ 模型。使用方法与上述基本相同，只不过需要额外指定一个量化参数。例如，要运行一个 AWQ 模型，例如 `Qwen2.5-7B-Instruct-AWQ` ："

#: ../../source/deployment/vllm.md:210 038c0aaf26994a3c9d9274986881928a
msgid "or GPTQ models like `Qwen2.5-7B-Instruct-GPTQ-Int4`:"
msgstr "或者是GPTQ模型比如 `Qwen2.5-7B-Instruct-GPTQ-Int4` ："

#: ../../source/deployment/vllm.md:229 d74f1b93664548c4bfc061c372713f12
msgid "Additionally, vLLM supports the combination of AWQ or GPTQ models with KV cache quantization, namely FP8 E5M2 KV Cache.  For example,"
msgstr "此外，vLLM支持将AWQ或GPTQ模型与KV缓存量化相结合，即FP8 E5M2 KV Cache方案。例如："

#: ../../source/deployment/vllm.md:249 3351cf60292647cb95d8baf14ebd96f1
msgid "Troubleshooting"
msgstr "常见问题"

#: ../../source/deployment/vllm.md:251 c45bb3ea4e0f481e8c831fe1bbdc0386
msgid "You may encounter OOM issues that are pretty annoying. We recommend two arguments for you to make some fix."
msgstr "您可能会遇到令人烦恼的OOM（内存溢出）问题。我们推荐您尝试两个参数进行修复。"

#: ../../source/deployment/vllm.md:254 982ce3867df24d03a03f2057df8defc5
msgid "The first one is `--max-model-len`. Our provided default `max_position_embedding` is `32768` and thus the maximum length for the serving is also this value, leading to higher requirements of memory. Reducing it to a proper length for yourself often helps with the OOM issue."
msgstr "第一个参数是 `--max-model-len` 。我们提供的默认最大位置嵌入（`max_position_embedding`）为32768，因此服务时的最大长度也是这个值，这会导致更高的内存需求。将此值适当减小通常有助于解决OOM问题。"

#: ../../source/deployment/vllm.md:257 171d5762619e4073a72aecffb25cba1d
msgid "Another argument you can pay attention to is `--gpu-memory-utilization`. vLLM will pre-allocate this much GPU memory. By default, it is `0.9`. This is also why you find a vLLM service always takes so much memory. If you are in eager mode (by default it is not), you can level it up to tackle the OOM problem. Otherwise, CUDA Graphs are used, which will use GPU memory not controlled by vLLM, and you should try lowering it. If it doesn't work, you should try `--enforce-eager`, which may slow down infernece, or reduce the `--max-model-len`."
msgstr "另一个您可以关注的参数是 `--gpu-memory-utilization` 。 vLLM将预分配该参数指定比例的显存。默认情况下，该值为 `0.9`。这也是为什么您发现一个vLLM服务总是占用大量内存的原因。如果你使用了eager模式（默认不是），您可以将其调高以应对OOM问题。反之，vLLM会使用CUDA Graphs，而CUDA Graphs会额外占用不受vLLM管理的显存；此时，您应当尝试降低`--gpu-memory-utilization`。如果还是无法解决，可以尝试`--enforce-eager`（这会影响推理效率）或缩小`--max-model-len`。"
